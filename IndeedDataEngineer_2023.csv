,Job_Title,Company,Location,Salary,Job_Description
0,"Data Engineer, Music",Spotify,"New York, NY•Remote",N,"Engineering
Data
Spotify is looking for a Data Engineer to join our Music Expression organization; the newly built team focused on empowering artists and fans to form deeper connections via new ways to express themselves and their music. The specific opening is for a mid-to-senior experienced Data Engineer to join the team building and managing the new Countdown Pages feature. In particular, this role also potentially contributes to the backend stack.
Location
New York or Remote Americas
Job type
Permanent
Music Expression is a highly collaborative team who enjoys tackling big challenges; our most recent successes have included enabling 40,000 artists to share a special message during Spotify Wrapped to over 300 million viewers. This team was also behind the most successful pre-release campaign in Spotify history: Taylor Swift’s Midnights album release.
What You'll Do
Design, build and maintain data pipelines using data processing frameworks like Scio on Google Cloud Platform.
Help drive optimization, testing, and tooling to improve data quality.
Collaborate with engineers, product managers, experts, and business partners, taking learning and leadership opportunities that will arise every single day.
Work in cross-functional agile teams to continuously experiment, iterate, and deliver on new product objectives.
Deliver testable, maintainable, and high-quality code and take operational responsibility for the components that you develop.
Share knowledge, promote standard methodologies, making your team the best version of itself through mentorship and constructive accountability
Be a technical leader within the team you work with and within Spotify in general
Hack on what you want during regular hack days and bi-annual hack weeks.
Work in an environment that supports your individual growth by providing you with challenging tasks to tackle and the freedom to acquire new skills in hack time, reading groups, lectures, and a variety of internal training courses.
Who You Are
You have worked with high volume heterogeneous data, and have production level experience with one or more higher level JVM based data processing frameworks such as Beam, Dataflow, Crunch, Spark, etc.
You are knowledgeable about data access and data storage techniques.
You have experience or strong interest to learn backend API development for distributed services.
Have an understanding of system design, data structures, and algorithms.
A self-motivated individual contributor and teammate that is able to multitask, prioritize and communicate progress in a constantly evolving environment.
Love building positive relationships with colleagues and multiple customers and can explain sophisticated topics in simple terms.
Where you'll be
We are a distributed workforce enabling our band members to find a work mode best for them!
Where in the world? For this role, it can be within the Americas region in which we have a work location
Prefer an office to work from home instead? Not a problem! We have plenty of options for your working preferences. Find more information about our Work From Anywhere options here
Working hours? We operate within the Eastern Standard time zone for collaboration
Our global benefits
Extensive learning opportunities, through our dedicated team, GreenHouse.
Flexible share incentives letting you choose how you share in our success.
Global parental leave, six months off - fully paid - for all new parents.
All The Feels, our employee assistance program and self-care hub.
Flexible public holidays, swap days off according to your values and beliefs.
Learn about life at Spotify
The United States base range for this position is $140,246-175,308, plus equity. The benefits available for this position include health insurance, six month paid parental leave, 401(k) retirement plan, monthly meal allowance, 23 paid days off, 13 paid flexible holidays, paid sick leave. This range encompasses multiple levels. Leveling is determined during the interview process. Placement in a level depends on relevant work history and interview performance. These ranges may be modified in the future.

Spotify is an equal opportunity employer. You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be forward-thinking! So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens.

Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the chance to enjoy and be passionate about these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service."
1,AWS Data Engineer,KPI Partners,Remote,"$140,000 - $160,000 a year","KPI Partners, A global consulting firm focused on strategy, technology, and digital transformation. We help companies tackle their most ambitious projects and build new capabilities. We provide solutions in Cloud, Data, Application Development & BI spaces.
We enable your growth
At KPI, you can become who you want to be and learn skills that will take you further in your career
Continuously upgrade yourself
Develop as a future leader
Drive cloud enablement around the world
Engineering Excellence
Enhance your engineering expertise with our unique approach
This program gives engineers the opportunity to excel in product and software engineering by learning our industry-leading practices, tools, and technologies to build excellence by enhancing their competencies and skills
Visit to Know more : https://www.kpipartners.com/
Title: AWS Data Engineer – Full-time Job Opportunity
Location: 100% remote
Time Zone: MST
Must Haves:
AWS - general (IAM, role and policies, Secret Manager, KMS)
Lambda functions with Python
SQS, SNS, Kinesis, EC2, EMR, S3
Node.js for data pulls using API calls
Languages: Relational SQL, NoSql, Python
Responsibilities:
Help establish the development and architectural standards for the team in areas of technical excellence.
Work collaboratively with Product Management, DevOps, and other software developers to develop applications that can be easily tested and perform well under a variety of network conditions.
Delight consumers by ensuring they have the data they need to inform decisions, where and when they need it.
Champion adherence to best-practice development methodologies.
Build and maintain data pipelines.
Design and code appropriate, scalable, and secure algorithms, data structures, and software components.
Follow good documentation practices.
Lead design and code reviews.
Facilitate automated testing so all teams can innovate at a rapid pace.
Mentor and build skills across the team through effective standards setting, code reviews, and pairing.
Tune production applications to maximize efficiency.
Demonstrated technical leadership skills with ability to conduct code reviews and architect solutions.
Develop ongoing metrics, analysis, and dashboards to guide important decisions.
Qualifications:
5+ years of experience developing complex enterprise applications
Great leadership skills
Extensive experience in AWS, GCP or Azure
Extensive experience working with relational and NoSql databases
Experience with Docker and Serverless architectures like AWS Lambda, DynamoDB, ECS, S3, Amazon Kinesis, EventBridge, SQS, CloudFormation, Terraform, and/or other similar cloud services.
Experience writing microservices/lambdas in Javascript/Typescript for data processing.
Experience working in a complex enterprise environment and developing complex programs
Experience with REST API architecture and development, especially using Swagger or Apigee.
Knowledge of Git including version control, branching, merging/rebasing, and pull requests.
Experience writing SQL and a procedural language (Python, R, etc.) for data handling. (Preferred: Snowflake DBT and SIngleStore DB).
Strong focus on automation including Continuous Integration / Deployment with writing unit and integration tests.
Experience in Agile/SCRUM Software Development Process
Experience implementing data analytics, visualization tools and programs using Tableau, Grafana, and Google Sheets.
Good security practices and experience writing code that manages customer data.
Impeccable communication and team skills with shared ownership of code and other deliverables.
Willingness to work with and learn new technologies.
Build architectural models with synchronous and asynchronous patterns to decouple, integrate and scale services.
Experience with real-time data processing.
Preferred Skills:
Familiar with GCP data stores including BigQuery, Bigtable and Datastore.
Knowledge of architecting solutions in GCP, specifically using: Cloud functions, Composer, Cloud pub/sub, Dataflow and Stackdriver.
Experience with team development tooling (especially with Jira and Github).
Familiarity with Docker architectures, and Terraform deployments.
Experience with MuleSoft and GraphQL
Expert in Node.js
Experience with distributed systems and federated authentication systems.
Experience with Maven/Gradle build systems.
Understanding of BFF (Backend-for-Frontend) patterns.
Experience with development of self-healing, reliable and reactive systems.
Job Type: Full-time
Salary: $140,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Experience:
AWS: 3 years (Required)
NoSQL: 1 year (Required)
Node.js: 1 year (Required)
Work Location: Remote

Health insurance"
2,Azure Data Engineer,Radiant Systems Inc,"Milwaukee, WI•Remote",N,"Azure Synapse (no data bricks please)
Some Data Modeling would be helpful
ADF workflow experience
GIT
Python
Spark
MS or Azure SQL
Azure DevOps experience helpful
Datawarehouse and ETL
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: Remote"
3,Data Engineer,Intone Networks,"Smithfield, RI",N,"Dynamic Work schedule - This is 5 days on site a month- in the same week then the remainder of the month is working from home. ( They can fly/drive into the office as well). If your candidate is not open to this please tell them they will not be considered. Fidelity determines the week they are working onsite, not the candidate. Location- Smithfield Rhode Island MUST HAVE: ETL development experience in Python and Java, AWS, SQL. NTH: snowflake, control M Hands on ETL development experience in Python and Java. Experience working on AWS cloud, Databases on AWS. SQL and data analysis Ideally looking for someone who has strong communication skills since they will be communicating with business. Looking for someone who is a self-starter and can work independently. Should be curious to learn and experiment. Primarily responsible for the development of large scale data efforts tied to the cloud such as moving data to new cloud based solutions and building data lakes etc. To accomplish this the resource uses AWS, Python, Snowflake, and other data driven technologies"
4,Data Engineer,Ghritachi Inc,"Dallas, TX•Remote",$50 - $55 an hour,"Hi,
Please find the below Job Description. If interested, please share your updated resume along with below details.
Job Title: Data Engineer with Python and ETL
Location: Dallas, TX (Remote)
Duration: Long Term
(In person Interviews are mandatory)
Responsibilities:
8 to 10 years of experience with Python, SQL, ETL and data visualization/exploration tools
Familiarity with the AWS ecosystem, specifically Redshift and RDS
Communication skills, especially for explaining technical concepts to nontechnical business leaders
Ability to work on a dynamic, research-oriented team that has concurrent projects
Bachelor’s degree (or equivalent) in computer science, information technology, engineering, or related discipline
Experience in building or maintaining ETL processes
Professional certification
Thanks & Regards,
Sekhar
Job Type: Contract
Salary: $50.00 - $55.00 per hour
Compensation package:
1099 contract
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 8 years (Required)
Python: 5 years (Required)
ETL: 5 years (Required)
SQL: 5 years (Required)
Work Location: Remote"
5,Data Engineer,BlueCross BlueShield of Tennessee,"1 Cameron Hill Circle, Chattanooga, TN 37402",N,"Provides application design, analysis, programming, testing and documentation for the development, integration, enhancement, and maintenance of information systems solutions
Job Description:
BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Job Responsibilities
Formulates and defines system scope and objectives through research and fact-finding to design, develop, modify, or integrate complex information systems.
Devises or modifies application systems and procedures to optimize functional requirements including capacity, operating time, response time, and form of desired results.
Designs, codes, tests, debugs, and documents programs, subroutines, and scripts.
May serve one or more project team roles, such as project lead, business systems analyst, or technical lead, for small to medium efforts or manage phases of medium to large efforts.
Maintains technical skill set for software languages, databases, platforms, operating systems, utilities and networks needed to support work assignments.
Responds to system failures and performance events by taking appropriate measures to reduce system downtime and eliminate recurrence of problems.
Regularly provides guidance and training to less-experienced analysts/programmers.
Job Qualifications
Education
Bachelors Degree in Computer Science or equivalent work experience required. Equivalent years of experience are determined as one year of technical experience for every year of college requested.
Experience
1 year - Experience with information technology concepts, application development methodology, terminology, and standards required
3 years - Systems and programming experience required
1 year - Expert working knowledge of at least one programming language in use at BCBST.
Skills\Certifications
Proven skill with information technology in use at BCBST is required
Proven analysis, design, and coding skills, and demonstrated success in leading large and complex projects
Demonstrated ability to interpret and translate technical and/or or complex concepts into information meaningful to project team members and/or business personnel.
Be organized, reliable, and able to manage multiple tasks with exceptional work ethic.
Leadership skills
Must be able to communicate effectively with both technical and non-technical co-workers
Job Specific Requirements:
Specific skills needed are:
C# and .NET 5 or above
WebApi and REST services
MongoDb
React.js
OAuth
Experience with cloud technologies is a plus
Preferred Skills:
Number of Openings Available:
0
Worker Type:
Employee
Worker Sub-Type:
Employee
Company:
BCBST BlueCross BlueShield of Tennessee, Inc.
Applying for this job indicates your acknowledgement and understanding of the following statements:
BCBST is an Equal Opportunity employer (EEO), and all employees and applicants will be entitled to equal employment opportunities when employment decisions are made. BCBST will take affirmative action to recruit, hire, train and promote individuals in all job classifications without regard to race, religion, color, age, sex, national origin, citizenship, pregnancy, veteran status, sexual orientation, physical or mental disability, gender identity, or any other characteristic protected by applicable law.
Further information regarding BCBST's EEO Policies/Notices may be found by reviewing the following page:
BCBST's EEO Policies/Notices
BlueCross BlueShield of Tennessee is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at BlueCross BlueShield of Tennessee via-email, the Internet or any other method without a valid, written Direct Placement Agreement in place for this position from BlueCross BlueShield of Tennessee HR/Talent Acquisition will not be considered. No fee will be paid in the event the applicant is hired by BlueCross BlueShield of Tennessee as a result of the referral or through other means.
All applicants will be advised that BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Tobacco-Free Hiring Statement
To further our mission of peace of mind through better health, effective 2017, BlueCross BlueShield of Tennessee and its subsidiaries no longer hire individuals who use tobacco or nicotine products (including but not limited to cigarettes, cigars, pipe tobacco, snuff, chewing tobacco, gum, patch, lozenges and electronic or smokeless cigarettes) in any form in Tennessee and where state law permits. A tobacco or nicotine free hiring practice is part of an effort to combat serious diseases, as well as to promote health and wellness for our employees and our community. All offers of employment will be contingent upon passing a background check which includes an illegal drug and tobacco/nicotine test. An individual whose post offer screening result is positive for illegal drugs or tobacco/nicotine and/or whose background check is verified to be unsatisfactory, will be disqualified from employment, the job offer will be withdrawn, and they may be disqualified from applying for employment for six (6) months from the date of the post offer screening results.
Resources to help individuals discontinue the use of tobacco/nicotine products include smokefree.gov or 1-800-QUIT-NOW."
6,Data Engineer (Remote),The Hartford,"Hartford, CT•Remote",N,"You are a driven and motivated problem solver ready to pursue meaningful work. You strive to make an impact every day & not only at work, but in your personal life and community too. If that sounds like you, then you've landed in the right place.
Ready to grow your career leveraging the latest DATA technologies? Join a fast-paced and talented Performance Analytics Data Engineering team to unlock Data Capabilities for The Hartford. You will have an opportunity to participate in the ETL lifecycle process in support of continuous DATA delivery, while growing your knowledge with emerging technologies.
The Performance Analytics Data Engineering group is seeking a Data Engineer to support Claims Insight Analytics with a focus on Global Specialty lines of business. This position combines design, development, and implementation of data-focused solutions as well as analysis to identify and develop innovative monitoring metrics to customers across Global Specialty Lines.
In this role the candidate will partner closely with the Process Owners and Analysts as an expert resource in providing data, business, and technical support for a wide range of solutions. The candidate should have the skills of a data research expert and be able to work in an iterative collaborative way in balancing the solution with the technical outcome. This individual will bring with them a creative solution design approach to data exploration and discovery.
Responsibilities
Provide technical expertise to analysts, and project teams, working closely in clarifying business needs and requirements, to enable design, development, integration, and production of data & reporting solutions.
Maintain expertise of programming languages including SQL and Python, software technologies, and broad knowledge of database systems
Support the development of advanced claim monitoring metrics, which includes becoming the subject matter expert of the data and ownership of metric definitions.
Assisting in deep dive data analysis for Global Specialty customers, utilizing both structured and unstructured data.
Collaboration within the team to support data needs and team members’ analyses.
Experience & Skills
Bachelor’s degree or equivalent experience in related field required
Demonstrated ability to build strong partnerships (essential to success in this role)
2+ years of experience working with relational databases and SQL development
Experience with data warehousing tools, databases, and concepts (e.g. Oracle, Snowflake, AWS, ETL processes, Python, TOAD, Thoughtspot)
Knowledge of Global Specialty products and systems preferred but not required
Ability to analyze source systems and provide technical solutions
Experience creating, debugging, and optimizing SQL queries
Self-starter with a willingness to become a data expert
Demonstrate a passion to both learn new skills and lead discovery of the data research
Results oriented with the ability to multi-task and adjust priorities when necessary
Ability to work both independently and in a team environment
Effective written and verbal communication skills
Claims DNA badges (if currently in the claims organization): core and intermediate data, core innovation, core communication.
Compensation
The listed annualized base pay range is primarily based on analysis of similar positions in the external market. Actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. The base pay is just one component of The Hartford’s total compensation package for employees. Other rewards may include short-term or annual bonuses, long-term incentives, and on-the-spot recognition. The annualized base pay range for this role is:
$68,080 - $142,800
Equal Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression/Religion/Age
About Us
|
Culture & Employee Insights
|
Diversity, Equity and Inclusion
|
Benefits
Data Engineer - GE08AE
-
Skills:"
7,Data Platform Engineer,Peloton,"New York, NY 10011•Hybrid remote","$171,600 - $223,000 a year","ABOUT THE ROLE
At Peloton, we treat Data as Product - a valuable asset and a critical piece of our decision making process. The mission of the Data Platform team is to democratize data and provide a cost-efficient, observable and reliable data platform that empowers and enables decentralized teams to accelerate their ability to safely use data to generate actionable insights and product features. Our core guiding principles are 1) enable data as a product 2) ensure data reliability and 3) optimize for data productivity.
We are looking for a Data Platform Engineer to join our Data Platform team to build out core data systems and tools that serve as the building blocks of our data platform. As a part of the Data Platform team, you will be working across a wide range of problems in the data ingestion, data compute, data transport, data orchestration, data interoperability and data privacy areas.
YOUR DAILY IMPACT AT PELOTON
Be working on one or more workstreams under Data Platform viz. Change Data Capture, Data Compute Frameworks, Data Modeling, Schemas & Serialization, Data Pipelining & Orchestration, Data Storage, Data Transport & Interoperability, Developer Portal, Data Lake Standards, Data Access & Control.
Be an enabler and force multiplier to improve data productivity by building out self-service data systems, services and tools.
Work with decentralized teams enabling them to create data pipelines and publish golden datasets.
Write technical specifications and efficient, high quality, well-tested code.
YOU BRING TO PELOTON
You have 2+ years of experience in engineering, especially in data infrastructure and data engineering
You have experience in a cloud environment like AWS and familiarity with tools like Airflow, Glue, Athena.
You have experience with Python or Scala and experience working on large distributed systems and frameworks like Spark/ Flink.
You are passionate and willing to learn about event-driven architecture, microservices, data reliability, data privacy and quality.
You are pragmatic without compromising on software quality and standards.
Base Salary: $171,600.00 to $223,000.00
The base salary range represents the low and high end of the anticipated salary range for this position based at our New York City headquarters. The actual base salary offered for this position will depend on numerous factors including individual performance, business objectives, and if the location for the job changes. Our base salary is just one component of Peloton's competitive total rewards strategy that also includes annual equity awards and an Employee Stock Purchase Plan as well as other region-specific health and welfare benefits.
As an organization, one of our top priorities is to maintain the health and wellbeing for our employees and their family. To achieve this goal, we offer robust and comprehensive benefits including:
Medical, dental and vision insurance
Generous paid time off policy
Short-term and long-term disability
Access to mental health services
401k, tuition reimbursement and student loan paydown plans
Employee Stock Purchase Plan
Fertility and adoption support and up to 18 weeks of paid parental leave
Child care and family care discounts
Free access to Peloton Digital App and apparel and product discounts
Commuter benefits and Citi Bike Discount
Pet insurance and so much more!
#LI-JM2
#LI-Hybrid
ABOUT PELOTON:
Peloton is the leading interactive fitness platform globally, with a passionate community of nearly 7 million Members in the US, UK, Canada, Germany, and Australia. Peloton makes fitness entertaining, approachable, effective, and convenient, while fostering social connections that motivate its Members to commit to their fitness journeys. An innovator at the nexus of fitness, technology, and media, Peloton reinvented the fitness industry by developing a first-of-its-kind subscription platform that seamlessly combines the best equipment, proprietary networked software, world-class streaming digital fitness and wellness content, and best-in-class fitness experts and Instructors.
Peloton is an equal opportunity employer and committed to creating an inclusive environment for all of our applicants. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. If you would like to request any accommodations from application through to interview, please email: applicantaccommodations@onepeloton.com
Please be aware that fictitious job openings, consulting engagements, solicitations, or employment offers may be circulated on the Internet in an attempt to obtain privileged information, or to induce you to pay a fee for services related to recruitment or training. Peloton does NOT charge any application, processing, or training fee at any stage of the recruitment or hiring process. All genuine job openings will be posted here on our careers page and all communications from the Peloton recruiting team and/or hiring managers will be from an @onepeloton.com email address.
If you have any doubts about the authenticity of an email, letter or telephone communication purportedly from, for, or on behalf of Peloton, please email applicantaccommodations@onepeloton.com before taking any further action in relation to the correspondence.
Peloton does not accept unsolicited agency resumes. Agencies should not forward resumes to our jobs alias, Peloton employees or any other organization location. Peloton is not responsible for any agency fees related to unsolicited resumes."
8,Data Engineer,Peach IT Professionals,"Braham, MN 55006•Hybrid remote",$40 - $70 an hour,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006"
9,Data Engineer,Falling Colors,New Mexico•Remote,"$85,000 - $95,000 a year","Falling Colors offers solutions for administering public, private, and charitable funds to organizations that share a mission of promoting the Social Determinants of Health. We track allocated spending and measure outcomes through a configurable software platform and supporting services, allowing customers to glean insight into the effectiveness of funded programs via reporting and analytics dashboards, driving outcome-oriented decisions to maximize the impact of future investments.
We are seeking a Data Engineer to join our data team supporting our data environment focusing on ETL and data warehouse development and design. Applications will be accepted from individuals with at least 4 years of experience that reside in New Mexico. Periodic travel to headquarters will be required, but this position is otherwise remote.
Falling Colors
Falling Colors is a small (~30 person) B-certified company that is women and LGBTQ+ founded, owned, and operated. We are a diverse and open-minded team, with a mix of genders, ethnicities, and interests. We have parents and non-parents, self-taught and formally educated folk, introverts and extroverts, semi-professional ballroom dancers, golfers, gardeners, rock climbers, yogis, and VR hobbyists. As a company, we value using our skills and time to improve the world around us — from behavioral health care to local educational opportunities. As a team, we strive to be kind, support each other, and work together, stepping forward to handle all the exciting things that get thrown our way.
At Falling Colors, we don’t just build technology: we build impact. We take great pride in what we do for our communities with our products, and further seek to improve the world in everything we do. Our passion project is The White Building, a space rich with history. You can read more about this project at thewhitebuilding (dot) com on Instagram @thewhitebuilding. We use a portion of our profits to fund our Falling Colors Foundation, which paid out over $200,000 in small business grants to help our local businesses make it through uncertain times during the pandemic. We pay our employees to volunteer and do our best to protect them from burnout with a generous PTO policy, flex time, and paid holidays. We protect our employees’ futures by partnering with a B-certified financial group that manages our 401k accounts (and we contribute—not match—a percentage to those accounts). We understand the critical role we play as a business in the six determinants of social health, and we want to be the company that sets the bar and inspires others to be better.
We are in Santa Fe, New Mexico, with satellite offices across the United States. This is a hybrid position for a candidate residing in New Mexico or Colorado. The candidate will therefore need to have a functional at-home workspace with reliable internet service and the ability to come work in the office.
Data Engineer Qualifications & Responsibilities
Responsibilities
Be an individual contributor who is excited to build new ETL pipelines, maintain and enhance existing pipelines, and become an expert on our data and infrastructure.
Collaborate on an interdisciplinary team to design and develop business intelligence solutions for behavioral health providers and administrators.
Work with data analysts and product support to translate business needs into specifications for data warehouse and pipeline design.
Design, implement, and maintain ETL and data cube solutions with an eye towards performance, security, and quality.
Preprocess and integrate heterogenous data sources.
Evaluate ETL performance and explore opportunities to automate processes, reduce bottlenecks, enhance quality, and improve overall usability.
Support the design, development, and implementation of data warehousing, reporting and analytics solutions that deliver information efficiently, allowing our clients to make informed, data-driven business decisions.
Produce ad-hoc reports and analyses when needed.
Have or develop excellent understanding of privacy rules related to sensitive health data our systems process to ensure appropriate handling of such sensitive data.
Create and maintain documentation in support of the team.
Qualifications
The Must Haves:
Bachelor’s degree in computer science or equivalent, or demonstrated experience and competency equivalent
At least 3 years of database development experience with a demonstrated proficiency in writing advanced tSQL to extract data from complex data structures.
At least 3 years’ experience designing and developing data warehouses with a demonstrated proficiency in dimensional modeling.
Designing and implementing ETL solutions using Azure Data Factory (ADF) and/or Integration Services (SSIS).
Experience with database design for transactional systems and data warehouses.
Experience working on an interdisciplinary team of engineers, analysts, data quality professionals, and subject matter experts.
A friendly team player attitude, comfortable wearing different hats on different projects and willing to take leadership or support roles as needed.
Comfort working in existing structure and process, with a focus on technical implementation and data.
A passion for working in service of a meaningful mission to improve the lives of vulnerable populations through technology and data
The Good to Haves:
Experience analyzing and optimizing slow running queries.
Experience writing DAX statements.
Experience with Streaming Data Pipelines.
Experience analyzing and optimizing slow running queries.
The Bonus Stuff:
Demonstrated knowledge of the unique security and analytical considerations in working with protected healthcare data and information systems.
Demonstrated experience designing and implementing disaster recovery solutions to hit RPO and RTO
Working knowledge of PowerShell and specifically dbatools.io
Experience with Azure cloud infrastructure
Experience administering SQL Server in a cloud environment
Python programming experience
Ability to debug and understand C# code
Familiarity with State of New Mexico public and behavioral health agencies
Diversity Statement
Falling Colors is committed to recruiting and retaining a diverse workforce. We recognize that the tech industry overall has failed to develop a workforce reflective of our population as a whole, and pledge to do our part to do better. We encourage people of color, differently abled people, veterans, women, and LGBTQ folks to apply for open positions. We believe that a diverse workforce made up of people with different backgrounds and life experiences makes us stronger and better able to serve the communities where we work to make a difference. We will consider all applicants without regard to race, color, national origin, religion, sexual or gender orientation or expression, marital or parental status, disability, age, or other protected basis.
Benefits
We offer:
A month of paid time off plus a month of holidays
Paid parental leave
Paid volunteering time
Generous 401k contribution
Generous health, vision, and dental plans (for employee and family, at no cost to our employees)
Employer-paid short- and long-term disability plans
Employer-paid life insurance
Professional growth and continuing education opportunities
Estate planning stipend
LifeLock subscription
Falling Colors is an Equal Opportunity Employer
What Next?
If you think this sounds like it could be you, we encourage you to apply. We do not expect anyone to have all the things we are looking for, and we are always happy to have a conversation.
To apply, please send your resume and a cover letter to apply @fallingcolors (dot) com. We look forward to getting your application!
Job Type: Full-time
Pay: $85,000.00 - $95,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
Are you a resident of New Mexico, or are you willing to relocate?
Experience:
data warehouse design and development: 3 years (Required)
database development: 3 years (Required)
Work Location: Remote

Health insurance"
10,Data Engineer - Flink,Amazon,"10300 Campus Point Dr Ste 200, San Diego, CA 92121",$72 - $80 an hour,"The successful candidate in this role will have:
Experience building and maintaining enterprise-scale (Terabyte - Exabyte) data pipelines.
Experience using modern open-source technologies and cloud services (SNS, SQS, MSK, ECS, EC2, DynamoDB, Kinesis, EMR, Kafka, Flink, Spark)
Experience with modern compression technologies (Orc, Spark)
Experience working with customers to model and onboard datasets to fit customer requirements.
Experience developing data pipeline parsers using Scala.
To follow up with any questions, please contact Ajitabh at # 408-907-2956
Job Type: Contract
Pay: $72.00 - $80.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Austin, TX: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
LinkedIn profile is required. Please share your LinkedIn profile
Experience:
Apache Flink: 1 year (Required)
Scala: 2 years (Required)
Parsing of Data: 2 years (Required)
AWS tools: 2 years (Required)
Work Location: One location"
11,"Data Engineer – Salesforce, SQL, SSIS – FULLY REMOTE WORK 42233","PRIMUS Global Services, Inc","Phoenix, AZ•Remote",N,"We have an immediate long-term opportunity with one of our key clients for a position of Data Engineer, to work on a remote basis.

The ideal applicant must have strong expertise with Salesforce, data movement, processing, and SSIS, as well as developing solutions for optimal data processing on the Salesforce platform.

A candidate who is API oriented with MuleSoft would be a huge plus; they are not fully involved right now, but will be by the end of the year.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Preeti
PRIMUS Global Services
Direct: 972-734-5373
Desk: 972-753-6500 Ext: 266
Email: jobs@primusglobal.com"
12,Data Engineer (L5),Netflix,Remote,N,"Remote, United States
Data Science and Engineering
At Netflix, our mission is to entertain the world. With 200+ million paid members in over 190 countries on millions of devices; enjoying TV series, documentaries, and feature films across a wide variety of genres and languages - Netflix is reinventing entertainment from end to end. We are revolutionizing how shows and movies are produced, pushing technological boundaries to efficiently deliver streaming video at a massive scale over the internet, and continuously improving the end-to-end user experience with Netflix across their member journey.

We pride ourselves on using data to inform our decision-making as we work towards our mission. This requires curating data across various domains such as Growth, Finance, Product, Content, and Studio. All of this data collection and curation is made possible thanks to the amazing Data Engineers of Netflix who bring this data to life.

Data Engineering at Netflix is a role that requires building systems to process data efficiently and modeling the data to power analytics. These solutions can range from batch data pipelines that bring to life business metrics to real-time processing services that integrate with our core product features. In addition, we require our Data Engineers to have a rich understanding of large distributed systems on which our data solutions rely. Candidates should have knowledge across several of these skill sets and usually need to be deep in at least one. As a Data Engineer, you also need to have strong communication skills since you will need to collaborate with business, engineering, and data science teams to enable a culture of learning. Learn more about the work of data engineers at Netflix.

Location of work: We are considering candidates who are willing to relocate to Los Gatos, California, as well as fully-remote candidates (remote in the US with occasional visits to Los Gatos) depending on the team your skills are most aligned with.

Who are you?
You strive to write elegant code, and you're comfortable with picking up new technologies independently
You are proficient in at least one major programming language (e.g. Java, Scala, Python) and comfortable working with SQL
You enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine learning models
You have a strong background in at least one of the following: distributed data processing or software engineering of data services, or data modeling
You are familiar with big data technologies like Spark or Flink and comfortable working with web-scale datasets
You have an eye for detail, good data intuition, and a passion for data quality
You appreciate the importance of great documentation and data debugging skills
You relate to and embody many of the aspects of the Netflix Culture. You love working independently while also collaborating and giving/receiving candid feedback
You are comfortable working in a rapidly changing environment with ambiguous requirements. You are nimble and take intelligent risks
At Netflix, we carefully consider a wide range of compensation factors to determine your personal top of market. We rely on market indicators to determine compensation and consider your specific job family, background, skills, and experience to get it right. These considerations can cause your compensation to vary and will also be dependent on your location.

The overall market range for roles in this area of Netflix is typically $150,000 - $750,000

This market range is based on total compensation (vs. only base salary), which is in line with our compensation philosophy. Netflix is a unique culture and environment. Learn more here."
13,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
14,Data Engineer,Konnectingtree,Remote,$45 - $50 an hour,"Greetings from KonnectingTree!
We are looking for a Data Engineer for one of our clients. This is a remote position with an Implementation partner. AWS Certification Mandatory.
Data Engineer with AWS Experience
Experience with PySpark/Spark
Experience in Python
Able to work independently
Able to work with the business team directly
Interested candidates kindly share your updated resume with mythili.saravanan@konenctingtree.com. Please reach me at 952-679-2916.
Job Type: Contract
Salary: $45.00 - $50.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Experience:
AWS Data Engineer: 5 years (Required)
Python: 5 years (Required)
PySpark: 5 years (Required)
Work Location: Remote
Speak with the employer
+91 952-679-2916"
15,Data Engineer (Remote),The Hartford,"Hartford, CT•Remote",N,"You are a driven and motivated problem solver ready to pursue meaningful work. You strive to make an impact every day & not only at work, but in your personal life and community too. If that sounds like you, then you've landed in the right place.
Ready to grow your career leveraging the latest DATA technologies? Join a fast-paced and talented Performance Analytics Data Engineering team to unlock Data Capabilities for The Hartford. You will have an opportunity to participate in the ETL lifecycle process in support of continuous DATA delivery, while growing your knowledge with emerging technologies.
The Performance Analytics Data Engineering group is seeking a Data Engineer to support Claims Insight Analytics with a focus on Global Specialty lines of business. This position combines design, development, and implementation of data-focused solutions as well as analysis to identify and develop innovative monitoring metrics to customers across Global Specialty Lines.
In this role the candidate will partner closely with the Process Owners and Analysts as an expert resource in providing data, business, and technical support for a wide range of solutions. The candidate should have the skills of a data research expert and be able to work in an iterative collaborative way in balancing the solution with the technical outcome. This individual will bring with them a creative solution design approach to data exploration and discovery.
Responsibilities
Provide technical expertise to analysts, and project teams, working closely in clarifying business needs and requirements, to enable design, development, integration, and production of data & reporting solutions.
Maintain expertise of programming languages including SQL and Python, software technologies, and broad knowledge of database systems
Support the development of advanced claim monitoring metrics, which includes becoming the subject matter expert of the data and ownership of metric definitions.
Assisting in deep dive data analysis for Global Specialty customers, utilizing both structured and unstructured data.
Collaboration within the team to support data needs and team members’ analyses.
Experience & Skills
Bachelor’s degree or equivalent experience in related field required
Demonstrated ability to build strong partnerships (essential to success in this role)
2+ years of experience working with relational databases and SQL development
Experience with data warehousing tools, databases, and concepts (e.g. Oracle, Snowflake, AWS, ETL processes, Python, TOAD, Thoughtspot)
Knowledge of Global Specialty products and systems preferred but not required
Ability to analyze source systems and provide technical solutions
Experience creating, debugging, and optimizing SQL queries
Self-starter with a willingness to become a data expert
Demonstrate a passion to both learn new skills and lead discovery of the data research
Results oriented with the ability to multi-task and adjust priorities when necessary
Ability to work both independently and in a team environment
Effective written and verbal communication skills
Claims DNA badges (if currently in the claims organization): core and intermediate data, core innovation, core communication.
Compensation
The listed annualized base pay range is primarily based on analysis of similar positions in the external market. Actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. The base pay is just one component of The Hartford’s total compensation package for employees. Other rewards may include short-term or annual bonuses, long-term incentives, and on-the-spot recognition. The annualized base pay range for this role is:
$68,080 - $142,800
Equal Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression/Religion/Age
About Us
|
Culture & Employee Insights
|
Diversity, Equity and Inclusion
|
Benefits
Data Engineer - GE08AE
-
Skills:"
16,Data Engineer,N,Remote,$65 - $70 an hour,"Job description
Job description
Design conceptual data architectures solution for data-centric analytics projects.
Analyze data sources, design and evaluate feasible technical solutions to be implemented by data engineers. The solutions might include database modeling and design, relational database architecture, metadata and repository creation and configuration management.
Understand the complexity of data and design systems and models to handle different data sources/formats, which includes structured, semi-structured, and unstructured, as well as stream processing.
Use data mapping, data mining and data transformational analysis tools to design and develop technical solution architectures.
Prepare and maintain accurate solution design and architectural documentations for delivery, platform and operations team.
Play a contributing role in the development and update of enterprise data architectural strategies, patterns, standards, processes and tools as new/unique data sources, use cases and requirements come up.
Support and adhere to the development of data management policies, standards and procedures.
Define integrated views of data drawing together data from across the enterprise, both in real-time and as extracts.
Collaborate with platform and IT to as need to finalize the right solution including pattern and architecture for the analytics use cases.
Address governance and security challenges associated with solutions.
Support the delivery team engineers in tuning queries for performance and scalability, help them package deployments and collaborate with IT to migrate code between Development, Quality Assurance (QA), and Production environments.
Support test automation, troubleshooting ETL job functionality, validating data, as well as create test data and table structures through use of SQL.
Design and develop data pipelines or ETL processes.
Must be able to work in an agile, rapid delivery environment.
5+ years of data strategy, architecture related experience designing, architecting, and implementing big data solutions.
9-10+ years of experience:
· ETL experience
· Natural Language Processing (NLP)
· Metadata tagging and link analysis
· Experience with script and/or parser development to extract structured and unstructured data from files
· Data warehousing, mining, analysis, self-service, or machine learning
· Data Modelling
· Technologies such as Informatica, SQL programming and hands-on experience in Python, R, APIs, Spark, Python or Kafka
· Big Data and Big data technologies
· Data visualization tools such as PowerBI, Qlik and Tableau
Experience with traditional ETL integration tools as well as more modern real-time messaging/integration platforms.
Experience with relational database modeling concepts and modeling for the modern big data platform.
Strong understanding of Operational, Relational, Analytical, Master, and Reference data models.
Deep understanding of data transfer formats such as XML, JSON, and Avro.
Understanding of Cloud technologies such as Microsoft Azure, Google Cloud, Amazon Web Services (AWS).
Knowledgeable in tools used to identify and manage structured and unstructured data.
Expert using data governance technologies such as Collibra or Informatica EDC.
Strong communicator at the technical levels
Strong, demonstrated writing and presentation skills
Job Type: Contract
Required Experience:
AWS/Python/Pyspark
Informatica: 7 years (Required)
SQL: 5 years (Required)
Data warehouse: 7 years (Required)
Job Type: Contract
Pay: $65.00 - $70.00 per hour
Compensation package:
1099 contract
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Experience:
AWS: 3 years (Required)
Python: 2 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Remote"
17,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
18,Data Engineer,Holman,Remote,"From $110,000 a year","Holman is currently accepting applications for the role of Data Platform Engineer
Principal Purpose of Position:
Design, develop, document and execute data solutions, tools and practices
Analysis of requirements at sufficient level of detail to allow ETL solution to be developed
Development of ETL job flows according to company standards for naming, performance, restartability and performance.
Support testing and remediation of defects in newly-developed/modified ETL workflows
Promote ETL workflows to PROD and provide ongoing support in PRODUCTION, including monitoring and troubleshooting
Ability to create Power BI Datasets to support the Analytic Delivery team
Evaluate emerging data platform technologies
Lead technology implementations
Follow and contribute to best practices for data management and governance
Collaborate with the Data Architecture team to understand and implement load processes for reporting and analytic data structures (data warehouses, data marts and data lakes)
Performance tune and troubleshoot processes under development and in production as necessary.
Work with the Data Architects to augment ERD’s as changes are developed
Develop, maintain, and extend reusable data components
Provide timely project and task updates to all concerned parties
Monitor production data integration jobs and correct failures in a timely manner
Create and manage incident reports as they pertain to data integration processes
Perform all other duties and special projects as assigned.
Required Experience/Skills
2+ years Azure exposure (Any Resources: Databases, Data Factory, Synapse Studio, Storage Account, Power Platform)
2+ years ANSI SQL experience
1+ years data modeling exposure
Advanced problem solving/Critical thinking mindset
Preferred Experience/Skills
Azure connectivity/authentication (service principals, managed identities, certificates)
Power BI Dataset creation/maintenance
Azure Resources: DevOps, Logic Apps, Gen 2 Storage, Purview
SQL Server, Oracle, Python, Spark
Education and/or Training:
Bachelor’s degree in Computer Science or equivalent work experience
Compensation: Starting at $110,00 USD
#LIREMOTE
Job Type: Full-time
Pay: From $110,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Monday to Friday
Experience:
ETL: 2 years (Preferred)
Azure: 2 years (Preferred)
Work Location: Remote

Health insurance"
19,Data Engineer,Mericaninc,"1 Alabama St SW, Atlanta, GA 30303",$60 - $65 an hour,"Job Role: Data engineer
Location: Hybrid model (Day 01 onsite)
Type: Contract
EXPERIENCE :
Python
Kafka or Kinesis
DevOps deployment model
AWS
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Schedule:
Day shift
Evening shift
Monday to Friday
Ability to commute/relocate:
Atlanta, GA 30303: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location"
20,Data Engineer I,"The Travelers Companies, Inc.","Hartford, CT","$102,600 - $169,200 a year","Who Are We?
Taking care of our customers, our communities and each other. That’s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.
Job Category
Technology
Compensation Overview
The annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.
Salary Range
$102,600.00 - $169,200.00
Target Openings
1
What Is the Opportunity?
As a contributing member of the Salesforce Agile Circle, you will be responsible for working with various Business areas and IT stakeholders to elicit, analyze, specify, and validate business and IT solutions. This role supports the creation of data centric features and user stories and ensures committed in progress work meets the needs of the business. The role works with a diverse stakeholder group across both Business and IT domains and across multiple Enterprise value streams.
What Will You Do?
Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions in support of our Salesforce integrations
Design data solutions.
Analyze sources to determine value and recommend data to include in analytical processes.
Incorporate core data management competencies including data governance, data security and data quality.
Collaborate within and across teams to support delivery and educate end users on data products/analytic environment.
Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate.
Test data movement, transformation code, and data components.
Perform other duties as assigned.
What Will Our Ideal Candidate Have?
Bachelor’s Degree in STEM related field or equivalent
Six years of related experience
Proficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices.
The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions.
Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on.
Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems.
Strong verbal and written communication skills with the ability to interact with team members and business partners.
Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities.
Solid SQL Server working knowledge – able to create and analyze store procedures
Working knowledge of MongoDB or other NoSQL databases
Working knowledge of pub-sub message exchange architecture, such as RabbitMQ or Kafka
Working knowledge of various message formats, such as JSON
AWS Foundational Cloud Practitioner certificate preferred
AWS experience and knowledge of various AWS messaging and storage patterns, such as S3 bucket, DynamoDB, etc.
What is a Must Have?
Bachelor’s degree or equivalent training with data tools, techniques, and manipulation.
Four years of data engineering or equivalent experience.
What Is in It for You?
Health Insurance: Employees and their eligible family members – including spouses, domestic partners, and children – are eligible for coverage from the first day of employment.
Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.
Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.
Wellness Program: The Travelers wellness program is comprised of tools and resources that empower you to achieve your wellness goals. In addition, our Life Balance program provides access to professional counseling services, life coaching and other resources to support your daily life needs. Through Life Balance, you’re eligible for five free counseling sessions with a licensed therapist.
Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.
Employment Practices
Travelers is an equal opportunity employer. We believe that we can deliver the very best products and services when our workforce reflects the diverse customers and communities we serve. We are committed to recruiting, retaining and developing the diverse talent of all of our employees and fostering an inclusive workplace, where we celebrate differences, promote belonging, and work together to deliver extraordinary results.

If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an
email
so we may assist you.

Travelers reserves the right to fill this position at a level above or below the level included in this posting.
To learn more about our comprehensive benefit programs please visit
http://careers.travelers.com/life-at-travelers/benefits/
."
21,Reference Data Engineer,BRMi,Virginia•Remote,N,"Overview:
BRMi is seeking a Reference Data Engineer

**Can be 100% remote in TX, NJ, NC, WV, AL, VA, MD, MO, DC, GA, or FL**

Click here to learn about BRMi's culture.

Click here to see BRMi’s Glassdoor reviews
Responsibilities:
Reference Data Management (using tools such as Informatica Reference 360, Ataccama, Profisee, or similar.)
Configure Data workflow management
Logical & Physical Data modeling
Data quality control & monitoring
Data Integration (Batch & Real Time)
Data Migration
Data profiling
Qualifications:
Hands on data service/programming lang. experience –
Informatica Reference 360, Ataccama, Profisee, or similar
Erwin
Azure Data Lake
Databricks
PySpark
SQL
API
Agile Delivery - Azure DevOps/Boards, JIRA

Desired – Data Stewardship exp., Data Governance exp. , Data Security exp. , Data Architecture, Synapse (Dedicated SQL Pool)
** BRMi will not sponsor applicants for work visas for this position.**
**This is a W2 opportunity only**

EOE/Minorities/Females/Vet/Disabled
We are an equal opportunity employer that values diversity and commitment at all levels. All individuals, regardless of personal characteristics, are encouraged to apply. Employment policies and decisions on employment and promotion are based on merit, qualifications, performance, and business needs. The decisions and criteria governing the employment relationship with all employees are made in a nondiscriminatory manner, without regard to race, religion, color, national origin, sex, age, marital status, physical or mental disability, medical condition, veteran status, or any other factor determined to be unlawful by federal, state, or local statutes."
22,Data Engineer,"TeamWorx Security, LLC","Columbia, MD 21046•Remote","$100,000 - $125,000 a year","Title: Data Engineer
Location: Remote
Position Type: Full-Time
Salary Range: $100k-$125k
U.S. Citizenship is required. Must be willing to submit for a security clearance.

The Opportunity
TeamWorx Security is looking for a highly motivated and adaptable Data Engineer with a drive to succeed in a fast-paced environment! The Data Engineer will work closely with both our technical team and our government customers to build capabilities and solutions to assess program effectiveness across a range of processes, projects, and systems. The candidate will apply a wide range of qualitative and/or quantitative methods to recommend performance improvements by producing thorough management reports and automation tools tailored to the specific needs of each program component.

What it Takes!
We are product-focused and love innovation. We look for leadership qualities in all of our teammates. You must be comfortable exploring, learning, and building cool technologies, making mistakes, and learning along the way. You should be self-driven and self-motivated with a great attitude! You should take ownership of projects and drive them to completion. We aim to challenge and grow our employees, and in return, their contributions guide our efforts to deliver maximum value to our customers. You must be comfortable working remotely, directly with our customers, and in our team environment.

What You'll Be Doing
The perfect candidate should have detailed knowledge of JavaScript, Google Apps Script, Google Workspace, data structures, analysis, and visualization. Knowledge of web-app development and programming languages such as CSS, HTML, and Python are desired. Strong attention-to-detail and the ability to produce clear and concise documentation are also essential. The Data Engineer is expected to:

Develop and maintain web applications using Google Apps Script and JavaScript
Extract and analyze data to provide unique insights and patterns specific to the necessary task
Validate the work of others by performing confirmatory data analysis
Collaborate with other team members to design, develop, and implement data-driven solutions
Create and maintain data pipelines and database architectures
Develop and implement data quality checks and processes
Apply knowledge and experience using industry-standard web development tools and programming languages
Utilize software packages such as Oracle, MySQL, Tableau, Google Sheets, and Excel at an advanced level to manipulate and display data

What We Think You'll Need
Bachelor’s in Computer Science, Data Science, Information Systems, or a related field (or equivalent experience)
Proficiency in JavaScript, Knowledge of other programming languages such as CSS, HTML, and Python is a plus
Experience with Google Apps Script, Google Workspace, and web application development
Ability to perform exploratory and confirmatory data analysis
Knowledge of descriptive analytics, trends, business intelligence, and business analytics
Strong problem-solving and analytical skills
Excellent communication and teamwork abilities
Self-motivation, adaptability, and drive to succeed in a fast-paced environment.
US citizenship required and must willing to submit for a security clearance

What’s in it for you?

No more bullet points for starters. Also, these awesome perks: Comprehensive Health, Dental, & Vision Insurance, Short-Term Disability & Life Insurance, Matching 401(k) Contributions, Paid Time Off including 11 Holidays and your Birthday, Professional Development, Fitness Membership, and Home Office allocations, Performance Bonuses & Profit Sharing, & Employee Referral Bonuses. But wait, there's more: TeamWorx Security is committed to our teammates and their professional goals. Being a small but growing company affords us the opportunity to work WITH YOU to reach your full potential.

A Little About Us and Our Team
We love to be inspired. We started TeamWorx Security to build, create, and dream about what inspires us, and because of this, our Maryland startup is quickly growing. We are a team of data scientists, software engineers, researchers, product developers, and operation-minded folks who specialize in developing applications and products that our customers love. Our mission is to reinvent the way technical and non-technical people work with technology. We believe in creating harmonious, automated solutions designed to improve our customers’ daily workflow, allowing them to act quickly and decisively about what matters most. We are looking for critical thinkers who excel at solving problems and have a passion for what they do to help us move our mission forward. This is our story; come tell us your story.

At TeamWorx Security, Inc., we hold our values as guiding principles in all that we do and encourage our community to embrace and embody them. Our core values are putting our employees first, being curious, being authentic, being scrappy, and honoring those we serve. These values drive our company culture, decision-making, and operations.

All qualified applicants will receive consideration for employment without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, or any other basis prohibited by law. TeamWorx Security will also consider the employment of qualified applicants with criminal histories, consistent with relevant laws."
23,Data Engineer,Clairvoyant,Remote,"$100,000 - $110,000 a year","Job type:FTE/W2
Location:Remote
What you’ll do:
Build and maintain data pipelines, data models, standardized datasets etc
Ingest data from multiple data sources (files, streaming, APIs) into our
datawarehouse
Work with stakeholders to understand requirements and implement efficient code to
accomplish the goals
Effectively gather input, ideas, and perspectives from developers, designers, and
product managers across the organization to identify opportunities for improvement,
collaboration, and impact
What you’ll bring:
6+ years of professional experience in Data Engineering
SQL: 3-5 years of experience writing complex queries for analytical and
data warehousing purposes. Experience writing DDL and DML statements. Thorough
understanding of data warehousing concepts and data model design
Python: Working knowledge of python and object orientated programming.
One year minimum
GCP/GBQ: Thorough experience working in the GCP ecosystem, and thorough
knowledge of GBQ. Two year minimum, critical requirement as main DB environment
will be GBQ
Composer/Airflow or similar pipeline orchestration tool: Working knowledge of
composer/airflow and creating/maintaining data pipeline orchestration through DAGs.
One year minimum
Working experience with a code editor, preferably VS Code, Dbeaver, or PyCharm
Experience using a repository to push code up through various environments (dev,
stg, prd) and understanding of CICD best practices. Experience with
BitBucket a plus. One year minimum
Job Types: Full-time, Contract
Salary: $100,000.00 - $110,000.00 per year
Schedule:
Monday to Friday
Experience:
Data engineer: 5 years (Required)
Python: 1 year (Required)
Sql: 3 years (Required)
GCP: 2 years (Required)
Work Location: Remote"
24,Data Engineer,Excelgens,Remote,$36 - $52 an hour,"Contract to hire role with NASDAQ. Remote role.
2 rounds of interview:
1. 90 Minutes python test
2. 30 minutes interview
Please check the JD below:
· Looking for resources to support the Data Analysis Team. Support includes design, development, and testing. These tasks include, but are not limited to the following features:
· Analyse environmental, social and governance (ESG) data to ensure the creation and configuration of our clients' accounts.
· Data modelling in YAML
· Indicator configuration YAML
· Apply Data Pipeline transformation using Python on client’s data.
Skills Required:
Python programming basic knowledge of SQL GIT Data Set Analysis Expectation: Data Set Analysis Interacting with Customer Support team - Dealing with Questions from (vendor & Customer Support team) Communication skills updating Documents. Maintain 100s of Data Sets Debugging, QA check, Alarms, writing code from Scratch Maintenance – bulk of work ETL not necessarily good to have.
Job Type: Contract
Salary: $36.00 - $52.00 per hour
Experience level:
6 years
Schedule:
8 hour shift
Monday to Friday
Experience:
Python: 4 years (Required)
SQL: 1 year (Preferred)
YAML: 3 years (Preferred)
Work Location: Remote"
25,AWS Data engineer,TekWisen Software Pvt. Ltd,Remote,$60 - $75 an hour,"Job Title: Data Engineer - AWS
Location: Madison WI 53703 - Remote
Duration: 5 Months
Job Type: Contract (c2c and w2)
Work Type: 100% Remote in USA
Job Description
As a Staff Data Engineer on Foundation Analytics, you will be responsible for building many key parts of the foundation data services platform. You’ll work in a collaborative Agile environment using the latest in engineering best practices with involvement in all aspects of the software development lifecycle. You will be responsible for ensuring the team makes sound design & configuration decisions to develop curated data products, apply standard architectural practices, and supports the Data Product Managers in evolving core data products. As a member of the data engineering team, you will help client to deliver impactful reporting products for our customers.
Publishing well written and tested code to production daily using technologies such as Linux, Docker, Kubernetes, AWS, Kafka and Python Drive data architecture and integration design and development discussions with engineering and other teams Investigate production issues and fine-tune our data pipelines Build a platform that will be the foundation for our customer facing reporting features, our machine learning initiatives, and internal product analytics
Perform rapid proto typing Participate in designing, developing key features and functionality of our data platform Continually improve the data platform development for high efficiency, throughput and quality of data Collaborate with team members with researching & brainstorming different solutions for technical challenges facing the team
Develop standard methodologies and mentor other engineers on the team to help make technical decisions on our projects and roadmap.,
Skills
7+ years of software development/data engineering experience 4+ years of hands-on experience of building scalable data platforms and/or reliable data pipelines
Proficiency in at least one of the following programming languages: Java, Python, Scala
Experience with AWS.
Experience in developing and operating high volume, high availability environments Working understanding of Kubernetes’ infrastructure and security best practices
Ability to work effectively in a dynamic, occasionally interrupt driven environment that includes geographically spread teams and customers BS degree in Engineering, CS, or equivalent
Education
Experience writing ETL jobs to help address various data engineering challenges
Strong understanding of Build tools and Deployment tools Familiarity with Kafka, Flink, Spark frameworks with validated understanding of at least one job scheduling tool: Airflow, Celery, AWS Step functions Tech Stack Our data pipelines are written in Java and Python based software stacks
We utilize many open-source technologies, including Spark, Flink, Hudi, Airflow Our software runs on AWS services like EMR and in Kubernetes, and integrates with AWS services S3, Athena, and Glue for data access
Job Type: Contract
Salary: $60.00 - $75.00 per hour
Experience level:
10 years
11+ years
Schedule:
8 hour shift
Experience:
software development: 9 years (Required)
Data Engineer: 9 years (Required)
data platforms: 7 years (Required)
data pipelines: 7 years (Required)
AWS: 9 years (Required)
ETL: 9 years (Required)
EMR systems: 9 years (Required)
Work Location: Remote"
26,Data Engineer,Northwestern Mutual,"Milwaukee, WI","From $73,570 a year","AT NORTHWESTERN MUTUAL, WE ARE STRONG, INNOVATIVE AND GROWING. WE INVEST IN OUR PEOPLE. WE CARE AND MAKE A POSITIVE DIFFERENCE.
Primary Duties & Responsibilities:
Apply engineering best practices in order to analyze, design, develop, deploy and support software solutions.
Develop software using continuous Deployment and integration practices.
Participate in an Agile implementation and maintenance of source control and release procedures.
Participate in Code Reviews and feedback to the team
Explain technical solutions to technical teams.
Contribute to a collaborative work environment in which all team members are respected regardless of their individual differences and are motivated to improve both their individual and team contributions.
Identify data quality issues and their root causes. Propose fixes and design data audits.
Qualifications:
Bachelor's Degree
1-3 years of professional experience.
At least 1 year of professional software engineering, debugging and software documentation experience.
Code Knowledge: Python, JVM (Java, Scala),Apache Spark, SQL.
Experience with Agile methodologies/DevOps environment.
Awareness of database structures, theories, principles, and practices.
Awareness of Data Integration Patterns and Tooling including ELT/ETL, EII, Replication, Event Streaming, Virtualization to support batch and real-time data needs.
Has or develops understanding of 1-3 subject areas/domains of data.
Self-motivated and able to work with minimal direction
Proficient programming skills.
Skills-Proficiency Level:

Agile Methodologies - Basic
Application Platforms - Intermediate
Data Auditing - Basic
Data Integrity - Intermediate
Data Privacy - Basic
Data Quality - Basic
DevOps - Basic
Domain Expertise - Basic
Engineering Practices - Intermediate
Integration Patterns - Basic
Programming Languages - Intermediate
Root Cause Analysis - Basic
Software Debugging - Basic
Software Documentation - Basic
Software Engineering - Intermediate
Strategic Thinking - Basic
Technical Communication - Intermediate
Written Communication - Intermediate
Compensation Range:
Pay Range - Start:
$73,570.00
Pay Range - End:
$105,100.00
Please note that this is the standard pay structure. Positions in certain locations (such as California) may provide an increase on the standard pay structure based on the location. Pleas e click here for additi onal information relating to location-based pay structures.
GROW YOUR CAREER WITH A BEST-IN-CLASS COMPANY THAT PUTS OUR CLIENT’S INTERESTS AT THE CENTER OF ALL WE DO. GET STARTED NOW!
We are an equal opportunity/affirmative action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender identity or expression, sexual orientation, national origin, disability, age or status as a protected veteran, or any other characteristic protected by law.
If you work or would be working in California, Colorado, New York City, Washington or outside of a Corporate location, please click here for information pertaining to compensation and benefits.

FIND YOUR FUTURE
We’re excited about the potential people bring to Northwestern Mutual. You can grow your career here while enjoying first-class perks, benefits, and commitment to diversity and inclusion.
Flexible work schedules
Concierge service
Comprehensive benefits
Employee resource groups"
27,Data Engineer,Konnectingtree,Remote,$45 - $50 an hour,"Greetings from KonnectingTree!
We are looking for a Data Engineer for one of our clients. This is a remote position with an Implementation partner. AWS Certification Mandatory.
Data Engineer with AWS Experience
Experience with PySpark/Spark
Experience in Python
Able to work independently
Able to work with the business team directly
Interested candidates kindly share your updated resume with mythili.saravanan@konenctingtree.com. Please reach me at 952-679-2916.
Job Type: Contract
Salary: $45.00 - $50.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Experience:
AWS Data Engineer: 5 years (Required)
Python: 5 years (Required)
PySpark: 5 years (Required)
Work Location: Remote
Speak with the employer
+91 952-679-2916"
28,Data Engineer,Falling Colors,New Mexico•Remote,"$85,000 - $95,000 a year","Falling Colors offers solutions for administering public, private, and charitable funds to organizations that share a mission of promoting the Social Determinants of Health. We track allocated spending and measure outcomes through a configurable software platform and supporting services, allowing customers to glean insight into the effectiveness of funded programs via reporting and analytics dashboards, driving outcome-oriented decisions to maximize the impact of future investments.
We are seeking a Data Engineer to join our data team supporting our data environment focusing on ETL and data warehouse development and design. Applications will be accepted from individuals with at least 4 years of experience that reside in New Mexico. Periodic travel to headquarters will be required, but this position is otherwise remote.
Falling Colors
Falling Colors is a small (~30 person) B-certified company that is women and LGBTQ+ founded, owned, and operated. We are a diverse and open-minded team, with a mix of genders, ethnicities, and interests. We have parents and non-parents, self-taught and formally educated folk, introverts and extroverts, semi-professional ballroom dancers, golfers, gardeners, rock climbers, yogis, and VR hobbyists. As a company, we value using our skills and time to improve the world around us — from behavioral health care to local educational opportunities. As a team, we strive to be kind, support each other, and work together, stepping forward to handle all the exciting things that get thrown our way.
At Falling Colors, we don’t just build technology: we build impact. We take great pride in what we do for our communities with our products, and further seek to improve the world in everything we do. Our passion project is The White Building, a space rich with history. You can read more about this project at thewhitebuilding (dot) com on Instagram @thewhitebuilding. We use a portion of our profits to fund our Falling Colors Foundation, which paid out over $200,000 in small business grants to help our local businesses make it through uncertain times during the pandemic. We pay our employees to volunteer and do our best to protect them from burnout with a generous PTO policy, flex time, and paid holidays. We protect our employees’ futures by partnering with a B-certified financial group that manages our 401k accounts (and we contribute—not match—a percentage to those accounts). We understand the critical role we play as a business in the six determinants of social health, and we want to be the company that sets the bar and inspires others to be better.
We are in Santa Fe, New Mexico, with satellite offices across the United States. This is a hybrid position for a candidate residing in New Mexico or Colorado. The candidate will therefore need to have a functional at-home workspace with reliable internet service and the ability to come work in the office.
Data Engineer Qualifications & Responsibilities
Responsibilities
Be an individual contributor who is excited to build new ETL pipelines, maintain and enhance existing pipelines, and become an expert on our data and infrastructure.
Collaborate on an interdisciplinary team to design and develop business intelligence solutions for behavioral health providers and administrators.
Work with data analysts and product support to translate business needs into specifications for data warehouse and pipeline design.
Design, implement, and maintain ETL and data cube solutions with an eye towards performance, security, and quality.
Preprocess and integrate heterogenous data sources.
Evaluate ETL performance and explore opportunities to automate processes, reduce bottlenecks, enhance quality, and improve overall usability.
Support the design, development, and implementation of data warehousing, reporting and analytics solutions that deliver information efficiently, allowing our clients to make informed, data-driven business decisions.
Produce ad-hoc reports and analyses when needed.
Have or develop excellent understanding of privacy rules related to sensitive health data our systems process to ensure appropriate handling of such sensitive data.
Create and maintain documentation in support of the team.
Qualifications
The Must Haves:
Bachelor’s degree in computer science or equivalent, or demonstrated experience and competency equivalent
At least 3 years of database development experience with a demonstrated proficiency in writing advanced tSQL to extract data from complex data structures.
At least 3 years’ experience designing and developing data warehouses with a demonstrated proficiency in dimensional modeling.
Designing and implementing ETL solutions using Azure Data Factory (ADF) and/or Integration Services (SSIS).
Experience with database design for transactional systems and data warehouses.
Experience working on an interdisciplinary team of engineers, analysts, data quality professionals, and subject matter experts.
A friendly team player attitude, comfortable wearing different hats on different projects and willing to take leadership or support roles as needed.
Comfort working in existing structure and process, with a focus on technical implementation and data.
A passion for working in service of a meaningful mission to improve the lives of vulnerable populations through technology and data
The Good to Haves:
Experience analyzing and optimizing slow running queries.
Experience writing DAX statements.
Experience with Streaming Data Pipelines.
Experience analyzing and optimizing slow running queries.
The Bonus Stuff:
Demonstrated knowledge of the unique security and analytical considerations in working with protected healthcare data and information systems.
Demonstrated experience designing and implementing disaster recovery solutions to hit RPO and RTO
Working knowledge of PowerShell and specifically dbatools.io
Experience with Azure cloud infrastructure
Experience administering SQL Server in a cloud environment
Python programming experience
Ability to debug and understand C# code
Familiarity with State of New Mexico public and behavioral health agencies
Diversity Statement
Falling Colors is committed to recruiting and retaining a diverse workforce. We recognize that the tech industry overall has failed to develop a workforce reflective of our population as a whole, and pledge to do our part to do better. We encourage people of color, differently abled people, veterans, women, and LGBTQ folks to apply for open positions. We believe that a diverse workforce made up of people with different backgrounds and life experiences makes us stronger and better able to serve the communities where we work to make a difference. We will consider all applicants without regard to race, color, national origin, religion, sexual or gender orientation or expression, marital or parental status, disability, age, or other protected basis.
Benefits
We offer:
A month of paid time off plus a month of holidays
Paid parental leave
Paid volunteering time
Generous 401k contribution
Generous health, vision, and dental plans (for employee and family, at no cost to our employees)
Employer-paid short- and long-term disability plans
Employer-paid life insurance
Professional growth and continuing education opportunities
Estate planning stipend
LifeLock subscription
Falling Colors is an Equal Opportunity Employer
What Next?
If you think this sounds like it could be you, we encourage you to apply. We do not expect anyone to have all the things we are looking for, and we are always happy to have a conversation.
To apply, please send your resume and a cover letter to apply @fallingcolors (dot) com. We look forward to getting your application!
Job Type: Full-time
Pay: $85,000.00 - $95,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
Are you a resident of New Mexico, or are you willing to relocate?
Experience:
data warehouse design and development: 3 years (Required)
database development: 3 years (Required)
Work Location: Remote

Health insurance"
29,Data Engineer,Signify Technology,Remote,"$180,000 - $200,000 a year","Signify Technology is partnered with a client in the healthcare industry who is revolutionizing how healthcare companies are connected to their customers. Leveraging powerful applications with AI, they are rapidly growing and looking to add engineers who can provide solutions to solve challenging problems.
As a Senior Data Engineer, you will have the opportunity to work with a talented team of engineers while working with Python, Databricks, and marketing analytic software.
Requirements:
4 years of experience with Python and Spark
Experience with marketing analytics tools
Experience with SQL
Bachelors in Computer Science
Preferred Candidates also have:
Experience with ML applications
Experience working with AI
Masters in Computer Science
We cannot provide sponsorship for this role and are not working with 3rd parties.
Job Type: Full-time
Salary: $180,000.00 - $200,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Experience:
Python: 4 years (Required)
Work Location: Remote

Health insurance"
30,Junior Data Engineer,AlixPartners,"Southfield, MI•Hybrid remote",N,"At AlixPartners, we solve the most complex and critical challenges by moving quickly from analysis to action when it really matters; creating value that has a lasting impact on companies, their people, and the communities they serve. By understanding, respecting, and honoring the needs of our employees, clients, and communities, AlixPartners actively promotes an inclusive environment. We strongly believe in the value that diversity brings to our experiences and are committed to the perpetual enhancements of initiatives, policies, and practices. We hold ourselves accountable by providing the space for authenticity, growth, and equity for everyone. www.alixpartners.com
AlixPartners has embraced a hybrid work model to provide flexibility and support our employees' work-life integration. Our hybrid model combines a mix of in-person at an AlixPartners office and remote working.
What you'll do
In this important role on AlixPartners' IT Data Engineering & Shared Services team, you will provide on-site engineering and support for our Database and all database-related services, including managing database schema design, implementation, operation, and monitoring.
The Data Engineer is a full-time role located in Southfield, Michigan, and reports to the Head of Client Technology Operations & Data Services. Paid relocation is not available for this position.
Maintain the integrity of databases and applications (as directed) upon which the business units rely on for uninterrupted business operations.
Run and monitor database and application performance queries; design and implement testing routines to identify and resolve (de-bug) conflicts and potential operating malfunctions, using the Enterprise monitoring solutions and other utilities.
Adhere to and perform against internal SLAs to the business.
Participate in the monthly maintenance cycle as well as the on-call rotation.
Effectively communicate within the database team to ensure all team members are adequately informed of changes, detected issues, and other items necessary to maintain high levels of user satisfaction.
Troubleshoot and resolve malfunctions with databases, applications, and related infrastructure; escalate to other technology support resources as needed.
Schedule, communicate, perform and monitor both regular and ad hoc database maintenance activities in accordance with defined policies and procedures.
Administer database systems security and assign appropriate access privileges to end-users; monitor security issues and report potential problems and concerns.
Participate in the creation (or modification) of documentation activities related to system design, configuration, and operation; support policies and compliance procedures.
Additional responsibilities as identified. This description is not designed to encompass a comprehensive listing of required activities, duties, or responsibilities.
What you'll need
Bachelors' degree preferred.
Minimum one (1) year of commercial experience; SQL Server as a DBA preferred.
Administration of Microsoft SQL Server 2008/2012/2014/2016/2017 or above preferred.
Basic T-SQL programming preferred.
Database performance monitoring, tuning and optimization preferred.
Microsoft SQL 2017 preferred.
Advanced T-SQL programming preferred.
Familiar with SSIS, SSAS cubes & SSRS reports.
Experience in Cloud Services (e.g., Azure, Amazon).
Experience in a 24x7 service delivery model supporting mission-critical business operations.
Excellent written and verbal communication skills in English.
Willingness to work outside of normal U.S. business hours, and as unique projects/needs arise.
Ability to work full time in an office and remote environment; physically able to sit/stand at a computer and work in front of a computer screen for significant portions of the workday.
Must become familiar with, and promote and abide by, our Core Values as defined by the AlixPartners' Code of Conduct and foster an inclusive environment with people at all levels of an organization.
The firm offers a comprehensive benefits program including health, vision, dental, disability, 401K, tuition reimbursement, identity theft protection, and mental wellness support. Employees will also receive a generous paid leave policy including vacation/personal time starting at 5.67 hours per pay period, sick time up to 80 hours annually, parental leave, and twelve holidays.
AlixPartners is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to, among other things, race, color, religion, sex, sexual orientation, gender identity, national origin, age, status as a protected veteran, or disability. AlixPartners is a proud Silver award-winning Veteran Friendly Employer.
#LI-KL1
#LI-Hybrid"
31,Data Engineer - All Levels,N,United States,"$6,201 - $14,777 a month","Job Description
Duties for this role include but not limited to: supporting the design, build, test and maintain data pipelines at big data scale. Assists with updating data from multiple data sources. Work on batch processing of collected data and match its format to the stored data, make sure that the data is ready to be processed and analyzed. Assisting with keeping the ecosystem and the pipeline optimized and efficient, troubleshooting standard performance, data related problems and provide L3 support. Implementing parsers, validators, transformers and correlators to reformat, update and enhance the data. Provides recommendations to highly complex problems. Providing guidance to those in less senior positions.

Additional Job Description:
Data Engineers play a pivotal role within Dataworks, focused on creating and driving engineering innovation and facilitating the delivery of key business initiatives. Acting as a “universal translator” between IT, business, software engineers and data scientists, data engineers collaborate across multi-disciplinary teams to deliver value. Data Engineers will work on those aspects of the Dataworks platform that govern the ingestion, transformation, and pipelining of data assets, both to end users within FedEx and into data products and services that may be externally facing. Day-to-day, they will be deeply involved in code reviews and large-scale deployments.

Essential Job Duties & Responsibilities

Understanding in depth both the business and technical problems Dataworks aims to solve
Building tools, platforms and pipelines to enable teams to clearly and cleanly analyze data, build models and drive decisions
Scaling up from “laptop-scale” to “cluster scale” problems, in terms of both infrastructure and problem structure and technique
Collaborating across teams to drive the generation of data driven operational insights that translate to high value optimized solutions.
Delivering tangible value very rapidly, collaborating with diverse teams of varying backgrounds and disciplines
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases
Interacting with senior technologists from the broader enterprise and outside of FedEx (partner ecosystems and customers) to create synergies and ensure smooth deployments to downstream operational systems

Skill/Knowledge Considered a Plus
Technical background in computer science, software engineering, database systems, and distributed systems.
Keen understanding of transportation and logistics domain with the ability to identify high opportunity areas and design approaches and solutions that generate and capture value.
Development experience in and familiarity with Microsoft Azure.
Knowledge of and demonstrated experience in building CI/CD pipelines.
Developing and operationalizing capabilities and solutions including under near real-time high-volume streaming conditions.
Hands-on development skills with the ability to work at the code level and help debug hard to resolve issues.
Demonstrated ability to deliver technical projects, often working under tight time constraints to deliver value.
An ‘engineering’ mindset, willing to make rapid, pragmatic decisions to improve performance and accelerate progress.
Comfort with working with distributed teams on code-based deliverables, using version control systems and code reviews.
DevOps: In terms of technology, must have all the knowledge like coding, scripting, database, yaml, json and IaC.
Demonstrated expertise working with some of the following common languages and tools:
Spark (Scala and PySpark), HDFS, Kafka and other high-volume data tools.
SQL and NoSQL storage tools, such as MySQL, Postgres, Cassandra, MongoDB and ElasticSearch.
Pandas, Scikit-Learn, Matplotlib, TensorFlow, Jupyter and other Python data tools

Minimum Qualifications:
Data Engineer II :
Bachelor's Degree in Computer Science, Information Systems, a related quantitative field such as Engineering or Mathematics or equivalent formal training or work experience. Two (2) years equivalent work experience in measurement and analysis, quantitative business problem solving, simulation development and/or predictive analytics. Strong knowledge in data engineering and machine learning frameworks including design, development and implementation of highly complex systems and data pipelines. Strong knowledge in Information Systems including design, development and implementation of large batch or online transaction-based systems. Strong understanding of the transportation industry, competitors, and evolving technologies. Experience as a member of multi-functional project teams. Strong oral and written communication skills. A related advanced degree may offset the related experience requirements.
Sponsorship is not available for Data Engineer II role.

Data Engineer III:
Bachelor’s Degree in Information Systems, Computer Science or a quantitative discipline such as Mathematics or Engineering and/or equivalent formal training or work experience. Five (5) years equivalent work experience in measurement and analysis, quantitative business problem solving, simulation development and/or predictive analytics. Extensive knowledge in data engineering and machine learning frameworks including design, development and implementation of highly complex systems and data pipelines. Extensive knowledge in Information Systems including design, development and implementation of large batch or online transaction-based systems. Strong understanding of the transportation industry, competitors, and evolving technologies. Experience providing leadership in a general planning or consulting setting. Experience as a senior member of multi-functional project teams. Strong oral and written communication skills. A related advanced degree may offset the related experience requirements.

Data Engineer Lead:
Bachelor’s Degree in Information Systems, Computer Science, or a quantitative discipline such as Mathematics or Engineering and/or equivalent formal training or work experience. Seven (7) years equivalent work experience in measurement and analysis, quantitative business problem solving, simulation development and/or predictive analytics. Extensive knowledge in data engineering and machine learning frameworks including design, development and implementation of highly complex systems and data pipelines. Extensive knowledge in Information Systems including design, development and implementation of large batch or online transaction-based systems. Strong understanding of the transportation industry, competitors, and evolving technologies. Experience providing leadership in a general planning or consulting setting. Experience as a leader or a senior member of multi-function project teams. Strong oral and written communication skills. A related advanced degree may offset the related experience requirements.

Domicile / Relocation Information:
This position can be domiciled anywhere in the United States.
The ability to work remotely within the United States may be available based on business need.

Application Criteria:
Upload current copy of Resume (Microsoft Word or PDF format only) and answer job screening questionnaire.

Additional Information
Colorado, Nevada, Connecticut, New York, California, Rhode Island and Washington Residents Only - Compensation: Monthly Salary: $6,201.00 - $14,777.00. This compensation range is provided as a reasonable estimate of the current starting salary range for this role. Factors that may be used to determine your actual salary may include but are not limited to your specific skills, your work location, how many years of experience you have, and comparison to other employees already in this role.

Born out of FedEx, a pioneer that ships nearly 20 million packages a day and manages endless threads of information, FedEx Dataworks is an organization rooted in connecting the physical and digital sides of our network to meet today's needs and address tomorrow's challenges.
We are creating opportunities for FedEx, our customers, and the world at large by:
Exploring and harnessing data to define and solve true problems;
Removing barriers between data sets to create new avenues of insight;
Building and iterating on solutions that generate value;
Acting as a change agent to advance curiosity and performance.
At FedEx Dataworks, we are making supply chains work smarter for everyone.

Employee Benefits: medical, dental, and vision insurance; paid Life and AD&D insurance; tuition reimbursement; paid sick leave; paid parental leave, paid vacation, paid military leave, and additional paid time off; geographic pay ranges; 401k with Company match and incentive bonus potential; sales Incentive compensation for selling roles.

Dataworks does not discriminate against qualified individuals with disabilities in regard to job application procedures, hiring, and other terms and conditions of employment. Further, Dataworks is prepared to make reasonable accommodations for the known physical or mental limitations of an otherwise qualified applicant or employee to enable the applicant or employee to be considered for the desired position, to perform the essential functions of the position in question, or to enjoy equal benefits and privileges of employment as are enjoyed by other similarly situated employees without disabilities, unless the accommodation will impose an undue hardship. If a reasonable accommodation is needed, please contact DataworksTalentAcquisition@corp.ds.fedex.com."
32,Data Engineer I,"ConstructConnect, Inc","Cincinnati, OH 45209",N,"ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.
The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.
The Opportunity
The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects to build and train predictive analytic models, design linear regression models using data
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Experience writing complex SQL Queries, Stored Procedures and Views
Experience building and optimizing data pipelines, architectures and data sets.
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
Understanding of version control and automated CI/CD with technologies such as Gitlab/ GitHub
1 – 3 years professional experience managing Data Lakes and Data Warehouses
Bachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)"
33,Snowflake Data Engineer,NextRow Digital,Remote,"$79,613 - $185,640 a year","Snowflake Data Engineer
Duration: Long Term
Remote Work
This is a 100% remote job.
Must have 3 verifiable references.
Job Responsibilities:
This position will require qualified T-SQL Developers to take the lead in the following tasks:
Rationalizing and mapping data between transactional and dimensional database
models/systems
Working closely with the database and system administrators to develop stored
procedures and ETL processes related to the data warehouse system.
Working in collaboration with vendors and other agency staff to design, develop and
manage the creation Synapse pipelines.
Ability to work independently and cooperatively as part of a team.
Ability to work under severe time constraints.
Must possess analytical and complex problem-solving skills.
REQUIRED SKILLS/EXPERIENCE
Extensive experience with development of stored procedures and ETL processes using T-SQL
Thorough understanding of data warehouse design hierarchies such as star and snowflake schemas
Use of ALM tools for work item management, version control, code analysis, and testing
Broad and extensive knowledge of the software development process and its technologies
Familiarity with continuous integration
Experience creating and scheduling elastic jobs
Experience with designing and modeling database structures based on business use cases
PREFERENCE TO BE GIVEN TO THE CANDIDATES WITH:
Experience with student information management systems and K-12 data and reporting
Experience with Common Education Data Standards (CEDS)
Experience with designing and modeling database structures based on business use cases
Azure Data Engineer Associate certification
Experience working with Azure Synapse Pipelines
Job Types: Full-time, Contract
Salary: $79,612.68 - $185,640.48 per year
Schedule:
8 hour shift
Experience:
Snowflake: 5 years (Required)
Azure: 4 years (Required)
Data warehouse: 5 years (Required)
T-SQL: 7 years (Required)
Work Location: Remote"
34,Data Engineer I,"The Travelers Companies, Inc.","Hartford, CT","$102,600 - $169,200 a year","Who Are We?
Taking care of our customers, our communities and each other. That’s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.
Job Category
Technology
Compensation Overview
The annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.
Salary Range
$102,600.00 - $169,200.00
Target Openings
1
What Is the Opportunity?
As a contributing member of the Salesforce Agile Circle, you will be responsible for working with various Business areas and IT stakeholders to elicit, analyze, specify, and validate business and IT solutions. This role supports the creation of data centric features and user stories and ensures committed in progress work meets the needs of the business. The role works with a diverse stakeholder group across both Business and IT domains and across multiple Enterprise value streams.
What Will You Do?
Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions in support of our Salesforce integrations
Design data solutions.
Analyze sources to determine value and recommend data to include in analytical processes.
Incorporate core data management competencies including data governance, data security and data quality.
Collaborate within and across teams to support delivery and educate end users on data products/analytic environment.
Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate.
Test data movement, transformation code, and data components.
Perform other duties as assigned.
What Will Our Ideal Candidate Have?
Bachelor’s Degree in STEM related field or equivalent
Six years of related experience
Proficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices.
The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions.
Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on.
Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems.
Strong verbal and written communication skills with the ability to interact with team members and business partners.
Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities.
Solid SQL Server working knowledge – able to create and analyze store procedures
Working knowledge of MongoDB or other NoSQL databases
Working knowledge of pub-sub message exchange architecture, such as RabbitMQ or Kafka
Working knowledge of various message formats, such as JSON
AWS Foundational Cloud Practitioner certificate preferred
AWS experience and knowledge of various AWS messaging and storage patterns, such as S3 bucket, DynamoDB, etc.
What is a Must Have?
Bachelor’s degree or equivalent training with data tools, techniques, and manipulation.
Four years of data engineering or equivalent experience.
What Is in It for You?
Health Insurance: Employees and their eligible family members – including spouses, domestic partners, and children – are eligible for coverage from the first day of employment.
Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.
Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.
Wellness Program: The Travelers wellness program is comprised of tools and resources that empower you to achieve your wellness goals. In addition, our Life Balance program provides access to professional counseling services, life coaching and other resources to support your daily life needs. Through Life Balance, you’re eligible for five free counseling sessions with a licensed therapist.
Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.
Employment Practices
Travelers is an equal opportunity employer. We believe that we can deliver the very best products and services when our workforce reflects the diverse customers and communities we serve. We are committed to recruiting, retaining and developing the diverse talent of all of our employees and fostering an inclusive workplace, where we celebrate differences, promote belonging, and work together to deliver extraordinary results.

If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an
email
so we may assist you.

Travelers reserves the right to fill this position at a level above or below the level included in this posting.
To learn more about our comprehensive benefit programs please visit
http://careers.travelers.com/life-at-travelers/benefits/
."
35,Data Engineer - Jr/Mid Level (Remote),pulseData,United States•Remote,"$75,000 - $120,000 a year","Join us as we pursue our vision to eliminate preventable sickness and transform kidney care using AI and data!

At pulseData, we provide value-based healthcare organizations and their care teams a patented data intelligence platform that helps deliver better care at less cost. As a company, we leverage data science and machine learning for Hospitals and Healthcare companies, to identify patients at risk of costly and avoidable medical outcomes.

You will become a member of pulseData's engineering team in this role. As our Data Engineer, you will assist in the developing, deploying, and maintaining our product pipelines and work in an environment of growth and collaboration.

The Role:
You'll be developing, deploying, and maintaining our production data pipeline which produces risk scores and patient reports vital to the workflows of doctors and care coordinators.
You'll be ensuring product deliverables are executed reliably and accurately on a regular basis.
You'll be managing and monitoring client interfaces to ensure timely delivery of data.
You'll be growing your skillset in a diverse and challenging environment that offers the latest technologies.

Your Background:
Completed Degree - We are open to candidates with 0 to 4 years of experience.
Any experience managing a data pipeline in a cloud environment is a plus.
We use open-source technologies like Apache Spark, Kubernetes, Airflow in addition to Google Cloud services and our own proprietary healthcare analytics library.
You possess a growth mindset with the ability to work autonomously and with teammates remotely.
Experience working on healthcare data is a plus!
This role does not offer sponsorship

The Perks:
Salary between $75k to $120k
Potential bonus and equity
Unlimited PTO - so you can stay at your best year round!
A remote working environment - you can live anywhere in the US!
Top tier benefits include medical, dental, and vision insurance - 85% of which is covered by pulseData.
Fun gym discounts, a 401k, and a new laptop + equipment.
Company offsite adventures throughout the US each year.
$300 annual home office stipend
Tons of room for growth and the ability to learn new things.
+ lots of other fun and exciting things!

75,000 to 120,000
Chronic Kidney Disease (CKD) is a substantially under-diagnosed condition that affects 37 million people in the US. Today, clinical and care management teams use our platform to identify the most at risk-individuals and proactively deliver care in advance of adverse events, delivering better clinical outcomes at a lower cost. Our clients are risk-bearing healthcare enterprises: ACOs, kidney clinics and nephrology practices, large health systems, managed care and health insurance companies. Together, we use data science to deliver more human care.

pulseData is backed by some of the best investors in the industry, recently raising a $16.5M Series A led by Bain Capital and Two Bear Capital."
36,"Data Engineer – Salesforce, SQL, SSIS – FULLY REMOTE WORK 42233","PRIMUS Global Services, Inc","Phoenix, AZ•Remote",N,"We have an immediate long-term opportunity with one of our key clients for a position of Data Engineer, to work on a remote basis.

The ideal applicant must have strong expertise with Salesforce, data movement, processing, and SSIS, as well as developing solutions for optimal data processing on the Salesforce platform.

A candidate who is API oriented with MuleSoft would be a huge plus; they are not fully involved right now, but will be by the end of the year.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Preeti
PRIMUS Global Services
Direct: 972-734-5373
Desk: 972-753-6500 Ext: 266
Email: jobs@primusglobal.com"
37,Data Engineer - Flink,Amazon,"10300 Campus Point Dr Ste 200, San Diego, CA 92121",$72 - $80 an hour,"The successful candidate in this role will have:
Experience building and maintaining enterprise-scale (Terabyte - Exabyte) data pipelines.
Experience using modern open-source technologies and cloud services (SNS, SQS, MSK, ECS, EC2, DynamoDB, Kinesis, EMR, Kafka, Flink, Spark)
Experience with modern compression technologies (Orc, Spark)
Experience working with customers to model and onboard datasets to fit customer requirements.
Experience developing data pipeline parsers using Scala.
To follow up with any questions, please contact Ajitabh at # 408-907-2956
Job Type: Contract
Pay: $72.00 - $80.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Austin, TX: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
LinkedIn profile is required. Please share your LinkedIn profile
Experience:
Apache Flink: 1 year (Required)
Scala: 2 years (Required)
Parsing of Data: 2 years (Required)
AWS tools: 2 years (Required)
Work Location: One location"
38,Sr. Data Engineer,Accrette Information Technology Professional Solutions Inc.,Remote,"$87,409 - $173,190 a year","Job Purpose:
Responsible for visualizing and designing enterprise data management framework. Analyses the data needs of thecompany and creates the data roadmap.
The role demands the deployment of theright data architecture to convert business need into data and system requirements, align business processes with IT systems, and manage the complex flow of data and information within the organization
Work Experience
Must have worked on Data Lake projects and involved in supporting data orchestration activities as part of building and enhancing
Experience of architecting data solution across hybrid (cloud, on premise) data platforms
Exposure to cloud integration/ cloud DWH implementation. (Snowflake, Azure Data Factory, Databricks etc.)
Deep knowledge and experience of the different database technologies - Oracle, SQL Server, PostgreSQL
Experience in large-scale Data Migration Projects
Experience of ETL & ELT Tools such as Informatica IICS.
Experience in Performance Tuning of ETL scripts to reduce the CPU time/load timing
Experience with DevOps and Monitoring on Azure.
Experience of mapping key Enterprise data entities to business capabilities and applications
Principal Accountabilities
Providing technical and data leadership to organization
Providing operation guidance to the application development team, IT and other functions.
Understand the strategic business mission, vision, direction and goals
Understand the future business capabilities (KPI, metrics, goals) etc.
Identify current capabilities and areas of pain
Identify gaps and areas of opportunity
Understand the current data policies and procedure
Gather Data via interviews, workshops, focus group discussion
Baseline and benchmark best PA and common solutions
Bring outside in view on latest tech stack on data management, Data Orchestration, DevOps and Source Code Management insight for Structural as well as semi / unstructured data ( across on-prem as well as on cloud)
Prepare framework to review projects to ensure short term and long terms data strategy are followed by project delivery teams
Build a framework of principles to ensure data integrity across the business
Identify and evaluate new data management technologies
Develop and Integrate technical functionality (e.g. scalability, security, performance, data recovery, reliability, etc.)
Required professsional software development experience
3+ years of programming experience in React.
2+ years of application development using Azure Cloud native architecture
Extensive knowledge in: React.js, Typescript, JSON, Javascript, CSS, UI UX design and development, Agile delivery
Strong proficiency in JavaScript, including the JavaScript object model
Thorough understanding of React.js and its core principles
Experience with popular React.js and React native
Familiarity with RESTful APIs
Familiarity with code versioning tools such as GIT
Should have knowledge about HTML, CSS design that is compatible across multiple browsers, ADA/CATO complaint
Should have knowledge about working in an agile environment & Azure Cloud platform
Job Types: Full-time, Contract
Salary: $87,408.67 - $173,189.72 per year
Compensation package:
Bonus pay
Schedule:
8 hour shift
Experience:
Data modeling: 3 years (Preferred)
Work Location: Remote"
39,Data Engineer,"TeamWorx Security, LLC","Columbia, MD 21046•Remote","$100,000 - $125,000 a year","Title: Data Engineer
Location: Remote
Position Type: Full-Time
Salary Range: $100k-$125k
U.S. Citizenship is required. Must be willing to submit for a security clearance.

The Opportunity
TeamWorx Security is looking for a highly motivated and adaptable Data Engineer with a drive to succeed in a fast-paced environment! The Data Engineer will work closely with both our technical team and our government customers to build capabilities and solutions to assess program effectiveness across a range of processes, projects, and systems. The candidate will apply a wide range of qualitative and/or quantitative methods to recommend performance improvements by producing thorough management reports and automation tools tailored to the specific needs of each program component.

What it Takes!
We are product-focused and love innovation. We look for leadership qualities in all of our teammates. You must be comfortable exploring, learning, and building cool technologies, making mistakes, and learning along the way. You should be self-driven and self-motivated with a great attitude! You should take ownership of projects and drive them to completion. We aim to challenge and grow our employees, and in return, their contributions guide our efforts to deliver maximum value to our customers. You must be comfortable working remotely, directly with our customers, and in our team environment.

What You'll Be Doing
The perfect candidate should have detailed knowledge of JavaScript, Google Apps Script, Google Workspace, data structures, analysis, and visualization. Knowledge of web-app development and programming languages such as CSS, HTML, and Python are desired. Strong attention-to-detail and the ability to produce clear and concise documentation are also essential. The Data Engineer is expected to:

Develop and maintain web applications using Google Apps Script and JavaScript
Extract and analyze data to provide unique insights and patterns specific to the necessary task
Validate the work of others by performing confirmatory data analysis
Collaborate with other team members to design, develop, and implement data-driven solutions
Create and maintain data pipelines and database architectures
Develop and implement data quality checks and processes
Apply knowledge and experience using industry-standard web development tools and programming languages
Utilize software packages such as Oracle, MySQL, Tableau, Google Sheets, and Excel at an advanced level to manipulate and display data

What We Think You'll Need
Bachelor’s in Computer Science, Data Science, Information Systems, or a related field (or equivalent experience)
Proficiency in JavaScript, Knowledge of other programming languages such as CSS, HTML, and Python is a plus
Experience with Google Apps Script, Google Workspace, and web application development
Ability to perform exploratory and confirmatory data analysis
Knowledge of descriptive analytics, trends, business intelligence, and business analytics
Strong problem-solving and analytical skills
Excellent communication and teamwork abilities
Self-motivation, adaptability, and drive to succeed in a fast-paced environment.
US citizenship required and must willing to submit for a security clearance

What’s in it for you?

No more bullet points for starters. Also, these awesome perks: Comprehensive Health, Dental, & Vision Insurance, Short-Term Disability & Life Insurance, Matching 401(k) Contributions, Paid Time Off including 11 Holidays and your Birthday, Professional Development, Fitness Membership, and Home Office allocations, Performance Bonuses & Profit Sharing, & Employee Referral Bonuses. But wait, there's more: TeamWorx Security is committed to our teammates and their professional goals. Being a small but growing company affords us the opportunity to work WITH YOU to reach your full potential.

A Little About Us and Our Team
We love to be inspired. We started TeamWorx Security to build, create, and dream about what inspires us, and because of this, our Maryland startup is quickly growing. We are a team of data scientists, software engineers, researchers, product developers, and operation-minded folks who specialize in developing applications and products that our customers love. Our mission is to reinvent the way technical and non-technical people work with technology. We believe in creating harmonious, automated solutions designed to improve our customers’ daily workflow, allowing them to act quickly and decisively about what matters most. We are looking for critical thinkers who excel at solving problems and have a passion for what they do to help us move our mission forward. This is our story; come tell us your story.

At TeamWorx Security, Inc., we hold our values as guiding principles in all that we do and encourage our community to embrace and embody them. Our core values are putting our employees first, being curious, being authentic, being scrappy, and honoring those we serve. These values drive our company culture, decision-making, and operations.

All qualified applicants will receive consideration for employment without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, or any other basis prohibited by law. TeamWorx Security will also consider the employment of qualified applicants with criminal histories, consistent with relevant laws."
40,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
41,Data Engineer,Arraya Solutions,Remote,N,"***Prospective candidates must be in Eastern or Central Time Zone and much be able to be employed as a W2 employee***
Arraya Solutions, a leading Mid-Atlantic technology consulting firm and Managed Services provider located just outside Philadelphia, is looking for a Data Engineer to join our team!
We are a culture that embraces change, values family and are actively involved with the community. Our team consists of people with positive attitudes who are interested in growing their knowledge around technology and leaders that are heavily involved in day-to-day activities.
Job Summary
The Data Engineering Team is a part of the Modeling and Data Sciences technology organization. This team supports key initiatives and improves the competitiveness and operational efficiencies in our existing and next-generation offerings by developing digital solutions and technologies in condition monitoring, reliability/risk modeling, dynamic modeling, IIoT, and new technology development & evaluation. This team works in close collaboration with the Engineering, Technology, and Operational teams to deliver competitive world-leading solutions. The position will be on the technical ladder where the incumbent will be provided with learning and advancement opportunities focusing on long-term career growth and success.
Essential Job Responsibilities:
Working with IT and business resources, support the Data Warehouse strategy and Business Intelligence initiatives:
Lead development activities to migrate the current on-premises SQL Server DW to Snowflake on AWS.
Gather requirements for Data Warehouse improvements and translate them to high level design documents including Physical Data Models. Provide technical architecture vision and recommend strategy/solutions.
Design and develop transformation processes and data structures for the Data Warehouse following best practice procedures.
Lead resolution of Data Warehouse load issues.
Support the analytical needs of the business users. Build strong relationships to help identify opportunities to enhance the analytical capabilities of the Data Warehouse.
Partnering with business stakeholders and technical report developers, establish, maintain, and promote consistent methodology for reporting and analytics deployment. Perform ad hoc data analysis to meet business unit data validation needs.
Participate in Data Quality initiatives and lead the data transformation component design to improve and maintain high quality data. Support performance tuning.
Foster teamwork through cooperative interactions with co-workers. Where needed, ensure project integration by communicating activities and status to project manager, appropriate project team members, and business users.
Job Qualifications:
BS degree, preferably in Information Technology, Management Information Systems, Computer Science or similar discipline.
Post-college training in Data Management and vendor tool use
4-6+ years of Data Warehousing/ BI experience.
This role serves as the primary technical resource for managing and moving data in and out of the Data Warehouse. The candidate must:
Be well versed in Data Warehouse design.
Have hands-on experience with ETL and ELT methodologies and tools.
Have experience with SQL Server 2016 and SSIS.
Experience with business reporting requirements analysis.
Familiarity with Data Management and Business Intelligence tools, such as Business Objects, Microsoft Power BI, QlikSense, Tableau, Looker, AWS S3 and Redshift, or Snowflake.
Demonstrated ability to clearly communicate with all levels within an organization.
Have experience leading small or offshore development teams
understanding and experience with building and deploying Business Intelligence and analytics applications.
Familiarity with Financial, Sales, Marketing, and Logistics reporting environments.
Strong sense of leadership, strong analytical skills, excellent communication Have expert skills and good technical abilities.
Preferred candidates will have experience with:
SSAS.
Analytics design and development is preferred.
Hands on development experience with .Net and/or Java is a plus.
Comfortable working in a fast-paced team. Capable of grasping new concepts quickly and acting with a sense of urgency.
Preferred candidate may have certifications like:
CBIP (Certified Business Intelligence Professional)
CDMP (Certified Data Management Professional)
Vendor-specific certifications
Job Types: Full-time, Contract
Schedule:
8 hour shift
Application Question(s):
What city/State are you currently located in?
Are you able to work as a W2 employee?
Work Location: Remote"
42,Data Engineer,Starschema,"Arlington, VA•Remote",N,"Company Description

About Starschema

At Starschema we believe that data has the power to change the world and data-driven organizations are leading the way. We help organizations use data to make better business decisions, build smarter products, and deliver more value for their customers, employees and investors. We dig into our customers’ toughest business problems, design solutions and build the technology needed to address today’s unique challenges.

What you can expect as a Starschema team member
As a member of the Starschema team, you will be on the front lines of digital transformation, working with some of the most innovative Fortune 500 companies to drive innovation and realize the promise of data-driven cultures. You will learn and use the latest data-centric technologies along with the core industry technologies.
Our team is inclusive and fun. While we take our work seriously, we know how to have a good time while doing so. We encourage everyone to share their opinions and ideas, and our leadership wants to hear everyone’s input no matter what role they play in the organization.

Job Description

As a Data Engineer at Starschema, you will bring business value for our clients through end to end development, optimization and operation of automated reporting, data lakes and related software platforms. You will use the latest technologies like Apache Airflow, Apache Kafka, Apache Spark and AWS etc. We are seeking for experienced medior and senior professionals for our open position.
What will You do:
Build and maintain database/bigdata clusters;
Build dashboards for infrastructure management and reporting;
Design and deploy infrastructure management strategies to meet up time and monitoring SLA’s;
Deploy code release in QA and PROD;
Participate in building unit/performance/integration tests working with database developers;
Participate in database SQL optimization plan;
Deploy configuration and automation tools to remove manual steps in deploying, upgrading, and scaling systems and software across all environment.

Qualifications

We want to hear from you if You have:
At least 3 years of experience in data engineering field;
Solid background in Python and SQL;
Experience building data solutions using big data tools: Airflow, Spark, Kafka, AWS;
Experience with data pipeline and workflow management tools
Hands-on experience with requirements analysis, design, coding and testing patterns;
Has experience in engineering (commercial and open source) software platforms and large-scale data infrastructures;
Experience working with cloud computing environments;
Excellent communications skills in English (both written and oral);
Intelligent, communicative team-player personality, interested in and willing to learn new skills and technologies.

Additional Information

What's In It For You:
Remote work: You can work remotely from anywhere within the USA. Plus if you are based in Washington D.C area.
Eligibility: We are unable to support work visa for this specific position so we are open to receive application from candidates who are eligible to work in the USA.
Benefits & Community: A healthy lifestyle and the feeling of belonging are important to us, for both body and mind. We provide:
401K Insurance with matching
Employee Assistance Program (EAP)
Technical/Professional trainings
Remote work / Home Office opportunity
Start date: The sooner the better, but if you currently work somewhere and have a notice period, it is still fine, we will wait for the right person!"
43,Data Engineer,Falling Colors,New Mexico•Remote,"$85,000 - $95,000 a year","Falling Colors offers solutions for administering public, private, and charitable funds to organizations that share a mission of promoting the Social Determinants of Health. We track allocated spending and measure outcomes through a configurable software platform and supporting services, allowing customers to glean insight into the effectiveness of funded programs via reporting and analytics dashboards, driving outcome-oriented decisions to maximize the impact of future investments.
We are seeking a Data Engineer to join our data team supporting our data environment focusing on ETL and data warehouse development and design. Applications will be accepted from individuals with at least 4 years of experience that reside in New Mexico. Periodic travel to headquarters will be required, but this position is otherwise remote.
Falling Colors
Falling Colors is a small (~30 person) B-certified company that is women and LGBTQ+ founded, owned, and operated. We are a diverse and open-minded team, with a mix of genders, ethnicities, and interests. We have parents and non-parents, self-taught and formally educated folk, introverts and extroverts, semi-professional ballroom dancers, golfers, gardeners, rock climbers, yogis, and VR hobbyists. As a company, we value using our skills and time to improve the world around us — from behavioral health care to local educational opportunities. As a team, we strive to be kind, support each other, and work together, stepping forward to handle all the exciting things that get thrown our way.
At Falling Colors, we don’t just build technology: we build impact. We take great pride in what we do for our communities with our products, and further seek to improve the world in everything we do. Our passion project is The White Building, a space rich with history. You can read more about this project at thewhitebuilding (dot) com on Instagram @thewhitebuilding. We use a portion of our profits to fund our Falling Colors Foundation, which paid out over $200,000 in small business grants to help our local businesses make it through uncertain times during the pandemic. We pay our employees to volunteer and do our best to protect them from burnout with a generous PTO policy, flex time, and paid holidays. We protect our employees’ futures by partnering with a B-certified financial group that manages our 401k accounts (and we contribute—not match—a percentage to those accounts). We understand the critical role we play as a business in the six determinants of social health, and we want to be the company that sets the bar and inspires others to be better.
We are in Santa Fe, New Mexico, with satellite offices across the United States. This is a hybrid position for a candidate residing in New Mexico or Colorado. The candidate will therefore need to have a functional at-home workspace with reliable internet service and the ability to come work in the office.
Data Engineer Qualifications & Responsibilities
Responsibilities
Be an individual contributor who is excited to build new ETL pipelines, maintain and enhance existing pipelines, and become an expert on our data and infrastructure.
Collaborate on an interdisciplinary team to design and develop business intelligence solutions for behavioral health providers and administrators.
Work with data analysts and product support to translate business needs into specifications for data warehouse and pipeline design.
Design, implement, and maintain ETL and data cube solutions with an eye towards performance, security, and quality.
Preprocess and integrate heterogenous data sources.
Evaluate ETL performance and explore opportunities to automate processes, reduce bottlenecks, enhance quality, and improve overall usability.
Support the design, development, and implementation of data warehousing, reporting and analytics solutions that deliver information efficiently, allowing our clients to make informed, data-driven business decisions.
Produce ad-hoc reports and analyses when needed.
Have or develop excellent understanding of privacy rules related to sensitive health data our systems process to ensure appropriate handling of such sensitive data.
Create and maintain documentation in support of the team.
Qualifications
The Must Haves:
Bachelor’s degree in computer science or equivalent, or demonstrated experience and competency equivalent
At least 3 years of database development experience with a demonstrated proficiency in writing advanced tSQL to extract data from complex data structures.
At least 3 years’ experience designing and developing data warehouses with a demonstrated proficiency in dimensional modeling.
Designing and implementing ETL solutions using Azure Data Factory (ADF) and/or Integration Services (SSIS).
Experience with database design for transactional systems and data warehouses.
Experience working on an interdisciplinary team of engineers, analysts, data quality professionals, and subject matter experts.
A friendly team player attitude, comfortable wearing different hats on different projects and willing to take leadership or support roles as needed.
Comfort working in existing structure and process, with a focus on technical implementation and data.
A passion for working in service of a meaningful mission to improve the lives of vulnerable populations through technology and data
The Good to Haves:
Experience analyzing and optimizing slow running queries.
Experience writing DAX statements.
Experience with Streaming Data Pipelines.
Experience analyzing and optimizing slow running queries.
The Bonus Stuff:
Demonstrated knowledge of the unique security and analytical considerations in working with protected healthcare data and information systems.
Demonstrated experience designing and implementing disaster recovery solutions to hit RPO and RTO
Working knowledge of PowerShell and specifically dbatools.io
Experience with Azure cloud infrastructure
Experience administering SQL Server in a cloud environment
Python programming experience
Ability to debug and understand C# code
Familiarity with State of New Mexico public and behavioral health agencies
Diversity Statement
Falling Colors is committed to recruiting and retaining a diverse workforce. We recognize that the tech industry overall has failed to develop a workforce reflective of our population as a whole, and pledge to do our part to do better. We encourage people of color, differently abled people, veterans, women, and LGBTQ folks to apply for open positions. We believe that a diverse workforce made up of people with different backgrounds and life experiences makes us stronger and better able to serve the communities where we work to make a difference. We will consider all applicants without regard to race, color, national origin, religion, sexual or gender orientation or expression, marital or parental status, disability, age, or other protected basis.
Benefits
We offer:
A month of paid time off plus a month of holidays
Paid parental leave
Paid volunteering time
Generous 401k contribution
Generous health, vision, and dental plans (for employee and family, at no cost to our employees)
Employer-paid short- and long-term disability plans
Employer-paid life insurance
Professional growth and continuing education opportunities
Estate planning stipend
LifeLock subscription
Falling Colors is an Equal Opportunity Employer
What Next?
If you think this sounds like it could be you, we encourage you to apply. We do not expect anyone to have all the things we are looking for, and we are always happy to have a conversation.
To apply, please send your resume and a cover letter to apply @fallingcolors (dot) com. We look forward to getting your application!
Job Type: Full-time
Pay: $85,000.00 - $95,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
Are you a resident of New Mexico, or are you willing to relocate?
Experience:
data warehouse design and development: 3 years (Required)
database development: 3 years (Required)
Work Location: Remote

Health insurance"
44,Data Engineer,Wevision LLC,"Irvine, CA 92602•Remote",$55 - $85 an hour,"Job description
We build services, data platform and machine learning based optimization engines for every aspect of advertising, including targeting, decisioning, pricing, personalization, inventory forecasting, attribution and full-funnel measurements. Our tenant is a strong tech team to deliver E2E solutions covering tech areas ranging from research, bigdata, microservices to data applications with front-end tools. Our team is seeking a software developer who will be an outstanding addition and will be responsible for development of high-available and high-concurrent backend services or data solutions. The right person for this role should have experience on either full stack components, or microservices or bigdata platforms. If you are someone who is proactive, hardworking, and enthusiastic in either these domains, this is a phenomenal role for you!
WHAT YOU’LL DO
Build components of large scale data platform for online streaming data and offline batch data from ETL pipelines, data processing, operational data store and AI feature stores.
Continuously improve performance, scalability and availability for microservices of advertising targeting, decisioning and ranking.
Own features of bigdata applications to fit evolving business with realtime metrics, measurable insights and industry leading user experience.
Drive adoption of the best engineering practices, including the use of design patterns, CI/CD, code review and automated integration testing.
Chip in disruptive innovation and apply new ground breaking technologies
As a key member of the team, contribute to all aspects of the software lifecycle: design, experimentation, implementation and testing.
Collaborate with program managers, product managers, and researchers in an open and innovative environment.
WHAT TO BRING
BS in computer science or engineering.
3+ years of professional programming and design experience in Scala, Java, Python, and etc.
2+ years of experience with Hadoop ecosystem e.g. HBase, Hive, Spark/Flink, Impala, Presto, Click House, Druid and etc.
Knowledge of system, application design and architecture
Passion for technology, open to interdisciplinary work
NICE-TO-HAVES
Experience with processing large amount of data at petabyte level.
Experience in digital video advertising or digital marketing domain.
Experience with CRM, DMP, user portrait and audience insights.
Experience with Airflow, Kafka, MemSQL, Docker, AWS, Terraform, Spinnaker, K8S, and etc.
Experience in at least one widely used Web framework (React.js, Vue.js, Angular, etc.) and good knowledge of Web stack HTML, CSS, Webpack.
Job Types: Full-time, Contract
Pay: $55.00 - $85.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Education:
Bachelor's (Required)
Experience:
Python: 1 year (Required)
AWS: 1 year (Preferred)
Scala: 1 year (Required)
Work Location: Hybrid remote in Irvine, CA 92602

Health insurance"
45,Data Engineer I,"ConstructConnect, Inc","Cincinnati, OH 45209",N,"ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.
The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.
The Opportunity
The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects to build and train predictive analytic models, design linear regression models using data
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Experience writing complex SQL Queries, Stored Procedures and Views
Experience building and optimizing data pipelines, architectures and data sets.
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
Understanding of version control and automated CI/CD with technologies such as Gitlab/ GitHub
1 – 3 years professional experience managing Data Lakes and Data Warehouses
Bachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)"
46,Data Engineer,"TeamWorx Security, LLC","Columbia, MD 21046•Remote","$100,000 - $125,000 a year","Title: Data Engineer
Location: Remote
Position Type: Full-Time
Salary Range: $100k-$125k
U.S. Citizenship is required. Must be willing to submit for a security clearance.

The Opportunity
TeamWorx Security is looking for a highly motivated and adaptable Data Engineer with a drive to succeed in a fast-paced environment! The Data Engineer will work closely with both our technical team and our government customers to build capabilities and solutions to assess program effectiveness across a range of processes, projects, and systems. The candidate will apply a wide range of qualitative and/or quantitative methods to recommend performance improvements by producing thorough management reports and automation tools tailored to the specific needs of each program component.

What it Takes!
We are product-focused and love innovation. We look for leadership qualities in all of our teammates. You must be comfortable exploring, learning, and building cool technologies, making mistakes, and learning along the way. You should be self-driven and self-motivated with a great attitude! You should take ownership of projects and drive them to completion. We aim to challenge and grow our employees, and in return, their contributions guide our efforts to deliver maximum value to our customers. You must be comfortable working remotely, directly with our customers, and in our team environment.

What You'll Be Doing
The perfect candidate should have detailed knowledge of JavaScript, Google Apps Script, Google Workspace, data structures, analysis, and visualization. Knowledge of web-app development and programming languages such as CSS, HTML, and Python are desired. Strong attention-to-detail and the ability to produce clear and concise documentation are also essential. The Data Engineer is expected to:

Develop and maintain web applications using Google Apps Script and JavaScript
Extract and analyze data to provide unique insights and patterns specific to the necessary task
Validate the work of others by performing confirmatory data analysis
Collaborate with other team members to design, develop, and implement data-driven solutions
Create and maintain data pipelines and database architectures
Develop and implement data quality checks and processes
Apply knowledge and experience using industry-standard web development tools and programming languages
Utilize software packages such as Oracle, MySQL, Tableau, Google Sheets, and Excel at an advanced level to manipulate and display data

What We Think You'll Need
Bachelor’s in Computer Science, Data Science, Information Systems, or a related field (or equivalent experience)
Proficiency in JavaScript, Knowledge of other programming languages such as CSS, HTML, and Python is a plus
Experience with Google Apps Script, Google Workspace, and web application development
Ability to perform exploratory and confirmatory data analysis
Knowledge of descriptive analytics, trends, business intelligence, and business analytics
Strong problem-solving and analytical skills
Excellent communication and teamwork abilities
Self-motivation, adaptability, and drive to succeed in a fast-paced environment.
US citizenship required and must willing to submit for a security clearance

What’s in it for you?

No more bullet points for starters. Also, these awesome perks: Comprehensive Health, Dental, & Vision Insurance, Short-Term Disability & Life Insurance, Matching 401(k) Contributions, Paid Time Off including 11 Holidays and your Birthday, Professional Development, Fitness Membership, and Home Office allocations, Performance Bonuses & Profit Sharing, & Employee Referral Bonuses. But wait, there's more: TeamWorx Security is committed to our teammates and their professional goals. Being a small but growing company affords us the opportunity to work WITH YOU to reach your full potential.

A Little About Us and Our Team
We love to be inspired. We started TeamWorx Security to build, create, and dream about what inspires us, and because of this, our Maryland startup is quickly growing. We are a team of data scientists, software engineers, researchers, product developers, and operation-minded folks who specialize in developing applications and products that our customers love. Our mission is to reinvent the way technical and non-technical people work with technology. We believe in creating harmonious, automated solutions designed to improve our customers’ daily workflow, allowing them to act quickly and decisively about what matters most. We are looking for critical thinkers who excel at solving problems and have a passion for what they do to help us move our mission forward. This is our story; come tell us your story.

At TeamWorx Security, Inc., we hold our values as guiding principles in all that we do and encourage our community to embrace and embody them. Our core values are putting our employees first, being curious, being authentic, being scrappy, and honoring those we serve. These values drive our company culture, decision-making, and operations.

All qualified applicants will receive consideration for employment without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, or any other basis prohibited by law. TeamWorx Security will also consider the employment of qualified applicants with criminal histories, consistent with relevant laws."
47,Data Engineer,Arraya Solutions,Remote,N,"***Prospective candidates must be in Eastern or Central Time Zone and much be able to be employed as a W2 employee***
Arraya Solutions, a leading Mid-Atlantic technology consulting firm and Managed Services provider located just outside Philadelphia, is looking for a Data Engineer to join our team!
We are a culture that embraces change, values family and are actively involved with the community. Our team consists of people with positive attitudes who are interested in growing their knowledge around technology and leaders that are heavily involved in day-to-day activities.
Job Summary
The Data Engineering Team is a part of the Modeling and Data Sciences technology organization. This team supports key initiatives and improves the competitiveness and operational efficiencies in our existing and next-generation offerings by developing digital solutions and technologies in condition monitoring, reliability/risk modeling, dynamic modeling, IIoT, and new technology development & evaluation. This team works in close collaboration with the Engineering, Technology, and Operational teams to deliver competitive world-leading solutions. The position will be on the technical ladder where the incumbent will be provided with learning and advancement opportunities focusing on long-term career growth and success.
Essential Job Responsibilities:
Working with IT and business resources, support the Data Warehouse strategy and Business Intelligence initiatives:
Lead development activities to migrate the current on-premises SQL Server DW to Snowflake on AWS.
Gather requirements for Data Warehouse improvements and translate them to high level design documents including Physical Data Models. Provide technical architecture vision and recommend strategy/solutions.
Design and develop transformation processes and data structures for the Data Warehouse following best practice procedures.
Lead resolution of Data Warehouse load issues.
Support the analytical needs of the business users. Build strong relationships to help identify opportunities to enhance the analytical capabilities of the Data Warehouse.
Partnering with business stakeholders and technical report developers, establish, maintain, and promote consistent methodology for reporting and analytics deployment. Perform ad hoc data analysis to meet business unit data validation needs.
Participate in Data Quality initiatives and lead the data transformation component design to improve and maintain high quality data. Support performance tuning.
Foster teamwork through cooperative interactions with co-workers. Where needed, ensure project integration by communicating activities and status to project manager, appropriate project team members, and business users.
Job Qualifications:
BS degree, preferably in Information Technology, Management Information Systems, Computer Science or similar discipline.
Post-college training in Data Management and vendor tool use
4-6+ years of Data Warehousing/ BI experience.
This role serves as the primary technical resource for managing and moving data in and out of the Data Warehouse. The candidate must:
Be well versed in Data Warehouse design.
Have hands-on experience with ETL and ELT methodologies and tools.
Have experience with SQL Server 2016 and SSIS.
Experience with business reporting requirements analysis.
Familiarity with Data Management and Business Intelligence tools, such as Business Objects, Microsoft Power BI, QlikSense, Tableau, Looker, AWS S3 and Redshift, or Snowflake.
Demonstrated ability to clearly communicate with all levels within an organization.
Have experience leading small or offshore development teams
understanding and experience with building and deploying Business Intelligence and analytics applications.
Familiarity with Financial, Sales, Marketing, and Logistics reporting environments.
Strong sense of leadership, strong analytical skills, excellent communication Have expert skills and good technical abilities.
Preferred candidates will have experience with:
SSAS.
Analytics design and development is preferred.
Hands on development experience with .Net and/or Java is a plus.
Comfortable working in a fast-paced team. Capable of grasping new concepts quickly and acting with a sense of urgency.
Preferred candidate may have certifications like:
CBIP (Certified Business Intelligence Professional)
CDMP (Certified Data Management Professional)
Vendor-specific certifications
Job Types: Full-time, Contract
Schedule:
8 hour shift
Application Question(s):
What city/State are you currently located in?
Are you able to work as a W2 employee?
Work Location: Remote"
48,Data Pipeline Engineer,Fuge Technologies Inc,"700 Gw Pkwy, McLean, VA 22101",$65 - $70 an hour,"Location: McLean, VA (3 days onsite -Tuesday, Wednesday, and Thursday)
no relocation candidates
Duration: 6 months contract, the opportunity for extension
Interview information:
Rounds: 2 Rounds ***2nd round will be In-Person and On-site***
Duration: 1st round - 45 Minutes, 2nd round - 30 Minutes
Additional Notes: 1st Round: MS Teams, Video Mandatory; 2nd Round: In-Person
Slots: 1st round: 4/17 @ 11:00am & 3:00pm; 4/18 @ 11:00am & 3:00pm
Must Haves: Not looking for an administrator here! Must have hands-on experience with the UNIX system, experience on the user side, and coding with Shell and Autosys. Expert-level SQL is required. Preference for experience with Snowflake. Must have hands-on experience with Hadoop.
Top skills required: Unix shell scripting, AutoSys, Hadoop, SQL, GIT, Jira, Bitbucket, and Jenkins
Control M: nice to have
Responsibilities include:
Evaluate and establish Operations Industrialization processes and work across various internal and external business customers and areas to help enhance technologies, operational and process efficiencies, and to improve the overall process. This includes participation in large-scale projects.
Interact with business teams and other stakeholders to understand and recommend operational processes and solutions to meet diverse, complex business requirements.
Manage GIT repositories, create baselines, and help in deployments.
Contribute and participate in the automation of the operational process and code reviews.
Develop and leverage SQL for validating data as well as SQL statements.
Run adhoc jobs in production as and when required to fix issues related to processes, data, etc.
Work in a small group and will need to support various ad hoc and on-going requests. This includes production operations support, development of procedures, etc.
Support end-user operational loads, and data issues and resolve them in a timely manner.
Qualifications:
Bachelor’s degree in Computer Science/Information Technology or related field and/or equivalent experience.
Preferred Skills:
5+ years of experience with the UNIX operating system mainly with LINUX (Mandatory).
3+ years of Shell scripting experience (Mandatory).
3+ years of Autosys experience (Mandatory).
3+ years of current and hands-on experience with SQL (Mandatory).
2+ years of experience with Hive queries in Big Data Platform - Hadoop (Mandatory).
Executing and monitoring jobs in a UNIX environment (Mandatory).
2+ years of experience with Source Code Control systems, preferably GIT, Bitbucket, etc. (Mandatory).
2+ years of AWS experience (optional).
2+ years of experience with Snowflake (preferred).
Running SQL queries from the command line.
Must be comfortable with analyzing large datasets (preferred).
Excellent oral and written communication skills.
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Mclean, VA 22101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location"
49,Associate Data Engineer - Remote,UnitedHealth Group,"Minnetonka, MN 55345•Remote","$56,300 - $110,400 a year","At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us and start doing your life's best work.(sm)
You’ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.
Primary Responsibilities:
Develop ETL workflows on cloud technologies like Azure and GCP
Design, develop, implement, and run data solutions that improve data efficiency, reliability, and quality, and are performant by design
Layer in instrumentation in the development process so that data pipelines can be monitored. Measurements are used to detect internal problems before they result into user visible outages or data quality issues
Embrace continuous learning of engineering practices to ensure industry best practices and technology adoption, including DevOps, Cloud and Agile thinking
Maintain high quality documentation of data definitions, transformations, and processes to ensure data governance and security
Identifies solutions to non-standard requests and problems
Solves moderately complex problems and/or conducts moderately complex analyses
You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
Required Qualifications:
Undergraduate degree or equivalent experience
4+ years of hands-on experience writing code or developing ETL pipelines with industry standard ETL tools
4+ years of experience in SQL , solid SQL knowledge
2+ years of experience in Data Engineering and coding on technologies like Java, Python or Scala
1+ years of experience with CICD tools such as Jenkins, GitHub, Maven etc.
Preferred Qualifications:
Cloud experience (Azure/AWS/GCP)
Proficient in building relationship with stakeholder and maintaining it during the course of the project/program
Proficient in working with cross functional teams
Careers with UnitedHealthcare. Work with a Fortune 5 organization that’s serving millions of people as we transform health care with bold ideas. Bring your energy for driving change for the better. Help us improve health access and outcomes for everyone, as we work to advance health equity, connecting people with the care they need to feel their best. As an industry leader, our commitment to improving lives is second to none.
*All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy
California, Colorado, Connecticut, Nevada, New York, Rhode Island, or Washington Residents Only: The salary range for California, Colorado, Connecticut, Nevada, New York, Rhode Island or Washington residents is $56,300 to $110,400. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.
At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.
Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.
UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment."
50,Data Engineer,"Second Wave Delivery Systems, LLC",Remote,N,"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build and maintain scalable, high-performance data processing systems (batch and/or streaming)
Lead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable
Design our data models for optimal storage and retrieval and to meet critical product and business requirements
Contribute to tooling & standards to improve the productivity and quality of output across the company
Work and consult with various non-technical stakeholders
Participate in interviewing and onboarding of new team members

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
5+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks
Expert knowledge of relational databases and query authoring (SQL).
Strong experience developing in one of the following - Python, Java, Scala
Experience with building and managing data pipelines
Experience with big data technology, e.g: Airflow, Spark, Google Cloud Composer

Preferred Qualifications
Demonstrated experience developing machine learning algorithms in the healthcare field
Experience extracting and correlating medical findings from unstructured data such as PDF charts
Experience with big data analysis tools such as BigQuery and Looker"
51,TS/SCI Data Engineer,Gridiron IT,"Washington, DC",$65 - $73 an hour,"Seeking a Data Engineer local to Washington, DC.
Active Top Secret/SCI Clearance Required
Duties
Develop and design data pipelines to support an end-to-end solution.
Develop and maintain artifacts i.e., schemas, data dictionaries, and transforms related to ETL processes.
Integrate data pipelines with AWS cloud services to extract meaningful insights.
Manage production data within multiple datasets ensuring fault tolerance and redundancy.
Design and develop robust and functional dataflows to support raw data and expected data.
Provide Tier 3 technical support for deployed applications and dataflows.
Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc.
Skills
Amazon Web Services (AWS)
Database Administration
Data Engineering
ETL Architecture and Development
End-to-End Processes
API Development
Extract, Transform, and Load (ETL)
Data Pipeline
Tier 3 Technical Support
Preferred
Database administration and development experience will be a plus for consideration.
Experience with cloud message APIs and usage of push notifications.
Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security.
Job Type: Full-time
Pay: $65.00 - $73.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Experience level:
5 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 4 years (Preferred)
AWS: 2 years (Preferred)
Data Engineering: 4 years (Preferred)
Security clearance:
Top Secret (Required)
Work Location: In person

Health insurance"
52,"Data Engineer, Election Insights (all-levels)",The Washington Post,"Washington, DC",N,"Job Description
The Washington Post is hiring a data engineer for our Election Insights team, to work on
The Post’s live election model
ahead of the 2024 presidential election.
The Election Insights team focuses on leveraging political data,
partnering with the newsroom
, engineering, and product teams to help The Post tell stories. This role is part of Elections Engineering, a cross-functional group of full-stack engineers and data scientists, with varied technical and academic backgrounds, responsible for delivering live election results across The Post’s platforms.
The role will be focused on The Post’s live election model in preparation for the election in 2024. In particular, we’re looking for candidates who would be comfortable working on deploying our model in a production environment and have an interest in data science. We also work on other data engineering projects related to elections, such as voter registration data and campaign finance. Most of our work is in Python, though we have some analyses running in R.
We typically work at a higher level of abstraction than a single story, and aim to apply big data and data science tools so they are reusable and easy to deploy. Our ideal candidates are creative problem-solvers, who are comfortable writing production level code to solve real-world newsroom problems. This position is remote-friendly for work in the US.
Motivation
You have a desire to contribute to The Post’s journalism through technology.
You are passionate about refining and improving team process, culture, documentation and communication while tackling complex data science and engineering challenges.
You want to work at a deeper level of abstraction than a single story or beat.
You are excited about learning new technologies and applying them to the practice of journalism.
Responsibilities
Work on the software components of our live election model including deployment, observability, participating in code reviews, and more. You will also contribute to some of the underlying data science work for the model.
Work closely with our data scientists and our back-end engineers to develop and maintain pipelines for election data.
Help prepare data for our live election model, as well as work on our tooling to make this data preparation easier and more efficient.
Improve The Post’s ability to report on elections by deriving insights from our machine learning algorithms and big data tools, both for internal stakeholders and for Post readers. This may include occasional work on deadline or in a breaking news situation.
Assist in attracting and hiring team members who want to work at the intersection of journalism and technology by actively participating in recruiting – and contributing feedback on our overall hiring process.
Qualifications
A minimum of one year of experience. Experience should be in one or both: building and maintaining backend/data engineering applications, or with developing and deploying data science tools and applications.
Proficiency across some of the languages and frameworks used in our data science stack such as Python, AWS services, and R.
A desire to work on cutting edge big data and machine learning tools in a team setting.
Experience with election data is not necessary, but is a plus.
Levels
If you have less than three years, but at least one year, we’d love to evaluate you for a data engineer role.
If you have more than three years but less than about seven years, we’d love to evaluate you for a senior data engineer role.
If you have more than about seven years, we’d love to talk to you about a principal data engineer role.
Wherever you are in your life or career, The Washington Post offers comprehensive and inclusive benefits for every step of your journey:
Competitive medical, dental and vision coverage
Company-paid pension and 401(k) match
Three weeks of vacation and up to three weeks of paid sick leave
Nine paid holidays and two personal days
20 weeks paid parental leave for any new parent
Robust mental health resources
Backup care and caregiver concierge services
Gender affirming services
Pet insurance
Free Post digital subscription
Leadership and career development programs
Benefits may vary based on the job, full-time or part-time schedule, location, and collectively bargained status.
The Post strives to provide its readers with high-quality, trustworthy news and information while constantly innovating. That mission is best served by a diverse, multi-generational workforce with varied life experiences and perspectives. All cultures and backgrounds are welcomed.
The innovation doesn’t end in the Newsroom – dozens of teams power The Washington Post. We are now hiring the next innovator – how will you Impact Tomorrow?
#washpostlife"
53,Data Engineer,Starschema,"Arlington, VA•Remote",N,"Company Description

About Starschema

At Starschema we believe that data has the power to change the world and data-driven organizations are leading the way. We help organizations use data to make better business decisions, build smarter products, and deliver more value for their customers, employees and investors. We dig into our customers’ toughest business problems, design solutions and build the technology needed to address today’s unique challenges.

What you can expect as a Starschema team member
As a member of the Starschema team, you will be on the front lines of digital transformation, working with some of the most innovative Fortune 500 companies to drive innovation and realize the promise of data-driven cultures. You will learn and use the latest data-centric technologies along with the core industry technologies.
Our team is inclusive and fun. While we take our work seriously, we know how to have a good time while doing so. We encourage everyone to share their opinions and ideas, and our leadership wants to hear everyone’s input no matter what role they play in the organization.

Job Description

As a Data Engineer at Starschema, you will bring business value for our clients through end to end development, optimization and operation of automated reporting, data lakes and related software platforms. You will use the latest technologies like Apache Airflow, Apache Kafka, Apache Spark and AWS etc. We are seeking for experienced medior and senior professionals for our open position.
What will You do:
Build and maintain database/bigdata clusters;
Build dashboards for infrastructure management and reporting;
Design and deploy infrastructure management strategies to meet up time and monitoring SLA’s;
Deploy code release in QA and PROD;
Participate in building unit/performance/integration tests working with database developers;
Participate in database SQL optimization plan;
Deploy configuration and automation tools to remove manual steps in deploying, upgrading, and scaling systems and software across all environment.

Qualifications

We want to hear from you if You have:
At least 3 years of experience in data engineering field;
Solid background in Python and SQL;
Experience building data solutions using big data tools: Airflow, Spark, Kafka, AWS;
Experience with data pipeline and workflow management tools
Hands-on experience with requirements analysis, design, coding and testing patterns;
Has experience in engineering (commercial and open source) software platforms and large-scale data infrastructures;
Experience working with cloud computing environments;
Excellent communications skills in English (both written and oral);
Intelligent, communicative team-player personality, interested in and willing to learn new skills and technologies.

Additional Information

What's In It For You:
Remote work: You can work remotely from anywhere within the USA. Plus if you are based in Washington D.C area.
Eligibility: We are unable to support work visa for this specific position so we are open to receive application from candidates who are eligible to work in the USA.
Benefits & Community: A healthy lifestyle and the feeling of belonging are important to us, for both body and mind. We provide:
401K Insurance with matching
Employee Assistance Program (EAP)
Technical/Professional trainings
Remote work / Home Office opportunity
Start date: The sooner the better, but if you currently work somewhere and have a notice period, it is still fine, we will wait for the right person!"
54,Data Engineer,Nike,"200 Newbury St., Boston, MA 02116",N,"Become part of the Converse Team

Converse is a place to explore potential, break barriers and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At Converse, it’s about each person bringing skills and passion to a challenging and constantly evolving world to make things better as a team.
Converse, Inc. Boston, MA. Work closely with Project Management and Business teams to completely define specifications to ensure the project acceptance. Involved in preparation of functional and technical specifications with different cross teams. Lead team, defining solution options, providing estimates on effort and risk, and evaluating technical feasibility in Agile development process, including Scrum and Kanban. Work on troubleshooting data and analytics issues and perform root cause analysis to proactively resolve issues. Develop data extracts and feeds from the full spectrum of systems in the Converse ecosystem, including transactional ERP systems, POS data, product and merchandising systems. Engineer data products for a variety of Operations analytics use cases, ranging from reporting and data visualization to advanced analytics/machine learning use cases. Support designing technical specifications and data transformation models for junior developers. Ensure development is on track and meets specifications as defined by product management and the business. Responsible for data integrity of current platform and QA of new releases. Support the development and maintenance of backlog items and solution feature. Participate in sprint planning activities from a development perspective. Responsible for designing cloud-based data architecture using AWS stacks. Design and develop Python data science and data engineering libraries dealing with structured and unstructured data. Work with a variety of database types (SQL/NoSQL, columnar, object-oriented) and diverse data formats. Responsible for ETL with Spark and building data pipelines/orchestrations in Airflow and working on ETL tools like Matillion. Responsible for DevOps toolchain and Continuous Development, Continuous Integration and Automated Testing using Jenkins. Ensure and use data engineering for advanced analytics/data science and Software development skills.
Applicant must have a Bachelor’s degree in Computer Science, Information Systems, or Information Technology and 5 years of progressive post-baccalaureate experience in the job offered or a related occupation. Experience must include:
Data warehousing;
ETL or ELT;
Amazon Web Service (AWS) Cloud Services, including AWS S3, AWS Lambda, AWS EC2, AWS EMR or AWS DynamoDB;
Relational Database Management Systems (RDBMS), such as Oracle, Teradata, SQL Server or Snowflake;
Database Development with writing stored procedures, functions, triggers, cursors or SQL queries;
Hadoop, HDFS, Hive or Spark;
Programming languages, including Java or Python;
Business Intelligence Tools, such as Tableau;
Unix Shell scripting; and
Version control systems, such as Git, Bitbucket or Github
#LI-DNP
Converse is more than a company; it’s a worldwide advocate for self-expression. This belief motivates our employees, permeates our working environment and inspires our products. No two of us look or think exactly alike. We are each one-of-a-kind. Individually and as a culture, we have the freedom to create and grow professionally. Generous benefits packages only sweeten the experience. From Boston to Shanghai, from Brand Design to Finance, Converse is a brand that celebrates the unique and creative people of the world. Together, we’re different."
55,Data Engineer,Gateway Professional Network,Remote,"$95,000 - $145,000 a year","Apply Directly: https://gatewaypn.applicantstack.com/x/detail/a2bwypr26k74
Data Engineer
Job Description for Full-Time Position
Location: US Remote (must reside in US and we are unable to sponsor Visas at this time)
Pay Scale: $95,000 – $145,000 per Annum (based on experience)
Reports to: Ramon Navarro
Job Description Summary:
Individual to contribute to GPN Tech transformation assuming accountability for data engineering lifecycle including research, proof of concepts, design, development, test, deployment, and maintenance of enterprise-scale data integration solutions leveraging Microsoft Azure PaaS offerings. Extensive experience working on Agile Scrum/DevOps teams employing the latest CI/CD cloud-first best practices. Potential to become an SME on Platform capabilities (ingestion, storage, processing, and presentation patterns) and extend future-state strategic roadmap features.
What You’ll Do: Responsibilities
Provide technical direction to delivery scrum teams, extending patterns and establishing new ones as required
Create and review technical designs of data integrations across various systems within the enterprise and with external business partners over multiple transmission channels, data formats, and processing patterns
Extend/implement CICD pipelines and containerization strategy
Responsible for ETL and SQL development using Domain Architecture and Interface based programming
Build processes and diagnostics tools to troubleshoot, maintain and optimize solutions and respond to customer and production issues
Experience You’ll Need: Qualifications
Data Engineer with in-depth knowledge of developing data-driven solutions using Azure App Services, Function Apps, API Service Management, Active Directory, Azure Storage, Data Factory, Databricks, RedisCache, SQL DB, Key Vault, Service Bus, Event Hub and Application Insights
Bachelor’s degree in computer science or related discipline; r Equivalent Work Experience
5+ years of experience leveraging cloud data storage technologies specifically Microsoft Azure offerings
5+ years of experience developing on modern data ingestion tools/platforms/protocols (Kafka, APIs, EventHubs, ServiceBus, etc.)
5+ Experience with data integration best practices (ETL/ELT patterns, Data Factory, Streams Analytics, etc.)
5+ years of experience in ETL and SQL Programming
2+ years of experience in C# and T-SQL
3+ years of experience working with Microsoft Azure DevOps and CI/CD best practices
3+ years of experience working in an Agile Scrum environment
Experience with various data structure optimization techniques on a Data Streaming Platform
Experience with Power BI or other similar data visualization tools
Bonus Points: Additional qualifications if you have them!
Experience with Machine Learning and Artificial Intelligence based solutions
Knowledge of data regulations and compliance policies (e.g., HIPAA)
Data Visualization experience, or strong affinity for leveraging data to help transform/optimize business functions
Big Data experience
IT healthcare experience
IoT experience
Who We Are:
GPN Technologieswas founded in 2007 with the mission of providing “big business” infrastructure to independent practitioners in the ophthalmic industry. The company’s driving initiative has been empowering independents to be profitable and competitive in today’s market by making high-tech, business-critical tools accessible and meaningful in small business settings.
Our analytics platforms have revolutionized the way practitioners view, understand, and act on their business data. We have continued to expand and hone those platforms to help those practitioners succeed and thrive in an increasingly competitive, fast-paced marketplace.
Thousands of practitioners across the country are serving their patients, their team members, and their bottom lines more efficiently with EDGEPro.
Job Type: Full-time
Pay: $95,000.00 - $145,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Paid time off
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
Do you require Visa sponsorship?
Experience:
Relational databases: 2 years (Preferred)
Azure: 2 years (Preferred)
Big data: 2 years (Preferred)
SQL: 2 years (Required)
Microsoft SQL Server: 2 years (Required)
C#: 2 years (Required)
.NET: 3 years (Required)
Work Location: Remote"
56,Software Engineer- Data Analysis,Humana,"Louisville, KY 40202•Remote",N,"The Software Engineer 2 codes software applications based on business requirements. Work assignments are varied and frequently require interpretation and independent determination of the appropriate courses of action.
Responsibilities
Humana is seeking to hire multiple Software Engineers focused on data analysis and database management systems. Software Engineers will have following responsibilities:
Standardize the quality assurance procedure for software.
Oversee testing and debugging and develops fixes.
Research complaints and makes necessary adjustments and/or recommendations to resolve complex software related issues.
Understand department, segment, and organizational strategy and operating objectives, including their linkages to related areas.
Make decisions regarding own work methods, occasionally in ambiguous situations, and requires minimal direction and receives guidance where needed.
Follow established guidelines/procedures.
Required Qualifications
Bachelor's Degree in Computer Science or related field or equivalent experience
0-3 years of experience in systems analysis or application programming development
Experience / knowledge of SQL RDMS and/or non-SQL DMS.
Strong communication skills
Must be passionate about contributing to an organization focused on continuously improving consumer experiences.
Preferred Qualifications
Master's Degree
Experience with DevOps and Agile methodology
Experience with Cloud services for Azure and/or GCP
Scheduled Weekly Hours
40

Not Specified
0"
57,Data Engineer,Falling Colors,New Mexico•Remote,"$85,000 - $95,000 a year","Falling Colors offers solutions for administering public, private, and charitable funds to organizations that share a mission of promoting the Social Determinants of Health. We track allocated spending and measure outcomes through a configurable software platform and supporting services, allowing customers to glean insight into the effectiveness of funded programs via reporting and analytics dashboards, driving outcome-oriented decisions to maximize the impact of future investments.
We are seeking a Data Engineer to join our data team supporting our data environment focusing on ETL and data warehouse development and design. Applications will be accepted from individuals with at least 4 years of experience that reside in New Mexico. Periodic travel to headquarters will be required, but this position is otherwise remote.
Falling Colors
Falling Colors is a small (~30 person) B-certified company that is women and LGBTQ+ founded, owned, and operated. We are a diverse and open-minded team, with a mix of genders, ethnicities, and interests. We have parents and non-parents, self-taught and formally educated folk, introverts and extroverts, semi-professional ballroom dancers, golfers, gardeners, rock climbers, yogis, and VR hobbyists. As a company, we value using our skills and time to improve the world around us — from behavioral health care to local educational opportunities. As a team, we strive to be kind, support each other, and work together, stepping forward to handle all the exciting things that get thrown our way.
At Falling Colors, we don’t just build technology: we build impact. We take great pride in what we do for our communities with our products, and further seek to improve the world in everything we do. Our passion project is The White Building, a space rich with history. You can read more about this project at thewhitebuilding (dot) com on Instagram @thewhitebuilding. We use a portion of our profits to fund our Falling Colors Foundation, which paid out over $200,000 in small business grants to help our local businesses make it through uncertain times during the pandemic. We pay our employees to volunteer and do our best to protect them from burnout with a generous PTO policy, flex time, and paid holidays. We protect our employees’ futures by partnering with a B-certified financial group that manages our 401k accounts (and we contribute—not match—a percentage to those accounts). We understand the critical role we play as a business in the six determinants of social health, and we want to be the company that sets the bar and inspires others to be better.
We are in Santa Fe, New Mexico, with satellite offices across the United States. This is a hybrid position for a candidate residing in New Mexico or Colorado. The candidate will therefore need to have a functional at-home workspace with reliable internet service and the ability to come work in the office.
Data Engineer Qualifications & Responsibilities
Responsibilities
Be an individual contributor who is excited to build new ETL pipelines, maintain and enhance existing pipelines, and become an expert on our data and infrastructure.
Collaborate on an interdisciplinary team to design and develop business intelligence solutions for behavioral health providers and administrators.
Work with data analysts and product support to translate business needs into specifications for data warehouse and pipeline design.
Design, implement, and maintain ETL and data cube solutions with an eye towards performance, security, and quality.
Preprocess and integrate heterogenous data sources.
Evaluate ETL performance and explore opportunities to automate processes, reduce bottlenecks, enhance quality, and improve overall usability.
Support the design, development, and implementation of data warehousing, reporting and analytics solutions that deliver information efficiently, allowing our clients to make informed, data-driven business decisions.
Produce ad-hoc reports and analyses when needed.
Have or develop excellent understanding of privacy rules related to sensitive health data our systems process to ensure appropriate handling of such sensitive data.
Create and maintain documentation in support of the team.
Qualifications
The Must Haves:
Bachelor’s degree in computer science or equivalent, or demonstrated experience and competency equivalent
At least 3 years of database development experience with a demonstrated proficiency in writing advanced tSQL to extract data from complex data structures.
At least 3 years’ experience designing and developing data warehouses with a demonstrated proficiency in dimensional modeling.
Designing and implementing ETL solutions using Azure Data Factory (ADF) and/or Integration Services (SSIS).
Experience with database design for transactional systems and data warehouses.
Experience working on an interdisciplinary team of engineers, analysts, data quality professionals, and subject matter experts.
A friendly team player attitude, comfortable wearing different hats on different projects and willing to take leadership or support roles as needed.
Comfort working in existing structure and process, with a focus on technical implementation and data.
A passion for working in service of a meaningful mission to improve the lives of vulnerable populations through technology and data
The Good to Haves:
Experience analyzing and optimizing slow running queries.
Experience writing DAX statements.
Experience with Streaming Data Pipelines.
Experience analyzing and optimizing slow running queries.
The Bonus Stuff:
Demonstrated knowledge of the unique security and analytical considerations in working with protected healthcare data and information systems.
Demonstrated experience designing and implementing disaster recovery solutions to hit RPO and RTO
Working knowledge of PowerShell and specifically dbatools.io
Experience with Azure cloud infrastructure
Experience administering SQL Server in a cloud environment
Python programming experience
Ability to debug and understand C# code
Familiarity with State of New Mexico public and behavioral health agencies
Diversity Statement
Falling Colors is committed to recruiting and retaining a diverse workforce. We recognize that the tech industry overall has failed to develop a workforce reflective of our population as a whole, and pledge to do our part to do better. We encourage people of color, differently abled people, veterans, women, and LGBTQ folks to apply for open positions. We believe that a diverse workforce made up of people with different backgrounds and life experiences makes us stronger and better able to serve the communities where we work to make a difference. We will consider all applicants without regard to race, color, national origin, religion, sexual or gender orientation or expression, marital or parental status, disability, age, or other protected basis.
Benefits
We offer:
A month of paid time off plus a month of holidays
Paid parental leave
Paid volunteering time
Generous 401k contribution
Generous health, vision, and dental plans (for employee and family, at no cost to our employees)
Employer-paid short- and long-term disability plans
Employer-paid life insurance
Professional growth and continuing education opportunities
Estate planning stipend
LifeLock subscription
Falling Colors is an Equal Opportunity Employer
What Next?
If you think this sounds like it could be you, we encourage you to apply. We do not expect anyone to have all the things we are looking for, and we are always happy to have a conversation.
To apply, please send your resume and a cover letter to apply @fallingcolors (dot) com. We look forward to getting your application!
Job Type: Full-time
Pay: $85,000.00 - $95,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
Are you a resident of New Mexico, or are you willing to relocate?
Experience:
data warehouse design and development: 3 years (Required)
database development: 3 years (Required)
Work Location: Remote

Health insurance"
58,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
59,Data Engineer (P),DCI Solutions,"Washington, DC","$140,000 - $180,000 a year","DCI Job Requirement for: Data Engineer
Location: Georgetown, Washington D.C.
Job Description:
Serve as a Data Engineer
Deploy groundbreaking technical solutions to solve the government’s hardest problems
Use advanced skillsets to: Build Data Structures, Maximize Storage Systems, & Develop frontend frameworks
Write code to build and maintain enterprise data integration pipelines
Partner with Production Development teams to roll out new features and products
Implement production-level solutions to advance Government missions, including: Configuration, Application development, & Data integration
Work effectively with a team of technical and non-technical individuals to achieve national security objectives
Qualifications:
BS in Computer Science / Engineering / Physics / Mathematics(or related career field)
5+ years of Data/Software Engineering experience
Analytical mindset and eagerness to solve technical problems with: Data structures, AI/ML, Storage systems, Front-end frameworks, & other technical tools
Proficiency with one or more programming languages, such as: Python, Typescript/JavaScript, & Similar languages
Ability to continuously learn
Comfortable making decision with minimal supervisory input
Must be able to participate in on-call rotations (business hours only)
Willingness to travel 10% of time (mainly in greater DC area) *Note*: This position is 4 Days a week on-site in Georgetown, Washington, D.C. 1 out of every 3 weeks will be at Ft. McNair, Washington D.C.
Active DoD Top-Secret Clearance
Job Type: Full-time
Job Type: Full-time
Pay: $140,000.00 - $180,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Signing bonus
Schedule:
Monday to Friday
Experience:
Python: 3 years (Preferred)
AI/ML: 2 years (Preferred)
Software Engineering: 5 years (Preferred)
Typescript or JavaScript: 5 years (Preferred)
Data Engineer: 5 years (Preferred)
Security clearance:
Top Secret (Required)
Work Location: One location

Health insurance"
60,Data Engineer,Rocket Mortgage,"Detroit, MI 48226",N,"As a Technology team member, you’re empowered to make an impact, employ your entrepreneurial spirit and build a career customized by you because at Rocket, you can. We are creating digital products that solve life’s most complex moments. You’ll get the chance to shape the future of tech, have your voice heard, get ahead in your career and develop your skills. With a tech career here, there's no limit to what you can achieve.
Minimum Qualifications
Bachelor's degree in computer science, information technology, or a related field or equivalent experience
Preferred Qualifications
3 years of experience working with database tools
3 years of programming experience using Python and C#
3 years of experience working with SQL server integration services or ETL tools
3 years of experience working with data integration tools
Proficiency in the Microsoft Office suite
Experience working with ETL tools
Knowledge of data integration tools
Software programming languages, such as Python and C#
Job Summary
As a Data Engineer, you'll work with database engineers to design, develop and maintain the infrastructure of data within the data warehouse, including setting the ETL (extract, transform, load) processes, bringing in new data sources, modifying existing data sources, making sure data is clean, complete and consumable, as well as designing data models within the data warehouse.
In this role, you'll work as part of one or more project teams and be responsible for designing and building mechanisms to move, integrate, cleanse and publish large volume datasets. This is a developer role with a specialty in data and requires deep knowledge of a variety of programming languages and design patterns.
Responsibilities
Design and support the new and evolving sources of data being brought into the data warehouse
Work closely with data architects and follow best practices for data management consumption
Work closely with business analysts to work through business requirements and develop processes to provide the needed data visibility via the data warehouse and reporting platform
Model application layer and metadata design
Design and create automated applications and reporting solutions
Work closely with front-end developers to ensure data is being brought in and data integrity is being maintained
Monitor and troubleshoot performance issues on the data warehouse servers
Benefits and Perks
Our team members fuel our strategy, innovation and growth, so we ensure the health and well-being of not just you, but your family, too! We go above and beyond to give you the support you need on an individual level and offer all sorts of ways to help you live your best life. We are proud to offer eligible team members perks and health benefits that will help you have peace of mind. Simply put: We’ve got your back. Check out our full list of Benefits and Perks.
Who We Are
Rocket Companies® is a Detroit-based company made up of businesses that provide simple, fast and trusted digital solutions for complex transactions. The name comes from our flagship business, now known as Rocket Mortgage®, which was founded in 1985. Today, we’re a publicly traded company involved in many different industries, including mortgages, fintech, real estate, automotive and more. We’re insistently different in how we look at the world and are committed to an inclusive workplace where every voice is heard. We’re passionate about the work we do, and it shows. We’ve been ranked #1 for Fortune’s Best Large Workplaces in Financial Services and Insurance List in 2022, named #5 on People Magazine’s Companies That Care List in 2022 and recognized as #7 on Fortune’s list of the 100 Best Companies to Work For in 2022.
Disclaimer
This is an outline of the primary responsibilities of this position. As with everything in life, things change. The tasks and responsibilities can be changed, added to, removed, amended, deleted and modified at any time by the leadership group.
We are proud equal opportunity employers and committed to providing an inclusive environment based on mutual respect for all candidates and team members. Employment decisions, including hiring decisions, are not based on race, color, religion, national origin, sex, physical or mental disability, sexual orientation, gender identity or expression, age, military or veteran status or any other characteristic protected by state or federal law. We also provide reasonable accommodations to qualified individuals with disabilities in accordance with state and federal law.
Colorado, New York City, California, and Washington Candidates Only. The salary range for this position is seventy thousand dollars to one hundred forty-seven thousand five hundred dollars. The position may also be eligible for an annual bonus and other employment-related benefits including, but not limited to, medical, dental, and vision benefits, 401K retirement plan, and paid-time off. More information regarding these benefits and others can be found here. The information regarding compensation and other benefits included in this paragraph is only an estimate and is subject to revision from time to time as the Company, in its sole and exclusive discretion, deems appropriate. The Company may determine during its review of the proposed compensation and benefits provided for this position, that the compensation and benefits for such position should be reduced. In no event will the Company reduce the compensation for the position to a level below the applicable jurisdictional minimum wage rate for the position."
61,"Data Engineer, Analytics - Generative AI",Meta,Remote,"$165,000 - $232,000 a year","Generative AI represents a huge opportunity to unleash the creativity of the billions of people globally who use our technologies. For years, teams at Meta have been leading the industry in AI research. We are uniquely positioned to adopt an end-to-end approach to generative AI that few organizations can offer, through building breakthrough product experiences using generative AI, generative AI research across all modalities, world-class applied AI/ML product engineering, scalable, high-performance AI infrastructure and deep experience building tools that enable social connection and expression.As a Data Engineer on the generative AI team at Meta, you can help bring the transformative potential of generative AI to people and businesses around the world as we build a new class of generative AI experiences. As we continue to expand and create, we have a lot of exciting work ahead of us!


Data Engineer, Analytics - Generative AI Responsibilities:
Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems.
Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve.
Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights visually in a meaningful way.
Define and manage SLA for all data sets in allocated areas of ownership.
Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership.
Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains.
Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources.
Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts.
Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts.
Influence product and cross-functional teams to identify data opportunities to drive impact.
Mentor team members by giving/receiving actionable feedback.



Minimum Qualifications:
5+ years experience in the data warehouse space.
5+ years experience in custom ETL design, implementation and maintenance.
5+ years experience with object-oriented programming languages.
7+ years experience with schema design and dimensional data modeling.
7+ years experience in writing SQL statements.
Experience analyzing data to identify gaps and inconsistencies.
Experience managing and communicating data warehouse plans to internal clients.



Preferred Qualifications:
Experience working with either a MapReduce or an MPP system.
Knowledge and practical application of Python.
Experience working autonomously in global teams.
Experience influencing product decisions with data.





Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics. You may view our Equal Employment Opportunity notice here. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. We may use your information to maintain the safety and security of Meta, its employees, and others as required or permitted by law. You may view Meta's Pay Transparency Policy, Equal Employment Opportunity is the Law notice, and Notice to Applicants for Employment and Employees by clicking on their corresponding links. Additionally, Meta participates in the E-Verify program in certain locations, as required by law"
62,Data Engineer (US Remote Eligible),General Mills,"Minneapolis, MN 55426•Remote",N,"Job Description:
Associate Data Engineer

Overview:

Technology at General Mills accelerates process transformation and business growth around the globe. To achieve business success, the Digital & Technology team uses leading edge technology, innovative thinking and agile processes. One of General Mills’ key technology priorities is driving business action through connected data.

The General Mills Digital & Technology organization is currently in the process of building a cloud data platform to advance the data-driven decision making capabilities of our enterprise. If you are an agile learner, have strong problem solving skills and are able to function as part of a highly technical, cross functional team, we would like to hear from you.

General Mills is seeking a Data Engineer to act as a key technical leader in our organization.

Key Responsibilities:
Act as a key technical leader within General Mills
Design, create, code, and support a variety of ETL solutions (potentially including but not limited to: Talend Studio, SQL, Python, Scala, Kafka, Google Big Query, or others)
Generate and implement your own ideas on how to improve the operational and strategic health of big data ecosystem
Participate in the evaluation, implementation and deployment of emerging tools & process in the big data space.
Partner with business analysts and solutions architects to deliver business initiatives.
Collaboratively troubleshoot technical and performance issues in the big data ecosystem

Minimum Qualifications:
Bachelor’s Degree; Computer Science, MIS, or Engineering preferred
2+ years of related technical experience, preferred Cloud data experience
Database development experience
Job Scheduling experience
Process mindset with experience creating, documenting and implementing standard processes
Development experience using Python and/or Spark
Effective verbal and written communication and influencing skills
Effective analytical and technical skills
Ability to work in a team environment
Ability to research, plan, organize, lead, and implement new processes or technology

Preferred Qualifications:
Scala or Java development experience
Familiarity with Kafka
Familiarity with the Linux operating system
Experience with agile techniques or methods
Additional Considerations:
Applicants must be currently authorized to work in the U.S. General Mills will not sponsor applicants for a U.S. work visa.
Relocation support will not be offered to applicants within the United States.
International relocation or international remote working arrangements (outside of the US) will not be considered.
The location for this position is Minneapolis, MN - remote work applications will be considered."
63,Data Engineer,Northwestern Mutual,"Milwaukee, WI","From $73,570 a year","AT NORTHWESTERN MUTUAL, WE ARE STRONG, INNOVATIVE AND GROWING. WE INVEST IN OUR PEOPLE. WE CARE AND MAKE A POSITIVE DIFFERENCE.
Primary Duties & Responsibilities:
Apply engineering best practices in order to analyze, design, develop, deploy and support software solutions.
Develop software using continuous Deployment and integration practices.
Participate in an Agile implementation and maintenance of source control and release procedures.
Participate in Code Reviews and feedback to the team
Explain technical solutions to technical teams.
Contribute to a collaborative work environment in which all team members are respected regardless of their individual differences and are motivated to improve both their individual and team contributions.
Identify data quality issues and their root causes. Propose fixes and design data audits.
Qualifications:
Bachelor's Degree
1-3 years of professional experience.
At least 1 year of professional software engineering, debugging and software documentation experience.
Code Knowledge: Python, JVM (Java, Scala),Apache Spark, SQL.
Experience with Agile methodologies/DevOps environment.
Awareness of database structures, theories, principles, and practices.
Awareness of Data Integration Patterns and Tooling including ELT/ETL, EII, Replication, Event Streaming, Virtualization to support batch and real-time data needs.
Has or develops understanding of 1-3 subject areas/domains of data.
Self-motivated and able to work with minimal direction
Proficient programming skills.
Skills-Proficiency Level:

Agile Methodologies - Basic
Application Platforms - Intermediate
Data Auditing - Basic
Data Integrity - Intermediate
Data Privacy - Basic
Data Quality - Basic
DevOps - Basic
Domain Expertise - Basic
Engineering Practices - Intermediate
Integration Patterns - Basic
Programming Languages - Intermediate
Root Cause Analysis - Basic
Software Debugging - Basic
Software Documentation - Basic
Software Engineering - Intermediate
Strategic Thinking - Basic
Technical Communication - Intermediate
Written Communication - Intermediate
Compensation Range:
Pay Range - Start:
$73,570.00
Pay Range - End:
$105,100.00
Please note that this is the standard pay structure. Positions in certain locations (such as California) may provide an increase on the standard pay structure based on the location. Pleas e click here for additi onal information relating to location-based pay structures.
GROW YOUR CAREER WITH A BEST-IN-CLASS COMPANY THAT PUTS OUR CLIENT’S INTERESTS AT THE CENTER OF ALL WE DO. GET STARTED NOW!
We are an equal opportunity/affirmative action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender identity or expression, sexual orientation, national origin, disability, age or status as a protected veteran, or any other characteristic protected by law.
If you work or would be working in California, Colorado, New York City, Washington or outside of a Corporate location, please click here for information pertaining to compensation and benefits.

FIND YOUR FUTURE
We’re excited about the potential people bring to Northwestern Mutual. You can grow your career here while enjoying first-class perks, benefits, and commitment to diversity and inclusion.
Flexible work schedules
Concierge service
Comprehensive benefits
Employee resource groups"
64,Azure Data Engineer,Integration Developer Network LLC,Remote,$70 - $75 an hour,"Skills:
Strong Experience with Azure Cloud
Strong Experience With Databricks
Strong Experience With Python Programming
Experience with Spark, Hadoop, Hive, Pig etc. Proven expertise with extracting data from a wide variety of sources and transforming the data as needed.
Previous experience with statistical modeling and deep learning frameworks / libraries is a definite plus.
Strong Experience With Python Programming
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Azure: 10 years (Required)
Databricks: 8 years (Required)
ADLs: 8 years (Required)
Cosmos DB: 7 years (Required)
ADF: 7 years (Required)
Work Location: Remote
Speak with the employer
+91 7328323606"
65,"Application Software Engineer, Data",SpaceX,"12301 Crenshaw Blvd, Hawthorne, CA 90250","$120,000 - $145,000 a year","SpaceX was founded under the belief that a future where humanity is out exploring the stars is fundamentally more exciting than one where we are not. Today SpaceX is actively developing the technologies to make this possible, with the ultimate goal of enabling human life on Mars.
APPLICATION SOFTWARE ENGINEER, DATA
The application software team is the central nervous system of SpaceX – we create mission critical applications that are used throughout SpaceX to accelerate launch vehicle production and flight as well as systems that allow Starlink to grow into a worldwide fast, reliable Internet service.
We are looking for engineers who treat fellow teammates with fairness, respect, and support.
Our team is creating systems to ingest and store concurrent streams of data from many always-on assets to manage the world's largest satellite constellation, reusable rockets, and Dragon spacecraft. Other applications range from platforms that enable rapid build and reuse of Starship, designing the next generation manufacturing software that will be used in high throughput factories for Starlink, and public facing systems where customers can join our Starlink network globally.
We work closely with engineers throughout the company to create and update our systems with respect to crewed launches, Starship flights, changes to the Starlink network and much more.
Aerospace experience is not required to be successful here - rather we look for smart, motivated, collaborative engineers who love solving problems and want to make an impact on a super inspiring mission. You will have full ownership of challenging problems, working with a team of enthusiastic engineers to design and produce solutions that enable SpaceX to move towards our goals at a rapid pace. The success of the missions at SpaceX depends on the software that you and your team produce.
RESPONSIBILITIES:
Develop highly reliable and scalable data pipelines to empower engineers across SpaceX
Create new applications that improve how the business at SpaceX operates
Collaborate with peers on architecture, design, and code reviews
Build prototypes to prove out key design concepts and quantify technical constraints
Own all aspects of software engineering and product development
Deep dive into business problems, find efficient solutions and apply first principles thinking
BASIC QUALIFICATIONS:
Bachelor's degree in computer science, data science, engineering, math, physics, or scientific discipline; OR 2+ years of professional experience building software in lieu of a degree
Experience in full stack development, software engineering, data engineering, or data science
PREFERRED SKILLS AND EXPERIENCE:
Programming experience in Python, C#, Java, Scala, Go or similar languages
Experience working with in-stream, big data processing and analytics using Apache Kafka, Spark, Flink, SQL or similar
Experience with relational and non-relational databases, data lakes e.g. HBase, Hive, Delta Lake, PostgreSQL, CockroachDB or similar
Experience with data exploration tools like Grafana, Jupyter Notebooks, Metabase, PowerBI or similar
Good understanding of version control, testing, continuous integration, build, deployment and monitoring
Some front-end experience in Angular, React, or similar JavaScript framework
Good understanding of statistics, machine learning algorithms and frameworks
ADDITIONAL REQUIREMENTS:
Willing to work extended hours and weekends when needed
COMPENSATION AND BENEFITS:
Pay Range:
Application Software Engineer/Level I: $120,000.00 - $145,000.00/per year
Application Software Engineer/Level II: $140,000.00 - $170,000.00/per year
Your actual level and base salary will be determined on a case-by-case basis and may vary based on the following considerations: job-related knowledge and skills, education, and experience.

Base salary is just one part of your total rewards package at SpaceX. You may also be eligible for long-term incentives, in the form of company stock, stock options, or long-term cash awards, as well as potential discretionary bonuses and the ability to purchase additional stock at a discount through an Employee Stock Purchase Plan. You will also receive access to comprehensive medical, vision, and dental coverage, access to a 401(k) retirement plan, short & long-term disability insurance, life insurance, paid parental leave, and various other discounts and perks. You may also accrue 3 weeks of paid vacation & will be eligible for 10 or more paid holidays per year. Exempt employees are eligible for 5 days of sick leave per year.
ITAR REQUIREMENTS:
To conform to U.S. Government space technology export regulations, including the International Traffic in Arms Regulations (ITAR) you must be a U.S. citizen, lawful permanent resident of the U.S., protected individual as defined by 8 U.S.C. 1324b(a)(3), or eligible to obtain the required authorizations from the U.S. Department of State. Learn more about the ITAR here.
SpaceX is an Equal Opportunity Employer; employment with SpaceX is governed on the basis of merit, competence and qualifications and will not be influenced in any manner by race, color, religion, gender, national origin/ethnicity, veteran status, disability status, age, sexual orientation, gender identity, marital status, mental or physical disability or any other legally protected status.
Applicants wishing to view a copy of SpaceX's Affirmative Action Plan for veterans and individuals with disabilities, or applicants requiring reasonable accommodation to the application/interview process should notify the Human Resources Department at (310) 363-6000."
66,TS/SCI Data Engineer,Gridiron IT,"Washington, DC",$65 - $73 an hour,"Seeking a Data Engineer local to Washington, DC.
Active Top Secret/SCI Clearance Required
Duties
Develop and design data pipelines to support an end-to-end solution.
Develop and maintain artifacts i.e., schemas, data dictionaries, and transforms related to ETL processes.
Integrate data pipelines with AWS cloud services to extract meaningful insights.
Manage production data within multiple datasets ensuring fault tolerance and redundancy.
Design and develop robust and functional dataflows to support raw data and expected data.
Provide Tier 3 technical support for deployed applications and dataflows.
Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc.
Skills
Amazon Web Services (AWS)
Database Administration
Data Engineering
ETL Architecture and Development
End-to-End Processes
API Development
Extract, Transform, and Load (ETL)
Data Pipeline
Tier 3 Technical Support
Preferred
Database administration and development experience will be a plus for consideration.
Experience with cloud message APIs and usage of push notifications.
Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security.
Job Type: Full-time
Pay: $65.00 - $73.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Experience level:
5 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 4 years (Preferred)
AWS: 2 years (Preferred)
Data Engineering: 4 years (Preferred)
Security clearance:
Top Secret (Required)
Work Location: In person

Health insurance"
67,Data Engineer,Clairvoyant,Remote,"$100,000 - $110,000 a year","Job type:FTE/W2
Location:Remote
What you’ll do:
Build and maintain data pipelines, data models, standardized datasets etc
Ingest data from multiple data sources (files, streaming, APIs) into our
datawarehouse
Work with stakeholders to understand requirements and implement efficient code to
accomplish the goals
Effectively gather input, ideas, and perspectives from developers, designers, and
product managers across the organization to identify opportunities for improvement,
collaboration, and impact
What you’ll bring:
6+ years of professional experience in Data Engineering
SQL: 3-5 years of experience writing complex queries for analytical and
data warehousing purposes. Experience writing DDL and DML statements. Thorough
understanding of data warehousing concepts and data model design
Python: Working knowledge of python and object orientated programming.
One year minimum
GCP/GBQ: Thorough experience working in the GCP ecosystem, and thorough
knowledge of GBQ. Two year minimum, critical requirement as main DB environment
will be GBQ
Composer/Airflow or similar pipeline orchestration tool: Working knowledge of
composer/airflow and creating/maintaining data pipeline orchestration through DAGs.
One year minimum
Working experience with a code editor, preferably VS Code, Dbeaver, or PyCharm
Experience using a repository to push code up through various environments (dev,
stg, prd) and understanding of CICD best practices. Experience with
BitBucket a plus. One year minimum
Job Types: Full-time, Contract
Salary: $100,000.00 - $110,000.00 per year
Schedule:
Monday to Friday
Experience:
Data engineer: 5 years (Required)
Python: 1 year (Required)
Sql: 3 years (Required)
GCP: 2 years (Required)
Work Location: Remote"
68,Snowflake Data Engineer,NextRow Digital,Remote,"$79,613 - $185,640 a year","Snowflake Data Engineer
Duration: Long Term
Remote Work
This is a 100% remote job.
Must have 3 verifiable references.
Job Responsibilities:
This position will require qualified T-SQL Developers to take the lead in the following tasks:
Rationalizing and mapping data between transactional and dimensional database
models/systems
Working closely with the database and system administrators to develop stored
procedures and ETL processes related to the data warehouse system.
Working in collaboration with vendors and other agency staff to design, develop and
manage the creation Synapse pipelines.
Ability to work independently and cooperatively as part of a team.
Ability to work under severe time constraints.
Must possess analytical and complex problem-solving skills.
REQUIRED SKILLS/EXPERIENCE
Extensive experience with development of stored procedures and ETL processes using T-SQL
Thorough understanding of data warehouse design hierarchies such as star and snowflake schemas
Use of ALM tools for work item management, version control, code analysis, and testing
Broad and extensive knowledge of the software development process and its technologies
Familiarity with continuous integration
Experience creating and scheduling elastic jobs
Experience with designing and modeling database structures based on business use cases
PREFERENCE TO BE GIVEN TO THE CANDIDATES WITH:
Experience with student information management systems and K-12 data and reporting
Experience with Common Education Data Standards (CEDS)
Experience with designing and modeling database structures based on business use cases
Azure Data Engineer Associate certification
Experience working with Azure Synapse Pipelines
Job Types: Full-time, Contract
Salary: $79,612.68 - $185,640.48 per year
Schedule:
8 hour shift
Experience:
Snowflake: 5 years (Required)
Azure: 4 years (Required)
Data warehouse: 5 years (Required)
T-SQL: 7 years (Required)
Work Location: Remote"
69,AWS Data Engineer,Sthavirsoft Inc,"11 New Jersey Avenue, Trenton, NJ 08629",$50 - $65 an hour,"AWS Data Engineer
Client: State of New Jersey - DOH
Job ID: 708389
Location: Trenton , NJ
Duration: 1+ years (Multiple Year Contract)
Interview mode: Skype
Note: This will be a hybrid office work environment with a 3 day in office requirement
Job Description
This senior consultant is for an Cloud data Engineer that will lead in data pipelines and data infrastructure as well as build or assist in any data engineering tasks.
This is an ongoing role beyond that of the dates above.
The State of NJ is seeking an Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with pyspark.
Experience using AWS Glue and EMR to construct data pipelines
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka,etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow,etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
.
It is a strong possibility for extension beyond the stated date above.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long term desire to consider this position for employment directly with the State. It is not inevitable, just a possibility a few years in the future.
few years in the future.
Job Type: Contract
Salary: $50.00 - $65.00 per hour
Ability to commute/relocate:
Trenton, NJ 08629: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS EMR: 3 years (Preferred)
Pyspark: 3 years (Preferred)
Data warehouse: 4 years (Preferred)
Data lake: 4 years (Preferred)
Python: 5 years (Preferred)
Work Location: One location"
70,Data Pipeline Engineer,Fuge Technologies Inc,"700 Gw Pkwy, McLean, VA 22101",$65 - $70 an hour,"Location: McLean, VA (3 days onsite -Tuesday, Wednesday, and Thursday)
no relocation candidates
Duration: 6 months contract, the opportunity for extension
Interview information:
Rounds: 2 Rounds ***2nd round will be In-Person and On-site***
Duration: 1st round - 45 Minutes, 2nd round - 30 Minutes
Additional Notes: 1st Round: MS Teams, Video Mandatory; 2nd Round: In-Person
Slots: 1st round: 4/17 @ 11:00am & 3:00pm; 4/18 @ 11:00am & 3:00pm
Must Haves: Not looking for an administrator here! Must have hands-on experience with the UNIX system, experience on the user side, and coding with Shell and Autosys. Expert-level SQL is required. Preference for experience with Snowflake. Must have hands-on experience with Hadoop.
Top skills required: Unix shell scripting, AutoSys, Hadoop, SQL, GIT, Jira, Bitbucket, and Jenkins
Control M: nice to have
Responsibilities include:
Evaluate and establish Operations Industrialization processes and work across various internal and external business customers and areas to help enhance technologies, operational and process efficiencies, and to improve the overall process. This includes participation in large-scale projects.
Interact with business teams and other stakeholders to understand and recommend operational processes and solutions to meet diverse, complex business requirements.
Manage GIT repositories, create baselines, and help in deployments.
Contribute and participate in the automation of the operational process and code reviews.
Develop and leverage SQL for validating data as well as SQL statements.
Run adhoc jobs in production as and when required to fix issues related to processes, data, etc.
Work in a small group and will need to support various ad hoc and on-going requests. This includes production operations support, development of procedures, etc.
Support end-user operational loads, and data issues and resolve them in a timely manner.
Qualifications:
Bachelor’s degree in Computer Science/Information Technology or related field and/or equivalent experience.
Preferred Skills:
5+ years of experience with the UNIX operating system mainly with LINUX (Mandatory).
3+ years of Shell scripting experience (Mandatory).
3+ years of Autosys experience (Mandatory).
3+ years of current and hands-on experience with SQL (Mandatory).
2+ years of experience with Hive queries in Big Data Platform - Hadoop (Mandatory).
Executing and monitoring jobs in a UNIX environment (Mandatory).
2+ years of experience with Source Code Control systems, preferably GIT, Bitbucket, etc. (Mandatory).
2+ years of AWS experience (optional).
2+ years of experience with Snowflake (preferred).
Running SQL queries from the command line.
Must be comfortable with analyzing large datasets (preferred).
Excellent oral and written communication skills.
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Mclean, VA 22101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location"
71,Data Engineer I,Bloom Insurance,Remote,N,"Data Engineer I
To support internal and external clients via processing and handling of data. To generate data solutions for ongoing immediate day to day business needs.

Essential Functions
Day to day functions include the following:
Design data models and develop database structures in Microsoft SQL server.
Write various database objects like stored procedures, functions, views, triggers for various front end applications.
Write SQL scripts, create SQL agent jobs to automate tasks like data importing, exporting, cleansing tasks.
Create database deployment packages for deploying changes.
Identify & repair inconsistencies in data, database tuning, query optimization.
Able to generate ad hoc data on demand.
Able to identify best practices, documentation, communicate all aspects of projects in a clear, concise manner
Develop simple SSIS packages to perform various ETL functions including data cleansing, manipulating, importing, exporting.
Develop & maintain client facing reports by using various data manipulation techniques in SSRS and Visual Studio.
Documentation
Optimization recommendations
Day to day troubleshooting
.NET Programming as needed

Education/Experience
BA, BS, or Masters in computer science/related field preferred or an equivalent combination of education and experience derived from at least 2 years of professional work experience
Solid experience with various versions of MS SQL Server and TSQL programming
Microsoft Certified DBA a plus

Skills/Knowledge
Strong experience in writing efficient SQL code
Working knowledge of SQL Server Management Studio (SSMS)
Knowledge of SQL Server Reporting Services (SSRS)
Knowledge of SQL Server Integration Services (SSIS)
Knowledge of Red Gate DBA Tool Belt (SQL Compare, SQL Data Compare, SQL Source Control) a plus
Knowledge of data science technologies is a plus
Clear, concise communication skills, excellent organizational skills
Highly self-motivated and directed
Keen attention to detail
High level of work intensity in a team environment
High integrity and values-driven
Eager for professional development
Experience and understanding of source control management a plus
What We Offer
At Bloom, we offer an engaging, supportive work environment, great benefits, and the opportunity to build the career you always wanted. Benefits of working for Bloom include:
Competitive compensation
Comprehensive health benefits
Long-term career growth and mentoring
About Bloom
As an insurance services company licensed in 48 contiguous U.S. states, Bloom focuses on enabling health plans to increase membership and improve the enrollee experience while reducing costs. We concentrate on two areas of service: technology services and call center services and are committed to ensuring our state-of-the-art software products and services provide greater efficiency and cost savings to clients.
Ascend Technology ™
Bloom provides advanced sales and enrollment automation technology to the insurance industry through our Ascend ™. Our Ascend™ technology platform focuses on sales automation efficiencies and optimizing the member experience from the first moment a prospect considers a health plan membership.

Bloom is proud to be an Equal Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law."
72,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
73,Data Engineer,PCS Wireless,Remote,N,"Location: Remote / Florham Park, NJ / Miami, FL
PCS Wireless is the global leader in the secondary market for connected devices. PCS Wireless partners with OEMs, operators, and retailers to unlock value in their and their customers’ devices. We specialize in the responsible redistribution of new and pre-owned phones, tablets, accessories, and other wireless equipment. PCS Wireless’ partners leverage our Asset Recovery, Trade-In, Device Processing and Device Diagnostics services to sustainably improve their bottom line.
Founded in 2001, our team, through entrepreneurial spirit and can-do attitude, takes on challenges with grit and excitement. We are a community who welcomes challenges and diversity of opinion, always expressed with respect.
If you are looking to be part of a human company, where you can deploy your energy to help implement extraordinary growth, where you can feel part of a team focused in developing a business that is connecting people and maybe making the world a little better place, then come join us as we grow into the next phase of PCS Wireless.
We are seeking a skilled Data Engineer to join our data team. The ideal candidate will be responsible for designing, building, and maintaining the infrastructure that enables our organization to store, process, and analyze large volumes of data. The successful candidate will work closely with our data scientists and analysts to ensure that they have access to clean, reliable data for their work.
Responsibilities:
Design and implement data pipelines that move data from various sources into a central repository or data lake.
Develop and maintain data warehouses and data marts that enable data analysis and reporting.
Create and maintain ETL (extract, transform, load) processes that clean and transform data for analysis.
Build and manage big data platforms such as Hadoop, Spark, or NoSQL databases.
Design and implement data security and privacy measures to protect sensitive data.
Collaborate with data scientists and analysts to provide access to the data they need for their work.
Automate data processing and data quality checks to ensure that data is accurate and reliable.
Develop and maintain data documentation and data dictionaries.
Evaluate new data technologies and tools and make recommendations to improve the organization's data capabilities.
Troubleshoot data infrastructure issues and optimize performance.
Qualifications:
Bachelor's degree in computer science, engineering, or a related field preferred.
2+ years of experience in data engineering or a related field.
Experience with big data technologies such as Hadoop, Spark, or NoSQL databases.
Strong programming skills in Python, Java, or another language.
Experience with ETL (extract, transform, load) processes and data pipeline design.
Familiarity with data warehousing and data modeling concepts.
Experience with cloud-based data solutions such as AWS, Google Cloud Platform, or Azure.
Strong problem-solving skills and the ability to work in a fast-paced environment.
Excellent communication skills and the ability to work effectively in a team.
Job Type: Full-time
Benefits:
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Work Location: Remote

Health insurance"
74,Data Engineer - FinTech,EquiLend,"New York, NY•Hybrid remote",N,"Company Overview
At Equilend we pride ourselves on being a leading global provider of trading, post-trade, securities market data, regulatory technology and clearing services in the securities finance industry.
Our journey began in 2000 when 10 of leading global financial institutions including Bank of America, BlackRock, Goldman Sachs, J.P. Morgan, Morgan Stanley, State Street and UBS came together with a view to increasing automation and efficiencies in the Securities Finance space. Since then, we've built our global footprint including offices in New York, Boston, Toronto, London, Dublin, Hong Kong, Tokyo, New Jersey, Pune and Chandigarh.
As a business we're committed to nurturing long-term relationships with our clients, creating an environment where our people can learn and grow, and continuing to foster an innovative and supportive environment.
Role Responsibilities
Design and develop ETL data pipelines using Python on Spark.
You will combine the discipline of software engineering with the knowledge and experience of building data solutions to deliver business value.
As a data engineer, you'll help deploy data pipelines and processes in a production-safe manner
Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility
Implement processes and systems to monitor data quality, ensuring production data is accurate and available.
Work closely with all business units and engineering teams to develop a strategy for long-term data platform architecture.
Required Skills
3+ years of relevant commercial experience
Have excellent knowledge of Python/PySpark or other Object Oriented languages such as Scala or Java
Experience with scheduling and monitoring tools such as Airflow or similar
Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
A background in software development in high-volume environments
Worked with stream processing technologies (such as Kafka, Spark and AWS Kinesis)
Working knowledge of AWS is highly desired, but not required
Thrive in a diverse, open and collaborative agile environment
Role Location Benefits
Hybrid working – you can choose which 100 days per year to work remotely where you prefer!
Generous vacation days each year which increase with your levelling
Discretionary annual bonus
Health Insurance coverage from day 1
401(k) Employer matching contribution plan
Regular team events – both in the office and offsite!
Weekly remote yoga sessions and paid access to a mindfulness-based mental health app
Diversity and Inclusion
We are committed to diversity and inclusion and believe that it is essential to our success as a company. We value and respect all individuals, regardless of their background, and recognize that diversity brings a wealth of perspectives and skills that help us innovate and excel and we welcome applications from individuals of all backgrounds and experiences.
If you require any accommodations during the application process please let us know.
#LI-Hybrid"
75,Data Engineer,Johns Hopkins University,"Baltimore, MD 21209•Remote","$96,950 - $133,350 a year","IT@JH Enterprise Business Solutions (EBS) is seeking a Data Engineer who is responsible for implementation and maintenance of data marts and warehouses by integrating various data sources into a pre-defined data model and/or semantic layer, and support of the development of front-end dashboards and reports.

Specific Duties & Responsibilities:
Primary Responsibilities:
Collaborates with data engineering and BI teams across the enterprise on the overall enterprise data and BI tools' architecture.

Provides technical guidance, mentoring and team building; and work closely with EBS functional teams and other IT and business teams in providing business intelligence solutions and support.

Performs hardware and software evaluations, recommendations and installations; implementation of production systems, applications, and technology platforms; security administration; backup and recovery planning, capacity planning; performance monitoring and tuning; and development of policies, standards, and procedures related to the JHU and JHHS enterprise environment.

Concurrently support integration with multiple data warehouse landscapes and environments to monitor, detect, diagnose, and correct problems using SAP, Microsoft, Splunk and other 3rd party software to assure the integrity, availability, security, and recovery of data.

Provides expertise on all support and maintenance for BusinessObjects and related systems.

Secondary Responsibilities:
Manages enterprise BI environment, partnering with developers across the enterprise - config, administration, support & maintenance.

Scales enterprise BI platforms to reduce cost & improve support & management.

Implements and configures integration and cataloging tools.

Collaborates with other BI and DW Teams (BusinessObjects, Tableau, O365/PowerBI).

Gain proficiency in Snowflake to build capacity for integrations from various sources.

Provides recommendations and document technological improvements including systems sizing, backup/restore and archiving strategies using a strong working knowledge of very large enterprise scaled SAN, OS, Network and advanced hardware and software concepts.

Develops and execute project plans, provide technical guidance and support; and create technical documentation.

Works collaboratively with business community and IT colleagues to define enterprise BI practices.

Collaborates & consults w/other IT teams for BI and development, as well as Analytic teams and other project teams to ensure operational support for major initiatives (Reports, Dashboard, Scorecards, Analytics).

Specific Devices, Software, Projects:
Management of the BusinessObjects Enterprise platform and all related components
Integration of multiple BI platforms and DW environments for cataloging

Snowflake another data warehouse and data lake technologies

Scale/size of Area, Project and/or System Supported:
Supports 10,000+ users in day-to-day reporting activities.

Supports several IT and BI development teams.
Works closely with system and data owners across the enterprise, data stewards and administrators of various source systems and applications

Collaborates closely with other technical teams on architecture, design and integration.

On-call requirements:
Off hours support not expected; only in event of unlikely system events.

Special Knowledge, Skills, and Abilities:
Expert knowledge of BusinessObjects and working knowledge of other BI platforms, such as: PowerBI, Tableau.
Knowledge of relational databases (MS-SQL), ETL tools and practices and data integration techniques.
Familiarity with data quality/profiling tools and practices.
Working understanding of OLTP and OLAP data modeling and normalization concepts.
Ability to establish and document data lineage, metadata.
Familiarity with code versioning tools (GIT) and agile development.
Working knowledge of reference/master data tools and practices.
Demonstrated knowledge of project management, requirements gathering, software development methodologies and project documentation practices.
Excellent written and oral communications.

Work Location:
100% Remote

Minimum Qualifications

Bachelor's degree required.
Five years of related work experience focused within database managementand design and business requirement gathering.
Additional experience may be substituted for education.

Preferred Qualifications

Experience in design, development and implementation of data visualizations using a variety of tools/platforms; and business intelligence tools including dashboards (such as: PowerBI, Tableau).
Experience with data analysis, including trending and development/tuning of KPIs.
Experience with using statistical languages and statistical packages in programming languages (e.g., Python, R).
Experience with Big Data technologies such as Hadoop or MapReduce.
Experience with modern web technologies, protocols & languages (REST, SOAP, JSON, HTML/XHTML, XML, XSLT).

Classified Title: Data Engineer
Working Title: Data Engineer
Role/Level/Range: ATP/04/PG
Starting Salary Range: $96,950-$133,350-$169,760
Employee group: Full Time
Schedule: Mon-Fri 8:30am-5:00pm
Exempt Status: Exempt
Location: Remote
Department name: 10001627-IT@JH Enterprise Business Solutions
Personnel area: University Administration

Total Rewards
The referenced salary range is based on Johns Hopkins University's good faith belief at the time of posting. Actual compensation may vary based on factors such as geographic location, work experience, market conditions, education/training and skill level. Johns Hopkins offers a total rewards package that supports our employees' health, life, career and retirement. More information can be found here: https://hr.jhu.edu/benefits-worklife/

Please refer to the job description above to see which forms of equivalency are permitted for this position. If permitted, equivalencies will follow these guidelines:
JHU Equivalency Formula: 30 undergraduate degree credits (semester hours) or 18 graduate degree credits may substitute for one year of experience. Additional related experience may substitute for required education on the same basis. For jobs where equivalency is permitted, up to two years of non-related college course work may be applied towards the total minimum education/experience required for the respective job.

**Applicants who do not meet the posted requirements but are completing their final academic semester/quarter will be considered eligible for employment and may be asked to provide additional information confirming their academic completion date.

The successful candidate(s) for this position will be subject to a pre-employment background check. Johns Hopkins is committed to hiring individuals with a justice-involved background, consistent with applicable policies and current practice. A prior criminal history does not automatically preclude candidates from employment at Johns Hopkins University. In accordance with applicable law, the university will review, on an individual basis, the date of a candidate's conviction, the nature of the conviction and how the conviction relates to an essential job-related qualification or function.

The Johns Hopkins University values diversity, equity and inclusion and advances these through our key strategic framework, the JHU Roadmap on Diversity and Inclusion .

Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

EEO is the Law

Learn more:
https://www.eeoc.gov/sites/default/files/migrated_files/employers/poster_screen_reader_optimized.pdf

Accommodation Information

If you are interested in applying for employment with The Johns Hopkins University and require special assistance or accommodation during any part of the pre-employment process, please contact the Talent Acquisition Office at jhurecruitment@jhu.edu . For TTY users, call via Maryland Relay or dial 711. For more information about workplace accommodations or accessibility at Johns Hopkins University, please visit accessibility.jhu.edu .

Johns Hopkins has mandated COVID-19 and influenza vaccines, as applicable. Exceptions to the COVID and flu vaccine requirements may be provided to individuals for religious beliefs or medical reasons. Requests for an exception must be submitted to the JHU vaccination registry. For additional information, applicants for SOM positions should visit https://www.hopkinsmedicine.org/coronavirus/covid-19-vaccine/ and all other JHU applicants should visit https://covidinfo.jhu.edu/health-safety/covid-vaccination-information/ .

The following additional provisions may apply, depending on campus. Your recruiter will advise accordingly.

The pre-employment physical for positions in clinical areas, laboratories, working with research subjects, or involving community contact requires documentation of immune status against Rubella (German measles), Rubeola (Measles), Mumps, Varicella (chickenpox), Hepatitis B and documentation of having received the Tdap (Tetanus, diphtheria, pertussis) vaccination. This may include documentation of having two (2) MMR vaccines; two (2) Varicella vaccines; or antibody status to these diseases from laboratory testing. Blood tests for immunities to these diseases are ordinarily included in the pre-employment physical exam except for those employees who provide results of blood tests or immunization documentation from their own health care providers. Any vaccinations required for these diseases will be given at no cost in our Occupational Health office.

Note: Job Postings are updated daily and remain online until filled."
76,Data Engineer/Data Analyst,Innova Solutions Inc.,"Richmond, VA 23229•Hybrid remote",$60 - $65 an hour,"Position Summary
The person will have a mix of highly technical data quality controls development, data analysis and reporting responsibilities to include writing complex SQL queries, some python code analysis, extensive data analysis, building DQ controls metrics reports, defining tech data controls strategy, working with metadata, architecture and development teams on the resolution to DQ controls issues
Primary Skill
MySQL
Secondary Skill
Tertiary Skill
Required Skills
SQL and DQ Controls Experience
DQ Controls Development and Strategy Skills
Business Analyst
Data Architecture
Desired Skills
Python, metadata, business intelligence reporting, BA
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Richmond, VA 23173: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
MySQL: 5 years (Required)
Data Quality: 5 years (Required)
Work Location: Hybrid remote in Richmond, VA 23173"
77,Platform Data Engineer,Recruiting From Scratch,"Livonia, MI 48154•Remote","$140,000 - $250,000 a year","Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Data Engineer - Platform
A Career with our Market Intelligence Team:
Our Market Intelligence team is responsible for developing proprietary research products and providing data and research management services for investment teams to support their pursuit of superior, risk-adjusted returns. We leverage innovative alternative data sources, advanced data analytics and technologies, and deep fundamental research to create high-quality compliant and differentiated research. Backed by the full resources of our company, our sector aligned teams of fundamental researchers, data scientists, data strategists, relationship managers, and data engineers collaborate to solve important research problems in partnership with the Firm’s investment and compliance professionals.

What you’ll do
Platform Engineers build solutions for processing big and unstructured data sets. Our team works closely with portfolio managers and data scientists to understand the potential business value of data sets and ultimately build data processing pipelines around those data sources. In this role, you will:
Develop big data processing pipelines for new data sources containing structured and unstructured data
Build platform infrastructure using Hadoop technologies
Build and support visualization and exploration capabilities around our big data sets
Maintain knowledge of new technology developments and conduct proof of concepts to evaluate new technologies

What’s required
We want you to join us if you have extensive experience or demonstrated interest in big data technologies. Other requirements include:
2+ years of experience in Data Engineering or related field
Commitment to the highest ethical standards
Strong experience in Python Development
Experience with Spark or Scala
Ability to devise novel and innovative solutions to challenges
Knowledge of/experience with graph databases is a plus

We take care of our people
We invest in our people, their careers, their health, and their well-being. We want you to concentrate on success and leave the rest to us. When you work here, we provide:
Fully-paid health care benefits
Generous parental and family leave policies
Mental and physical wellness programs
Tuition assistance
A 401(k) savings program with an employer match and more
Salary Range: $140,000-$250,000 base."
78,Azure Data Engineer,Integration Developer Network LLC,Remote,$70 - $75 an hour,"Skills:
Strong Experience with Azure Cloud
Strong Experience With Databricks
Strong Experience With Python Programming
Experience with Spark, Hadoop, Hive, Pig etc. Proven expertise with extracting data from a wide variety of sources and transforming the data as needed.
Previous experience with statistical modeling and deep learning frameworks / libraries is a definite plus.
Strong Experience With Python Programming
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Azure: 10 years (Required)
Databricks: 8 years (Required)
ADLs: 8 years (Required)
Cosmos DB: 7 years (Required)
ADF: 7 years (Required)
Work Location: Remote
Speak with the employer
+91 7328323606"
79,"Senior Engineer, Data Delivery",Sephora,"San Francisco, CA 94105•Remote","$137,000 - $153,000 a year","Job ID: 225441
Location Name: FSC REMOTE SF/NY/DC -173(USA_0173)
Address: FSC, Remote, CA 94105, United States (US)
Job Type: Full Time
Position Type: Regular
Job Function: Information Technology
Remote Eligible:Yes

Company Overview:
At Sephora we inspire our customers, empower our teams, and help them become the best versions of themselves. We create an environment where people are valued, and differences are celebrated. Every day, our teams across the world bring to life our purpose: to expand the way the world sees beauty by empowering the Extra Ordinary in each of us. We are united by a common goal - to reimagine the future of beauty.

The Opportunity:
Technology
Our technology team works fast and smart. With San Francisco as our home, we take bringing new tech to market seriously, developing the latest in mobile technologies, scalable architecture, and the coolest in-store client experience. We love what we do and we have fun doing it. The Technology group is comprised of motivated self-starters and true team players that are absolutely integral to the growth of Sephora and our future success.

Your role at Sephora:
As a “Senior Engineer, Delivery”, you will design and implement solutions and work alongside the product engineering team, evaluating new features and architecture while overseeing end to end flow. Reporting to the Director, Engineering, you will work closely with other team members like Data architects, Data Engineers, ML Engineers and Product management to understand what the business is trying to achieve, design optimal data models and most optimal ways for data-movements that helps optimize Compute, Processing and Data storage within Data Platform. This role demands excellent solution delivery skills, communication and software engineering best practice. Come be a part of a team that is starting this new journey.

Responsibilities
Architect and maintain our code base for ETL and ELT pipelines, large batch/ microbatch processing and streaming systems.
Build out the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ADF, Spark, Kafka or similar technologies.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc
Familiarity with designing and implementing innovative data services solutions using Spring boot, ReactJS, NoSQL, or other UI and API related technologies
Assure governance of processes in delivery management and production as per selected delivery model
For the top management and stakeholders - act as a single point of responsibility over any delivery-related matters, including escalations, upsells, ramp-downs, etc
Accountable for the technical leadership regarding the delivery. Ensuring a sound and future-proof architecture is planned and the implementation meets the technical quality standards.
Comfortable writing stories and associated acceptance criteria for agile/scrum workflow
Coordinate between multiple disciplines, stakeholders
Ensure that projects are delivered in line with Sephora processes and methodologies; Well-versed in different delivery models, with focus on agile approaches
Establish a strategy of continuous delivery risk management that enables proactive decisions and actions throughout the delivery life cycle
Measure and improve delivery productivity
Serve as a consultant to the Data Engineers in support of quality and timely delivery of work
Perform production support and deployment activities

We’re excited about you if you have:
3+ years experience in delivering projects in Data and Analytics, Big Data, Data Warehousing, Business Intelligence. Familiar with relevant technological solutions, industry best practices
Good understanding of data engineering challenges and proven experience with data platform engineering (batch and streaming, ingestion, storage, processing, management, integration, consumption)
At least 4 years of experience in gathering and understanding customer business requirements to introduce Big Data technologies
Familiar with multiple Big Data technology stacks (e.g. Hadoop, Yarn, HDFS, MapReduce, Hive, Spark, Kafka, etc)
Experience with data visualization, aware of various tools, technologies
Good understanding of various delivery approaches, well versed in agile
Understanding data related security challenges
Experience with one or more leading cloud providers (AWS/Azure/GCP), leading role in on-prem to cloud migration project(s)
Ability to manage multiple tasks and projects in a fast-moving environment
Experience working with marketing data structures using Google Cloud Storage, Cloud SQL, GCP Cloud Data Flow, Google Cloud Data Prep, BigQuery & Compute Engine, Cloud Big Table, Cloud CDN for Google Marketing & G Suite.
Experience working in an agile team
Familiar with scaling microservices in high traffic environments
Experience with performance management, logging, and monitoring tools
Experience with common open-source technologies at scale, such as Kafka, Cassandra, MongoDB, Nginx, Redis, ElasticSearch
Experience in building multi-tier, high availability applications with modern web technologies such as NoSQL, MongoDB, SparkML, and TensorFlow.
Problem-Solving Skills: be able to look at an issue that needs to be solved and come up with solutions quickly.

You’ll love working here because:
The people. You will be surrounded by some of the most talented, supportive, smart, and kind leaders and teams – people you can be proud to work with.
The product. Employees enjoy a product discount and receive free product (“gratis”) various times throughout the year. (Think your friends and family love you now? Just wait until you work at Sephora!)
The business. It feels good to win – and Sephora is a leader in the retail industry, defining experiential retail with a digital focus and creating the most loved beauty community in the world…with the awards and accolades to back it up.
The perks. Sephora offers comprehensive medical benefits, generous vacation/holiday time off, commuter benefits, and “Summer Fridays” (half-days every Friday between Memorial and Labor Day)…and so much more.
The LVMH family. Sephora’s parent company, LVMH, is one of the largest luxury groups in the world, providing support to over 70 brands such as Louis Vuitton, Celine, Marc Jacobs, and Dior.

Working at Sephora’s Field Support Center (FSC)
Our North American operations are based in the heart of San Francisco’s Financial District, but you won’t hear us call it a headquarters – it’s the Field Support Center (FSC). At the FSC, we support our stores in providing the best possible experience for every client. Dedicated teams cater to our client’s every need by creating covetable assortments, curated content, compelling storytelling, smart strategy, skillful analysis, expert training, and more. It takes a lot of curious and confident individuals, disrupting the status quo and taking chances. The pace is fast, the fun is furious, and the passion is real. We never rest on our laurels. Our motto? If it’s not broken, fix it.

The annual base salary range for this position is $137,000.00 - $153,000.00 The actual base salary offered depends on a variety of factors, which may include, as applicable, the applicant’s qualifications for the position; years of relevant experience; specific and unique skills; level of education attained; certifications or other professional licenses held; other legitimate, non-discriminatory business factors specific to the position; and the geographic location in which the applicant lives and/or from which they will perform the job. Individuals employed in this position may also be eligible to earn bonuses. Sephora offers a generous benefits package to full-time employees, which includes comprehensive health, dental and vision plans; a superior 401(k) plan, various paid time off programs; employee discount/perks; life insurance; disability insurance; flexible spending accounts; and an employee referral bonus program.

While at Sephora, you’ll enjoy…

The people. You will be surrounded by some of the most talented leaders and teams – people you can be proud to work with.
The learning. We invest in training and developing our teams, and you will continue evolving and building your skills through personalized career plans.
The culture. As a leading beauty retailer within the LVMH family, our reach is broad, and our impact is global. It is in our DNA to innovate and, at Sephora, all 40,000 passionate team members across 35 markets and 3,000+ stores, are united by a common goal - to reimagine the future of beauty.

You can unleash your creativity, because we’ve got disruptive spirit. You can learn and evolve, because we empower you to be your best. You can be yourself, because you are what sets us apart. This, is the future of beauty. Reimagine your future, at Sephora.

Sephora is an equal opportunity employer and values diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, ancestry, citizenship, gender, gender identity, sexual orientation, age, marital status, military/veteran status, or disability status. Sephora is committed to working with and providing reasonable accommodation to applicants with physical and mental disabilities.

Sephora will consider for employment all qualified applicants with criminal histories in a manner consistent with applicable law.

As a condition of employment, Sephora requires all newly hired employees to be fully vaccinated against COVID-19 by their start date unless they have requested and received an exemption due to a qualifying medical condition, a sincerely held religious belief or practice, or a requirement by law."
80,Data Engineer,N,Remote,N,"Poggio Labs is building the future of customer data applications
We are early entrants in a massive market, funded by top-tier investors, designing solutions together with our inspiring customers.
Working at Poggio Labs
Data engineers at Poggio Labs have autonomy over their projects, design features from scratch, and release to production. We automate as much as possible. We value curiosity, self-awareness, and collaboration as much as technical excellence.
As a founding member of the of the data team, you'll own building big pieces of our novel architecture together with insanely bright and motivated peers.
Seeking experienced product builders
Data warehouse, pipelines, and integration
Scalable cloud infrastructure, serving data to modular applications
Full-time, salary, equity, & benefits. Health, dental, vision, life, disability, 401(k), personal & professional development. We don't track PTO, but we recommend a minimum 3 weeks per year.
Poggio Labs is an equal opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.
This policy applies to all employment practices within our organization, including hiring, recruiting, promotion, termination, layoff, recall, leave of absence, compensation, benefits, training, and apprenticeship. Poggio Labs makes hiring decisions based solely on qualifications, merit, job performance, and business needs at the time."
81,Data Engineer,Fastechnowiz Solutions Inc,"Westlake, TX 76262",$57.63 - $65.00 an hour,"Position Details:
Title: Data Engineer
Industry: Banking & Financial
Duration: 12 Months- Long term
Location: Smithfield RI/ Durham, NC/ Westlake, TX
Top Skills: Informatica, SQL, Snowflake, python
Required Qualifications
ETL developer with Informatica
Strong SQL – Snowflake and SQL Server will be a huge plus.
Strong Analysis skills
Working knowledge of Unix OS /Shell scripting
Basic Python knowledge is required.
Good working knowledge of Control-M/Automation tools.
Some experience in DevOps
Production Support will be required – one week every 3 months.
Excellent interpersonal and communication skills
Excellent collaboration skills to work with multiple teams in the organization.
Additional Experience
Experience with Metadata management solutions / Data lineage is a plus
Learn New technologies and evaluate new products, participating in Proof of Concepts (POCs) is a plus
Vendor management is a plus
Some QA/Testing experience is a plus
Some Kubernetes / Docker experience is a plus
Strong communication and presentation skills
Job Type: Contract
Salary: $57.63 - $65.00 per hour
Benefits:
Health insurance
Parental leave
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Westlake, TX 76262: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 2 years (Required)
SQL: 3 years (Required)
Data warehouse: 3 years (Required)
Work Location: One location

Health insurance"
82,"Data Engineer, Global Payments - USDS",TikTok,"New York, NY","$102,400 - $221,760 a year","Responsibilities
TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Mumbai, Singapore, Jakarta, Seoul and Tokyo.

Why Join Us
At TikTok, our people are humble, intelligent, compassionate and creative. We create to inspire - for you, for us, and for more than 1 billion users on our platform. We lead with curiosity and aim for the highest, never shying away from taking calculated risks and embracing ambiguity as it comes. Here, the opportunities are limitless for those who dare to pursue bold ideas that exist just beyond the boundary of possibility. Join us and make impact happen with a career at TikTok.

About USDS
At TikTok, we're committed to a process of continuous innovation and improvement in our user experience and safety controls. We're proud to be able to serve a global community of more than a billion people who use TikTok to creatively express themselves and be entertained, and we're dedicated to giving them a platform that builds opportunity and fosters connection. We also take our responsibility to safeguard our community seriously, both in how we address potentially harmful content and how we protect against unauthorized access to user data.

U.S. Data Security (“USDS”) is a standalone department of TikTok in the U.S. This new security-first division was created to bring heightened focus and governance to our data protection policies and content assurance protocols to keep U.S. users safe. Our focus is on providing oversight and protection of the TikTok platform and user data in the U.S., so millions of Americans can continue turning to TikTok to learn something new, earn a living, express themselves creatively, or be entertained. The teams within USDS that deliver on this commitment daily span Trust & Safety, Security & Privacy, Engineering, User & Product Ops, Corporate Functions and more.

About the Team
The Global Payment team of the US Tech Service department of TikTok provides all-round payment solutions for the company's overseas products, overseas commercialization, and the company's overseas travel and procurement, including channel access, product order design, user interaction, capital management, tax and exchange optimization, settlement Reconciliation and so on. In this role, you'll have the opportunity to develop and manage the complex challenges of scale with your expertise in large-scale system design.

Responsibilities
Build data pipelines to portray business status, based on a deep understanding of our fast changing business and data-driven approach.
Extract information and signals from a broad range of data and build hierarchies to accomplish analytical and mining goals for “Packaged Business Capability” such as user-growth, gaming and searching.
Keep improving the integrity of data pipelines to provide a comprehensive data service.
Qualifications
Bachelor's degree in Computer Science, Statistics, Data Science or a related field.
Skilled in SQL and additional object-oriented programming language (e.g. Scala, Java, or Python).
Experience in issue tracking and problem solving on data pipelines.
Fast business understanding and collaborative in teamwork.
Experience working with user growth is a plus.
TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.

TikTok is committed to providing reasonable accommodations during our recruitment process. If you need assistance or an accommodation, please reach out to us at lois.chen@tiktok.com.
Job Information
The base salary range for this position in the selected city is $102400 - $221760 annually.



Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.



At ByteDance/TikTok our benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support ByteDancers to give their best in both work and life. We offer the following benefits to eligible employees:



We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care.



Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off(PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability.



We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice."
83,People Analytics Data Viz Engineer (Talent Acquisition),General Motors,Remote,"$102,384 - $130,824 a year","Job Description

People Analytics CoE (Community of Expertise) is looking for a People Analytics Data Visualization Engineer to join its newly formed Talent Acquisition Analytics team. In this role, you will be responsible for designing, developing, building, and managing Global Talent Acquisition (GTA) dashboards that provide key insights on our internal and external talent attraction and hiring activities.

You will be responsible for managing a portfolio of Talent Acquisition Power BI dashboards and moving them to the next level where insights and recommendations are seamlessly embedded in every user interaction and every single chart. You will also be responsible for designing, building, and managing the People Analytics data technology stack and infrastructure in partnership with IT. You will support the maintenance and expansion of data sets that are used by the People Analytics team to accelerate access to critical business insights and deliver insights at scale. You will provide meaningful, easy to understand and actionable insights by connecting talent acquisition data with critical business outcomes through powerful data visualizations.

Your storytelling, consulting, and technical skills as well as ability to extract key insights from multiple data sources and to put business problems into visual context will have a significant impact on our ability to enable data-driven insights across the organization. This role requires a strong dashboard and visualization technical development experience and analytical consulting skills. What is in it for you? You will have a chance to influence our talent strategy, help design insights that matter and be part of a great team that puts innovation and curiosity at the center of everything we do.

Responsibilities:
Streamline and automate Talent Acquisition reports and dashboards
Design, develop, test, and implement Power BI dashboards and visualizations supporting Talent Acquisition dashboards, projects, and ad-hoc requests
Manage portfolio of existing and future Talent Acquisition Power BI dashboards and deliverables (data preparation, security design, access management, data visualizations design, testing, optimization and automation, enhancement roadmap, portfolio catalog, development prioritization, user adoption and training)
Lead the implementation of enhanced dashboard access and data sharing security framework for Talent Acquisition dashboards including UX training and enhancement development
Deliver meaningful insights, trends, and analysis across all HR practice areas using a variety of internal and external data sources
Participate in gathering requirements and consulting with customers to understand business problems and find the best way to visualize outcomes
Partner with members of the Talent Acquisition Analytics and People Analytics Data Visualizations & Analytics Solutions teams to enhance existing data sources and automate the data preparation process
Use data to tell stories by creating visually appealing reports, Power BI dashboards, and presentations for our internal customers
Build and manage a Talent Acquisition Analytics Roadmap highlighting data requirements that will support the development of analytical products and solutions
Manage the integration of multiple internal and external data sources in partnership with IT
Support managing and enhancing existing People Analytics infrastructure (database management, product enhancement roadmap etc.)
Ingest new data sources into the existing People Analytics infrastructure
Partner with members of the Talent Acquisition Analytics, People Analytics Data Visualizations & Analytics Solutions and HROS teams to enhance existing data sources and data visualizations and automate the data preparation process
Establish a process for managing backlog and prioritization criteria for data related requests
Support the automation of dashboards, ongoing and ad-hoc reports, and data science models

Requirements:
5+ years of dashboard development in Power BI and data visualization experince providing data-driven recommendations
5+ years leading dashboard development in People Analytics or Talent Analytics roles
5+ years of data management and/or data engineering experience supporting People Analytics and/or Talent Analytics teams
Very good understanding of Talent Acquisition data and candidate management processes (talent attraction, sourcing, talent acquisition, talent development, workforce planning, DEI (diversity, equity, and inclusion), candidate onboarding & experience sentiment, etc.)
Experience developing and implementing enterprise-scale dashboards and reports, implementing row-level security models, and blending, optimizing, and automating multiple data sources
Thorough understanding of data visualization design best practices (Power BI, executive presentations)
Proven applied knowledge of scripting languages for data retrieval and data transformation (SQL) (R and Python are a plus)
Advanced knowledge and experience working with relational databases, query authoring (SQL) and familiarity with a variety of databases
Experience implementing row-level security models, and blending, optimizing, and automating multiple data sources
Experience working with IT on data management enhancements (co-development model)
Analytical thinker and creative designer able to bring data to life
Ability to translate business requirements into a production dashboard, take complex datasets and create key takeaways and recommendations using several data sources
Great data analytics, data accuracy, and consulting skills
Performance tuning experience related to data visualizations
Passionate about using data to tell stories
Strong analytical and communication skills, and ability to work effectively with business and technical associates
Experience building ETLs (Extract, Transform, Load), manipulating data at scale and managing data infrastructure is a plus
Experience building and optimizing data pipelines, managing governance and access, designing architecture that support dashboards development and automation is a plus
Experience using infographics and content management tools (e.g., WordPress) is a plus
Experience using KNIME is a plus

Remote: The position can be performed remotely from the US most of the time, but the employee may be asked to come on-site approximately four times per year

Compensation: The expected base compensation for this role is : $102,384 - $130,824. Actual base compensation within the identified range will vary based on factors relevant to the position.

Bonus Potential: An incentive pay program offers payouts based on company performance, job level, and individual performance.

Benefits: GM offers a variety of health and wellbeing benefit programs. Benefit options include medical, dental, vision, Health Savings Account, Flexible Spending Accounts, retirement savings plan, sickness and accident benefits, life insurance, paid vacation & holidays.

#LI-remote

About GM

Our vision is a world with Zero Crashes, Zero Emissions and Zero Congestion and we embrace the responsibility to lead the change that will make our world better, safer and more equitable for all.

Why Join Us

We aspire to be the most inclusive company in the world. We believe we all must make a choice every day - individually and collectively - to drive meaningful change through our words, our deeds and our culture. Our Work Appropriately philosophy supports our foundation of inclusion and provides employees the flexibility to work where they can have the greatest impact on achieving our goals, dependent on role needs. Every day, we want every employee, no matter their background, ethnicity, preferences, or location, to feel they belong to one General Motors team.

Benefits Overview

The goal of the General Motors total rewards program is to support the health and well-being of you and your family. Our comprehensive compensation plan incudes, the following benefits, in addition to many others:
Paid time off including vacation days, holidays, and parental leave for mothers, fathers and adoptive parents;
Healthcare (including a triple tax advantaged health savings account and wellness incentive), dental, vision and life insurance plans to cover you and your family;
Company and matching contributions to 401K savings plan to help you save for retirement;
Global recognition program for peers and leaders to recognize and be recognized for results and behaviors that reflect our company values;
Tuition assistance and student loan refinancing;
Discount on GM vehicles for you, your family and friends.

Diversity Information

General Motors is committed to being a workplace that is not only free of discrimination, but one that genuinely fosters inclusion and belonging. We strongly believe that workforce diversity creates an environment in which our employees can thrive and develop better products for our customers. We understand and embrace the variety through which people gain experiences whether through professional, personal, educational, or volunteer opportunities.GM is proud to be an equal opportunity employer.

We encourage interested candidates to review the key responsibilities and qualifications and apply for any positions that match your skills and capabilities.

Equal Employment Opportunity Statements

GM is an equal opportunity employer and complies with all applicable federal, state, and local fair employment practices laws. GM is committed to providing a work environment free from unlawful discrimination and advancing equal employment opportunities for all qualified individuals. As part of this commitment, all practices and decisions relating to terms and conditions of employment, including, but not limited to, recruiting, hiring, training, promotion, discipline, compensation, benefits, and termination of employment are made without regard to an individual's protected characteristics. For purposes of this policy, ""protected characteristics"" include an individual's actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth, lactation and related medical conditions), gender identity or gender expression, sexual orientation, weight, height, marital status, military service and veteran status, physical or mental disability, protected medical condition as defined by applicable state or local law, genetic information, or any other characteristic protected by applicable federal, state or local laws and ordinances. If you need a reasonable accommodation to assist with your job search or application for employment, email us atCareers.Accommodations@GM.comor call us at 800-865-7580. In your email, please include a description of the specific accommodation you are requesting as well as the job title and requisition number of the position for which you are applying."
84,Data Engineer,LiveBy,"Lincoln, NE•Remote","$85,000 - $130,000 a year","We are looking for an experienced Data Software Engineer with 2+ years of experience in API and Data normalization, and expertise in Amazon Web Services (AWS), JavaScript, and Python. In this role, you will be responsible for developing, testing, and deploying data-driven software products. You should have a strong understanding of various technologies and databases, including SQL and NoSQL, as well as experience working with API and data normalization.

You will collaborate closely with software developers, product managers, and other stakeholders to ensure that software products meet the business’s goals and objectives. You will be expected to provide technical input on product design and development, as well as troubleshoot any issues that arise.

In addition to being comfortable with API and data normalization, the ideal candidate should have excellent problem-solving skills, communication, and collaboration skills. You should be able to work with a variety of stakeholders, provide feedback on product design and development, and effectively troubleshoot any issues that arise. You should have demonstrated the ability to design long-term solutions capable of adapting to changes in product design and business requirements. If you have the experience and skills necessary for this role, we want to hear from you!

Job Responsibilities:
Designing and developing data models and pipelines to support a variety of products
Implementing data standards, data governance, and best practices in data engineering
Building data architectures to support real-time and batch data processing
Developing and managing dashboards to visualize data and provide insights
Performing data profiling and analysis, data cleansing and data validation
Implementing ETL processes, data models, and data warehouse solutions
Researching and developing new technologies and solutions for data engineering
Collaborating with teams to define and improve data quality, data integration, and data security standards
Monitoring and troubleshooting data engineering solutions and pipelines
Qualifications and Experience:
2+ years of professional experience in data engineering with demonstrated growth in knowledge and responsibility
Expertise in AWS, API Gateway, Javascript, Serverless Architectures utilizing AWS Lambda, Glue and Fargate
Database experiences with PostgresSQL, Mongo, Casandra, DocumentDB or equivalent
Experience or background in GIS and real estate technologies a plus
Experienced in developing data engineering solutions that prioritize scalability, performance, reliability, and cost-effectiveness
Proven track record of success in producing high-quality data engineering solutions that meet customer requirements, as well as staying up to date on the latest developments in the field"
85,Data Engineer (Remote),Carvana,"Carmel, IN•Remote",N,"About ADESA...
ADESA, a Carvana-owned company, currently operates 56 locations throughout the US. Our Vehicle Service & Logistics Centers, some up to 200 acres, provide a wide array of vehicle services including repair & reconditioning, and auction remarketing. Many of our sites serve as market hub distribution centers. Our inventory comprises hundreds of thousands of vehicles across North America from retail to commercial, OEM & more.
We're excited about the future! As an industry leader, ADESA is poised for a multi-year expansion including huge investments in facilities, massive sales growth, and an ever-increasing inventory of vehicles! We are looking for great people who want to take this journey with us!
About the position...
We are looking for a data engineer with cloud data warehouse experience and a passion for turning data into information. You will develop data pipelines and data models that underpin our KPIs and business processes. This exciting, fast-paced role requires excellent organizational skills, critical thinking, problem-solving, and teamwork to enable business partners to make informed decisions.
What you'll be doing...
Implement data models and data engineering solutions for our cloud data warehouse
Work with business stakeholders to discover ROI and key success metrics to guide our investments in technology and improve our online operations
Analyze data to ensure quality and help assess whether business value can be achieved
Develop SQL to prototype concepts and troubleshoot issues
Create visual solutions to deliver to stakeholders
What you should have…
Experience engineering data ingestion and transformation solutions
Demonstrated ability to understand and implement data models, particularly Data Warehouse use cases
SQL and RDBMS experience (ex. Snowflake, RedShift, Oracle, SQL Server, MySQL, etc.)
Experience with cloud warehousing and analytics (ex. Snowflake, BigQuery, etc.)
OOP or functional programming experience (ex. Python, JavaScript, etc.)
Excellent analytical and problem-solving skills with the ability to analyze and break down problems
Drive to match data solutions to business goals
Ability to visualize data (Tableau experience preferred)
Strong oral and written communication skills
Motivated self-starter, ability to initiate and drive work to completion
Legal stuff…
Hiring is contingent on passing a complete background check. This role is eligible for visa sponsorship.
Carvana is an equal employment opportunity employer. All applicants receive consideration for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, marital status, national origin, age, mental or physical disability, protected veteran status, or genetic information, or any other basis protected by applicable law. Carvana also prohibits harassment of applicants or employees based on any of these protected categories.
Please note this job description is not designed to contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice."
86,Data Pipeline Engineer,Fuge Technologies Inc,"700 Gw Pkwy, McLean, VA 22101",$65 - $70 an hour,"Location: McLean, VA (3 days onsite -Tuesday, Wednesday, and Thursday)
no relocation candidates
Duration: 6 months contract, the opportunity for extension
Interview information:
Rounds: 2 Rounds ***2nd round will be In-Person and On-site***
Duration: 1st round - 45 Minutes, 2nd round - 30 Minutes
Additional Notes: 1st Round: MS Teams, Video Mandatory; 2nd Round: In-Person
Slots: 1st round: 4/17 @ 11:00am & 3:00pm; 4/18 @ 11:00am & 3:00pm
Must Haves: Not looking for an administrator here! Must have hands-on experience with the UNIX system, experience on the user side, and coding with Shell and Autosys. Expert-level SQL is required. Preference for experience with Snowflake. Must have hands-on experience with Hadoop.
Top skills required: Unix shell scripting, AutoSys, Hadoop, SQL, GIT, Jira, Bitbucket, and Jenkins
Control M: nice to have
Responsibilities include:
Evaluate and establish Operations Industrialization processes and work across various internal and external business customers and areas to help enhance technologies, operational and process efficiencies, and to improve the overall process. This includes participation in large-scale projects.
Interact with business teams and other stakeholders to understand and recommend operational processes and solutions to meet diverse, complex business requirements.
Manage GIT repositories, create baselines, and help in deployments.
Contribute and participate in the automation of the operational process and code reviews.
Develop and leverage SQL for validating data as well as SQL statements.
Run adhoc jobs in production as and when required to fix issues related to processes, data, etc.
Work in a small group and will need to support various ad hoc and on-going requests. This includes production operations support, development of procedures, etc.
Support end-user operational loads, and data issues and resolve them in a timely manner.
Qualifications:
Bachelor’s degree in Computer Science/Information Technology or related field and/or equivalent experience.
Preferred Skills:
5+ years of experience with the UNIX operating system mainly with LINUX (Mandatory).
3+ years of Shell scripting experience (Mandatory).
3+ years of Autosys experience (Mandatory).
3+ years of current and hands-on experience with SQL (Mandatory).
2+ years of experience with Hive queries in Big Data Platform - Hadoop (Mandatory).
Executing and monitoring jobs in a UNIX environment (Mandatory).
2+ years of experience with Source Code Control systems, preferably GIT, Bitbucket, etc. (Mandatory).
2+ years of AWS experience (optional).
2+ years of experience with Snowflake (preferred).
Running SQL queries from the command line.
Must be comfortable with analyzing large datasets (preferred).
Excellent oral and written communication skills.
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Mclean, VA 22101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location"
87,Data Engineer,TalkingPoints,Remote,N,"About TalkingPoints
TalkingPoints is an award-winning education technology non-profit organization with the mission to unlock the potential of low-income, underserved families to support their children’s learning by partnering with their children’s schools. We are building a one-of-a-kind product in the market, a multilingual family engagement platform that connects families and teachers using human and artificial intelligence powered, two-way translated messages and personalized supports — eliminating language, time limitations, and know-how as barriers to school/family connection.

Millions of teachers and families are using TalkingPoints in their classrooms, schools and districts across the United States. TalkingPoints works with key school district partners such as Oakland Unified, Seattle Public Schools, Boston Public Schools, Buffalo Public School, Wake County and NYC DOE. We have won awards from NYU, MIT, Google, Forbes and are backed by Google, Eric Schmidt, AT&T, Cisco, and Stanford University to name a few.

TalkingPoints is at a critical inflection point in the organization's growth. We continue to see exponential growth in both our user base and our own internal structure. It's an important time to join our team and build on our momentum and ability to support students and families during this critical time in which education equity gaps are more concerning than ever. Learn more.

Diversity: we celebrate it, we support it, and we thrive on it!

The opportunity
As our Data Engineer, you will be responsible for developing and maintaining automated ETL/ ELT data pipelines for migrating data to and from our centralized data warehouse. The Data Engineer will utilize database tools/technologies including AWS, Snowflake, and dbt, and will collaborate with non-technical stakeholders to understand data needs and build interactive dashboards to support self-service access patterns and decentralized ownership of data assets.

The ideal candidate is passionate about improving equity in education for under-resourced students through AI-based, data-informed solutions. The Data Engineer will report to the Head of Data, Analytics, and Research and will collaborate with a cross-functional team of skilled data engineers and researchers.

Responsibilities:
Technical Implementation and Stakeholder Engagement
Develop and maintain automated CI/CD data pipelines for migrating, storing, and transforming data across the life cycle from ingestion to consumption
Optimize existing data pipelines to improve speed and scalability
Develop and maintain unit, integration, and regression tests for ensuring the reliability and accuracy of data transformations
Work with stakeholders to gather requirements and co-develop analytics dashboards for self-service access
Debug and develop fixes for data integrity issues
Collaborate with data engineers and researchers to develop machine learning solutions for improving product implementation fidelity, informing new product development, and evaluating product efficacy
Stay up to date with industry developments and recommend new technologies and approaches to improve data infrastructure and processes

Who you are
3+ years of experience in a data-focused, technical role
Successful history of manipulating, processing, and extracting value from large disconnected datasets
Experience developing cloud-based data solutions using AWS and/or GCP services
Experience using cloud-based data services such as Snowflake and/or Databricks
Experience implementing event-driven data solutions using streaming technologies such as Kinesis and/or Kafka
Experience implementing CI/CD processes for improving the reliability and speed of ETL/ELT pipeline development
Advanced SQL knowledge and expertise in query optimization
Knowledge of data modeling and data warehousing concepts
Strong ability to communicate effectively and collaborate with technical and non-technical stakeholders
Excellent problem-solving and critical-thinking skills

Nice to haves:
Bachelor’s degree in Computer Science, Information Technology, or a related field
Domain knowledge in education and current educational technology trends
Experience with SIS integrations using tools such as Clever, Classlink, and/or PowerSchool
Experience designing REST API methods
Experience working with non-relational databases such as MongoDB
Experience supporting machine learning and/or research applications
Experience in ETL tools/technologies such as dbt

Overall fit with our org culture - we look for team members who are
User and mission-oriented: we are devoted to our mission with empathy towards the communities we serve.
All-in together: we go above and beyond the job description, working together as a team.
Agile & action-oriented: we get things done, knowing that our progress is urgent to our communities.
Always Learning: we are always learning, with a growth mindset, to reach our full potential.
Boldly Courageous: we take risks in order to achieve big things.
Resourceful: We are creative, solution-oriented, and scrappy.

What we offer
An incredible opportunity to build a mission-driven, rapidly growing startup tech nonprofit
Competitive salary, full coverage health insurance, benefits, and unlimited time off
401K match
Annual professional development benefit
Flexibility to work remotely
Fun, smart, dynamic, and motivated team

How to apply
Please submit your resume, a brief paragraph describing your interest, and a link to your LinkedIn profile"
88,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
89,Data Center Engineer 1,Tekrek Solutions Inc,"1075 W Entrance Dr, Auburn Hills, MI 48326",$40 - $45 an hour,"Requirements:
Bachelor's degree in Mechanical, Electrical Engineering, or IT required.
Minimum of 3 years' experience in facilities infrastructure. (must have experience in Data Center facilities and High Power Electrical equipment with knowledge of HVAC equipment)
Must understand As-Built drawings and electrical and mechanical operations of a Data Center.
Knowledge of various data center technologies and a strong understanding of high-availability data centers.
Understanding of network design principles.
Minimum of 3 years' experience in budget/finance experience.
Minimum of 3 years' experience in project management.
Read and interpret electrical and mechanical blueprints
On call 24 x 7 in a rotation order.
Able to lift 30 lbs.
Preferred Qualifications:
Master's degree
UST and AST Certification
PMP Certification
Capable of leading design and planning sessions including leading the collection of business requirements, determining the scope of the project, and aligning business needs with the appropriate technology.
Exceptional program/project management track record.
Excellent team work skills.
Strategic thinker with the ability to grasp business goals, strategies, and challenges.
Proven ability to multi-task is critical, excellent communication skills, both written and verbal, strong conflict management skills.
Carries out responsibilities in a professional, courteous manner at all times.
Notes:
Candidates must be able to work onsite at ICT.
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Benefits:
Health insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Auburn Hills, MI 48326: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data center: 3 years (Required)
Mechanical: 1 year (Required)
Electrical: 1 year (Required)
Finance/Budget: 1 year (Required)
Project management: 1 year (Required)
Security clearance:
Confidential (Preferred)
Work Location: One location

Health insurance"
90,Azure Data Engineer,Integration Developer Network LLC,Remote,$70 - $75 an hour,"Skills:
Strong Experience with Azure Cloud
Strong Experience With Databricks
Strong Experience With Python Programming
Experience with Spark, Hadoop, Hive, Pig etc. Proven expertise with extracting data from a wide variety of sources and transforming the data as needed.
Previous experience with statistical modeling and deep learning frameworks / libraries is a definite plus.
Strong Experience With Python Programming
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Azure: 10 years (Required)
Databricks: 8 years (Required)
ADLs: 8 years (Required)
Cosmos DB: 7 years (Required)
ADF: 7 years (Required)
Work Location: Remote
Speak with the employer
+91 7328323606"
91,Data Engineer,GovDocs,"Saint Paul, MN 55121•Hybrid remote","$90,000 - $120,000 a year","Flexible Work Model:
We believe that the best way for us to grow, is through a work environment that allows us more flexibility whereby employees can be together in the office where interactions can happen with higher frequency and effectiveness (collaboration and team-engagement) – especially when dealing with complex problems and business innovation, balanced with work-from-home where we have more focused, uninterrupted time with minimal distractions for dedicated project/productive work.
Best Places to Work 2021:
GovDocs was named one of the 2021 Best Places to Work by the Minneapolis/St. Paul Business Journal!
Being named an honoree was no easy feat, as they received over 300 nominations for this year’s award. GovDocs was one of the top-scoring Minnesota businesses honored in the medium company category (50-250 employees) for creating a fun, challenging, and rewarding workplace.
Read more about GovDocs as Best Places to Work 2021 here: https://bizj.us/1qbeau
Position Summary:
We are looking for a savvy Data Engineer to join our growing engineering team. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, and business systems team on data analytics and reporting initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities:
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure Data Lake technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Understanding of non-relational databases such as MongoDB desired
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Ability to build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Strong critical thinking, decision making, troubleshooting and problem-solving skills
Excellent oral and written communication skills
We are looking for a candidate with 3+ years of experience in a Data Engineer role. Bachelor’s Degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field a plus.
Technical Experience/Skills Required:
Experience in Azure Ecosystem (Azure Data lake, Azure Data Factory, Azure Data Bricks, Azure Storage, Cosmos DB, ADO)
Understanding and experience working with Microsoft Azure DevOps (work items, build/release, CICD)
Experience in Database Analysis and modern warehousing technologies
Understanding of code development best practices, process design and automation, security concepts, Agile concepts, tools, and technologies
Knowledge on Data Ingestion/ streaming tooling such as Kafka, Spark, or similar technologies
Familiarity with DevOps landscape, processes, standards, and tools
Experience in Agile frameworks and methodologies (Atlassian, Azure DevOps), Software Development Lifecycle (SDLC) experience is a plus.
Strong understanding of data Ingestion, data transformation, data management, data quality, and data lineage services and technologies
Compensation
This position is critical to the success of our business and the compensation package will be commensurate with candidate's experience and skills. Compensation will include base salary and performance-based incentives. Benefits include paid vacation, paid volunteer time and paid holidays, medical and dental, and matching 401(k).
Company Description
GovDocs serves companies in building and executing their employment law compliance programs in two primary ways:
Employment Law Posting Service: We manage all the complexities of identifying and providing the required set of employment law postings from the 1,700+ potential postings across the U.S. and Canada. Using proprietary technology, we allow companies to manage, track and verify postings at each of their locations – including our patent pending PosterCheck technology. Our Employment Law Posting Update service is used by almost 22% of the Fortune 500 and 30% of the Fortune 100.
Employment Law Compliance Software Solutions: This is a first to market Software-as-a-Service starting with our Minimum Wage product and Paid Leave products, which allow companies to identify and track which laws apply to their locations and employees, then provides all relevant data to make decisions. This service was created in response to requests from some of our largest customers who recognized that employment law expansion across and variances between jurisdictions (Federal, State and Local) were too difficult to manually track.
GovDocs has grown revenue annually by 18% since 2008 with a 96% customer retention rate, primarily due to our Postings Update Program both obtaining and retaining customers – as employment law postings are required by law, every company has an existing provider. Our Software Solutions are an entirely new line of business that gives us great growth potential to create a full Employment Law Compliance (ELC) platform of solutions, it is also challenging us to reinvent how we view urgency, innovation and teamwork.
You must be authorized to work in the United States. Immigration or work visa sponsorship will not be provided. In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire
Job Type: Full-time
Pay: $90,000.00 - $120,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Day shift
Monday to Friday
COVID-19 considerations:
Flexible work model with working from home options. We take action to protect the health and well-being of our colleagues by regular review of Covid-19 statistics and health guidelines.
Application Question(s):
Will you now or in the future require sponsorship for employment visa status?
What are your compensation requirements?
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Saint Paul, MN 55121

Health insurance"
92,[ No C2C ] Big Data - Informatica Developer / Support Engineer,Production Modeling Corporation,"Troy, MI","$145,000 - $157,000 a year","Note: This is not a C2C or 1099 position, per the client. This is a W2 contract engagement.
Location: Working for the MI office, remote. May asked to confirm your working location. May be required to visit the office in MI, VA, or TN to work with colleagues.
Duration: 3+ years
Description:
We are a prime vendor with the client
The following functions are the responsibility:
The Big Data Design and Support role will be responsible for the design and build of world class high-volume real-time data ingestion and processing frameworks and advanced analytics on big data platforms. He/She will research, develop, optimize, and innovate frameworks and patterns for enterprise scale data analysis and computations as part of our Big Data.
Strong Informatica knowledge and skills, especially Informatica Big Data Management.
Strong business/technical oral and written communication skills
Minimum 5 years work experience in IT and Data ecosystems
Experience in process analysis, requirements analysis and system engineering
Strong written and oral communication skills
Independent, structured and analytical work
4+ years’ programming/scripting languages Java and Scala, python, R, Pig
2+ years of experience of developing solutions in cloud environments (Azure, AWS etc) with focus on analytics stack.
2+ years of experience in big data streaming frameworks, data processing and real-time ingestion patterns.
Experience working with and evaluating open source technologies and demonstrated ability to make objective choices
Support on-call
Experience architecting highly scalable, highly concurrent and low latency systems
Experience scaling applications to processing multiple petabytes
Experience with Visualization Tools such as Tableau
Knowledge of software/data design methods, data structures, and modeling standards
Knowledge of Chef scripting, Docker containers
Working Experience with continuous integration and continuous delivery tools
Experience with multiple cloud computing platforms
Experience with more than one data streaming technologies
Understanding of Machine Learning skills (like Apache Mahout, Spark MLib)
Ability to communicate complex architectures and insights in a clear, precise, and actionable manner
Job Types: Full-time, Contract
Pay: $145,000.00 - $157,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee discount
Health insurance
Life insurance
Paid time off
Referral program
Vision insurance
Schedule:
8 hour shift
Work Location: In person

Health insurance"
93,Big Data Engineer,Jconnect Infotech Inc.,Remote,$50 - $60 an hour,"Position – Snowflake Data Engineer
Location – Initial Remote
Duration – Contract
Technical Skills: Python, AWS, SQL, Snowflake
Job Description:
Experience Required:
• Cloud data platform – Snowflake, SQL and deep knowledge of Data Ware house and datalake concepts
Experience on any data ingestion (ETL/ELT) tool.
Well versed in Snowflake architecture concepts.
Must have Hands-on experience with Snowflake utilities like Snow SQL, Snow pipe.
Must have implemented snowflake projects in production.
Strong problem Management, troubleshooting and analytical skills
Roles & Responsibilities:
Snowflake Developer is responsible for
Design, develop, test, deploy and maintain enterprise
level applications using snowflake platform.
Work with a variety of stockholders to understand
requirements and deliver solutions.
Create Integration design for projects that include
infrastructure details, major application flows/sequences and failover designs.
Take ownership of project and see it through completion.
Strong analytical and problem-solving skill.
Ability to work independently as well as part-of-team.
Job Type: Contract
Pay: $50.00 - $60.00 per hour
Schedule:
8 hour shift
Experience:
Snowflake: 3 years (Required)
SQL: 1 year (Preferred)
AWS: 4 years (Required)
Work Location: Remote"
94,Data Engineer,CVS Health,"Buffalo Grove, IL","$70,000 - $140,000 a year","CVS Health has a rich set of healthcare data of more than 150 million individuals and provides the best foundation for any ambitious data engineer to work on billions of records of healthcare data. The Data Engineering Advisor will lead the design of highly scalable and extensible data platforms including but not limited to RDBMS / Big Data / Cloud, which enables industrializing collection, storage, modeling, and analysis of massive data sets from heterogeneous channels.

Responsibilities:

1. Collect, analyze, and synthesize complex information from disparate sources into a single reporting view/data layer using SQL or similar tools. Summarize results and present findings to our non-analytic audiences
2. Conduct analyses of data, ranging from descriptive ad-hoc analyses to in-depth investigations of healthcare utilization trends/outcomes and prepare data summarization presentation for review by leadership team and key stakeholders
3. Apply problem-solving and analytical skills to conduct quality assurance testing on data from initial delivery to final merged product to ensure results are accurate and aligned with documented requirements
4. Support skill development of team by sharing knowledge, developing, and sharing new technical skills, and providing guidance and mentorship to other team members.
5. Ensure proper maintenance and centralized storage of key project documentation including business requirements, technical specifications, design documents, test plans, etc.
6. Construct and deliver written reports of analytic findings in a variety of formats (reports, PPT, including visualization of data and findings), formulate recommendations, and effectively present results to stakeholders
7. Lead discussions on data ingestion standardization, data conformance/quality and audit balance control
8. Build data semantic layer, test, validate and present to analytical and reporting team.
9. Design/develop data security model, views, tables for specific business use cases.
10. Build innovative, scalable, parameterized, efficient solutions around data

Pay Range
The typical pay range for this role is:
Minimum: 70,000
Maximum: 140,000

Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will take into account a wide range of factors, including location.

Required Qualifications
1 year of business experience in analytics or Advanced degree + 6 months’ experience
1 year of experience programming using R, Hadoop, Python, SQL or Advanced Degree
1 year of experience cloud computing or Advanced degree

Preferred Qualifications
Strong knowledge of analytics tools and languages to analyze large data sets from multiple data sources
Demonstrates proficiency in most areas of mathematical analysis methods, machine learning, statistical analyses, and predictive modeling and in-depth specialization in some areas.
Anticipates and prevents problems and roadblocks before they occur
Healthcare sector experience preferred
Demonstrates strong ability to communicate technical concepts and implications to business partners.
Experience as a Data Engineer within the cloud environment

Education
Bachelor’s Degree in Computer Science, Engineering, Statistics, Physics, Math, or related field:
or Advanced Degree

Preferred: Master’s Degree with coursework focused on advanced algorithms, mathematics in computing, data structures, or related field

Business Overview
Bring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities."
95,"Data Engineer, Analytics",Algo,"Troy, MI•Remote",N,"Responsibilities
Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems.
Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve.
Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights visually in a meaningful way.
Define and manage SLA for all data sets in allocated areas of ownership.
Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership.
Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains.
Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources.
Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts.
Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts.
Influence product and cross-functional teams to identify data opportunities to drive impact.
Mentor team members by giving/receiving actionable feedback.
Minimum Qualifications
5+ years experience in the data warehouse space.
5+ years experience in custom ETL design, implementation and maintenance.
5+ years experience with object-oriented programming languages.
5+ years experience with schema design and dimensional data modeling.
5+ years experience in writing SQL statements.
Experience analyzing data to identify gaps and inconsistencies.
Experience managing and communicating data warehouse plans to internal clients.
Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.
Preferred Qualifications
Experience working with either a MapReduce or an MPP system.
Knowledge and practical application of Python.
Experience working autonomously in global teams.
Experience influencing product decisions with data."
96,Data Engineer I,"ConstructConnect, Inc","Cincinnati, OH 45209",N,"ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.
The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.
The Opportunity
The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects to build and train predictive analytic models, design linear regression models using data
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Experience writing complex SQL Queries, Stored Procedures and Views
Experience building and optimizing data pipelines, architectures and data sets.
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
Understanding of version control and automated CI/CD with technologies such as Gitlab/ GitHub
1 – 3 years professional experience managing Data Lakes and Data Warehouses
Bachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)"
97,Data Engineer 3,WEX Inc.,Remote,"$97,500 - $130,000 a year","The base pay range represents the anticipated low and high end of the pay range for this position. Actual pay rates will vary and will be based on various factors, such as your qualifications, skills, competencies, and proficiency for the role. Base pay is one component of WEX's total compensation package. Most sales positions are eligible for commission under the terms of an applicable plan. Non-sales roles are typically eligible for a quarterly or annual bonus based on their role and applicable plan. WEX's comprehensive and market competitive benefits are designed to support your personal and professional well-being. Benefits include health, dental and vision insurances, retirement savings plan, paid time off, health savings account, flexible spending accounts, life insurance, disability insurance, tuition reimbursement, and more. For more information, check out the ""About Us"" section.
Salary Pay Range: $97,500.00 - $130,000.00"
98,Data Engineer,Starschema,"Arlington, VA•Remote",N,"Company Description

About Starschema

At Starschema we believe that data has the power to change the world and data-driven organizations are leading the way. We help organizations use data to make better business decisions, build smarter products, and deliver more value for their customers, employees and investors. We dig into our customers’ toughest business problems, design solutions and build the technology needed to address today’s unique challenges.

What you can expect as a Starschema team member
As a member of the Starschema team, you will be on the front lines of digital transformation, working with some of the most innovative Fortune 500 companies to drive innovation and realize the promise of data-driven cultures. You will learn and use the latest data-centric technologies along with the core industry technologies.
Our team is inclusive and fun. While we take our work seriously, we know how to have a good time while doing so. We encourage everyone to share their opinions and ideas, and our leadership wants to hear everyone’s input no matter what role they play in the organization.

Job Description

As a Data Engineer at Starschema, you will bring business value for our clients through end to end development, optimization and operation of automated reporting, data lakes and related software platforms. You will use the latest technologies like Apache Airflow, Apache Kafka, Apache Spark and AWS etc. We are seeking for experienced medior and senior professionals for our open position.
What will You do:
Build and maintain database/bigdata clusters;
Build dashboards for infrastructure management and reporting;
Design and deploy infrastructure management strategies to meet up time and monitoring SLA’s;
Deploy code release in QA and PROD;
Participate in building unit/performance/integration tests working with database developers;
Participate in database SQL optimization plan;
Deploy configuration and automation tools to remove manual steps in deploying, upgrading, and scaling systems and software across all environment.

Qualifications

We want to hear from you if You have:
At least 3 years of experience in data engineering field;
Solid background in Python and SQL;
Experience building data solutions using big data tools: Airflow, Spark, Kafka, AWS;
Experience with data pipeline and workflow management tools
Hands-on experience with requirements analysis, design, coding and testing patterns;
Has experience in engineering (commercial and open source) software platforms and large-scale data infrastructures;
Experience working with cloud computing environments;
Excellent communications skills in English (both written and oral);
Intelligent, communicative team-player personality, interested in and willing to learn new skills and technologies.

Additional Information

What's In It For You:
Remote work: You can work remotely from anywhere within the USA. Plus if you are based in Washington D.C area.
Eligibility: We are unable to support work visa for this specific position so we are open to receive application from candidates who are eligible to work in the USA.
Benefits & Community: A healthy lifestyle and the feeling of belonging are important to us, for both body and mind. We provide:
401K Insurance with matching
Employee Assistance Program (EAP)
Technical/Professional trainings
Remote work / Home Office opportunity
Start date: The sooner the better, but if you currently work somewhere and have a notice period, it is still fine, we will wait for the right person!"
99,Data Engineer - Advanced,Federal Reserve Bank of New York,"New York, NY•Hybrid remote","$96,000 - $120,000 a year","Company
Federal Reserve Bank of New York
Working at the Federal Reserve Bank of New York positions you at the center of the financial world with a unique perspective on national and international markets and economies. You will work in an environment with a diverse group of experienced professionals to foster and support the safety, soundness, and vitality of our economic and financial systems.

The Bank believes in work flexibility to balance the demands of work and life while also connecting and collaborating with our colleagues in person. Employees can expect to be in the office a couple of days per week as needed for meetings and team collaboration and should live within a commutable distance.
What we do:
The Data and Analytics chapter in the Technology Group builds data products that provide the organization with analytical capabilities in support of its mission. Reporting to the chapter lead for Data and Analytics, you will be part of a diverse, dynamic, and agile squad that is responsible for data pipelines, data integration, data quality, data visualization, self-service analytics and data catalog for the enterprise.
Your role as Data Engineer:
Quickly learn about the business domain and the associated data and analytics products that the team works on
Execute on the cloud migration using data platforms identified
Support and maintain existing data pipelines using legacy technology such as Informatica Power Center and Oracle while the cloud migration is underway
Develop new data pipelines as needed with wide-ranging source and target configurations in a customer facing role
Migrate on-premises data management products to AWS cloud as well as support hybrid configurations
Research, troubleshoot and recommend solutions to data integration and quality problems.
What we are looking for:
Technologist with background in data engineering and data integration with hands on experience
Experience in DW SaaS products such as Snowflake and AWS Data Services
Experience with ETL concepts and RDBMS
Alteryx and Tableau experience is preferred but not a must have
Experienced working in Agile product teams
Experience with Python in data engineering or application development
Understanding of fixed income products
Expertise in ETL and data integration techniques and practices
Knowledge of data architecture and data management best practices
Collaborative working style to support larger team goals and outcomes
Experience with data catalog tools like Collibra
Salary Range: $96000 - $120000 /year
We believe in transparency at the NY Fed. This salary range reflects a variety of skills and experiences candidates may bring to the job. We pay individuals along this range based on their unique backgrounds. Whether you’re stretching into the job or are a more seasoned candidate, we aim to pay competitively for your contributions.
Touchstone Behaviors set clear expectations for leading with impact at every stage of our careers and aspire to achieve in our continued growth and development.
Communicate Authentically: Empathetically engage one another with direct and transparent dialogue and listening. Actively discuss viewpoints with respect and compassion in a timely and candid manner, taking into account verbal and nonverbal cues. Ask questions, learn from each other, and share information widely to move the Bank's work forward.
Collaborate Inclusively: Inspire a diverse and inclusive environment that empowers others to contribute meaningfully. Intentionally bring a diverse set of people together to achieve positive business results.
Drive Progress: Grow and adapt to changing priorities in the Bank. Experiment with new concepts and take appropriate risk to drive innovation. Remain curious and action oriented, navigating through ambiguity and uncertainty to drive outcomes.
Develop Others: Equitably champion, mentor, and develop others to grow professionally. Demonstrate vulnerability and empathy to create a trusted environment.
Take Ownership: Establish an environment of action and excellence by holding self and others accountable to execute to the highest standard.
Benefits:
Our organization offers benefits that are the best fit for you at every stage of your career:
Fully paid Pension plan and 401k with Generous Match
Comprehensive Insurance Plans (Medical, Dental and Vision including Flexible Spending Accounts and HSA)
Subsidized Public Transportation Program
Tuition Assistance Program
Onsite Fitness & Wellness Center
And
more
Please note that the position requires access to confidential supervisory information and/or FOMC information, which is limited to ""Protected Individuals"" as defined in the U.S. federal immigration law. Protected Individuals include, but are not limited to, U.S. citizens, U.S. nationals, and U.S. permanent residents who either are not yet eligible to apply for naturalization or who have applied for naturalization within the requisite timeframe. Candidates who are permanent residents may be eligible for the information access required for this position if they sign a declaration of intent to become a U.S. citizen and pursue a path to citizenship and meet other eligibility requirements.
In addition, all candidates must undergo an enhanced background check, comply with all applicable information handling rules, and will be tested for all controlled substances prohibited by federal law, to include marijuana.
The Federal Reserve Bank of New York is committed to a diverse workforce and to providing equal employment opportunity to all persons without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, age, genetic information, disability, or military service.
This is not necessarily an exhaustive list of all responsibilities, duties, performance standards or requirements, efforts, skills or working conditions associated with the job. While this is intended to be an accurate reflection of the current job, management reserves the right to revise the job or to require that other or different tasks be performed when circumstances change.
Full Time / Part Time
Full time
Regular / Temporary
Regular
Job Exempt (Yes / No)
No
Job Category
Information Technology
Work Shift
First (United States of America)
The Federal Reserve Banks believe that diversity and inclusion among our employees is critical to our success as an organization, and we seek to recruit, develop and retain the most talented people from a diverse candidate pool. The Federal Reserve Banks are committed to equal employment opportunity for employees and job applicants in compliance with applicable law and to an environment where employees are valued for their differences.
Privacy Notice"
100,AWS Data engineer,TekWisen Software Pvt. Ltd,Remote,$60 - $75 an hour,"Job Title: Data Engineer - AWS
Location: Madison WI 53703 - Remote
Duration: 5 Months
Job Type: Contract (c2c and w2)
Work Type: 100% Remote in USA
Job Description
As a Staff Data Engineer on Foundation Analytics, you will be responsible for building many key parts of the foundation data services platform. You’ll work in a collaborative Agile environment using the latest in engineering best practices with involvement in all aspects of the software development lifecycle. You will be responsible for ensuring the team makes sound design & configuration decisions to develop curated data products, apply standard architectural practices, and supports the Data Product Managers in evolving core data products. As a member of the data engineering team, you will help client to deliver impactful reporting products for our customers.
Publishing well written and tested code to production daily using technologies such as Linux, Docker, Kubernetes, AWS, Kafka and Python Drive data architecture and integration design and development discussions with engineering and other teams Investigate production issues and fine-tune our data pipelines Build a platform that will be the foundation for our customer facing reporting features, our machine learning initiatives, and internal product analytics
Perform rapid proto typing Participate in designing, developing key features and functionality of our data platform Continually improve the data platform development for high efficiency, throughput and quality of data Collaborate with team members with researching & brainstorming different solutions for technical challenges facing the team
Develop standard methodologies and mentor other engineers on the team to help make technical decisions on our projects and roadmap.,
Skills
7+ years of software development/data engineering experience 4+ years of hands-on experience of building scalable data platforms and/or reliable data pipelines
Proficiency in at least one of the following programming languages: Java, Python, Scala
Experience with AWS.
Experience in developing and operating high volume, high availability environments Working understanding of Kubernetes’ infrastructure and security best practices
Ability to work effectively in a dynamic, occasionally interrupt driven environment that includes geographically spread teams and customers BS degree in Engineering, CS, or equivalent
Education
Experience writing ETL jobs to help address various data engineering challenges
Strong understanding of Build tools and Deployment tools Familiarity with Kafka, Flink, Spark frameworks with validated understanding of at least one job scheduling tool: Airflow, Celery, AWS Step functions Tech Stack Our data pipelines are written in Java and Python based software stacks
We utilize many open-source technologies, including Spark, Flink, Hudi, Airflow Our software runs on AWS services like EMR and in Kubernetes, and integrates with AWS services S3, Athena, and Glue for data access
Job Type: Contract
Salary: $60.00 - $75.00 per hour
Experience level:
10 years
11+ years
Schedule:
8 hour shift
Experience:
software development: 9 years (Required)
Data Engineer: 9 years (Required)
data platforms: 7 years (Required)
data pipelines: 7 years (Required)
AWS: 9 years (Required)
ETL: 9 years (Required)
EMR systems: 9 years (Required)
Work Location: Remote"
101,iPhone Data Analysis Engineer,Apple,"Austin, TX",N,"Summary
Posted: Feb 9, 2023
Weekly Hours: 40
Role Number: 200462069
Apple’s iPhone Hardware Engineering organization is looking for a highly motivated engineer with a real passion in both hardware and software. As a member of the iPhone modeling and algorithm team, you will work on opportunities to extend iPhone/iPad user experiences and improve battery life by studying data from the field, internal data sources and vendors. You will have the opportunity to influence the architectural roadmaps and high-level system specifications for the future iOS devices. This is an extraordinary cross-disciplinary opportunity and it requires intensive team collaboration effort coupled with proven system-engineering background.
Key Qualifications
Data analysis and automation - Significant expert at processing large volume of data, analysis, visualization analytics.
A deep understanding of probability, statistics, algorithms and mathematics.
Practical modern machine learning experiences in regression, classification and clustering.
Strong software skills (Matlab, C/C++, R, Python).
Excellent communication, persuasion and negotiation skills.
Proven self motivation and ability to work independently.
Understanding of control theory and how it relates to modeling Li-on batteries is a plus.
Understanding embedded system architecture is a plus.
Deep knowledge in key system-engineering architectural and implementation tradeoffs is a plus.
Description
We are looking for an engineer capable of handling challenges from “cradle to grave”. We love self-motivated teammates who are committed to continually innovate. Bring your passion and dedication and you will discover excitement and endless possibilities in the process of building our next-gen products in Apple. IN THIS ROLE, YOU WILL BE RESPONSIBLE FOR: - Support automated power data collection infrastructure for latest iPhone. - Build visualization tool for x-functional team to digest the power performance data - Perform data mining and data analysis across gigantic amount of data generated by multiple generation of iPhones - Leverage machine learning techniques to analyze the field data and provide the mentorship to the x-function teams for design decision making - Use case power analysis - Drive the future power-saving algorithms/techniques
Education & Experience
BS/MS/PHD EE Required
Additional Requirements"
102,Senior Data Engineer ( Remote - Eligible),Capital One,"McLean, VA•Remote",N,"Center 1 (19052), United States of America, McLean, Virginia
Senior Data Engineer ( Remote - Eligible)
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.

What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Capital One is open to hiring a Remote Employee for this opportunity.

Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies

Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of experience with Airflow
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices

At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.
Location is New York City: $161,900 - $184,800 for Senior Data Engineer
Location is San Francisco, California: $171,500 - $195,800 for Senior Data Engineer
Remote roles in other areas of New York & California, and across Colorado & Washington: $137,200 - $156,600 for Senior Data Engineer
Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter.
This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
No agencies please. Capital One is an Equal Opportunity Employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex, race, color, age, national origin, religion, physical and mental disability, genetic information, marital status, sexual orientation, gender identity/assignment, citizenship, pregnancy or maternity, protected veteran status, or any other status prohibited by applicable national, federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC)."
103,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
104,Data Engineer - Databricks,Penn Foster,Remote,N,"Position Summary:
If you have a passion for data, Penn Foster has the job for you. Come be a part of the fast-growing Data Engineering team as we migrate our existing Data Lake to an Azure Databricks platform. You will play a critical role in this highly visible and strategic project to revolutionize Penn foster’s data capabilities. The Data Engineer is primarily responsible for the development of Python/PySpark code. The ideal candidate will have experience converting existing processes from legacy platform to Python/PySpark. This new platform will allow Penn Foster to expand its Analytics and Machine Learning capabilities.
Essential Job Functions:
Developing Python, SQL and PySpark based applications
Participating in code reviews
Estimating level of effort for assigned tasks and adhering to schedules
Worked in with Agile development processes
Be a complete team player
Comfortable working in a fluid constantly changing environment
Strong sense of ownership for all work
Knowledge, Skills, Abilities:
5+ years of Data Engineering experience
5+ years of Python and Spark
5+ years of SQL
Experience with large data centric projects and data migration projects
Ability to learn and absorb existing and new data structures
Jira and Git exposure
Experience working in a cloud environment (Azure preferred)
Databricks exposure is a huge plus
Tableau experience is a plus
Familiarity with Agile and Iterative Development (Kanban preferred)
Excellent interpersonal and communication skills (written and verbal)
Ability to work independently and in a group
Self-starter attitude with initiative
Creativity
Ability to solve complex problems

Equal Employment Opportunity:
At Penn Foster we are proud to be an Equal Employment Opportunity employer. We are committed to creating a work environment that embraces and celebrates diversity. We encourage underrepresented groups to apply. We do not discriminate based on race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other status protected under federal, state, or local law.
About Us:
At Penn Foster, we are dedicated to helping over 300,000 students each year achieve their goals through affordable, accessible, career-focused learning. Our mission has remained the same since 1890: to enhance the lives of our students and clients through the acquisition of skills and credentials that can help them work toward their career and life goals. Together with our extensive partner network of leading employers, community-based organizations, and academic institutions, we close skills gaps and are building a workforce that’s prepared for the future job market. We aim to help businesses thrive by mobilizing their individual workers and energizing communities with opportunities for growth and progress. We are proud to play a role in the success of over 80% percent of our graduates that see improvement within their careers, as they inspire us to keep finding new ways to further our reach and broaden horizons. Join the Penn Foster movement and start working toward a better future today.
What We Offer:
We offer a competitive base salary, plus a robust benefits package that includes medical, dental, vision, flexible spending, generous paid time off, sponsored volunteer opportunities, parking & commuter benefits, a 401K with a company match, plus free access to all of our online programs.
F1VTuRj2Xy"
105,Data Engineer (Remote - US),Bon Secours Mercy Health,"Cincinnati, OH 45237•Remote",N,"At Bon Secours Mercy Health, we are dedicated to continually improving health care quality, safety and cost effectiveness. Our hospitals, care sites and clinicians are recognized for clinical and operational excellence.
Summary of Primary Function/General Purpose of Position
The Data Engineer works closely with a multidisciplinary Agile team to build high quality data pipelines driving analytic solutions. The solutions generate insights from the organization's connected data, enabling the organization to advance the data-driven decision-making capabilities of the organization's enterprise.
Essential Job Functions
Develops maintained data architecture and pipelines that adhere to ETL (Extract, Transform, and Load) principles and business goals.
Solves complex data problems to deliver insights that helps the ministry achieve goals.
Understands modeling process, major groups of algorithmic techniques, and model testing techniques.
Learns about machine learning, data science, computer vision, artificial intelligence, statistics, and applied mathematics.
Partners with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Understands platforms available and how they are used (e.g. Azure, Hadoop, Bluemix).
Performs tasks related to data ETL (Extract, Transform, and Load), data structuring and cleaning, and data pipeline creation.
Understands rules on data, privacy and security, and can ensure quality of data.
Integrates data from multiple sources.
This document is not an exhaustive list of all responsibilities, skills, duties, requirements, or working conditions associated with the job. Employees may be required to perform other job-related duties as required by their supervisor, subject to reasonable accommodation.
Licensing/Certification
None
Education
Bachelors (required)
Masters, Computer Science, Management Information Systems, or Engineering (preferred)
Doctorate, Computer Science, Management Information Systems, or Engineering (preferred)
Work Experience
1 year experience in a data engineering or architecture role (required)
3+ years' experience in a data engineering or architecture role (preferred)
Training
None
Language
None
Skills
SQL and data analysis
Programming (e.g., Python, Scala)
Development in cloud computing services (Azure)
Database development (e.g., Hadoop or BigQuery)
Data warehouse maintenance
BI tools (e.g., Tableau, PowerBI)
Data governance
Structured and unstructured data
Big-picture and detail-oriented thinking
Data modeling & pipeline building
Critical thinking
Relationship building
Ability to work with technical and non-technical stakeholders
Requirements translation
Agile work process
Continuous learner
Familiarity with machine learning
Microsoft Suite
Cloud platform knowledge (e.g., Azure Data Factory, ADLS Gen2, Snowflake)
Coding and scripting skills (e.g., Python, Spark)
Communication skills
Continuous integration tools (e.g., Jenkins, Maven, Git)
SQL Server
Translation of complex technical concepts
Understanding of interplay of analytics components with enterprise architecture
Working Conditions
Periods of high stress and fluctuating workloads may occur
General office environment
Physical Requirements
Lifting/ Carrying (0-50 lbs.) 0%
Lifting/ Carrying (50-100 lbs.) 0%
Push/ Pull (0-50 lbs.) 0%
Push/ Pull (50-100 lbs.) 0%
Stoop, Kneel 0%
Crawling 0%
Climbing 0%
Balance 0%
Bending 0%
Sitting 67-100%
Walking 0%
Standing 0%
Additional Physical Requirements/Hazards
None
Patient Population
Not applicable to this position
Many of our opportunities reward* your hard work with:

Comprehensive, affordable medical, dental and vision plans
Prescription drug coverage
Flexible spending accounts
Life insurance w/AD&D
Employer contributions to retirement savings plan when eligible
Paid time off
Educational Assistance
And much more

Benefits offerings vary according to employment status
All applicants will receive consideration for employment without regard to race, color, national origin, religion, sex, sexual orientation, gender identity, age, genetic information, or protected veteran status, and will not be discriminated against on the basis of disability. If you'd like to view a copy of the affirmative action plan or policy statement for Mercy Health – Youngstown, Ohio or Bon Secours – Franklin, Virginia; Petersburg, Virginia; and Emporia, Virginia, which are Affirmative Action and Equal Opportunity Employers, please email recruitment@mercy.com. If you are an individual with a disability and would like to request a reasonable accommodation as part of the employment selection process, please contact The Talent Acquisition Team at recruitment@mercy.com"
106,Senior Data Engineer,Southwest Airlines,"2430 Shorecrest Dr, Dallas, TX 75235","From $137,250 a year","Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. The Senior Data Engineer manages a variety of projects and applies advanced knowledge of software development and data transformation methodologies to identify and prevent production problems as well as implement immediate resolution efforts for application systems. For example, t his role also focus es on data integration and Extract, Transfer, Load (ETL) including working with data targets and sources of databases, flat files, XML, and JSON. They will also oversee and provide production support for Teradata and big data platforms. The Senior Data Engineer is a strong communicator with a strategic mindset who looks forward to being a part of impactful initiatives that drive the future of Technology at Southwest.

Additional details:
This role is offered as a remote workplace position, which may require travel for trainings , meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is limited group of states or localities ineligible for Employees to regularly perform their work off-site. Those ineligible locations are: Alaska, Delaware, New Jersey, North Dakota, South Dakota, Vermont, West Virginia, and Wyoming.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand the implications of new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material
Education
Required: High School Diploma or GED
Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training
Experience
Required: Advanced level experience, seasoned and specialized knowledge in:
Cloud infrastructure, DataLake
ETL experience ensuring source to target data integrity
Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque)
ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience
Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies
Agile development experience and Agile ceremonies and practices
Licensing/Certification
N/A
Physical Abilities
Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time
Ability to communicate and interact with others in the English language to meet the demands of the job
Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job
Other Qualifications
Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines
Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986
Must be at least 18 years of age
Must be able to comply with Company attendance standards as described in established guidelines
Pay & Benefits
Competitive market salary from $137,250 per year to $152,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you'll love:
Fly for free, as a privilege, on any open seat on all Southwest flights—your eligible dependents too.
Up to a 9.3% 401(k) Company match, dollar for dollar, per paycheck.*
Potential for annual ProfitSharing contribution toward retirement - when Southwest profits, you profit.**
Explore more Benefits you’ll love: swa.is/benefits

Pay amount doesn’t guarantee employment for any particular period of time
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made a t the discretion of the Company

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply."
107,Data Engineer,Violet Ink,"Newark, NJ 07107•Hybrid remote",From $60 an hour,"Key Job Responsibilities
· Analyze data needs and objectives within the broader journey.
· Source, analyze and organize raw data, prepare data for transformation and consumption.
· Identify ways to improve data governance, reliability, efficiency, and quality.
· Build applications ensuring that the code follows latest coding practices and industry standards.
· Build using modern design patterns and architectural principles.
· Ensure developed solutions remain compliant with all applicable Prudential standards.
· Solve complex problems and provides new perspective on existing problems.
· Develop through collaboration and deliver application component solutions.
· Develop high quality, well documented, and efficient code supporting testing and automation.
· Support product owner in defining future stories and tech lead in defining technical designs.
Competencies – Knowledge, Skills, Abilities
Candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. Should have experience using following software/tools:
Big data tools
Relational and NoSQL databases
Data pipeline and workflow management tools
AWS cloud services
Stream processing systems
Object oriented and scripting language
Build processes supporting data transformation, data structure, metadata, dependency, and workload management.
Successful history of manipulating, processing, and extracting value from large, disconnected structured and unstructured datasets.
Advanced working SQL knowledge and experience working with relational databases.
Experience building and optimizing data pipelines, architecture, and data sets.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Strong project management and organization skills.
Experience supporting and working with agile cross functional teams in a dynamic environment
Background in financial services functions strongly desirable.
Job Type: Contract
Pay: From $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newark, NJ 07107: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
No SQL: 1 year (Required)
Work Location: Hybrid remote in Newark, NJ 07107"
108,Contract Senior Data Integration BA/Engineer,Trf Technologies,"New York, NY",$100 - $120 an hour,"JOB TITLE : Contract Senior Data Integration BA/Engineer with Financial Services Experience
MUST be local to New York City. Work 3 days in office/2 days remote
Major Responsibilities:
BIG PLUS: Experience with Axiom or other Regulatory Reporting
This position is 30-40% Business Analyst/60-70% SQL/ETL Engineer
Experience designing and developing Enterprise Data Warehouse solutions.
Demonstrated proficiency with Data Analytics, Data Insights
Proficient writing SQL queries and programming including stored procedures and reverse engineering existing process
Leverage SQL, programming language (Python or similar) and/or ETL Tools (Azure Data Factory, Data Bricks, Talend and SnowSQL) to develop data pipeline solutions to ingest and exploit new and existing data sources.
Perform code reviews to ensure fit to requirements, optimal execution patterns and adherence to established standards.
Qualifications:
10+ years - Enterprise Data Management
10+ years - SQL Server based development of large datasets
5+ years with Data Architecture
3+ years’ experience in Finance / Banking industry – some understanding of Securities and
Banking products and their data footprints.
2+ years Python coding experience
Proficient with Data Visualization tools
Hands-on experience with Snowflake utilities such as SnowSQL and SnowPipe
Working knowledge of MS Azure configuration items with respect to Snowflake.
Hands-on experience with Tasks, Streams, Time travel, Optimizer, Metadata Manager, data sharing
Experience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modeling.
Previous experience leading an enterprise-wide Cloud Data Platform migration with strong architectural and design skills
Capable of discussing enterprise level services independent of technology stack
Experience with Cloud based data architectures, messaging, analytics
Superior communication skills
Cloud certification(s)
Any experience with Regulatory Reporting is a Plus
Education:
Minimally a BA degree within an engineering and/or computer science discipline
Master’s degree strongly preferred
Job Types: Full-time, Contract
Pay: $100.00 - $120.00 per hour
Schedule:
Day shift
Monday to Friday
Ability to commute/relocate:
New York, NY: Reliably commute or planning to relocate before starting work (Required)
Education:
Master's (Required)
Experience:
Axiom or other Regulatory Reporting: 1 year (Required)
SQL: 1 year (Required)
Enterprise Data Management: 10 years (Required)
Finance / Banking industry: 3 years (Required)
Work Location: In person"
109,Data Engineer - Databricks,Penn Foster,Remote,N,"Position Summary:
If you have a passion for data, Penn Foster has the job for you. Come be a part of the fast-growing Data Engineering team as we migrate our existing Data Lake to an Azure Databricks platform. You will play a critical role in this highly visible and strategic project to revolutionize Penn foster’s data capabilities. The Data Engineer is primarily responsible for the development of Python/PySpark code. The ideal candidate will have experience converting existing processes from legacy platform to Python/PySpark. This new platform will allow Penn Foster to expand its Analytics and Machine Learning capabilities.
Essential Job Functions:
Developing Python, SQL and PySpark based applications
Participating in code reviews
Estimating level of effort for assigned tasks and adhering to schedules
Worked in with Agile development processes
Be a complete team player
Comfortable working in a fluid constantly changing environment
Strong sense of ownership for all work
Knowledge, Skills, Abilities:
5+ years of Data Engineering experience
5+ years of Python and Spark
5+ years of SQL
Experience with large data centric projects and data migration projects
Ability to learn and absorb existing and new data structures
Jira and Git exposure
Experience working in a cloud environment (Azure preferred)
Databricks exposure is a huge plus
Tableau experience is a plus
Familiarity with Agile and Iterative Development (Kanban preferred)
Excellent interpersonal and communication skills (written and verbal)
Ability to work independently and in a group
Self-starter attitude with initiative
Creativity
Ability to solve complex problems

Equal Employment Opportunity:
At Penn Foster we are proud to be an Equal Employment Opportunity employer. We are committed to creating a work environment that embraces and celebrates diversity. We encourage underrepresented groups to apply. We do not discriminate based on race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other status protected under federal, state, or local law.
About Us:
At Penn Foster, we are dedicated to helping over 300,000 students each year achieve their goals through affordable, accessible, career-focused learning. Our mission has remained the same since 1890: to enhance the lives of our students and clients through the acquisition of skills and credentials that can help them work toward their career and life goals. Together with our extensive partner network of leading employers, community-based organizations, and academic institutions, we close skills gaps and are building a workforce that’s prepared for the future job market. We aim to help businesses thrive by mobilizing their individual workers and energizing communities with opportunities for growth and progress. We are proud to play a role in the success of over 80% percent of our graduates that see improvement within their careers, as they inspire us to keep finding new ways to further our reach and broaden horizons. Join the Penn Foster movement and start working toward a better future today.
What We Offer:
We offer a competitive base salary, plus a robust benefits package that includes medical, dental, vision, flexible spending, generous paid time off, sponsored volunteer opportunities, parking & commuter benefits, a 401K with a company match, plus free access to all of our online programs.
F1VTuRj2Xy"
110,Software Engineer - Data Processing and Machine Learning Platform,Hewlett Packard Enterprise,"Andover, MA 01810•Hybrid remote","$124,500 - $286,300 a year","Software Engineer - Data Processing and Machine Learning Platform

This role has been designated as ‘Edge’, which means you will primarily work outside of an HPE office.
Job Description:

Hewlett Packard Enterprise is the global edge-to-cloud company advancing the way people live and work. We help companies connect, protect, analyze, and act on their data and applications wherever they live, from edge to cloud, so they can turn insights into outcomes at the speed required to thrive in today’s complex world. Our culture thrives on finding new and better ways to accelerate what’s next. We know diverse backgrounds are valued and succeed here. We have the flexibility to manage our work and personal needs. We make bold moves, together, and are a force for good. If you are looking to stretch and grow your career our culture will embrace you. Open up opportunities with HPE.
Are you looking for a unique, truly innovative role? What if it could be with one of the most impactful IT companies in the world? Then we have the right opportunity— we are looking for a Software Engineer who will both provide support and lead teams through the Engineering development process and implementation of company’s products. Projects are typically shorter-term, less complex and more contained with a defined time frame. Programs are typically longer-term, multi-functional, multi-project with complex requirements and effort.
You will be joining an agile, empowered team, focused on building a brand new data processing platform that enables and powers the analytics that are at the core of HPE Greenlake. The HPE GLCP Data Platform provides abstractions, components and infrastructure to create a state-of-the-art platform for running data processing and machine learning workloads. The team leverages the latest big data and microservice-based technologies to build out the data processing platform – Apache Spark, Delta Lake, Apache Pulsar, Kubernetes, Istio, knative, GraphQL, TimescaleDB, Postgres. The development stack is Scala (cats, http4s, Caliban, shapeless, lagom).
Responsibilities:
Technical contributor, as a software developer, in a cross-functional development team, focused on building out a full featured data processing platform
Leverage big-data technologies for data processing, including Apache Spark, Kubernetes, Apache Pulsar, AWS (Lambda, S3)
Contribute to the data platform ecosystem, including implementation of microservices or serverless components using Scala, http4s
Work with the Data Scientists to create and evolve the data platform to enable both exploratory analytics and Machine Learning
Work with the DevOps engineers to design and build observability features (telemetry, tracing) and CI/CD
Develop unit, integration, system or any tests that are needed to help the team deliver value quickly, with a high degree of quality
Education and Experience
Bachelor/Master's degree in Computer Science/Engineering program, or equivalent
8+ years of Software Engineering experience
Knowledge and Skills
Strong software engineering/development skills. Practical experience with Apache Spark
Strong distributed systems knowledge
Data processing experience at scale
Practical experience designing and implementing microservices
Team player with a passion for learning, programming, automation, and data processing
Excellent analytical and problem solving skills
Excellent communications skills
Scala (practical experience with functional programming would be a plus)
AWS or other public cloud provider
Akka (experience with Lagom and overall experience with CQRS/Event Sourcing would be a plus)
What we can offer you:
Extensive benefits, a competitive salary and participation in the shared values and purpose that make Hewlett Packard Enterprise one of the world´s most attractive employers! At HPE, our goal is to provide equal opportunities, flexible work-life balance, and constantly evolving career growth.
If you are looking for challenges in an exciting, supportive, and international work environment, then we definitely want to hear from you. Continue the conversation by clicking apply now below, or directly via our Careers Portal at www.hpe.com/careers.
Then let’s stay connected!
Find out more about us and follow us on:
https://www.facebook.com/HPECareers
https://twitter.com/HPE_Careers
HPE is an Equal Employment Opportunity/ Veterans/Disabled/LGBT and Affirmative Action employer. We are committed to diversity and building a team that represents a variety of backgrounds, perspectives, and skills. We do not discriminate and all decisions we make are made on the basis of qualifications, merit, and business need. Our goal is to be one global diverse team that is representative of our customers, in an inclusive environment where we can continue to innovate and grow together.
#Diversity #unitedstates #hpeocto #octo #LI-Hybrid
Job:
Engineering
Job Level:
Expert

States with Pay Range Requirement
The expected salary/wage range for a U.S.-based hire filling this position is provided below. Actual offer may vary from this range based upon geographic location, work experience, education/training, and/or skill level. If this is a sales role, then the listed salary range reflects combined base salary and target-level sales compensation pay. If this is a non-sales role, then the listed salary range reflects base salary only. Variable incentives may also be offered. Information about employee benefits offered can be found at https://myhperewards.com/main/new-hire-enrollment.html.
Annual Salary: $124,500.00 - $286,300.00
Hewlett Packard Enterprise is EEO F/M/Protected Veteran/ Individual with Disabilities.

HPE will comply with all applicable laws related to employer use of arrest and conviction records, including laws requiring employers to consider for employment qualified applicants with criminal histories."
111,Senior Data Engineer ( Remote - Eligible),Capital One,"McLean, VA•Remote",N,"Center 1 (19052), United States of America, McLean, Virginia
Senior Data Engineer ( Remote - Eligible)
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.

What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Capital One is open to hiring a Remote Employee for this opportunity.

Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies

Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of experience with Airflow
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices

At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.
Location is New York City: $161,900 - $184,800 for Senior Data Engineer
Location is San Francisco, California: $171,500 - $195,800 for Senior Data Engineer
Remote roles in other areas of New York & California, and across Colorado & Washington: $137,200 - $156,600 for Senior Data Engineer
Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter.
This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
No agencies please. Capital One is an Equal Opportunity Employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex, race, color, age, national origin, religion, physical and mental disability, genetic information, marital status, sexual orientation, gender identity/assignment, citizenship, pregnancy or maternity, protected veteran status, or any other status prohibited by applicable national, federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC)."
112,Data Engineer,Apple,"Cupertino, CA",N,"Summary
Posted: Dec 22, 2021
Weekly Hours: 40
Role Number: 200327520
As part of our Video Engineering group, you’ll help deliver creative solutions to various problems that could impact the people all over the world. This Data Engineer will work closely with other members of the Video Engineering group to mine data, implement model evaluation pipeline, analyze large scale data, visualize data, and ensure the delivery is of the highest quality. This position will also require strong coding skills, presentation skills, and collaborating with multiple teams (ex: machine learning, cloud infrastructure support).
Key Qualifications
A curious mind
An obsession for quality
Background in Data science, Data mining, Multivariate statistics, Computer vision, Machine learning
Experience working with large scale data sets
Solid programming skills including:
Python
C/C++
Experience with data visualization and presentation, familiar with data analysis tools such as Tableau
Excellent problem solving and communication skills
Description
The responsibilities of this position includes the following for current and future products: - Implement algorithm evaluation methods - Analyze data and build data analysis tools - Deep-dive failure analysis - Discover new perspectives for old data - Produce / Present meaningful data visualization to higher-ups and across various involved teams
Education & Experience
Masters in Computer Science or relevant experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $104,000 and $190,000 annualized, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program."
113,Sr Java Data Engineer,Intone Networks,"Plano, TX",N,"QUALIFICATIONS · 5+ years of total IT experience · 3+ years of experience as a Data Engineer, having built large and complex data pipelines. · Experience designing, building and operationalizing large scale enterprise data solutions and applications using AWS data and analytics services in combination with other services/platforms o Spark, EMR, RedShift, Kinesis, Kinesis Firehose, Athena, Lambda & AWS Glue · Thorough knowledge of AWS concepts for use in day-to-day development regarding security, networking, and other concepts. o This will require understanding how IAM Roles, VPCs, Security Groups, Access Control Lists, Lifecycle Policies, etc., and will impact the products built in this role. o Must also know how to adjust those pieces. · Experience integrating with orchestration tools for data pipeline and workflow management such as: AWS Step Functions, Airflow, etc. · Experience implementing streaming big-data pipelines as well as batch job flows in AWS · Able to implement a change data capture (CDC). Needs to have this experience. · Able to support and contribute towards the building of end-to-end products on AWS in all aspects including Development, DevOps, designing, architecting, testing, and maintenance/operations. · Advanced working SQL knowledge and experience working with relational databases and NoSQL databases. · Experience designing and build production data pipelines from ingestion to consumption within a big data architecture, using Java, Scala. · Must have experience with infrastructure-as-code tools such as Terraform (preferably) or CloudFormation. · Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations. · Understanding of concepts regarding security, privacy, performance, etc. · Kubernetes experience for developing, deploying, and orchestrating microservices."
114,Data Engineer,TalkingPoints,Remote,N,"About TalkingPoints
TalkingPoints is an award-winning education technology non-profit organization with the mission to unlock the potential of low-income, underserved families to support their children’s learning by partnering with their children’s schools. We are building a one-of-a-kind product in the market, a multilingual family engagement platform that connects families and teachers using human and artificial intelligence powered, two-way translated messages and personalized supports — eliminating language, time limitations, and know-how as barriers to school/family connection.

Millions of teachers and families are using TalkingPoints in their classrooms, schools and districts across the United States. TalkingPoints works with key school district partners such as Oakland Unified, Seattle Public Schools, Boston Public Schools, Buffalo Public School, Wake County and NYC DOE. We have won awards from NYU, MIT, Google, Forbes and are backed by Google, Eric Schmidt, AT&T, Cisco, and Stanford University to name a few.

TalkingPoints is at a critical inflection point in the organization's growth. We continue to see exponential growth in both our user base and our own internal structure. It's an important time to join our team and build on our momentum and ability to support students and families during this critical time in which education equity gaps are more concerning than ever. Learn more.

Diversity: we celebrate it, we support it, and we thrive on it!

The opportunity
As our Data Engineer, you will be responsible for developing and maintaining automated ETL/ ELT data pipelines for migrating data to and from our centralized data warehouse. The Data Engineer will utilize database tools/technologies including AWS, Snowflake, and dbt, and will collaborate with non-technical stakeholders to understand data needs and build interactive dashboards to support self-service access patterns and decentralized ownership of data assets.

The ideal candidate is passionate about improving equity in education for under-resourced students through AI-based, data-informed solutions. The Data Engineer will report to the Head of Data, Analytics, and Research and will collaborate with a cross-functional team of skilled data engineers and researchers.

Responsibilities:
Technical Implementation and Stakeholder Engagement
Develop and maintain automated CI/CD data pipelines for migrating, storing, and transforming data across the life cycle from ingestion to consumption
Optimize existing data pipelines to improve speed and scalability
Develop and maintain unit, integration, and regression tests for ensuring the reliability and accuracy of data transformations
Work with stakeholders to gather requirements and co-develop analytics dashboards for self-service access
Debug and develop fixes for data integrity issues
Collaborate with data engineers and researchers to develop machine learning solutions for improving product implementation fidelity, informing new product development, and evaluating product efficacy
Stay up to date with industry developments and recommend new technologies and approaches to improve data infrastructure and processes

Who you are
3+ years of experience in a data-focused, technical role
Successful history of manipulating, processing, and extracting value from large disconnected datasets
Experience developing cloud-based data solutions using AWS and/or GCP services
Experience using cloud-based data services such as Snowflake and/or Databricks
Experience implementing event-driven data solutions using streaming technologies such as Kinesis and/or Kafka
Experience implementing CI/CD processes for improving the reliability and speed of ETL/ELT pipeline development
Advanced SQL knowledge and expertise in query optimization
Knowledge of data modeling and data warehousing concepts
Strong ability to communicate effectively and collaborate with technical and non-technical stakeholders
Excellent problem-solving and critical-thinking skills

Nice to haves:
Bachelor’s degree in Computer Science, Information Technology, or a related field
Domain knowledge in education and current educational technology trends
Experience with SIS integrations using tools such as Clever, Classlink, and/or PowerSchool
Experience designing REST API methods
Experience working with non-relational databases such as MongoDB
Experience supporting machine learning and/or research applications
Experience in ETL tools/technologies such as dbt

Overall fit with our org culture - we look for team members who are
User and mission-oriented: we are devoted to our mission with empathy towards the communities we serve.
All-in together: we go above and beyond the job description, working together as a team.
Agile & action-oriented: we get things done, knowing that our progress is urgent to our communities.
Always Learning: we are always learning, with a growth mindset, to reach our full potential.
Boldly Courageous: we take risks in order to achieve big things.
Resourceful: We are creative, solution-oriented, and scrappy.

What we offer
An incredible opportunity to build a mission-driven, rapidly growing startup tech nonprofit
Competitive salary, full coverage health insurance, benefits, and unlimited time off
401K match
Annual professional development benefit
Flexibility to work remotely
Fun, smart, dynamic, and motivated team

How to apply
Please submit your resume, a brief paragraph describing your interest, and a link to your LinkedIn profile"
115,Data Visualization Engineer,Colgate-Palmolive,"Piscataway, NJ 08854•Hybrid remote","$100,000 - $147,000 a year","No Relocation Assistance Offered
# 152409 - Piscataway, New Jersey, United States
Do you want to be part of a team that is building a future to smile about? What about having the opportunity to connect with others across the world, full of stimulating discussions, and making impactful contributions?

If this is how you see your career, Colgate is the place to be!

Our dependable household brands, dedicated employees, and sustainability commitments make us a company passionate about building a future to smile about for our employees, consumers, and surrounding communities. The pride in our brand fuels a workplace that encourages creative thinking, champions experimentation, and promotes authenticity which has contributed to our enduring success.

If you want to work for a company that lives by their values, then give your career a reason to smile...every single day.

The Data Engineering and Visualization team is looking for a Visualization Engineer with strong technical skills and business savvy to work with stakeholders across the organization to solve strategic problems that influence the company’s growth. The role will be responsible for working with our business partners to build visualizations that align with our strategic directives and teach and inspire our partners to use visualization tools independently. The work has a direct and measurable impact on issues that affect many aspects of company success (revenue growth management, digital transformation, environmental sustainability, etc.)

What you will do
Translate data analysis requests into solutions using a variety of data analysis/visualization tools
Understand complex business processes and translate them into requirements for visualizations
Influence the use of new visualization tools adopted by the company
Democratize data and insights through strategic data structuring and use of visualization tools

What we are looking for
Excellent communication skills and strong technical/business acumen to uncover and present insights that drive strategic decisions
Ability to manage multiple deliverables and meet deadlines
Excellent written and verbal communication skills: the ability to tell a compelling story with data
Strong verbal and written communication skills
Consistent attention to detail
Track record of working independently and solving problems creatively, with a sense of accountability

Required Qualifications
Bachelor's degree in Computer Science, Management Information Systems, or related field.
Experience with data visualization tools (such as Domo, Google Data Studio, Tableau, or similar)
Experience writing complex SQL queries to pull data into visualization tools
Hands-on experience in extracting data from various data sources and building complex data models (Star Schema/Snowflake)

#LI-Hybrid
Salary Range $100,000- $147,000 USD

Pay is based on several non discriminatory factors including but not limited to experience, education, skills and office location. In addition to your salary, Colgate-Palmolive offers a performance based bonus and competitive benefits package.

Equal Opportunity Employer
Colgate is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity, sexual orientation, national origin, ethnicity, age, disability, marital status, veteran status (United States positions), or any other characteristic protected by law.

Are you interested in working for Colgate-Palmolive? You can apply online and attach all relevant documents such as a cover letter and resume or CV. Applications received by e-mail are not considered in the selection process. Become part of our team. We look forward to your application.
Colgate-Palmolive is a leading global consumer products company, tightly focused on Oral Care, Personal Care, Home Care and Pet Nutrition. Colgate sells its products in over 200 countries and territories around the world under such internationally recognized brand names as Colgate, Palmolive, elmex, Tom’s of Maine, Sorriso, Speed Stick, Lady Speed Stick, Softsoap, Irish Spring, Protex, Sanex, Elta MD, PCA Skin, Ajax, Axion, Fabuloso, Soupline and Suavitel, as well as Hill’s Science Diet and Hill’s Prescription Diet.
For more information about Colgate’s global business, visit the Company’s web site at http://www.colgatepalmolive.com. To learn more about Colgate Bright Smiles, Bright Futures® oral health education program, please visit http://www.colgatebsbf.com. To learn more about Hill's and the Hill’s Food, Shelter & Love program please visit http://www.hillspet.com. To learn more about Tom’s of Maine please visit http://www.tomsofmaine.com.

Reasonable accommodation during the application process is available for persons with disabilities. Please contact Application_Accommodation@colpal.com with the subject ""Accommodation Request"" should you require accommodation."
116,Data Engineer,N,United States•Remote,N,"Numerator is looking for an experienced, talented and quick-thinking Data Engineer to join a Numerator Product team as their Data expert.
This is a unique opportunity where you will get the chance to work on the data engineering and infrastructure of one of Numerator's fastest growing products. Our technology harnesses consumer-related data in many ways including gamified mobile apps, sophisticated web crawling and enhanced Deep Learning algorithms to deliver an unmatched view of the consumer shopping experience.
As a Data Engineer on the TruView team, you will make an immediate impact as you help build out and expand the product platform as the customers demands grow.
This role requires a balance between hands-on data engineering, infrastructure-as-code deployments as well as involvement in operational data architecture and technology choices for a customer-facing product.

How You’ll Spend Your Time
Design, document, and lead development of end-to-end data models/pipelines for Numerator’s growing datasets and product offerings.
Work with business, product, and data science teams to understand end-user requirements or analytics needs to implement the most appropriate technologies and scalable data engineering practices


3+ years designing data warehouses and building data pipelines or in a data intensive engineering role (preferably Snowflake)
Proficiency in SQL and one major scripting language (preferably Python)
Experience with data modeling, ETL design and tooling (especially Airflow), and transforming data to meet business goals
Experience designing and deploying production solutions to the cloud with AWS, Azure or GCP
Experience with declarative infrastructure, Docker and Kubernetes ((e.g. Terraform, EKS)
Availability to participate in after-hours on-call support with your fellow engineers and help improve a team’s on-call process where necessary
Nice to Haves
Experience creating integrations, managing user permissions and optimizing queries in Snowflake
Autonomy, versatility, intellectual curiosity, and ability to thrive in a fast-paced organization
Experience in CI/CD and Monitoring software such as CircleCI, Github Actions, Prometheus, Coralogix, Jenkins, Splunk etc.
What we offer you
More data than you could imagine to play with!
Data that matters and that is shaping the impact of billion dollar brands.
Brilliant, motivated and passionate colleagues with whom to spend your time.
An inclusive and collaborative company culture- we work in an open environment while working together to get things done, and adapt to the changing needs as they come.
Market competitive total compensation package.
Volunteer time off and charitable donation matching.
Regular hackathons to build your own projects and work with people across the entire company
Strong support for career growth, including mentorship programs, leadership training, access to conferences and employee resource groups.
Great benefits package including health/vision/dental, exceptional maternity leave coverage, unlimited PTO, flexible schedule, 401K/RRSPs matching and much more.
If this sounds like something you would like to be part of, we’d love for you to apply! Don't worry if you think that you don't meet all the qualifications here. The tools, technology, and methodologies we use are constantly changing and we value talent and interest over specific experience.
#LI-MB1
#LI-Remote"
117,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
118,Distributed Systems Engineer (L5) - Data Platform,Netflix,Remote,N,"Remote, United States
Data Platform
At Netflix, we want to entertain the world and are constantly innovating on how entertainment is imagined, created and delivered to a global audience. We currently stream content in more than 30 languages in 190 countries, topping over 220 million paid subscribers and are expanding into new forms of entertainment such as gaming.

The data infrastructure teams at Netflix enable us to leverage data to bring joy to our members in many different ways. We provide centralized data platforms and tools for various business functions at Netflix, so they can utilize our data to make critical data-driven decisions. We do all the heavy lifting to make it easy for our business partners to work with data efficiently, securely, and responsibly. We aspire to lead the industry standard in building a world-class data infrastructure, as Netflix leads the way to be the most popular and pervasive destination for global internet entertainment.

We are looking for distributed systems engineers to help evolve and innovate our infrastructure as we work towards our ambitious goal of 500 million members worldwide. We are committed to building a diverse and inclusive team to bring new perspectives as we solve the next set of challenges. In addition, we are open to remote candidates. We value what you can do, from anywhere in the U.S.

Spotlight on Data Infrastructure Teams:
Storage and Insights | Learn More
Offers storage and insights products. Those products provide storage services to platforms, applications and users around the globe which are performant, secure and centrally managed; enabling platforms to move, store and efficiently archive data in the cloud. We offer a consistent mechanism to create & manage S3 resources, integrate Netflix ecosystem for access control and provide observability into the cost & resource lifecycle of these resources, by taking ownership of existing tools and shaping a more cohesive strategy.

This would be your dream job if you enjoy:
Solving real business needs at large scale by applying your software engineering and analytical problem solving skills.
Architecting and building a robust, scalable, and highly available distributed infrastructure.
Leading cross-functional initiatives and collaborating with engineers, product managers, and TPM across teams.
Sharing our experiences with the open source communities and contributing to Netflix OSS.
About you:
You have 5+ years of experience in building large-scale distributed systems or applications.
You are proficient in design and development of RESTful web services.
Experienced building and operating scalable, fault-tolerant, distributed systems
You are an expert in Java or other object-oriented programming languages. Python or Scala expertise is a plus.
Multi-threading is a challenge that you are comfortable tackling.
You have a BS in Computer Science or related field.
A few more things about us:

As a team, we come from many different countries and our fields of education range from the humanities to engineering to computer science. Our team includes product managers, program managers, designers, full-stack developers, distributed systems engineers, and data scientists. Folks have the opportunity to wear different hats, should they choose to. We strongly believe this diversity has helped us build an inclusive and empathetic environment and look forward to adding your perspective to the mix!

At Netflix, we carefully consider a wide range of compensation factors to determine your personal top of market. We rely on market indicators to determine compensation and consider your specific job family, background, skills, and experience to get it right. These considerations can cause your compensation to vary and will also be dependent on your location.

The overall market range for roles in this area of Netflix is typically $100,000 - $700,000

This market range is based on total compensation (vs. only base salary), which is in line with our compensation philosophy. Our culture is unique, and we tend to live by our values, so it’s worth learning more about Netflix here."
119,Data Engineer,Techsara solutions,"Techsara solutions in Washington, DC 20001",$65 - $70 an hour,"Hi,
Hope you are doing Great!
Please find the requirement below, If you find yourself comfortable with the requirement Please Reply back with your updated resume and will get back to you or I would really appreciate it if you can give me a call back at my contact Number.
Job Title: Data Engineer
Location: local to DC Metro for returning to the office in future- LOCALS ONLY
Remote: Yes for now
Job Description:
Data Engineer (Reference Data)
Data deliverables–
Reference Data Management (using tools such as Informatica Reference 360, Ataccama, Profisee, or similar.)
Configure Data workflow management
Logical & Physical Data modeling
Data quality control & monitoring
Data Integration (Batch & Real Time)
Data Migration
Data profiling
Hands on data service/programming lang. experience –
Informatica Reference 360, Ataccama, Profisee, or similar
Erwin
Azure Data Lake
Databricks
PySpark
SQL
API
Agile Delivery - Azure DevOps/Boards, JIRA
Desired – Data Stewardship exp., Data Governance exp. , Data Security exp. , Data Architecture, Synapse (Dedicated SQL Pool)
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Washington, DC 20001: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL (Required)
Informatica (Required)
PySpark, Erwin (Required)
Work Location: One location"
120,Data Engineer,Wawa,"Media, PA 19063•Hybrid remote",N,"Data Engineer
Wawa Corporate - Media, PA
Job Summary:
The Data Engineer role designs and develops scalable data solutions using data integration tools and technologies. The individual utilizes big data computation, data platforms and storage tools to create prototype and data products. Conduct build and testing of data pipelines and solutions. Additionally, Data Engineer integrates, tests data pipelines with Advance Analytics and AI platforms. Must be proficient with multiple data engineering and integration tools such as Scala, Python, Spark, Snowflake etc. in an AWS environment.
Principal Duties:
Responsible for designing and implementing solutions for loading both structured and semi-structured data design into multiple target data systems.
Design, develop, optimize, and maintain data pipelines and processes that adhere to data integration principles and business goals.
Solve complex data problems to deliver insights that helps our business to achieve their goals.
Code, test, and document new or modified data systems to create robust and scalable applications for data analytics.
Ensure that data pipelines are scalable, repeatable, and secure, and can serve multiple users within the company.
Design and implement data ingestion techniques for real time and batch processes for structured and semi-structured data sources into Wawa’s data lake and data warehouse platforms.
Understand complex business requirements and propose end to end and simplified enterprise information architecture solutions.
Develop and implement data design methods, data structures, and modeling standards which work with multiple business intelligence tools.
Work closely with Analytics team and implement their self-service and analytics requirements.
Work with Data Science practitioners and developers to make sure that all data solutions are
Collaborate with Analytics team to build solutions that enable business analytics. Develop quality scalable, tested, and reliable data services using industry best practices.
Manage all activities centered on obtaining data and loading into a data lake environment.
Assess the suitability and quality of candidate data sets for the Data Lake.
Balance business requirements with technical feasibility and set expectations on new projects. Recommend changes in development, maintenance and system standards.
Design and build integration components and interfaces in collaboration with Architects and Infrastructure Engineers as necessary. Perform unit, component, integration testing of software components including the design, implementation, evaluation and execution of unit and assembly test scripts.
Determine if the data received from the upstream systems are of good quality based on the rules and data quality validations defined and in case of any issues with the data quality analyze and come up with a preliminary summary of the root cause/issue.
Assist the Analytics team by leveraging Wawa’s Enterprise Data Platform ecosystem to design, and develop capabilities to deliver our solutions using Spark, Scala, Python and
Follow security standards for all data and tools that are being introduced in the team.
Basic Qualifications:
Bachelor’s degree in Computer Science/Engineering preferred
3-5+ years database, data integration experience
3+ years’ experience with Spark, Scala/Python, SQL and Big Data solutions
Preferred experience with Databricks and Snowflake
3+ years’ experience in designing and implementing the data architecture (conceptual, logical, physical & dimensional models).
Developing Enterprise Business Intelligence solutions on one or more of the following
EDW platforms: Snowflake, Redshift, Google Big Query
Experience implementing Big Data solutions using open source technologies
Strong knowledge of key scripting and programming languages such as Python, Java, Scala, etc
Experience with data integration tools such as Talend would be helpful
Experience designing and implementing various data pipeline patterns and strategies
Hands-on experience with dimensional modeling techniques and creation of logical and physical data models (entity relationship modeling, exposure to data warehouse design)
Strong knowledge of data security principles
Proven track record working with complex, interrelated systems and bringing that data together on Big Data platforms.
Wawa will provide reasonable accommodation to complete an application upon request, consistent with applicable law. If you require an accommodation, please contact our Associate Service Center.
Wawa, Inc. is an equal opportunity employer. Wawa maintains a work environment in which Associates are treated fairly and with respect and in which discrimination of any kind will not be tolerated. In accordance with federal, state and local laws, we recruit, hire, promote and evaluate all applicants and Associates without regard to race, color, religion, sex, age, national origin, ancestry, familial status, marital status, sexual orientation or preference, gender identity or expression, citizenship status, disability, veteran or military status, genetic information, domestic or sexual violence victim status or any other characteristic protected by applicable law. Unlawful discrimination will not be a factor in any employment decision.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Employee stock ownership plan
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
7 years
8 years
9 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Media, PA 19063: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What Big Data Tools/Technologies/Querying Languages do you have experience with (RedShift, Snowflake, Python, SQL, Google BigQuery, Spark, Scala, etc.)?
Experience:
Data Engineering: 7 years (Required)
Leadership: 3 years (Required)
Work Location: Hybrid remote in Media, PA 19063

Health insurance"
121,Data Engineer,HealthEquity,Remote,N,"Overview:
We are CONNECTING HEALTH AND WEALTH. Come be part of remarkable.

How you can make a difference

We are looking for a Data Engineer II who will be reporting directly to our Manager, Data Engineering . The position is responsible for data solutions critical to our company’s descriptive and predictive analytics. The successful candidate will perform a broad range of tasks, from mapping business problems to data solutions, developing ETL/ELT solutions, designing logical and physical data models, and promoting data quality.

What you’ll be doing

Development of ETL/ELT solutions (e.g., DBT/Python/C#) for the most critical and complex projects in collaboration with data architects
Develop an understanding of all HQY data and cross-platform relationships
Work closely with the business, mapping their most pressing business problems to data and analytic solutions.
Research and solve issues in unfamiliar technologies or areas of expertise
Implement new data flows in accordance with the data warehouse design
Utilize and improve automated deployment pipelines for the solution
Peer review of code and unit tests for fellow team members
Coach and mentor junior data engineers

What you will need to be successful

BS degree in Computer Science, Engineering, Mathematics, or a related field and 4 years of work experience
Ability and passion to deliver ETL/ELT solutions efficiently using the Agile development methodology
Problem solving and troubleshooting skills.
Demonstrated ability to innovate, drive assigned tasks to successful completion
Advanced knowledge of SQL and Relational Database Management Systems (e.g., Oracle, SQL Server, mySQL).
Knowledge of ETL processing tools (e.g., DBT, Databricks, Fivetran, SSIS, Talend).
Understanding of data modeling techniques (e.g., 3NF, Dimensional) and ability to apply them
Proficiency in programming languages common to Data and Analytics (e.g., Python, R, C#).
Some knowledge of massive parallel processing datastores (e.g., Databricks, Snowflake, Redshift).
Experience with Cloud Services (e.g., Azure, AWS).
A strong commitment to data security protocols and regulatory requirements

#LI-Remote
This is a remote position.
Benefits & Perks:
Medical, Dental, Vision
HSA contribution and match
Dependent Care FSA match
Uncapped Paid Time Off
401(k) match
Paid Parental Leave
Ongoing Education & Tuition Assistance
Gym/Fitness Reimbursement
Award Winning Wellness Program
Come be your authentic self:
Why work for HealthEquity

HealthEquity has a vision that by 2030 we will make HSAs as wide-spread and popular as retirement accounts. We are passionate about providing a solution that allows American families to connect health and wealth. Join us and discover a work experience where the person is valued more than the position. Click here to learn more.

Come be your authentic self

HealthEquity, Inc. is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, age, color, religion, sex, sexual orientation, gender identity, national origin, status as a qualified individual with a disability, veteran status, or other legally protected characteristics. HealthEquity is a drug-free workplace. For more information about our EEO policy, or about HealthEquity’s applicant disability accommodation, drug-free-workplace, background check, and E-Verify policies, please visit our Careers page.

HealthEquity is committed to your privacy as an applicant for employment. For information on our privacy policies and practices, please visit HealthEquity Privacy."
122,Data Center Engineer 1,Tekrek Solutions Inc,"1075 W Entrance Dr, Auburn Hills, MI 48326",$40 - $45 an hour,"Requirements:
Bachelor's degree in Mechanical, Electrical Engineering, or IT required.
Minimum of 3 years' experience in facilities infrastructure. (must have experience in Data Center facilities and High Power Electrical equipment with knowledge of HVAC equipment)
Must understand As-Built drawings and electrical and mechanical operations of a Data Center.
Knowledge of various data center technologies and a strong understanding of high-availability data centers.
Understanding of network design principles.
Minimum of 3 years' experience in budget/finance experience.
Minimum of 3 years' experience in project management.
Read and interpret electrical and mechanical blueprints
On call 24 x 7 in a rotation order.
Able to lift 30 lbs.
Preferred Qualifications:
Master's degree
UST and AST Certification
PMP Certification
Capable of leading design and planning sessions including leading the collection of business requirements, determining the scope of the project, and aligning business needs with the appropriate technology.
Exceptional program/project management track record.
Excellent team work skills.
Strategic thinker with the ability to grasp business goals, strategies, and challenges.
Proven ability to multi-task is critical, excellent communication skills, both written and verbal, strong conflict management skills.
Carries out responsibilities in a professional, courteous manner at all times.
Notes:
Candidates must be able to work onsite at ICT.
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Benefits:
Health insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Auburn Hills, MI 48326: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data center: 3 years (Required)
Mechanical: 1 year (Required)
Electrical: 1 year (Required)
Finance/Budget: 1 year (Required)
Project management: 1 year (Required)
Security clearance:
Confidential (Preferred)
Work Location: One location

Health insurance"
123,Data Engineer,SpaceX,"18390 NE 68th St, Redmond, WA 98052","$120,000 - $145,000 a year","SpaceX was founded under the belief that a future where humanity is out exploring the stars is fundamentally more exciting than one where we are not. Today SpaceX is actively developing the technologies to make this possible, with the ultimate goal of enabling human life on Mars.
DATA ENGINEER
At SpaceX we're leveraging our experience in building rockets and spacecraft to deploy Starlink, the world's most advanced broadband internet system. Starlink is the world's largest satellite constellation and is providing fast, reliable internet to 1M+ users worldwide. We design, build, test, and operate all parts of the system – thousands of satellites, consumer receivers that allow users to connect within minutes of unboxing, and the software that brings it all together. We've only begun to scratch the surface of Starlink's potential global impact and are looking for best-in-class engineers to help maximize Starlink's utility for communities and businesses around the globe.
As a Data Engineer, you will be responsible for developing the strategy, key metrics, tools, software services and processes for assessing how well key aspects of Starlink are scaling and the effectiveness of the Starlink Network in serving millions of users around the globe. You will work with operators, subsystem responsible engineers, software engineers, and network engineers inside the Starlink organization as well as key contacts with various major external partners to help ensure the growth of this program.
RESPONSIBILITIES:
Build and maintain mission-critical infrastructure, tools, processes, and custom software to objectively assess growth areas for the Starlink program
Automate the aggregation of metrics and detection of widespread application issues across Starlink
Establish and maintain relationship with key third party application/content owners
Lead technical investigations about chronic application-level issues
Build ground-based software systems that ingest, transform, and store data
Apply data analytics, models, and techniques to data products created by space vehicles
Create catalogs of data and tools that can be used by you and other teams to perform analytics
Fuse data from multiple sources to create usable information
BASIC QUALIFICATIONS:
Bachelor's degree in computer science, data science, physics, mathematics, or a STEM discipline; OR 2+ years of professional experience in data engineering in lieu of a degree
Development experience in an object-oriented programming language (i.e. C, C++, Python)
PREFERRED SKILLS AND EXPERIENCE:
Professional experience in analytics, data science, or machine learning
Experience using Spark, Presto, Flink, or Snowflake
Experience building solutions with Parquet, or similar storage formats
Knowledge of Kubernetes
Experience building solutions with in-stream data processing of structured and semi-structured data
Experience building predictive models and machine learning pipelines (clustering analysis, prediction, anomaly detection)
Experience in custom ETL design, implementation and maintenance
Experience handling large (TB+) datasets
Experience with developing and deploying tools used for data analysis
Ability to work effectively in a dynamic environment that includes working with changing needs and requirements
Ability to take on projects that require taking initiative and developing new expertise
ADDITIONAL REQUIREMENTS:
Must be willing to work extended hours and weekends as needed
COMPENSATION AND BENEFITS:

Pay range:
Data Engineer/Level I: $120,000.00 - $145,000.00/per year
Data Engineer/Level II: $140,000.00 - $170,000.00/per year

Your actual level and base salary will be determined on a case-by-case basis and may vary based on the following considerations: job-related knowledge and skills, education, and experience.
Base salary is just one part of your total rewards package at SpaceX. You may also be eligible for long-term incentives, in the form of company stock, stock options, or long-term cash awards, as well as potential discretionary bonuses and the ability to purchase additional stock at a discount through an Employee Stock Purchase Plan. You will also receive access to comprehensive medical, vision, and dental coverage, access to a 401(k)-retirement plan, short & long-term disability insurance, life insurance, paid parental leave, and various other discounts and perks. You may also accrue 3 weeks of paid vacation & will be eligible for 10 or more paid holidays per year. Exempt employees are eligible for 5 days of sick leave per year.
ITAR REQUIREMENTS:
To conform to U.S. Government space technology export regulations, including the International Traffic in Arms Regulations (ITAR) you must be a U.S. citizen, lawful permanent resident of the U.S., protected individual as defined by 8 U.S.C. 1324b(a)(3), or eligible to obtain the required authorizations from the U.S. Department of State. Learn more about the ITAR here.
SpaceX is an Equal Opportunity Employer; employment with SpaceX is governed on the basis of merit, competence and qualifications and will not be influenced in any manner by race, color, religion, gender, national origin/ethnicity, veteran status, disability status, age, sexual orientation, gender identity, marital status, mental or physical disability or any other legally protected status.
Applicants wishing to view a copy of SpaceX's Affirmative Action Plan for veterans and individuals with disabilities, or applicants requiring reasonable accommodation to the application/interview process should notify the Human Resources Department at (310) 363-6000."
124,Full Stack Engineer - Data Analytics,Costco Wholesale,"4401 4TH AVE S, Seattle, WA 98134","$127,500 - $162,500 a year","This is an environment unlike anything in the high-tech world and the secret of Costco’s success is its culture. The value Costco puts on its employees is well documented in articles from a variety of publishers including Bloomberg and Forbes. Our employees and our members come FIRST. Costco is well known for its generosity and community service and has won many awards for its philanthropy. The company joins with its employees to take an active role in volunteering by sponsoring many opportunities to help others. In 2021, Costco contributed over $58 million to organizations such as United Way and Children's Miracle Network Hospitals.
Costco IT is responsible for the technical future of Costco Wholesale, the third largest retailer in the world with wholesale operations in fourteen countries. Despite our size and explosive international expansion, we continue to provide a family, employee centric atmosphere in which our employees thrive and succeed. As proof, Costco ranks seventh in Forbes “World’s Best Employers”.
The Full Stack Engineer will perform development work across the Analytics and Data Science stack. The ideal candidate will demonstrate the ability to manage the completion of projects that involve databases, backend services, and OO/UI development. This position will require an individual to design, build, enhance, and implement solutions in a fast-paced Agile environment participating in scrum ceremonies and working closely with a highly motivated team.
If you want to be a part of one of the worldwide BEST companies “to work for”, simply apply and let your career be reimagined.

ROLE
Supports Systems Architects with the design of the overall architecture of a specific product/application, applying principles that promote availability, reusability, interoperability, and security into the design framework.
Performs development, optimization, and automation activities to support the implementation of a product/application.
Increases proficiency and understanding of relational and non-relational databases, distributed application architectures, user interfaces and user experience, quality assurance, security concerns, and business value creation.
Adopts engineering best practices to deliver higher quality and scalable solutions.
Provides the team with the development strategy, solution recommendations, and estimates for a given product/application.
Participates in scrum ceremonies (sprint planning, retrospective, showcases/demos, stand-ups, backlog refinement, etc.).
Creates and executes unit tests which validate the code changes made to support the implementation of a user story.
Uses test-driven development (TDD) practices to assist with the detection/recognition of software defects early in the development process.
Identifies deficiencies within a product/application’s code-base and identifies opportunities to improve overall code quality.
Works with team members to move user stories from the development backlog into testing and provides clarification when needed.
Estimates, plans, and manages all implementation tasks and reports on development progress.
Demonstrates a strong understanding of emerging technologies to support the development of new solutions.
Collaborates with team members (e.g. Systems Architects, Systems Analysts) to define project specifications and release documentation for all phases of the product development cycle from product definition to design through implementation.
Acts as a technical lead in the Full Stack Engineering space and delivers data and analytics products/applications/portals/APIs.
Conducts peer code reviews for the software changes made by other engineers within a team.
REQUIRED
5-10 years’ experience in data management and development.
Proficient working knowledge of code versioning tools such as Git, SVN, etc.
Familiarity with continuous integration.
Strong understanding of the software development process.
Strong knowledge of object oriented concepts.
Strong knowledge and experience working with databases.
Strong verbal and written communication skills and the ability to communicate to both technical and Business audiences.
Demonstrated ability to execute against iteration plans and deliver assignments within scope, schedule, and budget.
Ability to work with all management levels.
High integrity, accountability, and a positive attitude; willing to do what it takes to make the team successful.
Team player; good interpersonal and organizational skills. Creative, likes challenges.
Strong organizational and time management skills with ability to multitask and prioritize work effectively in a fast-paced working environment.
Extremely responsive; able to work under pressure in a crisis with a strong sense of urgency.
Responsible, conscientious, self-motivated, and able to work with limited supervision.
Detail-oriented and possesses strong problem-solving skills and ability to analyze for potential future issues.
Able to support off-hours work as required, including weekends, holidays, and 24/7 on call responsibilities on a rotational basis.
Recommended
Bachelor degree in Computer Science, Statistics, Data Science, Engineering, or related field or equivalent experience.
Minimum of 5 years’ experience in:
Integrating an application with other applications using web services.
Working with UI frameworks/libraries such as Angular and React.
Working knowledge of databases (DB2, SQL Server, Oracle etc.).
Programming experience with related technology such as XML, Java, various JavaScript frameworks, and SQL queries.
Cloud Development/Computing (preferably Azure)
Required Documents
Cover Letter
Resume

California applicants, please click here to review the Costco Applicant Privacy Notice.
Pay Range:
Level 3: $127,500 - $162,500
Level 4: $152,500 - $182,500, Bonus and Restricted Stock Unit (RSU) eligible
We offer a comprehensive package of benefits including paid time off, health benefits — medical/dental/vision/hearing aid/pharmacy/behavioral health/employee assistance, health care reimbursement account, dependent care assistance plan, commuter benefits, short-term disability and long-term disability insurance, AD&D insurance, life insurance, 401(k), stock purchase plan, SmartDollar financial wellness program, to eligible employees.
Costco is committed to a diverse and inclusive workplace. Costco is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or any other legally protected status. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to IT-Recruiting@costco.com
If hired, you will be required to provide proof of authorization to work in the United States. In some cases, applicants and employees for selected positions will not be sponsored for work authorization, including, but not limited to H1-B visas."
125,"Senior Software Engineer, Ads Data and Foundation Tech",TikTok,"New York, NY","$187,040 - $280,000 a year","Responsibilities
TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo.

Why Join Us
At TikTok, our people are humble, intelligent, compassionate and creative. We create to inspire - for you, for us, and for more than 1 billion users on our platform. We lead with curiosity and aim for the highest, never shying away from taking calculated risks and embracing ambiguity as it comes. Here, the opportunities are limitless for those who dare to pursue bold ideas that exist just beyond the boundary of possibility. Join us and make impact happen with a career at TikTok.

At Ads Data and Foundation team, we provide basic services such as sales performance management, rebate accounting, financial accounting, and post-paid strategy for TikTok commercial business to improve the efficiency of sales operations while safeguarding capital and meeting financial compliance requirements.

Responsibilities
Responsible for the design and development of applications and data services related to TikTok's sales accounting business.
Ability to continuously optimize the system so that it can evolve rapidly with business needs and support business development.
Qualifications
Bachelor's degree or equivalent practical experience in Computer Science, or related technical field.
3+ years of coding experience with a general purpose programming language such as Java/Python/Go.
Proficiency with common storage technologies such as MySQL/Redis/Memcached/Kafka.
Experience with big data technologies such as Hive, Flink, etc.
Good communication and project management skills.
Experience with business-to-business and sales management is a plus.
TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.

TikTok is committed to providing reasonable accommodations during our recruitment process. If you need assistance or an accommodation, please reach out to us at will.wong@bytedance.com.
Job Information
The base salary range for this position in the selected city is $187040 - $280000 annually.



Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.



At ByteDance/TikTok our benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support ByteDancers to give their best in both work and life. We offer the following benefits to eligible employees:



We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care.



Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off(PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability.



We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice."
126,Data Engineer,EHE Health,Remote,"$125,000 - $150,000 a year","Who we are:

EHE Health has led the way in preventive medicine for more than 100 years. EHE Health is an evidence-based preventive health program that partners with mid- and large-sized employers to provide their employees and beneficiaries an unparalleled, personalized medical experience, beginning with prevention. EHE Health customizes the patient journey to be holistic, convenient and attainable towards the goal of achieving better health.

EHE boasts the only national preventive and primary care network allowing our members to access world-class medical care in 39 states, Washington DC and Puerto Rico.

As an innovator and leader of helping our patients live longer, healthier lives with personalized preventive care, EHE Health has launched EHE Health Pulse, a range of virtual and in-person clinical encounters and experiences that provide individuals the ability to conveniently access health services, including our Health Mentorship and Health Navigation programs.

EHE is backed by Summit and DW Healthcare Partners, investment engines behind many of the world’s leading technology and healthcare enterprises. Their strategic involvement has helped EHE reach new heights as a pioneering force in preventive care.

What we’re looking for:

We are looking for a dynamic, self-motivated Data Engineer with a passion for optimizing and embedding analytics. As a Data Engineer, your goal is to cultivate a data-informed culture and create insights that will be leveraged across the entire organization. You have experience executing at a high-level, solving complex problems and delivering solutions with real business impact.

The ideal candidate has a strong background in data architecture, database design and data modeling. You will work with cross-functional teams to design, implement and maintain our data infrastructure while also creating dashboards and reporting insights to various stakeholders.

In this role, you will:
Build out data pipelines and dashboards for internal and external stakeholders
Build analytic systems translating operational and business logic into analytics solutions
Debug and troubleshoot existing data problems
Institute processes to ensure data pulled from various sources meets quality standards, is curated and enhanced for analytical use, driving towards a ""single source of truth""
Define enterprise blueprint for managing data as an asset and a utility, unified data formats, and strategy to continuously streamline data landscape with end users in mind
Build a deep understanding of the business and develop executive and operational dashboards using Quicksight that track KPIs and visually expose trends
Implement and maintain data security and privacy measures
Develop metrics to accurately measure business performance and enable every decision the benefit of real-world data
Take results to a logical and relevant end product with minimum supervision

What this role requires:
Bachelor’s degree in Computer Science, Information Systems or a related field
4+ years’ experience as a Data Engineer
Strong experience with AWS Glue, AWS Quicksight and AWS Redis
Robust knowledge of data architecture, database design and data modeling
Attention to detail and ability to manage multiple projects in a timely manner
Strong analytical and problem-solving skills
Working knowledge of databases is a must
Highly proficient with Python, JAVA, SQL and ETL processes
Ability to work independently and operate effectively in a cross-functional matrix environment
Excellent analytical, written and verbal communication skills

What we offer:
Competitive salary
Medical, dental, vision, life and disability insurance
Employer-matched 401(k) plan
Tuition reimbursement
Employee access to our preventive exam and services
Gym membership

The salary range for this role is $125,000 - $150,000 and is determined by a number of factors including the candidate’s experience, qualifications and skills.

EHE is committed to Equal Employment Opportunity and to attracting and retaining the most qualified employees.
WpwTEG30R9"
127,Advanced Data Engineer,Kroger Technology & Digital,Remote,"$150,000 - $159,000 a year","Accountable for leading activities that create deliverables to guide the direction, development, and delivery of technological responses to targeted business outcomes. Provide facilitation, analysis and design tasks required for the development of an enterprise's data and information architecture, focusing on data as an asset for the enterprise. Develop reusable standards, design patterns, guidelines, and configurations to evolve the technical infrastructure related to data and
information across the enterprise, including direct collaboration with 84.51. Demonstrate the company’s core values of respect, honesty, integrity, diversity, inclusion and safety.
Minimum Position Qualifications:
Bachelor's Degree in computer science, or software engineering, or related field
7+ years successful and applicable hands-on experience in the data development and principles including end-to-end design patterns.
7+ years proven track record of designing and delivering large scale, high quality operational or analytical data systems.
7+ years successful and applicable experience taking a lead role in building complex data solutions that have been successfully delivered to customers.
Any experience defining evolutionary data solutions and underlying technologies.
Demonstrated written and oral communication skills Basic understanding of network and data security architecture.
Strong knowledge of industry trends and industry competition
Knowledge in a minimum of two of the following technical disciplines: data warehousing, data management, analytics development, data science, application programming interfaces (APIs), data integration, cloud, servers and storage, and database management
Requirements:
Experience in Data modeling and advanced SQL techniques
Experience working on cloud migration methodologies and processes including tools like Databricks, Azure Data Factory, Azure Functions, and other Azure data services
Expert in SQL, Python, Spark, Databricks
Experience working with varied data file formats (Avro, json, csv) using PySpark for ingesting and transformation
Experience with DevOps process and understanding of Terraform scripting
Understanding the benefits of data warehousing, data architecture, data quality processes, data warehousing design and implementation, table structure, fact and dimension tables, logical and physical database design
Experience designing and implementing ingestion processes for unstructured and structured data sets
Experience designing and developing data cleansing routines utilizing standard data operations
Knowledge of data, master data, metadata related standards, and processes
Experience working with multi-Terabyte data sets, troubleshooting issues, performance tuning of Spark and SQL queries
Experience using Azure DevOps/Github actions CI/CD pipelines to deploy code
Microsoft Azure certifications are a plus
Company Overview: Kroger Family of Companies employs nearly half a million associates who serve over 11 million customers daily through a seamless shopping experience under a variety of banner names. At The Kroger Co., we are Fresh for Everyone™ and dedicated to our Purpose: To Feed the Human Spirit®. We are committed to creating #ZeroHungerZeroWaste communities by 2025. Careers with The Kroger Co. and our family of companies offer competitive wages, flexible schedules, benefits and room for advancement.
#GoSourcing
Job Type: Full-time
Pay: $150,000.00 - $159,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Stock options
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Application Question(s):
Link to LinkedIn Profile:
Education:
Bachelor's (Required)
Experience:
Azure: 5 years (Required)
SQL: 5 years (Preferred)
Data warehouse: 5 years (Preferred)
Terraform: 5 years (Required)
Azure Databricks: 5 years (Required)
Data Engineering: 10 years (Required)
Data Infrastructure: 5 years (Required)
License/Certification:
DP-203: Data Engineering on Microsoft Azure (Preferred)
Work Location: Remote

Health insurance"
128,Data Engineer,Pendrick Capital Partners,Remote,"$100,000 - $170,000 a year","Company Overview
Pendrick Capital Partners is a leader in helping U.S. healthcare providers manage their receivables. With a core belief of practicing a patient-first mindset, Pendrick is the best-in-class revenue cycle management partner with over 10-years of experience purchasing outstanding receivables. Pendrick’s recognized compliance program offers an unparalleled degree of risk reduction for our healthcare industry partners while increasing returns on patient responsibility balances.
As a Data Scientist at Pendrick Capital Partners, you’ll help us make better and faster decisions than ever before. We use the latest in cloud, analytical, and machine learning technologies to unlock big opportunities for the company’s executives. We have big goals for the next few years, and we could use your help to design, architect, and implement solutions that meet our growing needs for rapid and cutting-edge analytics and forecasting.
This role is for you if:
You have built machine learning models through all phases of development, from design through training, evaluation, validation, and implementation and can explain your decisions in a simple and concise way to non-technical experts,
You know how to strike the right balance between sharing your expertise and listening to others’ ideas, and
You love to learn how to apply cutting-edge technologies in a way that drives value for business decisions and can leverage several technologies and languages — SQL, R,
AWS, Spark, and more — to reveal the insights hidden within huge volumes of transaction data,
The Ideal Candidate is:
A big data wrangler. You have the skills to retrieve, combine, and analyze data from a variety of sources and structures, preferably using Spark and other open source technologies.
Technical. You’ve worked with open-source languages, you know how to develop reusable code, and you are passionate about continuing to improve. You have hands-on experience developing data science solutions using open-source tools and cloud computing platforms.
Statistically-minded. You’ve built models, validated them, and monitored them post- deployment. You know how to interpret a ROC curve and partial dependence plots. You have experience with multivariate linear and nonlinear models as well as unsupervised approaches including clustering, classification, and anomaly detection.
Forward-thinking. You know how to promote a culture of technical excellence and look for opportunities to reuse robust, resilient solutions wherever possible.
Basic Qualifications:
Bachelor’s Degree plus 2 years of experience in data analytics in the workplace, or
Master’s Degree plus 1 year in data analytics in the workplace, or PhD
At least 1 year of experience in open source programming languages for large-scale data analysis (preferably R)
At least 1 year of experience with machine learning
At least 1 year of experience with relational databases
Languages: Python & SQL required. C++ and R
Preferred Qualifications:
Master’s Degree in “STEM” field (Science, Technology, Engineering, or Mathematics) plus 3 years of experience in data analytics, or Ph.D. in “STEM” field (Science,
Technology, Engineering, or Mathematics)
At least 1 year working in financial, healthcare, or collections services
At least 1 year of experience working with AWS
At least 2 years experience in Spark/Databricks/Scala or R
At least 2 years experience with machine learning
At least 3 years experience with SQL
Git, Docker, Serverless, Lambda, ECS, AWS CLI, Boto3
Experience with consumer finance data is a plus
For more information about Pendrick Capital Partners, please visit our website at https://www.pendrickcp.com/
Job Type: Full-time
Pay: $100,000.00 - $170,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Referral program
Vision insurance
Compensation package:
Performance bonus
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
How many years of relational database experience do you have?
Experience:
AWS: 1 year (Preferred)
SQL: 1 year (Preferred)
C++: 1 year (Preferred)
Work Location: Remote

Health insurance"
129,Data Engineer,Procter & Gamble,"Cincinnati, OH","$146,000 - $172,000 a year","Our Team!
P&G has declared Advance Analytics as a key strategy and is designing truly modern, scalable Big Data applications. At the same time, P&G is broadly using Cloud platform to host scalable and fully automated solutions based on clusters and serverless cloud AI services.
Would you like to use your technical skills to design and manage scalable Data applications? Do you want to work in a team where your efforts will fuel P&G business? If so, this position is perfect for you! We are looking for Data Engineers to accelerate automation and increase effectiveness of our best-in-class Data Analytics group.
We are searching for self-motivated candidates, able to source and develop data transformations and models that will be used for advanced analytics. You will build scalable and automated solutions both in traditional cloud-based clusters and as serverless. You will work in partnership with data analysts to understand use cases, data needs, and outcome objectives. Your focus will be to create new solution or re-engineer legacy technology or manual approaches.
As a partner in our business, you will have project work and ad-hoc requests that require agility and often fast paced response to business needs.
Job Responsibilities
Partner with business stakeholders, upstream infrastructure platform teams, and downstream data consumers to understand and translate business requirements into technical design while working on advanced analytics use cases
Designing and developing applications that source data from various systems utilizing advanced programming in Scala, Python, js, shell scripting and SQL
Delivering analytical algorithms and applications (eg. machine learning)Engaging in proof of concepts and experiments together with business partners and analysts to deliver new advanced analytics use cases
Develop and support data pipelines, warehouses, data models, and reporting systems to tackle business opportunities
Lead and continuously improve end-to-end service delivery including strategic vendor management, operations, and product innovation.
Influence the future of new technologies and the ways in which P&G uses them
Work with multifunctional and multinational teams within and outside of P&G
Focus on key business cases development within Data & Analytics + Azure
Manage agile projects using cloud and hybrid solutions

Qualifications
Required
BS or MS in Business/Management Information Systems and/or Computer Science/Engineering, Programming/Software Development or Operations Research or Statistics, or with proven track of record from programming areas.
Experience in implementing projects & solutions using Microsoft Azure stack
Cloud Understanding
Python and SQL programming skills
Databricks, PySpark, Hive experience
Analytics, Data Modeling, and Data Provisioning (acquisition from various sources, transformation and sharing)
Ability to handle multiple priorities
Ability to work collaboratively across functions and organize work
Strong written and verbal communication skills to influence others
Preferred
Knowledge and/or experience with using or building CI/CD tools
Knowledge of SCRUM and DevOps methodologies
Pay Range: $146k-$172k
Compensation for roles at P&G varies depending on a wide array of non-discriminatory factors including but not limited to the specific office location, role, degree/credentials, relevant skill set, and level of relevant experience. At P&G compensation decisions are dependent on the facts and circumstances of each case. Total Rewards at P&G include salary + bonus (if applicable) + benefits. Your recruiter may be able to share more about our total rewards offerings and the specific salary range for the relevant location(s) during the hiring process
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status, disability status, age, sexual orientation, gender identity and expression, marital status, citizenship, HIV/AIDS status or any other legally protected factor.
Immigration sponsorship is not available for this role. As a general matter, Procter & Gamble does not sponsor candidates for nonimmigrant visas or permanent residency. However, Procter & Gamble may make exceptions on a discretionary basis. Any exceptions would be based on the Company's specific business needs at the time and place of recruitment as well as the particular qualifications of the individual.
Procter & Gamble participates in e-verify as required by law.
Qualified individuals will not be disadvantaged based on being unemployed.
Job locations:
Cincinnati, Ohio, United States Job Type: Full time Job categories: Information Technology Req No: R000057173"
130,Data Engineer,Pyx Health Inc,Remote,N,"Data Engineer
Loneliness is a devastating and pervasive problem, fraught with steep social, personal, and economic costs, and Pyx Health is looking for a
Senior Data Engineer with the ability to hit the ground running to help solve it. We're changing the landscape of care, resulting in improved
health. To do so requires ingesting, processing, and transforming large amounts of information. The ideal candidate will be dedicated and willing
to take ownership of the data processing systems, learn quickly, and work in a team environment with humility and compassion.
ONLY CANDIDATES RESIDING IN THE USA MAY APPLY.
Responsibilities:
Help make architectural standards a reality and always look for ways to improve.
Manage the hydration and data hygiene of a large data warehouse.
Eliminate technical debt across Database infrastructure; Build and optimize data ETL processes, and flat file ingestion mechanisms.
Maintain and improve existing stored procedures, views, and table schemas.
Administer large, high-volume production database environments.
Work closely with data analytics and development partners to create robust data solutions.
Strive to write code that is high quality and easily maintainable.
Work in a fast-paced, Agile environment where the requirements and needs are constantly evolving.
Keep up with new technologies when sensible and make improvements where possible.
Develop highly available database systems that can sustain operational and disaster scenarios specifically in multi-regional cloud
Required Skills:
Is self-motivated, needs very little oversight, can take ownership of the product, and can deliver features on time.
Is humble, approachable, easy to work with and enjoys tackling complex problems.
Is constantly seeking challenges and takes pride in creating solutions that clients love.
Has experience ingesting files with large amounts of data efficiently and expediently using ETL pipelines.
Has experience with Cloud Computing, preferably with Azure using tools like Azure Data Factory, Azure Databricks, and Azure DevOps.
Possesses strong knowledge of Python and relevant data transformation libraries such as Pandas and PySpark.
Has strong SQL development skills including stored procedures, writing performance-optimized views, and query analysis.
Familiarity with version control systems, preferably Git or BitBucket.
Understanding of Data Architecture and how it affects different environments such as Application, Analytics, and Processing.
The Ideal Candidate Will Also Have:
History of working in a high speed, aglie environment, using tools like JIRA, Conflucene, or other ticketing systems
Experience performing schema migrations across multiple environments.
Experience securing databases for compliance and security frameworks (e.g. HIPPA, HITECH, HITRUST, PCI, SOC).
Has experience administering RDBMS like SQL Server / MySQL / Oracle.
Interest or experience in developing AI models.
Knowledge of common health care file exchange formats.
Experience working in or with the healthcare industry or healthcare-related products that require compliance with HIPAA.
About Pyx Health
We solve some of the biggest problems in healthcare - loneliness and social isolation.
Pyx Health is a mobile solution that reduces loneliness and social isolation by connecting with your members outside of the traditional care
setting. By providing critical and timely interventions and addressing social determinants of health, we take care of health plan members when
they are most vulnerable, especially after a transition of care."
131,BI Data Engineer I,The Boston Beer Company,"Boston, MA•Hybrid remote",N,"We are currently hiring a BI Data Engineer I in Boston, MA (Hybrid) .
This is a great role on a team that is transforming Business Intelligence technology at Boston Beer. The Data Engineer is a technical role with SQL Server and Azure cloud data technology at its core and has ample opportunity for technical growth as we move from traditional data technologies to cloud data architecture. As Boston Beer takes on new challenges in areas like supply chain and digital, this role will be highly involved in ensuring our data platform is scalable, performant, and ready for the future.
Working within the Boston-based Business Intelligence team, this role is responsible for various database activities all centered on delivering impactful Business Intelligence solutions and maintaining and improving our database systems. This is a multi-faceted role that includes technical analysis, design, development, testing and maintenance as well as user interaction including requirements gathering, training, etc. This role will work on our on-premises SQL data warehouse platform and helping to build out the data platform of the future in Azure.
What You'll Brew :
Build out and migration to cloud data technologies, which may include (but not limited to)
Azure Data Lake, Azure SQL Pools, Apache Spark, Synapse, Azure Data Factory, SQL Server Database development work including tables, views, stored procedures and functions. SSIS development for Extract Transform Load (ETL) processes as well performance tuning and testing of BI structures and processes.
Participation in the design of data models to house and deliver essential reporting.
Automation with Powershell Scripting
Consultation with other IT personnel to perform essential analysis on source data
Collaborate with BI Team members and key BI power users on specific data designs and intended use.
Collaborate on requirements definition, design and build out of key data warehousing and integration deliverables for Supply Chain, Brand Insights, Digital, Sales and other areas.
SQL scripting to perform essential data maintenance functions including reclassifications, some master data setup and other
This role may also be involved in data preparation and modeling using the Power BI platform as well as Analysis Services-Tabular
Maintenance and extension of our SQL server jobs and Azure PowerShell runbooks for update of our SQL and Azure environments.
Partner with the BI team to promote and evolve design, development, and testing standards.
Adhere to IT department policies and procedures when performing tasks to ensure controls are in place and departmental goals are met.
Adhere to standards for testing, documentation, and change management.
What Ingredients You'll Bring:
Minimum Requirements
Bachelor’s degree in Computer Science, or a closely related field, or the equivalent
3+ years in a data integration or data warehouse development role, preferably in a Microsoft environment
3+ years hands-on development experience with SQL Server, SSIS
1+ years working with Azure cloud data technologies including migration, integration, and design.
Powershell experience a must.
Preferred Requirements
Microsoft Power BI platform, including DAX and Power Query expressions and Analysis Services tabular a big plus.
Python scripting a plus
MuleSoft, SAP Hana data experience helpful
Other cloud platforms such as AWS a plus
Some Perks:
Our people are our most important “ingredient.” We hire the best talent; and we reward, develop, and retain them too.
In addition to generous healthcare on day one, stock purchase plan, 401k and more, Full Time Boston Beer Coworkers have the following perks available*:
Tuition reimbursement
Fertility/adoption support
Free financial coaching
Health & wellness program and discounts
Professional development & training
Free beer!
*Talk to your recruiter about eligibility
Boston Beer Corporation is an equal opportunity employer and is committed to a diverse workforce. In order to help ensure reasonable accommodation for individuals protected by Section 503 of the Rehabilitation Act of 1973, the Vietnam Veteran’s Readjustment Act of 1974, and Title I of the Americans with Disabilities Act of 1990, applicants who wish to request accommodation in the job application process can contact jobs@bostonbeer.com for assistance.
#LI-AA1
#LI-Hybrid"
132,[ NO C2C candidates ] Informatica Data Warehouse Engineer,Production Modeling Corporation,"Troy, MI","$130,000 - $134,000 a year","Note: This is not a C2C or 1099 position, per the client. This is a W2 contract engagement. We are a prime vendor with the client
Location: the Troy MI area and/or Herndon, VA; must be able to relocate to either location within a reasonable time after hire.
Duration: 3+ years
Location: US based remote
Description:
Purpose of Role/Organizational Unit:
The Senior data engineer is Responsible for support and maintain data and big data platform environments and all the data related integration, processing, services and management applications, He/She will trouble shooting, research, develop, optimize, and innovate frameworks and patterns for enterprise scale data analysis. Development and delivery of DI Use Cases and improving Data Platform reliable and stability to make sure business use cases performance.
Strong Informatica knowledge and skills
BS degree, preferably in Computer science.
Good communication skills with right attitude to blend in with team
Minimum 5 years work experience in IT and Data ecosystems
Experience in process analysis, requirements analysis and system engineering
Experience in Data Interaction to design strong and solid data integration and processes to enable good quality data to business stakeholders • Strong written and oral communication skills
Independent, structured and analytical work
National/International availability.
Process and technology innovation with the business
IT experience in business systems analysis in a large scale environment across multiple systems
Business process improvement involving broad based information systems and utilizing data integration tools and techniques to effect business change
Strong technical knowledge and ability to express complex technical concepts in terms that are understandable to the business
ETL tools Informatica and programming language
Experience in data platform performance tuning and support
Functions/Responsibility Performed:
The following functions are the responsibility of the Data Warehouse Architect. This individual may be asked to solely perform these functions or may choose to delegate some of these functions to other member of their team, per agreement with the Data Warehouse Competency Center Team Lead. Ultimately, responsibility for ensuring that the functions are performed rests with this position.
Develops and implements business intelligence tools or applications for accessing and analyzing data and meta data
Works with System Admin group to manage, monitor performance usage and administer the analytic server and BI reporting tool
5+ year Big Data environment experience
Works with Data Stewards to develop business-oriented meta data definitions
Works with data stewards and data warehouse architect to identify data quality issues and design processes and procedures to improve/maintain quality
Participate and review design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates
5+ years working experience on Data integration environment
Experience with ETL design – workflows, mappings, testing, etc.
Ability to support and troubleshoot issues within Informatica
Familiarity with Informatica Data Quality,
Experience with Oracle database skills
Familiarity with Informatica development/trouble shooting skills
Develop documentation and maintain as needed
Optimize the ETL environment
Must be self-motivated and able to work independently, consistently and meet deadlines
Strong business/technical oral and written communication skills
Demonstrated analytical ability and creativity in developing and implementing plans and solutions for complex problems on medium size projects
Strong SQL skills
Educational Requirements:
 Bachelor’s Degree or equivalent amount of work experience in business or IT consulting
Job Types: Full-time, Contract
Pay: $130,000.00 - $134,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee discount
Health insurance
Life insurance
Paid time off
Referral program
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Troy, MI: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 5 years (Required)
Data Warehousing: 2 years (Required)
Big Data: 2 years (Required)
Work Location: In person

Health insurance"
133,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
134,Data Engineer,GovDocs,"Saint Paul, MN 55121•Hybrid remote","$90,000 - $120,000 a year","Flexible Work Model:
We believe that the best way for us to grow, is through a work environment that allows us more flexibility whereby employees can be together in the office where interactions can happen with higher frequency and effectiveness (collaboration and team-engagement) – especially when dealing with complex problems and business innovation, balanced with work-from-home where we have more focused, uninterrupted time with minimal distractions for dedicated project/productive work.
Best Places to Work 2021:
GovDocs was named one of the 2021 Best Places to Work by the Minneapolis/St. Paul Business Journal!
Being named an honoree was no easy feat, as they received over 300 nominations for this year’s award. GovDocs was one of the top-scoring Minnesota businesses honored in the medium company category (50-250 employees) for creating a fun, challenging, and rewarding workplace.
Read more about GovDocs as Best Places to Work 2021 here: https://bizj.us/1qbeau
Position Summary:
We are looking for a savvy Data Engineer to join our growing engineering team. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, and business systems team on data analytics and reporting initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities:
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure Data Lake technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Understanding of non-relational databases such as MongoDB desired
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Ability to build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Strong critical thinking, decision making, troubleshooting and problem-solving skills
Excellent oral and written communication skills
We are looking for a candidate with 3+ years of experience in a Data Engineer role. Bachelor’s Degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field a plus.
Technical Experience/Skills Required:
Experience in Azure Ecosystem (Azure Data lake, Azure Data Factory, Azure Data Bricks, Azure Storage, Cosmos DB, ADO)
Understanding and experience working with Microsoft Azure DevOps (work items, build/release, CICD)
Experience in Database Analysis and modern warehousing technologies
Understanding of code development best practices, process design and automation, security concepts, Agile concepts, tools, and technologies
Knowledge on Data Ingestion/ streaming tooling such as Kafka, Spark, or similar technologies
Familiarity with DevOps landscape, processes, standards, and tools
Experience in Agile frameworks and methodologies (Atlassian, Azure DevOps), Software Development Lifecycle (SDLC) experience is a plus.
Strong understanding of data Ingestion, data transformation, data management, data quality, and data lineage services and technologies
Compensation
This position is critical to the success of our business and the compensation package will be commensurate with candidate's experience and skills. Compensation will include base salary and performance-based incentives. Benefits include paid vacation, paid volunteer time and paid holidays, medical and dental, and matching 401(k).
Company Description
GovDocs serves companies in building and executing their employment law compliance programs in two primary ways:
Employment Law Posting Service: We manage all the complexities of identifying and providing the required set of employment law postings from the 1,700+ potential postings across the U.S. and Canada. Using proprietary technology, we allow companies to manage, track and verify postings at each of their locations – including our patent pending PosterCheck technology. Our Employment Law Posting Update service is used by almost 22% of the Fortune 500 and 30% of the Fortune 100.
Employment Law Compliance Software Solutions: This is a first to market Software-as-a-Service starting with our Minimum Wage product and Paid Leave products, which allow companies to identify and track which laws apply to their locations and employees, then provides all relevant data to make decisions. This service was created in response to requests from some of our largest customers who recognized that employment law expansion across and variances between jurisdictions (Federal, State and Local) were too difficult to manually track.
GovDocs has grown revenue annually by 18% since 2008 with a 96% customer retention rate, primarily due to our Postings Update Program both obtaining and retaining customers – as employment law postings are required by law, every company has an existing provider. Our Software Solutions are an entirely new line of business that gives us great growth potential to create a full Employment Law Compliance (ELC) platform of solutions, it is also challenging us to reinvent how we view urgency, innovation and teamwork.
You must be authorized to work in the United States. Immigration or work visa sponsorship will not be provided. In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire
Job Type: Full-time
Pay: $90,000.00 - $120,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Day shift
Monday to Friday
COVID-19 considerations:
Flexible work model with working from home options. We take action to protect the health and well-being of our colleagues by regular review of Covid-19 statistics and health guidelines.
Application Question(s):
Will you now or in the future require sponsorship for employment visa status?
What are your compensation requirements?
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Saint Paul, MN 55121

Health insurance"
135,Data Engineer,Stytch,California•Remote,"$150,000 - $300,000 a year","What we're looking for
Stytch is the platform for user authentication. We build infrastructure that sits in the critical path of our customer's applications. As a data engineer, you'll work on designing and building event-driven architecture systems to drive analytics insights and observability tooling for our customers.

What Excites You
Championing data-driven insights - you see data analytics and observability as a product critical to success
Solving problems with pragmatic solutions — you know when to make trade-offs between completeness and utility and you know when to cut scope to ship something good enough quickly
Building products that make developers lives easier — as a data engineer for a developer infrastructure company, what you build will have an immediate impact on our customers
Shaping the culture and growing the team through recruiting, mentorship, and establishing best practices
Learning new skills and technologies in a fast paced environment

What Excites Us
Comfort working in a modern data stack using tools like Snowflake, Redshift, DBT, Fivetran, ElasticSearch, and Kinesis
Appreciation for schema design and architecture that balance flexibility and simplicity
Experience designing and building highly reliable back-end and ETL systems
3+ years as a data or backend engineer

What Success Looks Like
Technical — build new, highly reliable services that our customers can depend on
Ownership — advocate for projects and solutions that you believe in and ship them to production
Leadership — level up your teammates by providing mentorship and guidance

Our Tech Stack
Data moves through Snowflake, ElasticSearch, MySQL, and Kinesis
Go and Node for application services
We run on AWS with Kubernetes for containerization
gRPC and protobufs for internal service communication

Expected base salary $150,000-$300,000. The anticipated base salary range is not inclusive of full benefits including equity, health care insurance, time off, paid parental leave, etc. This base salary is accurate based on information at the time of posting. Actual compensation for hired candidates will be determined using a number of factors including experience, skills, and qualifications.
We're looking to hire a GREAT team and that means hiring people who are highly empathetic, ambitious, and excited about building the future of user authentication. You should feel empowered to apply for this role even if your experience doesn't exactly match up to our job description (our job descriptions are directional and not perfect recipes for exactly what we need). We are committed to building a diverse, inclusive, and equitable workspace where everyone (regardless of age, education, ethnicity, gender, sexual orientation, or any personal characteristics) feels like they belong. We look forward to hearing from you!

Learn more about our team and culture here!"
136,Data Engineer,Recruiting From Scratch,Remote,"$150,000 - $200,000 a year","Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
From Our Client
Location: New York / Remote
Job Type: Full-time
Experience: 3+ years
About the role
Our mission is to increase the number of financially independent people. We believe we can achieve this goal by building tools that enable independent business owners to scale their businesses profitably. Our first product combines a virtual credit card system with dynamic spending limits and software tooling to help merchants grow and optimize their profitability. We are growing very fast - in less than five months, we grew to millions in card volume. We have a significant waitlist of customers waiting to use our product. We are looking to expand our headcount quickly to support the demand. Our investors include Solomon Hykes (founder of Docker), Paul Buchheit (founder of Gmail), Paul Graham (founder of Y Combinator), Robert Leshner (founder of compound.finance), and many more. We have raised over $30M from top-tier fintech investors.
WHAT YOU'LL BE WORKING ON
You'll be involved in projects of varying scope and complexity:
Build credit risk models that segment merchants based on their revenue and spend patterns to offer dynamic credit limits that change with business performance given the high seasonality and fast pace of ecommerce
Use machine learning tools to build realtime credit underwriting models leveraging alternative and traditional data sources
WHAT YOU'LL NEED
Passion for, or curiosity to learn, financial technology
Track record of high-quality shipping products and features at scale
Ability to turn business and product ideas into engineering solutions
Desire to work in a fast-paced environment, continuously grow and master your craft




WHAT WE’D LIKE TO SEE
Experience with building out data pipelines (e.g. should know data lakes, data warehouses, ETL, etc.)
Experience working with data analytics, algorithmic decision making, and real-time data systems
Experience with different business requirements on data freshness and retention
PLUSES
Proven experience and subject matter expertise in e-commerce payments (nice to have) or financial services.
Has worked with bank account data (e.g. worked with Plaid), payments data (perhaps Stripe), and/or other fintech data sources




Why you should join us:
Our mission is to increase the number of financially independent people. We believe we can achieve this goal by building tools that enable independent business owners to scale their businesses profitably. Our first product combines a virtual credit card system with dynamic spending limits and software tooling to help merchants grow and optimize their profitability. We are growing very fast - in less than five months, we have grown to millions in card volume.
We have a significant waitlist of customers waiting to use our product. We are looking to expand our headcount quickly to support the demand. Our investors include Solomon Hykes (founder of Docker), Paul Buchheit (founder of Gmail), Paul Graham (founder of Y Combinator), Robert Leshner (founder of compound.finance), and many more. We recently closed a financing round of $5M from a top-tier fintech investor.
Salary Range: $150,000-$250,000 base."
137,Data Engineer,Numerator,United States•Remote,N,"Numerator is looking for an experienced, talented and quick-thinking Data Engineer to join a Numerator Product team as their Data expert.
This is a unique opportunity where you will get the chance to work on the data engineering and infrastructure of one of Numerator's fastest growing products. Our technology harnesses consumer-related data in many ways including gamified mobile apps, sophisticated web crawling and enhanced Deep Learning algorithms to deliver an unmatched view of the consumer shopping experience.
As a Data Engineer on the TruView team, you will make an immediate impact as you help build out and expand the product platform as the customers demands grow.
This role requires a balance between hands-on data engineering, infrastructure-as-code deployments as well as involvement in operational data architecture and technology choices for a customer-facing product.

How You’ll Spend Your Time
Design, document, and lead development of end-to-end data models/pipelines for Numerator’s growing datasets and product offerings.
Work with business, product, and data science teams to understand end-user requirements or analytics needs to implement the most appropriate technologies and scalable data engineering practices


3+ years designing data warehouses and building data pipelines or in a data intensive engineering role (preferably Snowflake)
Proficiency in SQL and one major scripting language (preferably Python)
Experience with data modeling, ETL design and tooling (especially Airflow), and transforming data to meet business goals
Experience designing and deploying production solutions to the cloud with AWS, Azure or GCP
Experience with declarative infrastructure, Docker and Kubernetes ((e.g. Terraform, EKS)
Availability to participate in after-hours on-call support with your fellow engineers and help improve a team’s on-call process where necessary
Nice to Haves
Experience creating integrations, managing user permissions and optimizing queries in Snowflake
Autonomy, versatility, intellectual curiosity, and ability to thrive in a fast-paced organization
Experience in CI/CD and Monitoring software such as CircleCI, Github Actions, Prometheus, Coralogix, Jenkins, Splunk etc.
What we offer you
More data than you could imagine to play with!
Data that matters and that is shaping the impact of billion dollar brands.
Brilliant, motivated and passionate colleagues with whom to spend your time.
An inclusive and collaborative company culture- we work in an open environment while working together to get things done, and adapt to the changing needs as they come.
Market competitive total compensation package.
Volunteer time off and charitable donation matching.
Regular hackathons to build your own projects and work with people across the entire company
Strong support for career growth, including mentorship programs, leadership training, access to conferences and employee resource groups.
Great benefits package including health/vision/dental, exceptional maternity leave coverage, unlimited PTO, flexible schedule, 401K/RRSPs matching and much more.
If this sounds like something you would like to be part of, we’d love for you to apply! Don't worry if you think that you don't meet all the qualifications here. The tools, technology, and methodologies we use are constantly changing and we value talent and interest over specific experience.
#LI-MB1
#LI-Remote"
138,Senior Data Engineer,First American Financial Corporation,"Santa Ana, CA•Remote","$95,900 - $230,200 a year","Company Summary
Come join First American's Digital Title Group, newly formed to re-imagine and digitize the title search and examination process through Big Data, AI, document automation and modern, cloud-native application development. As a market leading title insurance company, powered by the nation's largest and most complete property information, ownership and recorded document database, First American is committed to advancing title automation and removing friction from the real estate closing process. Our modern title decisioning solutions create certainty and speed through data and analytics, delivered to real estate agents, lenders, title agents and homebuyers. Join a team that puts its People First! Since 1889, First American (NYSE: FAF) has held an unwavering belief in its people. They are passionate about what they do, and we are equally passionate about fostering an environment where all feel welcome, supported, and empowered to be innovative and reach their full potential. Our inclusive, people-first culture has earned our company numerous accolades, including being named to the Fortune 100 Best Companies to Work For® list for seven consecutive years. We have also earned awards as a best place to work for women, diversity and LGBTQ+ employees, and have been included on more than 50 regional best places to work lists. First American will always strive to be a great place to work, for all. For more information, please visit www.careers.firstam.com.
Job Summary
Senior Data Engineer (remote)
Summary
We're looking for Senior Data Engineers interested in transforming our industry by solving cutting-edge problems with modern technologies, want the benefit of working for an established real estate insurance leader and seek a culture awarded as a Fortune 100 Best Companies to Work.
What you’ll do:
Develop ETL pipelines involving transformation of nested data stored in JSON and Parquet files, using GLUE + Pyspark
Develop and maintain scalable data pipelines and build out new API integrations for data transfer
Provide guidance on data architecture for tables, indices, caching strategies, etc. to help improve performance
Collaborate with analysts to perform data analysis and troubleshoot data issues
Must-have:
5+ years of development experience with any of the following software languages: Python, R, Scala, Java, Kotlin, or C# (we use Python)
2+ years of experience in any of these technologies: Snowflake, BigQuery, Databricks, Spark in any flavor, HIVE, Hadoop, Cloudera or RedShift (we use PySpark & Snowflake)
Strong Database/SQL experience in any RDBMS (We use PostgreSQL)
Nice-to-have:
Experience in schema design, data ingestion/processing experience on Snowflake (or equivalent MPP)
Experience developing in a containerized local environment like: Docker, Rancher, Kubernetes
Experience in orchestrating data processing jobs using Step Function/Glue workflow/Apache Airflow (MWAA)
Bachelor's degree in Computer Science (or related field) or equivalent combination of education and experience
Pay Range: $95,900 - $230,200 annually
This hiring range is a good faith and reasonable estimate of the salary range of possible compensation at the time of the posting and is subject to change. The actual compensation offered will be determined by various factors, which may include a candidate’s education, training, experience, and geographic location.
#LI-MD1
#LI-REMOTE
First American invests in its employees' development and well-being, empowers them to provide superior customer service and encourages them to serve the communities where they live and work. First American is committed to diversity and inclusion. We are an equal opportunity employer.
Based on eligibility, First American offers a comprehensive benefits package including medical, dental, vision, 401k, PTO/paid sick leave and other great benefits like an employee stock purchase plan."
139,Senior Data Operations Engineer,BetterUp,"Austin, TX•Remote","$136,850 - $193,000 a year","Let's face it, a company whose mission is human transformation better have some fresh thinking about the employer/employee relationship.

We do. We can't cram it all in here, but you'll start noticing it from the first interview.

Even our candidate experience is different. And when you get an offer from us (and accept it), you get way more than a paycheck. You get a personal BetterUp Coach, a development plan, a trained and coached manager, the most amazing team you've ever met (yes, each with their own personal BetterUp Coach), and most importantly, work that matters.

This makes for a remarkably focused and fulfilling work experience. Frankly, it's not for everyone. But for people with fire in their belly, it's a game-changing, career-defining, soul-lifting move.

Join us and we promise you the most intense and fulfilling years of your career, doing life-changing work in a fun, inventive, soulful culture.

If that sounds exciting—and the job description below feels like a fit—we really should start talking.
We're looking for a Senior Data / DataOps engineer who cares deeply about their craft, and who wants to use their skills to bring about positive change in the world while working in a high-performing organization. On the Data Operations team, our mission is to architect, build and operate a world class data platform enabling teams to apply analytics and ML for social good. We are product engineers that strive to enable all BetterUppers to build data driven products that further our mission.
We're looking for someone who is comfortable in the rapidly changing nature of a startup environment but also adept at moving relentlessly forward: doing what needs to be done to unblock projects that truly deliver value to our users. At BetterUp we delight in supporting and pushing each other to bring out the best in our colleagues, and would love someone to join the team who shares our passions for empathy, excellence, and continuous improvement. We also deeply understand that a key to peak performance is balance, and our culture is focused on providing the support our people need to be able to bring their whole selves to bear in service of our mission.
What you'll do:
Data evangelist: Bring, build, and drive data culture and best practices, enabling the product and engineering org to build better, more reliable, and secure data pipeline and data-driven products and powering use cases spanning internal and customer-facing analytics, data science / ML needs, and in-app experiences
DevEx delighter: Use tooling and automation to deliver a developer experience that enables teams to quickly and easily build out data products following mature SDLC principles
System designer: Passion for building systems, platforms, and tools that people use. You'll use your expertise in the broader data ecosystem and the modern architectures, approaches, and emerging technologies in this space, on top of a strong foundation on the fundamentals of building distributed systems in the cloud.
Act as an owner: It may start with a proof of concept but it's not done until it's in production. Adept at moving projects forward and able to unblock projects regardless of where we are in the development lifecycle.
Do less, deliver more: Familiar with the terms YAGNI and yak shaving? Focus your efforts on high-impact initiatives that really move the needle.
Impress yourself: We hold ourselves to quality above and beyond something that ""just gets it done."" Each system or line of code is an opportunity to demonstrate craftspersonship.
Collaborate without ego: Work together with teams to drive cross-team and cross-functional technical roadmaps, and willing to take on roles small or large in order to further the mission at hand.
If you have some or all of the following, please apply:
4+ years of relevant data engineering, data infrastructure, DataOps / MLOps, DevOps, SRE, or general systems engineering experience (high growth startup experience is a plus)
A leader for your teammates and driver of large cross functional projects within your organization
Familiarity or expertise using and maintaining modern data platform technologies and services like Kafka, Airflow, Snowflake, Segment, Stitch, Fivetran, dbt, Looker, etc.
Familiarity or expertise using and maintaining ML tooling and platforms like AWS Sagemaker, GCP Vertex AI, BentoML, MLFlow, Kubeflow, etc.
Experience doing infrastructure-as-code using tools like Terraform, Ansible, Chef, etc., and a pathological inclination towards automation and CI/CD
Full lifecycle ownership up through production and experience with observability and monitoring tools like DataDog, Honeycomb, Sentry, etc.
Experience architecting and implementing data governance processes and tooling (such as data catalogs, lineage tools, role-based access control, PII handling)
Strong coding ability in Python (preferred) or other languages like Java, C#, Golang, etc., and a solid grasp of SQL fundamentals
Benefits:
At BetterUp, we are committed to living out our mission every day and that starts with providing benefits that allow our employees to care for themselves, support their families, and give back to their community.
Access to BetterUp coaching; one for you and one for a friend or family member
A competitive compensation plan with opportunity for advancement
Medical, dental and vision insurance
Flexible paid time off
Per year:
All federal/statutory holidays observed
4 BetterUp Inner Work days (https://www.betterup.co/inner-work)
5 Volunteer Days to give back
Learning and Development stipend
Company wide Summer & Winter breaks
Year-round charitable contribution of your choice on behalf of BetterUp
401(k) self contribution
We are dedicated to building diverse teams that fuel an authentic workplace and sense of belonging for each and every employee. We know applying for a job can be intimidating, please don't hesitate to reach out — we encourage everyone interested in joining us to apply.

BetterUp Inc. provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, disability, genetics, gender, sexual orientation, age, marital status, veteran status. In addition to federal law requirements, BetterUp Inc. complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
At BetterUp, we compensate our employees fairly for their work. Base salary is determined by job-related experience, education/training, residence location, as well as market indicators. The range below is representative of base salary only and does not include equity, sales bonus plans (when applicable) and benefits. This range may be modified in the future.
The base salary range for this role is $136,850 – $193,000.
We value your privacy. Your personal data will be processed in accordance with our Privacy Policy. If you have any questions about the privacy of your personal data or your rights with regards to your personal data, please reach out to support@betterup.co
#LI-Remote"
140,Data Engineer,TalkingPoints,Remote,N,"About TalkingPoints
TalkingPoints is an award-winning education technology non-profit organization with the mission to unlock the potential of low-income, underserved families to support their children’s learning by partnering with their children’s schools. We are building a one-of-a-kind product in the market, a multilingual family engagement platform that connects families and teachers using human and artificial intelligence powered, two-way translated messages and personalized supports — eliminating language, time limitations, and know-how as barriers to school/family connection.

Millions of teachers and families are using TalkingPoints in their classrooms, schools and districts across the United States. TalkingPoints works with key school district partners such as Oakland Unified, Seattle Public Schools, Boston Public Schools, Buffalo Public School, Wake County and NYC DOE. We have won awards from NYU, MIT, Google, Forbes and are backed by Google, Eric Schmidt, AT&T, Cisco, and Stanford University to name a few.

TalkingPoints is at a critical inflection point in the organization's growth. We continue to see exponential growth in both our user base and our own internal structure. It's an important time to join our team and build on our momentum and ability to support students and families during this critical time in which education equity gaps are more concerning than ever. Learn more.

Diversity: we celebrate it, we support it, and we thrive on it!

The opportunity
As our Data Engineer, you will be responsible for developing and maintaining automated ETL/ ELT data pipelines for migrating data to and from our centralized data warehouse. The Data Engineer will utilize database tools/technologies including AWS, Snowflake, and dbt, and will collaborate with non-technical stakeholders to understand data needs and build interactive dashboards to support self-service access patterns and decentralized ownership of data assets.

The ideal candidate is passionate about improving equity in education for under-resourced students through AI-based, data-informed solutions. The Data Engineer will report to the Head of Data, Analytics, and Research and will collaborate with a cross-functional team of skilled data engineers and researchers.

Responsibilities:
Technical Implementation and Stakeholder Engagement
Develop and maintain automated CI/CD data pipelines for migrating, storing, and transforming data across the life cycle from ingestion to consumption
Optimize existing data pipelines to improve speed and scalability
Develop and maintain unit, integration, and regression tests for ensuring the reliability and accuracy of data transformations
Work with stakeholders to gather requirements and co-develop analytics dashboards for self-service access
Debug and develop fixes for data integrity issues
Collaborate with data engineers and researchers to develop machine learning solutions for improving product implementation fidelity, informing new product development, and evaluating product efficacy
Stay up to date with industry developments and recommend new technologies and approaches to improve data infrastructure and processes

Who you are
3+ years of experience in a data-focused, technical role
Successful history of manipulating, processing, and extracting value from large disconnected datasets
Experience developing cloud-based data solutions using AWS and/or GCP services
Experience using cloud-based data services such as Snowflake and/or Databricks
Experience implementing event-driven data solutions using streaming technologies such as Kinesis and/or Kafka
Experience implementing CI/CD processes for improving the reliability and speed of ETL/ELT pipeline development
Advanced SQL knowledge and expertise in query optimization
Knowledge of data modeling and data warehousing concepts
Strong ability to communicate effectively and collaborate with technical and non-technical stakeholders
Excellent problem-solving and critical-thinking skills

Nice to haves:
Bachelor’s degree in Computer Science, Information Technology, or a related field
Domain knowledge in education and current educational technology trends
Experience with SIS integrations using tools such as Clever, Classlink, and/or PowerSchool
Experience designing REST API methods
Experience working with non-relational databases such as MongoDB
Experience supporting machine learning and/or research applications
Experience in ETL tools/technologies such as dbt

Overall fit with our org culture - we look for team members who are
User and mission-oriented: we are devoted to our mission with empathy towards the communities we serve.
All-in together: we go above and beyond the job description, working together as a team.
Agile & action-oriented: we get things done, knowing that our progress is urgent to our communities.
Always Learning: we are always learning, with a growth mindset, to reach our full potential.
Boldly Courageous: we take risks in order to achieve big things.
Resourceful: We are creative, solution-oriented, and scrappy.

What we offer
An incredible opportunity to build a mission-driven, rapidly growing startup tech nonprofit
Competitive salary, full coverage health insurance, benefits, and unlimited time off
401K match
Annual professional development benefit
Flexibility to work remotely
Fun, smart, dynamic, and motivated team

How to apply
Please submit your resume, a brief paragraph describing your interest, and a link to your LinkedIn profile"
141,"Senior Software Engineer, Data Warehouse",Reddit,"New York, NY•Remote","$183,500 - $273,000 a year","Reddit is a community of communities where people can dive into anything through experiences built around their interests, hobbies, and passions. Our mission is to bring community, belonging, and empowerment to everyone in the world. Reddit users submit, vote, and comment on content, stories, and discussions about the topics they care about the most. From pets to parenting, there's a community for everybody on Reddit and with over 50 million daily active users, it is home to the most open and authentic conversations on the internet. For more information, visit redditinc.com.
As a Senior Software Engineer - Data Warehouse, you will build production facing tools on top of Reddit's petabyte-scale warehouse, and work directly with data customers to support analytics and reporting needs. Your work will enable data scientists, machine learning engineers, and product teams to create and access information at a massive scale, and you will have opportunities to develop new data tools from the ground up. If you have a passion for building and maintaining high quality code, and want to improve how Reddit makes strategic decisions at the company level, then this is the team for you!
What You'll Learn:
You will be exposed to the full lifecycle of data at Reddit, and as a result will gain expertise on how to improve the data culture across the entire company
You will collaborate directly with data science, experimentation, infrastructure, machine learning, and senior leadership
You will interact with one of the largest and richest datasets in the world, work with leading data technologies, and lead efforts that allow the Data Warehouse team to improve the performance and reliability of our stack
What You'll Do:
Build and scale data orchestration services that support complex analysis across Reddit
Write production level code that processes billions of events per day using core python & SQL
Design and implement tooling for access management, monitoring, data controls, and self-service ETL creation
Own data quality for crucial systems at Reddit, and serve as a primary resource for data expertise
Define and manage SLAs for datasets that support production services, including an on-call rotation for Data Warehouse tools
Mentor other engineers on how to design, build, and evangelize services used by hundreds of engineers across Reddit
Who You Might Be:
4+ years experience in the data warehouse space
4+ years experience working with large scale ETL systems (implementation, strategy, and maintenance)
4+ years of experience building clean, maintainable, object-oriented code in a production environment
Expert in python who is comfortable working in a production environment
Strong SQL and/or experience as a database admin
Excellent communication skills to collaborate with stakeholders at all levels of the company
Experience working with terraform, airflow, or similar data processing tools
Benefits:
Comprehensive Health benefits
401k Matching
Workspace benefits for your home office
Personal & Professional development funds
Family Planning Support
Flexible Vacation & Reddit Global Days Off
4+ months paid Parental Leave
Paid Volunteer time off
Pay Transparency:
This job posting may span more than one career level.
In addition to base salary, this job is eligible to receive equity in the form of restricted stock units, and depending on the position offered, it may also be eligible to receive a commission. Additionally, Reddit offers a wide range of benefits to U.S.-based employees, including medical, dental, and vision insurance, 401(k) program with employer match, generous time off for vacation, and parental leave. To learn more, please visit https://www.redditinc.com/careers/.
To provide greater transparency to candidates, we share base pay ranges for all US-based job postings regardless of state. We set standard base pay ranges for all roles based on function, level, and country location, benchmarked against similar stage growth companies. Final offer amounts are determined by multiple factors including, skills, depth of work experience and relevant licenses/credentials, and may vary from the amounts listed below.
The base pay range for this position is $183,500-$273,000.
#LI-SV1
#LI-Remote
Reddit is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, please contact us at ApplicationAssistance@Reddit.com."
142,Data Engineer,GovDocs,"Saint Paul, MN 55121•Hybrid remote","$90,000 - $120,000 a year","Flexible Work Model:
We believe that the best way for us to grow, is through a work environment that allows us more flexibility whereby employees can be together in the office where interactions can happen with higher frequency and effectiveness (collaboration and team-engagement) – especially when dealing with complex problems and business innovation, balanced with work-from-home where we have more focused, uninterrupted time with minimal distractions for dedicated project/productive work.
Best Places to Work 2021:
GovDocs was named one of the 2021 Best Places to Work by the Minneapolis/St. Paul Business Journal!
Being named an honoree was no easy feat, as they received over 300 nominations for this year’s award. GovDocs was one of the top-scoring Minnesota businesses honored in the medium company category (50-250 employees) for creating a fun, challenging, and rewarding workplace.
Read more about GovDocs as Best Places to Work 2021 here: https://bizj.us/1qbeau
Position Summary:
We are looking for a savvy Data Engineer to join our growing engineering team. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, and business systems team on data analytics and reporting initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities:
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure Data Lake technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Understanding of non-relational databases such as MongoDB desired
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Ability to build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Strong critical thinking, decision making, troubleshooting and problem-solving skills
Excellent oral and written communication skills
We are looking for a candidate with 3+ years of experience in a Data Engineer role. Bachelor’s Degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field a plus.
Technical Experience/Skills Required:
Experience in Azure Ecosystem (Azure Data lake, Azure Data Factory, Azure Data Bricks, Azure Storage, Cosmos DB, ADO)
Understanding and experience working with Microsoft Azure DevOps (work items, build/release, CICD)
Experience in Database Analysis and modern warehousing technologies
Understanding of code development best practices, process design and automation, security concepts, Agile concepts, tools, and technologies
Knowledge on Data Ingestion/ streaming tooling such as Kafka, Spark, or similar technologies
Familiarity with DevOps landscape, processes, standards, and tools
Experience in Agile frameworks and methodologies (Atlassian, Azure DevOps), Software Development Lifecycle (SDLC) experience is a plus.
Strong understanding of data Ingestion, data transformation, data management, data quality, and data lineage services and technologies
Compensation
This position is critical to the success of our business and the compensation package will be commensurate with candidate's experience and skills. Compensation will include base salary and performance-based incentives. Benefits include paid vacation, paid volunteer time and paid holidays, medical and dental, and matching 401(k).
Company Description
GovDocs serves companies in building and executing their employment law compliance programs in two primary ways:
Employment Law Posting Service: We manage all the complexities of identifying and providing the required set of employment law postings from the 1,700+ potential postings across the U.S. and Canada. Using proprietary technology, we allow companies to manage, track and verify postings at each of their locations – including our patent pending PosterCheck technology. Our Employment Law Posting Update service is used by almost 22% of the Fortune 500 and 30% of the Fortune 100.
Employment Law Compliance Software Solutions: This is a first to market Software-as-a-Service starting with our Minimum Wage product and Paid Leave products, which allow companies to identify and track which laws apply to their locations and employees, then provides all relevant data to make decisions. This service was created in response to requests from some of our largest customers who recognized that employment law expansion across and variances between jurisdictions (Federal, State and Local) were too difficult to manually track.
GovDocs has grown revenue annually by 18% since 2008 with a 96% customer retention rate, primarily due to our Postings Update Program both obtaining and retaining customers – as employment law postings are required by law, every company has an existing provider. Our Software Solutions are an entirely new line of business that gives us great growth potential to create a full Employment Law Compliance (ELC) platform of solutions, it is also challenging us to reinvent how we view urgency, innovation and teamwork.
You must be authorized to work in the United States. Immigration or work visa sponsorship will not be provided. In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire
Job Type: Full-time
Pay: $90,000.00 - $120,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Day shift
Monday to Friday
COVID-19 considerations:
Flexible work model with working from home options. We take action to protect the health and well-being of our colleagues by regular review of Covid-19 statistics and health guidelines.
Application Question(s):
Will you now or in the future require sponsorship for employment visa status?
What are your compensation requirements?
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Saint Paul, MN 55121

Health insurance"
143,Data Engineer,Tegria,Remote,"$100,250 - $156,700 a year","Data Engineer
As a Data Engineer, your work at Tegria will center on strategic opportunities and implementation, process improvement, and growing Tegria as a company. This role will focus on developing, constructing, testing, and maintaining data structures, data pipelines, and architecture. You will recommend and implement ways to improve the readability, efficiency, and quality of data.  A Data Engineer at Tegria will also be focused on delivering data sets for data modeling, data mining, and production reporting.
The role you play
An effective Data Engineer will help the organization on a whole achieve success through:
Data Engineer-Specific Responsibilities
Create and maintain optimal data pipeline architecture while working within time and budget constraints (i.e. SSIS, Azure Data Factory, Apache Spark, Databricks, etc.)
Work with stakeholders to assist with data-related issues and support their data infrastructure needs
Consult, assess and provide recommendations for improvement in client’s current data architecture
Assemble large, complex, and disparate datasets to meet functional and non-functional business needs
Utilize multiple programming languages to develop the best solution for the client (i.e. SQL, R, Python, C++, etc.)
Research and develop processes for utilizing cloud-based services (i.e. Snowflake, Google Cloud Services, Amazon Web Services, and Microsoft Azure)
Create data tools for analytics and data science team members that will aid in the development and optimization of our current and future services into an industry leader in healthcare analytics
Client Engagement Delivery
Working independently or as part of a project team on a client engagement. Could be full-time on a single customer engagement or part-time across customers
Serving as a liaison between diverse IT and operations groups
Facilitating meetings and owning meeting scheduling and coordination, preparation, documentation, and follow-up
Utilizing, reviewing, and creating project tools and templates for assigned projects
Creating and maintaining project plans
Evaluating and documenting current-state processes through discovery and analysis. Presenting recommendations for improvements based on industry experience and best-practices
Facilitating future-state workflow, policy, and process design and planning
Building, testing, training, converting and/or deploying new infrastructure, workflows, policies, and processes
Participating in major milestone reviews and decision gates
Presenting to a wide variety of audiences
Documenting measurable outcomes resulting from initiatives through KPI analysis and impact tracking
Effectively utilizing communication, decision-making, and escalation pathways
Executing effective project wrap-up through outcomes documentation, lessons-learned, and leave-behind materials allowing customers to sustain ongoing operations
Mentoring Associate(s) on project activities and deliverables and collaborating with others on the same
Mentoring customer counterparts for successful, long-term ownership and growth
What we’re looking for
We expect:
5+ years of professional experience working as a Data Engineer, ETL Developer, Database Administrator, or similar positions
Experience working with SSIS and other data integration tools and a desire to stay current as the data engineering field advances
A deep conceptual knowledge and demonstrated practical understanding of data modeling techniques and best practices
In-depth understanding of data warehouse design with advanced knowledge in querying languages such as SQL
Experience working with unstructured and semi structured data (JSON, free-text entries, etc.)
Demonstrated ability in project management (waterfall and/or agile), and other organizational management such as risk management, or change management
Capable of and comfortable with working remotely
Capable of and comfortable with traveling to client sites as needed
We’d love to see:
Prior consulting experience
Experience with interface engines such as Epic Bridges, HL7 or FHIR
Some experience implementing, supporting, optimizing, and upgrading Epic
Certification in one or more Epic data model and/or application(s)
Formal project management certification – either PMP or CSM
Formal process improvement certification – ex: Lean Six Sigma or ITIL
Need a few more details?
Status: Exempt
Eligibility: Must be legally authorized to work in the US without sponsorship
Work Location: This position is remote. Must work in a location within the US.
Travel: Up to 25%
Benefits Eligibility: Eligible
Now, a little about us
At Tegria, we bring bold ideas and breakthroughs to improve care, technology, revenue, and operations in ways that move healthcare organizations from patient-centered to human-centered. We are helping healthcare put people first—both patients and those who dedicate their lives to delivering care.
And at the very core of this vital work is our incredibly talented people.
People with different backgrounds who welcome challenge and change. People who listen first, ask hard questions, and make decisions to cultivate a culture of equity and inclusion. People who chase after goals, growth, and generosity. We’re real. We’re nimble, and we believe in our mission to humanize healthcare.
Perks and benefits
Top talent deserves top rewards. We’ve carefully curated a best-in-class benefits package, meant to meet you wherever you are in your life and career.
Your health, holistically. We offer a choice of multiple health and dental plans with nationally recognized networks, as well as vision benefits, a total wellness program, and an employee assistance program for you and your family.
Your financial well-being. We offer competitive wages, retirement savings plans, company-paid disability and life insurance, pre-tax savings opportunities (HSA and/or FSA), and more.
And everything in between. Our lifestyle benefits are unrivaled, including professional development offerings, opportunities for remote work, and our favorite: a generous paid-time-off program, giving you the flexibility to plan a vacation, take time away for illness (or life’s important events), and shift your schedule to accommodate those unexpected curve balls thrown your way.
Tegria is an equal employment opportunity employer and provides equal employment opportunities (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. All qualified candidates are encouraged to apply.
Job Type: Full-time
Pay: $100,250.00 - $156,700.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Parental leave
Referral program
Vision insurance
Compensation package:
Bonus pay
Yearly pay
Experience level:
5 years
Schedule:
Monday to Friday
COVID-19 considerations:
Remote role, Certain positions require individuals to meet their client’s COVID-19 requirements (typically being fully vaccinated, including a booster)
Experience:
Data Engineering: 3 years (Required)
Work Location: Remote

Health insurance"
144,Senior Data Engineer (US Remote),Swyfft,"New York, NY•Remote","$110,000 - $165,000 a year","Swyfft Holdings, LLC, consists of Swyfft, LLC and Core Programs, LLC. Both are fast-growing, tech-enabled MGA’s that are disrupting the traditional insurance industry by re-imagining how you price and bind home insurance and commercial package products. From lightning-fast quotes to hassle-free claims servicing, Swyfft Holdings, LLC leverages big data to provide the very best customer service experience in the industry. We're growing, we’re expanding and we're looking for “tech-savvy” folks like you to join our team!

This position is a U.S. remote based opportunity. Some travel for team meetings and trainings may be required

About the Position:
As a Senior Data Engineer, you will be instrumental in supporting the development and use of our data systems. Your goal is to ensure that information flows timely and accurately to and from the organization as well as within. As a successful Senior Data Engineer will bring forth a strong understanding of databases and data analysis procedures. You are a highly technical SQL expert with strong problem-solving skills and excellent troubleshooting capabilities. In a perfect world, you might even have some previous experience working for an insurtech or an analytics measurement platform.

Key Responsibilities: (What you'll be asked to do)
Build efficient ways to organize, store and analyze data while maintaining availability and consistency.
Create processes and enforce policies for effective data management.
Formulate techniques for quality data collection to ensure adequacy, accuracy and legitimacy of data.
Devise and implement efficient and secure procedures for data handling and analysis with attention to all technical aspects.
Establish rules and procedures for data sharing with upper management and external stakeholders.
Support others in the daily use of data systems and ensure compliance to legal and company standards.
Provide assistance with reports and data extraction when needed.
Monitor and analyze information and data systems and evaluate their performance to discover ways of enhancing them (such as new technologies and upgrades).
Ensure digital databases and archives are protected from security breaches and data losses.
Troubleshoot data-related problems and authorize maintenance or modifications

The Successful Candidate: (what we're looking for)
You have a strong understanding of databases and data analysis procedures.
You have an analytical mindset and strong problem-solving skills.
You have excellent communication and collaboration skills.
You have intense attention to detail and quality assurance.

Some Requirements:
Expertise in SQL (MS and PostgreSQL).
Familiarity with modern database and information system technologies.
Expertise in both Tableau Desktop and Server.
7+ years of experience as a data manager.
Excellent understanding of data administration and management functions such as collection, analysis, and distribution.
Understanding of data warehousing and star schemas.
Basic familiarity with predictive analysis and data visualization techniques.
Solid understanding of R and Python environment configuration and basic programming.
Expert level in Microsoft Excel.
Understanding of spatial database functionality is a plus.

Education:
Bachelors’ degree or equivalent experience required in a related field.
Advanced degrees or Certifications are a plus.

Computer Skills:
You are familiar with predictive analysis and data visualization techniques using relevant tools such as: Tableau, Dataiku, R, Python.
Must be proficient with MS Office and other internal insurance related programs, systems or applications.
Ability to communicate effectively using programs such as Slack & MS Teams. You are comfortable sharing screens and video chatting.

Other:
Reliable high-speed internet connectivity required.
Designated quiet work from home space.

We Have a Great Benefits Package!
20 days of PTO annually
Medical, Dental, Vision
Short- and Long-Term Disability (Company Paid)
Life & AD&D (Company Paid)
Healthcare, Dependent Care and Transit FSA
401K with a generous matching contribution and no vesting schedule

Salary
$110,000 - $165,000

The salary range reflected above is a good faith estimate of base pay for the primary location of the position. The salary for this position ultimately will be determined based on the education, experience, knowledge, and abilities of the successful candidate.

It is the policy of Swyfft to provide equal employment opportunities to all employees and applicants for employment without regard to race, religion, color, ethnic origin, gender, gender identity, age, marital status, veteran status, sexual orientation, disability, or any other basis prohibited by applicable federal, state, or local law. EOE/AA/M/D/V/F.

Please Note: Swyfft is not accepting 3rd party agency resumes for this position, please do not forward resumes to our careers email address or Swyfft employees. Swyfft will not be responsible for any fees related to unsolicited resumes."
145,Senior Data Engineer,Expel,"Herndon, VA•Remote","$130,600 - $189,400 a year","Do you have a unique passion for data? On one hand you're a doer who loves to think about ways to build data pipelines, drive data quality and work with product and R&D teams to enable their projects. On the other hand you're a teacher and coach who loves seeing others understand and be successful with data.
Your experience wrangling data over the years has taught you a few valuable lessons. Firstly, you've come to accept that data comes in all shapes and sizes (and almost never the shape and size that you actually need right now). Secondly, you've realized that data engineering is much more than a quick shoots-and-ladders game of getting a pile of data from point A to B... in fact data quality and reliability are what sets apart the orgs that get this right. Finally, and most importantly, you've realized that the truly satisfying moments are the ones where you get to see the actual impact of your work and how it's helped others around you.
Expel is looking for someone with a passion for data quality, a vision for data modeling and drive to organize data. You'll have autonomy to help shape the future of data engineering at Expel by bringing your ideas to improve what we do and how we do it. You'll work across the company to understand the needs of your collaborators and help deliver solutions that allow them to self-service data analysis. You'll work closely with data scientists and engineers to enable research, support implementation of machine learning models and new product features.
Our team values collaboration and a ""how can I help?"" attitude. So, if you're someone who cares about making data engineering understandable to everyone, and finds mentorship exciting, we should talk!
What Expel can do for you
Provide opportunities where the decisions you make have a direct impact on our company and our customers
Empower you to scale yourself by teaching others what responsible data engineering is and how it can make an impact
Challenge you to push the boundaries of our data engineering, security, and technology vision
Provide an entertaining, small and highly transparent startup environment
What you can do for Expel
Bring others along with you on your journey by ensuring stakeholders understand our data
Work across our operations, engineering and service delivery teams, ensuring Expel can easily deploy, monitor, and manage all of our machine learning capabilities, and can easily self-service analysis of data that impacts their business organization.
Foster an atmosphere of experimentation, continuous learning, and improvement, while also driving a culture of quality
Stay aware of emerging techniques and papers that might benefit Expel
Contribute to an environment of mentorship and coaching of data engineering processes and tools
Contribute to Expel's external voice through blog posts and conference speaking
What you should bring with you
Passion for teaching and mentoring others about data engineering techniques and best practices
Ability and desire to collaborate and work with teams throughout the organization to foster research and innovation
Ability and desire to communicate technical concepts to audiences of various technical expertise
A passion for improving and building data platforms
Extensive experience with SQL and Python, BigQuery experience is a plus
Experience writing and deploying containerized application code (we use docker and kubernetes).
Experience building data pipelines (DBT, Airflow, Beam) with a solid understanding of data modeling, data access, and data storage techniques and best practices
Experience analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency
Experience enabling the development of machine learning models by helping to enable new research datasets, building training pipelines, and facilitating model inference at scale.
5+ years of data engineering experience with a deep understanding of engineering and quality best practices, particularly in SaaS environments
Additional information
The base salary range for this role is between $130,600 USD and $189,400 USD + bonus eligibility and equity.
We believe in paying transparently and equitably. Your salary will ultimately be based on factors such as your experience, skills, team equity, and market data. You'll also be eligible for unlimited PTO (which we model and encourage), work location flexibility, up to 24 weeks of parental leave, and really excellent health benefits.
We're only hiring those authorized to work in the United States.
We're an Equal Opportunity Employer: You'll receive consideration for employment without regard to race, sex, color, religion, sexual orientation, gender identity, national origin, protected veteran status, or on the basis of disability.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
#LI-Remote"
146,Data Analytics Engineer,State Farm,"Bloomington, IL 61701",N,"Overview:
We are not just offering a job but a meaningful career! Come join our passionate team!

As a Fortune 50 company, we hire the best employees to serve our customers, making us a leader in the insurance and financial services industry. State Farm embraces diversity and inclusion to ensure a workforce that is engaged, builds on the strengths and talents of all associates, and creates a Good Neighbor culture.

We offer competitive benefits and pay with the potential for an annual financial award based on both individual and enterprise performance. Our employees have an opportunity to participate in volunteer events within the community and engage in a learning culture. We offer programs to assist with tuition reimbursement, professional designations, employee development, wellness initiatives, and more!

Visit our Careers page for more information on our benefits, locations and the process of joining the State Farm team!
Responsibilities:
As a Data Engineer you will:
Work closely with data scientists and business experts to develop modeling solutions for actuarial and underwriting business problems
Building and maintaining data pipelines for the development, implementation, execution, validation, monitoring, and improvement of data science solutions
Establish business domain knowledge for State Farm data sources
Investigate, recommend, and initiate acquisition of new data resources from internal and external data sources
Identify critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that support and extend quantitative analytic deployment solutions
Qualifications:
We are looking for Candidates who have-
Required Skills:
Experience in programming languages such as Python, SAS, R
Experience with any opensource database such as PostgreSql, MySQL. Etc.
Experience with developing solutions on AWS or other distributed compute platforms.
Ability to learn and adopt new technologies and languages
Critical thinking skills to challenge current thinking and apply right technology to solve problems.
Bachelor’s Degree in Computer Science, Software Engineering, or related field
Preferred Skills:
Experience with the Model Building Lifecycle
Experience with CI/CD systems, preferably with GitLab, Jenkins, or AWS Code Deploy.
Experience using deployment automation technologies, preferably Terraform
Experience with P&C Insurance Data

****Applicants are required to be eligible to lawfully work in the U.S. immediately; employer will not sponsor applicants for U.S. work authorization (e.g. H-1B visa) for this opportunity*****
Office Location: Corporate office located in Bloomington, IL, OR State Farm Hubs: Richardson, TX; Dunwoody, GA; Phoenix AZ.

SFARM
#JOA
#LI-MC2

S"
147,Big Data Engineer,Jconnect Infotech Inc.,Remote,$50 - $60 an hour,"Position – Snowflake Data Engineer
Location – Initial Remote
Duration – Contract
Technical Skills: Python, AWS, SQL, Snowflake
Job Description:
Experience Required:
• Cloud data platform – Snowflake, SQL and deep knowledge of Data Ware house and datalake concepts
Experience on any data ingestion (ETL/ELT) tool.
Well versed in Snowflake architecture concepts.
Must have Hands-on experience with Snowflake utilities like Snow SQL, Snow pipe.
Must have implemented snowflake projects in production.
Strong problem Management, troubleshooting and analytical skills
Roles & Responsibilities:
Snowflake Developer is responsible for
Design, develop, test, deploy and maintain enterprise
level applications using snowflake platform.
Work with a variety of stockholders to understand
requirements and deliver solutions.
Create Integration design for projects that include
infrastructure details, major application flows/sequences and failover designs.
Take ownership of project and see it through completion.
Strong analytical and problem-solving skill.
Ability to work independently as well as part-of-team.
Job Type: Contract
Pay: $50.00 - $60.00 per hour
Schedule:
8 hour shift
Experience:
Snowflake: 3 years (Required)
SQL: 1 year (Preferred)
AWS: 4 years (Required)
Work Location: Remote"
148,Science Data Operations Engineer 1,JPL/NASA,"4800 Oak Grove Dr, Pasadena, CA 91109",N,"Job Details
New ideas are all around us, but only a few will change the world. That’s our focus at JPL. We ask the biggest questions, then search the universe for answers—literally. We build upon ideas that have guided generations, then share our discoveries to inspire generations to come. Your mission—your opportunity—is to seek out the answers that bring us one step closer. If you’re driven to discover, create, and inspire something that lasts a lifetime and beyond, you’re ready for JPL.
Located in Pasadena, California, JPL has a campus-like environment situated on 177 acres in the foothills of the San Gabriel Mountains and offers a work environment unlike any other: we inspire passion, foster innovation, build collaboration, and reward excellence.
Do you excel in an environment that values exploration and discovery? We have a universe of opportunities waiting for you!
The Jet Propulsion Laboratory (JPL) is NASA’s lead center for robotic exploration of the solar system. Our core competency is the end-to-end implementation of unprecedented robotic space missions to study Earth, the Solar System, and the Universe.
The Instrument Software and Science Data Systems (Org. 398) Section within JPL consist of multidisciplinary teams of engineers and technologists who provide expertise across the domains of instrument operations and science data systems.
We are responsible for:
Safely controlling JPL remote sensing instruments.
Transforming data collected by these instruments into scientific measurements and preserving them for future analyses.
Providing context and understanding to the measurements.
Making the measurements and related information accessible to a broad and global user community: scientists, operations, decision makers and society at large.
Our engineering teams build and operate high performance data processing, management and analysis systems capable of handling petabyte scale datasets in support of science discovery, research, operations and applications. They support JPL and NASA missions, as well as other science-based projects. Our research and technology development teams create new onboard and ground based technologies for data processing, analysis, modeling, reasoning, visualization, management, access and analytics that are infused into our data systems.
JPL, located in Pasadena, California, has a casual, campus-like environment situated on 177 acres in the foothills of the San Gabriel Mountains and offers a work environment unlike any other: we inspire passion, foster innovation, build collaboration, and reward excellence.
We are proud to be part of NASA and Caltech, as we explore the universe and make history through new discoveries.
We aim to do things never done before and to go places few can go. We dare mighty things…do you?
The Instrument Software and Science Data Systems Section is seeking a Science Data Operations Engineer.
Job Responsibilities:
Will be a member of the Science Data Operations Engineering Group reporting to the Technical Group Supervisor. We are seeking a highly motivate and enthusiastic Data Operations Engineer to join our team, supporting missions such as the NASA-ISRO Synthetic Aperture Radar (NISAR) mission. The successful candidate will play a key role in assisting the processing, management, and distribution of large volumes of science data generated by Earth orbiting satellites operated by the JPL team, enabling groundbreaking scientific research in Earth observation.
Required Skills:
Educational background: A bachelor's degree in Engineering, Computer Science, Earth Science, Remote Sensing, or a related field.
Basic programming skills: Familiarity with at least one programming language, such as Python, C++, or Java, for data processing and scripting tasks.
Cloud computing fundamentals: Basic understanding of cloud computing concepts and exposure to cloud-based platforms such as AWS.
Data management: Knowledge of data storage, organization, and management principles, including ensuring data integrity, accessibility, and security.
Problem-solving skills: Ability to analyze and solve problems related to data processing, storage, and distribution.
Teamwork and collaboration: Ability to work effectively both independently and collaboratively within interdisciplinary teams.
Communication skills: Good written and verbal communication skills, with the ability to present complex information clearly and concisely to diverse audiences.
Adaptability: Willingness to learn new technologies and methodologies, adapting to the evolving requirements of the NISAR mission.
Time management: Ability to effectively manage time and prioritize tasks to meet deadlines and project milestones.
Desired Skills:
Programming languages: Proficiency in Python which is commonly used for data processing and scripting tasks. Understanding of Jupyter Notebooks and Boto a plus.
Cloud platforms: Knowledge of cloud-based platforms such as AWS, Azure, or Google Cloud, and familiarity with their respective services and tools. Understanding of High Performance computing platforms a plus.
Containerization and orchestration: Familiarity with containerization technologies like Docker and orchestration tools such as Kubernetes or serverless computing, which are essential for efficient deployment and management of cloud-based applications.
Synthetic Aperture Radar (SAR) data processing: Interest or background in SAR data processing techniques, including SAR interferometry and polarimetry, which are relevant to the NISAR mission.
Large-scale data storage: Exposure to large-scale data storage solutions, including distributed file systems, object storage, and cloud-based services.
Geospatial data formats and standards: Familiarity with geospatial data formats, metadata standards, and coordinate reference systems used in Earth observation and remote sensing.
JPL has a catalog of benefits and perks that span from the traditional to the unique. This includes a variety of health, dental, vision, wellbeing, and retirement plans, paid time off, learning, rideshare, childcare, flexible schedule, parental leave and many more. Our focus is on work-life balance, and living healthy, fulfilling lives as we Dare Mighty Things Together. For benefits eligible positions, benefits are effective the first day of the month coincident with or immediately following the employee’s start date.
For further benefits information click Benefits and Perks
The hiring range displayed below is specifically for those who will work in or reside in the location listed. In extending an offer, Jet Propulsion Laboratory considers factors including, but not limited to, the candidate’s job related skills, experience, knowledge, and relevant education/training. Hiring range for this job may be adjusted based on primary work location outside of Pasadena, California. This adjusted range will be provided to candidates by the Recruiter when applicable.
The typical full time equivalent annual hiring range for this job in Pasadena, California.
$83,096 - $99,528
JPL is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to sex, race, color, religion, national origin, citizenship, ancestry, age, marital status, physical or mental disability, medical condition, genetic information, pregnancy or perceived pregnancy, gender, gender identity, gender expression, sexual orientation, protected military or veteran status or any other characteristic or condition protected by Federal, state or local law.
In addition, JPL is a VEVRAA Federal Contractor.
EEO is the Law.
EEO is the Law Supplement
Pay Transparency Nondiscrimination Provision
The Jet Propulsion Laboratory is a federal facility. Due to rules imposed by NASA, JPL will not accept applications from citizens of designated countries or those born in a designated country unless they are Legal Permanent Residents of the U.S or have other protected status under 8 U.S.C. 1324b(a)(3). The Designated Countries List is available here."
149,Data Engineer,DSMH LLC,"Chicago, IL 60661",$50 - $55 an hour,"Position’s Contributions to Work Group:
The mission of the Services Analytics team is to enable services growth goals by ambitiously transforming the way we serve our customers by providing insights to deliver outstanding customer experience, serve as a source for execution and strategic planning to drive tool and service adoption and sales to users growth. The client uses quantitative methods such as business simulations, data mining, modelling and advanced statistical techniques to solve problems. We are looking for a candidate who will be responsible for building, enhancing, and maintaining data visualization tools, performing deep-dive analysis, and building reporting to identify insights. This role will provide support to our Managed Accounts team.
Typical Day:
Key Job Responsibilities may include, but are not limited to: Enhancements to existing dashboards to improve capabilities and expand insights as well as creation of new dashboards Manage large and complex datasets to optimize business insights Other tasks as assigned by manager
Interaction with team: Role will support Global Accounts Team in Peoria which includes higher level management members. Team prefers to meet in person regularly.
Work environment: Minimum one to two days per week in person or more based on business need.
Education Requirements:
Technical Skills
- -3 years of experience working with: SQL Alteryx Tableau or Power BI Top Candidates Will Also Have Experience with: ETL tools Python Excel Statistical Analysis (Experience inclusive of internships)
Job Type: Contract
Pay: $50.00 - $55.00 per hour
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Chicago, IL 60661: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 3 years (Required)
SQL: 2 years (Required)
Work Location: One location"
150,Data Analytics Engineer,Ameresco,Remote,N,"Ameresco, Inc. (NYSE:AMRC) is a leading cleantech integrator and renewable energy asset developer, owner and operator. Our comprehensive portfolio includes energy efficiency, infrastructure upgrades, asset sustainability and renewable energy solutions delivered to clients throughout North America and Europe. Our solutions range from upgrades to facility’s energy infrastructure to the development, construction and operation of renewable energy plants combined with tailored financial solutions. We foster an entrepreneurial, collaborative, and forward-thinking culture that thrives with innovation, diversity of thought, and inclusion. We are excited with all that the future holds for our industry, planet, and communities.
Ameresco Asset Sustainability Group (ASG) provides best in class asset management software and consulting services to our customers throughout North America. These customers include Schools, Higher Education, Public Housing, Municipalities, State & Federal Agencies, Commercial and Industrial real asset portfolios. Utilizing our propriety AssetPlanner® software suite, with modules and applications covering Capital Planning, Maintenance Management, Energy Management, Carbon Management and more, ASG is well-positioned to advise, enable, and support Ameresco customers to make better decisions around capital funding appropriation, reducing operating costs, and building towards a more sustainable future.
The successful candidate will work with the AMERESCO’s Asset Sustainability Group to develop and deploy analytics solutions across our portfolio of energy efficiency and distributed energy projects.
Responsibilities:
Initial onboarding of new sites, including creation and validation of point/entity maps, configuration of data pipelines and tuning of fault detection and diagnostic routines;
Develop and maintain data pipelines to capture, process and transform raw data from energy and building automation system for analysis and reporting;
Refine data quality control procedures for energy billing, interval meter, and building automation system data to identify and correct errors and inconsistencies in the data;
Support development and deployment of sensor and system fault detection and diagnostic algorithms.
Develop new visualizations and reports to effectively communicate building energy performance issues and opportunities for optimization to stakeholders;
Support ongoing development of measurement and verification, monitoring based commissioning, and predictive maintenance offerings;
Work with teams through AMERESCO to identify and exploit opportunities to streamline and automate existing processes to increase quality and productivity; and
Perform other duties as required.
Minimum Qualifications:
BS in Computer Science, Mechanical or Electrical Engineering is required.
Demonstrated proficiency using MS Excel and Python for data analysis is required.
Additional Qualifications:
Excellent analytical and problem-solving skills.
Strong communication and collaboration skills to work effectively with stakeholders across organization.
Experience with building automation systems and renewable energy systems is desirable.
AMERESCO challenges the brightest, most talented and creative individuals in the industry by providing an environment that embraces initiative, diversity, and achievement along with comprehensive rewards, including people-oriented insurance, investment, and incentive plans.
Equal Opportunity/Affirmative Action Employer/Women/Minorities/Veteran/Disability."
151,Data Engineer,NuTechs LLC,"Troy, MI•Hybrid remote","$100,000 - $130,000 a year","Data Engineer
Troy (2-3 Days) / Remote (3-2 Days) - Local candidates preferred

Actively seeking a Data Engineer who possesses a strong passion for designing, optimizing, refactoring, and upgrading complex data solutions. The Insights team is a new team with an exponential growth opportunity – both in terms of technology and personnel, and they are looking for someone to share their mission that will transform their company’s future. With this position, you will have a rare opportunity to use your talents, passions, and expertise to help drive this massive change in how we build, organize, and optimize our backend systems and processes across all product lines. This position offers excellent career growth and promotional opportunities, and stellar compensation.

Required Experience (Must have)
Minimum 5+ years’ data engineering experience working with designing data models in both column store and relational databases, incorporating disparate data to solve complex business needs.
Minimum Bachelor’s degree or Graduate degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field.
Experience building and optimizing data pipelines, architectures, database modeling and data sets that meet business requirements.
Experience in building the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources in ‘big data’ technologies or similar. Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience in building processes supporting data transformation, analytics tools, data structures, metadata, dependency, and workload management. Assist Data Science team that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and optimizing our product into an innovative industry leader.
Experience in using Azure DevOps, or Jira project management/requirements management.
Experience with big data tools: Hadoop, Spark, Kafka, or a related technology.
Proficient in programming languages such as SQL, Python, R, Shell Scripting
Desired Experience (Nice to have)
Experience with NoSQL databases, including Postgres and Cassandra is a plus
Strong working experience with technologies like AWS/Databricks and (minimum 3+ years’ experience) working with big data, including expertise designing data models in both column store and relational databases, incorporating disparate data to solve complex business needs
AWS lambda functions (Python preferred)
AWS databases (Aurora, DynamoDB, or Redshift preferred)
AWS storage services (EC2, S3 preferred)
Data Lake experience or similar (AWS preferred)
Primary Responsibilities
Developing and implementing an overall organizational data strategy that is in line with business processes. The strategy includes data model designs, database development, implementation and management of data warehouses and data analytics systems.
Identifying data sources, both internal and external and work out a plan for data management that is aligned with organizational data strategy.
Coordinating and collaborating with cross-functional teams, stakeholders, and vendors for the smooth functioning of the enterprise data system.
Managing end-to-end data architecture, from selecting the platform, designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.
Identifying, designing, and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition."
152,Lead Software Engineer - Data Integration - Telecommute,UnitedHealth Group,"Eden Prairie, MN 55344•Remote","$97,300 - $176,900 a year","Combine two of the fastest-growing fields on the planet with a culture of performance, collaboration, and opportunity and this is what you get. Leading edge technology in an industry that's improving the lives of millions. Here, innovation isn't about another gadget, it's about making health care data available wherever and whenever people need it, safely and reliably. There's no room for error. Join us and start doing your life's best work.(sm)
You’ll enjoy the flexibility to telecommute* from anywhere within the U.S. as you take on some tough challenges.
Primary Responsibilities:
Support all phases of data integration projects through Discovery, Estimation, Design, Analysis, Development, Testing and Implementation of APIs
Effectively lead and own delivery for the work of the development team while using technical skills as a hands-on software engineer
Design and implement product features in collaboration with business and IT stakeholders. Design reusable components, frameworks, and libraries
Remain current on industry trends in integration including but not limited to API technologies, event driven architecture and industry best practices to lead the assessment, design, testing, implementation, and support of new and existing solutions
Participate in an Agile/Scrum methodology to deliver high-quality software releases through Sprints
Contribute to code review process to ensure development work adheres to standards, specifications, and best practices. Help the teams improve efficiency and quality of the development
Work with the business and across technology to analyze complex business requirements, then manage the design or redesign of complex solutions. Advise on viability and feasibility of alternative approaches
Provide oversight, mentorship, and technical leadership to junior team members to ensure timely execution of project deliverables
Provide governance oversight to ensure adherence to development strategy, standards, tools, and procedures. Guide the development teams to consistently apply appropriate development principles and standards during all phases of the development life cycle
Troubleshoot production support issues post-deployment and come up with solutions as required
Maintain up-to-date skills in API development technologies and a broad understanding of the business, business needs, and how applications support the business
Provide expertise in the selection and implementation of development tools and best practices. Establish and maintain productive working relationships with business partners and project teams
Work very closely with architecture groups and drive solutions
Experience with integration tools such as MuleSoft / FiveTran / CData sync would be advantageous

You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.

Required Qualifications:
Bachelor's degree in computer science, software engineering, information systems or combination of education and related work experience
5+ years of professional experience in Software Engineering field with exposure to API development
5+ years of experience architecting services with MuleSoft or other integration solution such as FiveTran, cData or other similar tools
5+ years of experience with API management solutions such as MuleSoft, Apigee, Mashery, APIExchange or other similar tools
2+ years of experience on CI/CD methodologies using GITHUB, Jenkins or other similar tools.
Demonstrated knowledge of ability to translate customer's high-level requirements into technical specifications for the IT organization and manage changes to such specifications
Solid understanding of the Software Development Life Cycle (SDLC)
Ability to travel on occasion for team meetings, 1-2 times/year
Full COVID-19 vaccination is an essential job function of this role. Candidates located in states that mandate COVID-19 booster doses must also comply with those state requirements. UnitedHealth Group will adhere to all federal, state and local regulations as well as all client requirements and will obtain necessary proof of vaccination, and boosters when applicable, prior to employment to ensure compliance. Candidates must be able to perform all essential job functions with or without reasonable accommodation.
Preferred Qualifications:
Apply analytical critical thinking skills for process development or problem resolution

To protect the health and safety of our workforce, patients and communities we serve, UnitedHealth Group and its affiliate companies require all employees to disclose COVID-19 vaccination status prior to beginning employment. In addition, some roles and locations require full COVID-19 vaccination, including boosters, as an essential job function. UnitedHealth Group adheres to all federal, state and local COVID-19 vaccination regulations as well as all client COVID-19 vaccination requirements and will obtain the necessary information from candidates prior to employment to ensure compliance. Candidates must be able to perform all essential job functions with or without reasonable accommodation. Failure to meet the vaccination requirement may result in rescission of an employment offer or termination of employment
Careers with Optum. Here's the idea. We built an entire organization around one giant objective; make health care work better for everyone. So when it comes to how we use the world's large accumulation of health-related information, or guide health and lifestyle choices or manage pharmacy benefits for millions, our first goal is to leap beyond the status quo and uncover new ways to serve. Optum, part of the UnitedHealth Group family of businesses, brings together some of the greatest minds and most advanced ideas on where health care has to go in order to reach its fullest potential. For you, that means working on high performance teams against sophisticated challenges that matter. Optum, incredible ideas in one incredible company and a singular opportunity to do your life's best work.(sm)
Colorado, Connecticut or Nevada Residents Only: The salary range for Colorado residents is $97,300 to $176,900. The salary range for Connecticut / Nevada residents is $97,300 to $176,900. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.
*All Telecommuters will be required to adhere to UnitedHealth Group’s Telecommuter Policy.

Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.
UnitedHealth Group is a drug-free workplace. Candidates are required to pass a drug test before beginning employment."
153,Software Engineer - Data,Saildrone,"1050 W Tower Ave, Alameda, CA 94501","$130,000 - $190,000 a year","About Us
At Saildrone, we sustainably explore, map, and monitor the oceans to understand, protect, and preserve our world. Saildrone provides real-time access to critical data from any ocean on earth, 24/7/365, and uses proprietary software applications to transform that data into actionable insights and intelligence. Saildrone’s fleet of uncrewed surface vehicles (USVs), powered by renewable wind and solar power, have a minimal carbon footprint and operate without the need for a crewed support vehicle. We work with governments, civil agencies, foundations, universities, and private companies around the globe to drive better information about our oceans and seas—from sailing into the eye of a category 4 hurricane to obtain new data about how storms intensify collecting new CO2 data in hard-to-reach areas, and counting fish biomass to inform sustainable fishery management, to mapping the ocean floor and reducing illegal fishing and drug trafficking. As a result of our work, we have been included on Fast Company’s annual list of the “World’s Most Innovative Companies for 2022”, earned the 2022 Ocean Awards’ Innovation Award, won “Best Tech For Good” in the 2022 Timmy Awards, and were recognized by Andreessen-Horowitz’s American Dynamism 50 list of the “Top 50 companies kickstarting American renewal. Saildrone’s hurricane mission was included as one of the New York Times’ “21 Things That Happened for the First Time in 2021,” and Popular Science's “100 Greatest Innovations of 2021.”
We are based in Alameda, CA, with offices in Washington DC, St. Petersburg, FL, and Fall River, MA, and operate our missions worldwide. We are backed by top-tier investors in the frontier-tech and sustainability sectors, including Social Capital, Capricorn, Lux Capital, BOND Capital, and Emerson Collective.
This is an exciting opportunity with a fast-growing team at the cutting-edge intersection of big data services and autonomous hardware. You will be an integral part of a high-performing multi-disciplinary delivering high impact for humanity and future generations.

The Role
Data is Everything. The Saildrone fleet of ocean-going vessels tirelessly sails the world's seas piecing together the world's largest high-resolution meteorological oceanic dataset at a rate, precision and price point that was previously impossible. With NOAA approved data precision and demonstrated reliability, two other datasets that you'll be working with are ocean floor depth mapping and maritime domain awareness, which is anything associated with the safety and security of our oceans, such as illegal fishing and drug enforcement.
You will build the data pipeline powering both our external APIs and internal business intelligence about our drones. An effort spanning architecture, development, reliability, and scale, it is critical in our mission to deliver planetary insights.

Required skills and experience
3+ years of software engineering experience
Experience in data pipeline/platform software development in cloud environments Familiarity with pub/sub or stream processing technologies
Experience with SQL and relational databases
Familiarity with data warehouses, and querying structured and semi-structured data
Experience optimizing backend services and designing REST APIs
Desired skills and experience
Expertise in Python or Node.js programming languages
Familiar with Protobuf or Parquet data file formats
Experience with developing and deploying distributed services with Docker/Kubernetes
Experience with automation/orchestration tools like Ansible or Salt for executing commands on remote systems
Experience working with time series, geospatial, geo-temporal, or map-tile data would be a huge bonus
Experience with data from drones or robots, such as those from ROS-based systems, would also be very helpful
Knowledge of visualization tools such as Jupyter Notebooks, Tableau, AWS Quicksight, Streamlit
Experience building data pipelines for Machine Learning applications
Working knowledge of Athena/Presto/Trino, BigQuery, Redshift, Snowflake, or Vertica data querying technologies
NoSQL experience with Cassandra, DynamoDB, Redis, or MongoDB
Familiar with Argo, Airflow, or Luigi workflow orchestration engines
Experience with serverless functions, such as AWS Lambda
BS in Computer Science, Data Engineering or related field
Physical Requirements
Work is performed on a computer and requires ability to operate a keyboard and other peripheral devices.
Location: This position is in Alameda, CA. Our waterfront office offers beautiful views of San Francisco Bay in always sunny Alameda. Even our walls have good karma, our offices mix software development with a hardware production line in the former airplane hangar used to film 'The Matrix'.
Benefits:
Medical, dental and vision plans for you and your dependents.
Short and relaxing ferry ride from the Ferry Building for SF residents
Enhanced Parental Leave Programs
Competitive benefits including excellent medical, life insurance, 401k plan

Catch up on the latest news about us:
Interagency Public-Private Partnership Sends Uncrewed Saildrone to Explore Remote Alaskan Waters - NOAA OER (August 11, 2022)
An Unprecedented View Inside a Hurricane – EOS (May 6, 2022)
Meet the sailing robots trying to solve climate change – The Hill (Mar. 18, 2022)
They Sailed Into a Hurricane, Now these Unmanned Saildrones are Seeking Data on Our Carbon Uptake – CNN (Dec. 9, 2021)
Saildrone Catches a $100M C Breeze to Build More Robo-boats – TechCrunch (Oct. 18, 2021)
With A Sight, Sound And Radar Picture, Saildrone Could Build An AI Database Of Everything In The Ocean – Forbes (Aug. 16, 2021)
Changing Arctic Environment Could Also Change Technologies Needed to Protect It, Experts Say – SeaPower Magazine (Aug. 2, 2021)
Autonomous Research Vehicle Completes Ocean Crossing from San Francisco to Hawaii – Hydro International (July 13, 2021)
Saildrone Featured Videos Playlist

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Individual compensation packages are based on geographic location, scope of the role, relevant experience, and the ability to deal with complexity and problem solve within our organization, among other factors
Individuals who require employer sponsorship to remain employed in the United States now or in the future will not be considered for hire for certain positions.
In accordance with Saildrone’s mandatory employee COVID-19 vaccination policy, please be advised that all employees are required to be vaccinated and boosted to safeguard the health of our employees and their families, our customers and visitors and the community at large.
Any unsolicited resumes/candidate profiles submitted through our website or to personal email accounts of employees of Saildrone are considered property of Saildrone and are not subject to payment of agency fees.

#LI-AG1
#LI-JK1
#LI-LP1"
154,Associate Data Engineer,The Trevor Project,Remote,"$80,000 - $90,000 a year","The Trevor Project is committed to the full inclusion of all qualified individuals. As part of this commitment, we will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact accommodationrequest@thetrevorproject.org.

About Trevor
The Trevor Project is the leading suicide prevention and crisis intervention organization for LGBTQ young people. We are a non-profit that provides 24/7 life-saving support via phone, text, and chat. We also operate the world’s largest safe space social networking site for LGBTQ youth, and run innovative research, education, and advocacy programs. We’ve been saving lives every day for over 20 years.

Role: Associate Data Engineer
Reports to: Software Engineering Manager
Location: This role will be remote in the continental US, Alaska, or Hawaii
Hours: Full-time
Salary Range: $80,000 - $90,000

Overview of the role
The Data Engineer will work closely with the Data Engineering team and help with expanding Trevor’s core Data competencies. You will be directly involved in helping with day-to-day data defects that affect existing dashboards and reports, buildingout data integrity tests, helping with the audit of table schemas, and helping maintain various data pipelining tools.

Who You Are
Experienced. Proven knowledge of databases and design, with an understanding of SQL and its concepts, with proficiency in at least one general purpose programming language, such as Python, JavaScript, Scala, Java, etc. Experience with main Google Cloud Platform services is preferred.
Collaborative. Demonstrated ability to communicate ideas clearly - both written and verbal - for audiences, both internal and external. Recognized success in creating relationships with cross-functional teams from varying levels, working closely with colleagues to brainstorm ideas, discuss pros/cons, and develop ways forward.
Communicative. Proven skill in distilling complex information into compelling, easily digestible narratives; presents information in an inspiring and persuasive way.
Service Delivery and Adaptability. Proven focus on proactive and inclusive issue resolution and continuous improvement for all systems and processes.Demonstrated flexibility in the midst of change, being able to successfully juggle multiple deliverables, and adapting to new situations with fresh ideas or innovative approaches.
Mission and Culture Alignment. Demonstrates awareness and support for The Trevor Project’s mission and vision: to end suicide and address mental health crises in the LGBTQ youth community, and create a world where all LGBTQ young people see a bright future for themselves.
What You'll Do
Contribute to our critical mission of ending suicide among lesbian, gay, bisexual, transgender, queer & questioning young people, while fostering an inclusive approach to your direct work and engagement with Trevor colleagues across the organization
Continually grow your LGBTQ competency and awareness
Improve and expand a robust Google Cloud BigQuery schema built using Data Build Tool (DBT)
Maintain and build data pipelines to provide growth and analytics team with cleaned data
Expand existing health alerts and tests for data pipelines to prevent invalid data being ingested into data warehouse
Work with a small but growing data team in providing the best data insights possible to Trevor stakeholders
Communicate technical problems to teammates (including product managers, quality assurance analysts, and software engineers) in clear and insightful ways
Other relevant tasks, duties, or special projects as assigned

Why Trevor?
A career that truly makes a difference in the lives of LGBTQ young people–every single day
Comprehensive health insurance, including plan coverage for various gender affirmation care (and we pay 100% of your employee contribution premiums for medical, dental, vision, AND basic life insurance)
Flexible Spending Accounts
Employee Assistance Program to help with confidential emotional support, work life solutions, financial solutions, legal assistance, or online support
Generous vacation and 12 paid holidays (one of our holidays is Harvey Milk Day!) plus two floating holidays , and three half-day Fridays during the summer
Pet insurance
Remote work from the continental US, Alaska, or Hawaii (we provide the technology, a monthly internet reimbursement, and a reimbursement to outfit your work-from-home space!)
Professional and Learning Development Trainings/Education: including a professional subscription to LinkedIn Learning, providing access to more than 13,000 high quality on-demand courses.
Online Subscription to Headspace, a digital mindfulness and meditation platform

The Trevor Project provides equal employment opportunities (EEO) to all employees and qualified applicants for employment without regard to race, color, religion, gender, gender identity or expression, ancestry, sexual orientation, national origin, age, handicap, disability, marital status, or status as a veteran. The Trevor Project complies with all applicable laws."
155,Data Engineer,Wawa,"Media, PA 19063•Hybrid remote",N,"Data Engineer
Wawa Corporate - Media, PA
Job Summary:
The Data Engineer role designs and develops scalable data solutions using data integration tools and technologies. The individual utilizes big data computation, data platforms and storage tools to create prototype and data products. Conduct build and testing of data pipelines and solutions. Additionally, Data Engineer integrates, tests data pipelines with Advance Analytics and AI platforms. Must be proficient with multiple data engineering and integration tools such as Scala, Python, Spark, Snowflake etc. in an AWS environment.
Principal Duties:
Responsible for designing and implementing solutions for loading both structured and semi-structured data design into multiple target data systems.
Design, develop, optimize, and maintain data pipelines and processes that adhere to data integration principles and business goals.
Solve complex data problems to deliver insights that helps our business to achieve their goals.
Code, test, and document new or modified data systems to create robust and scalable applications for data analytics.
Ensure that data pipelines are scalable, repeatable, and secure, and can serve multiple users within the company.
Design and implement data ingestion techniques for real time and batch processes for structured and semi-structured data sources into Wawa’s data lake and data warehouse platforms.
Understand complex business requirements and propose end to end and simplified enterprise information architecture solutions.
Develop and implement data design methods, data structures, and modeling standards which work with multiple business intelligence tools.
Work closely with Analytics team and implement their self-service and analytics requirements.
Work with Data Science practitioners and developers to make sure that all data solutions are
Collaborate with Analytics team to build solutions that enable business analytics. Develop quality scalable, tested, and reliable data services using industry best practices.
Manage all activities centered on obtaining data and loading into a data lake environment.
Assess the suitability and quality of candidate data sets for the Data Lake.
Balance business requirements with technical feasibility and set expectations on new projects. Recommend changes in development, maintenance and system standards.
Design and build integration components and interfaces in collaboration with Architects and Infrastructure Engineers as necessary. Perform unit, component, integration testing of software components including the design, implementation, evaluation and execution of unit and assembly test scripts.
Determine if the data received from the upstream systems are of good quality based on the rules and data quality validations defined and in case of any issues with the data quality analyze and come up with a preliminary summary of the root cause/issue.
Assist the Analytics team by leveraging Wawa’s Enterprise Data Platform ecosystem to design, and develop capabilities to deliver our solutions using Spark, Scala, Python and
Follow security standards for all data and tools that are being introduced in the team.
Basic Qualifications:
Bachelor’s degree in Computer Science/Engineering preferred
3-5+ years database, data integration experience
3+ years’ experience with Spark, Scala/Python, SQL and Big Data solutions
Preferred experience with Databricks and Snowflake
3+ years’ experience in designing and implementing the data architecture (conceptual, logical, physical & dimensional models).
Developing Enterprise Business Intelligence solutions on one or more of the following
EDW platforms: Snowflake, Redshift, Google Big Query
Experience implementing Big Data solutions using open source technologies
Strong knowledge of key scripting and programming languages such as Python, Java, Scala, etc
Experience with data integration tools such as Talend would be helpful
Experience designing and implementing various data pipeline patterns and strategies
Hands-on experience with dimensional modeling techniques and creation of logical and physical data models (entity relationship modeling, exposure to data warehouse design)
Strong knowledge of data security principles
Proven track record working with complex, interrelated systems and bringing that data together on Big Data platforms.
Wawa will provide reasonable accommodation to complete an application upon request, consistent with applicable law. If you require an accommodation, please contact our Associate Service Center.
Wawa, Inc. is an equal opportunity employer. Wawa maintains a work environment in which Associates are treated fairly and with respect and in which discrimination of any kind will not be tolerated. In accordance with federal, state and local laws, we recruit, hire, promote and evaluate all applicants and Associates without regard to race, color, religion, sex, age, national origin, ancestry, familial status, marital status, sexual orientation or preference, gender identity or expression, citizenship status, disability, veteran or military status, genetic information, domestic or sexual violence victim status or any other characteristic protected by applicable law. Unlawful discrimination will not be a factor in any employment decision.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Employee stock ownership plan
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
7 years
8 years
9 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Media, PA 19063: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What Big Data Tools/Technologies/Querying Languages do you have experience with (RedShift, Snowflake, Python, SQL, Google BigQuery, Spark, Scala, etc.)?
Experience:
Data Engineering: 7 years (Required)
Leadership: 3 years (Required)
Work Location: Hybrid remote in Media, PA 19063

Health insurance"
156,Data Engineer Associate,Carrot Fertility,"San Francisco, CA•Remote","$81,000 - $108,000 a year","About Carrot:
Carrot Fertility is the leading global fertility care platform for women, who are often at the center of fertility care decisions and consequences. Plus, Carrot serves people of every age, race, income, sex, sexual orientation, gender, marital status, and geography. Trusted by hundreds of multinational employers, health plans, and health systems, Carrot's comprehensive clinical program delivers industry-leading cost savings for employers and award-winning experiences for millions of people worldwide. Whether there is a need for care through fertility preservation, male-factor infertility, pre-pregnancy, IVF, pregnancy and postpartum, adoption, gestational surrogacy, or menopause, Carrot supports members and their families through many of the most memorable and meaningful moments of their lives.
The Role:
Carrot is seeking a Data Engineer Associate to join our rapidly growing Business Intelligence function as it scales to meet - and anticipate - the needs of the organization and its clients. As an Associate Data Engineer at Carrot, you will design, program, and test data processing applications to improve our data infrastructure. You will participate in all aspects of data engineering by supporting our existing data integrations and data pipelines, data ingestions, exchanges, and traditional data warehouse and business intelligence systems. You will collaborate with cross-functional teams to support data-driven decision-making and contribute to the development of data architectures and solutions.
This is an exciting opportunity to work with cutting-edge technologies and cloud-based platforms to build scalable and reliable data pipelines and databases. You will have the opportunity to work with a team of talented data engineers, data scientists, and business intelligence analysts to support data-driven initiatives that have a positive impact on people's lives. You will also have the chance to learn from experienced senior data engineers to further develop your technical skills and knowledge in data engineering.
The Team:
The Business Intelligence team at Carrot is a highly cross-functional team that is central to Carrot's long-term success. The growing team is led by our Senior Director of Analytics and Business Intelligence and includes data engineers, data scientists, and business intelligence analysts.
Minimum Qualifications:
Experience writing database queries in SQL
Experience with database relational modeling
Experience with Python development
Experience working with version control systems (Git preferred)
Preferred Qualifications:
Experience with unit and integration testing in Python
Experience with Snowflake, dbt and/or Prefect
Experience working with AWS cloud services
Ability to communicate effectively with technical & non-technical team members across the organization
Quick learner, with ability to quickly grasp new concepts and excel
Compensation:
Carrot offers a holistic Total Rewards package designed to support our employees in all aspects of their life inside and outside of work, including health and wellness benefits, retirement savings plans, short- and long-term incentives, parental leave, family-forming assistance, and a competitive compensation package. The expected base salary for this position will range from $81,000 - $108,000. Actual compensation may vary from posted base salary depending on your confirmed job-related skills and experience.
Why Carrot?
Carrot has received national and international recognition for its pioneering work, including Best Diversity, Equity, & Inclusion Product from the Anthem Awards, Fast Company's Most Innovative Companies, CNBC's 100 Barrier Breaking Startups, and more. Carrot is regularly featured in media reporting on issues related to the future of work, women in leadership, healthcare innovation and diversity, equity, and inclusion, including MSNBC, The Economist, Bloomberg, The Wall Street Journal, CNBC, National Public Radio, Harvard Business Review, and more.
Carrot is fully distributed, with teams in more than 40 states across the United States and dozens of countries around the world. It has received numerous workplace awards, including Fortune's Best Workplaces in Healthcare, Quartz's Best Companies for Remote Workers, and Great Place to Work and Age-Friendly Employer certifications. Learn more at carrotfertility.com."
157,Data Solutions Engineer I,HealthEquity,Remote,N,"Overview:
We are CONNECTING HEALTH AND WEALTH. Come be part of remarkable.

How you can make a difference
With moderate direction, the Data Solutions Engineer is responsible for implementing electronic inbound and outbound data feeds for clients and third-party administrators in accordance with established SLA’s OLA’s With limited direction, this position works with Oracle packages and procedures, Microsoft SQL Server stored procedures, XML, and data objects to identify and accurate data issues and system defects. In addition, this position involves writing SQL queries and scripts to support custom reporting requests, resolves data integrity issues and recommends process automation for large Oracle and Microsoft SQL Server database applications.

What you’ll be doing
Communicate professionally in a sophisticated verbal and written form with internal and external clients, vendors, and third-party administrators. This includes, but is not limited to, creating meeting agendas, hosting/facilitating meetings, handling the flow of information between groups or departments to ensure issues are resolved effectively, collaborating and coordinating with other business units to support efforts to maintain data integrity, and educates clients and vendors on file formats, and addresses file issues.
Analyze, identify, and resolve data conditions and anomalies in an efficient and effective manner by writing Microsoft SQL and/or Oracle scripts by linking multiple tables, grouping, unions, loops, cursors and sub-queries and function calls to manipulate and analyze data for modification.
Create detailed mapping plans to implement inbound or outbound participant file feeds (including eligibility, enrollment, claims, and payroll information).
Produce written specifications to capture business process requirements for proposed solutions.
Develop, maintain, and provide documentation on processes, scripts and case resolution procedures for future reference or use.
Expertly handle crises and present a positive demeanor and forward thinking.
Apply an understanding of and testing of flat files and inbound eligibility files and maps, additionally basic function of outbound, 834, trigger, and claims files

What you will need to be successful
Two years or more experience handling electronic data files or maintaining large amounts of data.
Two or more of experience writing Microsoft SQL/Oracle scripts to querying large datasets for comparison and analysis, linking multiple tables, grouping, unions, loops, cursors and sub-queries and function calls to manipulate and analyze data for modification.
Two years or more of implementing files and performing moderate problem solving in a client-facing environment.
Intermediate SQL knowledge - write/run research queries - Oracle or MSSQL
Intermediate SQL knowledge- use standard scripts/packages to modify data
Beginner level understanding of ETL and the concept of electronic files. Being able to review file specifications.
Intermediate knowledge of Excel to include Formula’s and V-lookup,
Intermediate knowledge of Text Editing software such as Notepad, Ultra Edit, etc.
Intermediate written and verbal communication skills
Intermediate level problem solving skills
Shows willingness to learn new methods, procedures, or techniques. Shifts strategy or approach in response to the demands of a situation.
Strong organizational skills and the ability to prioritize engineering decisions
A shown ability to adapt to the changing demands of business
Passion for helping people tackle problems.
A commitment to data security

#LI-Remote
This is a remote position.
Benefits & Perks:
Medical, Dental, Vision
HSA contribution and match
Dependent Care FSA match
Uncapped Paid Time Off
401(k) match
Paid Parental Leave
Ongoing Education & Tuition Assistance
Gym/Fitness Reimbursement
Award Winning Wellness Program
Come be your authentic self:
Why work for HealthEquity

HealthEquity has a vision that by 2030 we will make HSAs as wide-spread and popular as retirement accounts. We are passionate about providing a solution that allows American families to connect health and wealth. Join us and discover a work experience where the person is valued more than the position. Click here to learn more.

Come be your authentic self

HealthEquity, Inc. is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, age, color, religion, sex, sexual orientation, gender identity, national origin, status as a qualified individual with a disability, veteran status, or other legally protected characteristics. HealthEquity is a drug-free workplace. For more information about our EEO policy, or about HealthEquity’s applicant disability accommodation, drug-free-workplace, background check, and E-Verify policies, please visit our Careers page.

HealthEquity is committed to your privacy as an applicant for employment. For information on our privacy policies and practices, please visit HealthEquity Privacy."
158,Data Engineer,"Jefferson - Center City, Philadelphia, PA","Philadelphia, PA 19107",N,"PRIMARY FUNCTION:
Should be a strong technical Data Engineer with Microsoft Azure experience, a demonstrated ability to design, develop, and deliver to support Enterprise Analytics.
ESSENTIAL FUNCTIONS:
Delivers data engineering and data warehousing solutions by ingesting, integrating, and curating data to deliver information to the business and stakeholders
Implements data engineering using Microsoft Azure services like ADLS, Synapse Analytics, Azure Data Factory, Dedicated SQL Pool, Serverless Pool, Apache Spark Pool, Azure SQL DB, Azure Event Hub, Azure Stream Analytics, Azure Functions, Azure Analysis Service, and Databricks etc.
Applies and refines established development standards and coordinates teams of Data Engineers, Data Modelers, Integration Architects, ETL Developers, and BI Analysts across multiple projects.
Collaborates closely with business users and stakeholders to define data for the design, development, and deployment of new solutions that support strategic business priorities.
Develops processes to ingest data from different sources to Azure Data Lake.
Ensures seamless integration of data across the enterprise and drives automation of common and repeated tasks
Develops large scale data structures and pipelines to organize, collect, and standardize data that helps generate insights and addresses reporting needs
Provides coaching and training to junior and new team members on standards, processes, and documentation
Interacts with co-workers, visitors, and other staff consistent with the iSCORE values of Jefferson.
OTHER FUNCTIONS AND COMPETENCIES:
This role will work as part of a large team of Data Engineers, Data Modelers, Integration Architects, and ETL Developers under the management of the Enterprise Data Warehouse Manager and will need to have good collaboration skills.
EDUCATIONAL/TRAINING REQUIREMENTS:
Bachelor’s degree (or foreign equivalent) in Computer Science, Information Technology, or a related field along with three years of experience with: Data Engineering, Data Warehouse architecture, ETL (Extract, Transform and Load) processes; Microsoft Azure Synapse Analytics; Azure Data Factory/Synapse Pipeline; Azure Data Lake; Azure DevOps; CI/CD; Oracle and/or SQL Server Database; Python
EXPERIENCE REQUIREMENTS:
At least two years of Data Engineering and Data Warehousing work experience
At least two years of significant experience with Microsoft Azure
Must have experience in hybrid data processing methods (batch and streaming)
Must have design experience and a proven track record of designing and conducting development of well-structured, maintainable Data Engineering & Data Warehousing solutions.
Must be able to balance technical and resource management responsibilities in a very fast-paced environment.
Covid Vaccination is a requirement for employment at Jefferson for employees working at Jefferson’s clinical entities or at the University. If you are not currently vaccinated you will be required to receive the vaccination prior to hire date if you are offered employment, unless you request and receive an approved medical or religious exemption from Jefferson.
Jefferson includes Thomas Jefferson University and Jefferson Health, a dynamic university and health system with broad reach across the Delaware Valley. Jefferson is the second largest employer in Philadelphia and the largest health system in Philadelphia based on total licensed beds.
Through the merger of Thomas Jefferson University and Philadelphia University in 2017, our University includes ten colleges and four schools. We are an NCAA Division II university and an R2 national doctoral university offering undergraduate and graduate-level programs that provide students with a forward-thinking education in architecture, business, design, engineering, fashion and textiles, health, medicine and social science.
Jefferson Health, the clinical arm of Thomas Jefferson University, has grown from a three-hospital academic health center in 2015, to an 18-hospital health system through mergers and combinations that include hospitals at Abington Health, Aria Health, Kennedy Health, Magee Rehabilitation and Einstein Healthcare Network. We have over 50 outpatient and urgent care centers; ten Magnet®-designated hospitals (recognized by the ANCC for nursing excellence); the NCI-designated Sidney Kimmel Cancer Center (one of only 70 in the country and one of only two in the region); and one of the largest faculty-based telehealth networks in the country. In 2021, Jefferson Health became the sole owner of HealthPartners Plan, a not-for-profit health maintenance organization in Southeastern Pennsylvania. We are the first health system regionally to create an aligned payer-provider partnership.
Jefferson’s mission, vision and values create an organization that attracts the best and the brightest students, faculty, staff, and healthcare professionals, as well as the most visionary leaders to drive exceptional results.
OUR MISSION: We improve lives.
OUR VISION: Reimagining health, education and discovery to create unparalleled value
OUR VALUES: Put People First, Be Bold & Think Differently and Do the Right Thing
As an employer, Jefferson maintains a commitment to provide equal access to employment. Jefferson values diversity and encourages applications from women, members of minority groups, LGBTQ individuals, disabled individuals, and veterans."
159,"IT Data Engineer – Geospatial Analytics, Corporate Development (Remote)",Weyerhaeuser,"Seattle, WA•Remote","$92,500 - $138,700 a year","Description
At Weyerhaeuser, our IT team is on a mission to help make Weyerhaeuser the world’s premier timber, land and forest products company. We’re not just in the cloud, we’re implementing technology that keeps us at the forefront of innovation in our industry. We run a mix of commercial software and fully integrated, custom, cloud-native applications built on AWS and Microsoft Dynamics365 that support critical operations and a dynamic ecommerce business. If you want to be part of a world-class technology team changing the world we live in – come grow with us!
This position will have the flexibility to report out of Weyerhaeuser’s Seattle Headquarters, Weyerhaeuser’s Atlanta office or work remotely provided you are currently based (or plan to move to) one of the following states: Alabama, Arkansas, Arizona, California, Colorado, Florida, Georgia, Idaho, Louisiana, Maine, Michigan, Mississippi, North Carolina, Oklahoma, Oregon, Pennsylvania, South Carolina, Texas, Utah, Virginia, Washington or West Virginia. This role is not open to those who live in other states or countries.
We are building a modern, spatial data analytics platform using cloud technologies and are looking for a talented IT Data Engineer who is highly motivated and team oriented to help us deliver and operate it. The selected candidate will become part of a passionate, fast-moving organization of IT professionals who partner closely with Weyerhaeuser’s Portfolio Analytics, Portfolio Management, Natural Resource and Natural Climate Solutions, Land Development and Recreation businesses to create sustainable solutions for our planet.
Roles and Responsibilities:
Work with our business partners, data science, and IT teams to design, develop, and maintain geospatial data analytics platforms and pipelines, curated data models, and analytic solutions.
Develop and deploy pre and post processing methods for advanced analytical geospatial products developed by Weyerhaeuser data scientists.
Proactively identify ways to improve data reliability, efficiency, and quality. Identify incomplete data and data integration opportunities.
Embrace DevOps to quickly deploy high quality solutions with standard CI/CD and IaaC practices.
Train, mentor, and share knowledge with other team members and data analysts.
Follow change management processes and procedures consistent with IT controls.
Participate in on-going support of data products.
Participate in agile/scrum sprint ceremonies including planning, standups, reviews, and retrospectives.
Optimize usability and performance within cost constraints.
Perform root cause analysis of system issues.
Qualifications
Bachelor’s degree, or equivalent experience in Computer Science, Geography/GIS, Information Technology or related field.
Typically requires a minimum of four (4) to six (6) years of experience engineering end to end geospatial and data analytic solutions.
Ability to manage a diverse workload and with minimal supervision.
Possess a strong work ethic, exceptional customer service, and commitment to delivering results to the organization.
Outstanding verbal, written, and presentation skills to both technical and non-technical audience at all levels within the organization.
A passionate and creative problem solver.
A team player who excels at building relationships and enjoys working both collaboratively and as an individual contributor.
Commitment to expand knowledge and skills, is coachable, and a fast learner with natural curiosity and a hunger for self-improvement.
Technical Qualifications
Knowledge of orchestrating and operationalizing geospatial and non-geospatial data pipelines with diverse structured and semi structured internal and external data sources.
Knowledge of data modelling and data warehousing techniques with large and complex datasets.
Ability to write complex SQL queries and understand the methods to tune query performance.
Experience with programming languages such as Python, R, C#
Experience developing and enabling data and analytical solutions with cloud and data platforms (e.g. AWS, Azure, Snowflake, PowerBI).
Experience with Azure DevOps and deployment automation (e.g. CI/CD pipelines).
Understanding of infrastructure as code tools such as CloudFormation and Terraform
Understanding of machine learning concepts and tooling
Travel
Occasional travel required (approximately 1-2 times per year).
Compensation: This role is eligible for our annual merit-increase program, and we are targeting a salary range of $92,500 - $138,700 based on your level of skills, qualifications and experience. You will also be eligible for our Annual Incentive Program, which offers a cash bonus targeting 10% of base pay. Potential plan funding may range from zero to two times that target.
Benefits: When you join our team, you and your family will be covered by our comprehensive health benefits plan, which includes medical, dental, vision, and basic life insurance. We also support personal volunteerism, sponsor a host of diversity networks, promote mentoring, and provide training and development opportunities to help you chart your path to a fulfilling career.
Retirement: Employees may enroll in our company’s 401k plan, which includes a paid company match in addition to our annual contribution equal to 5% of your base salary.
Paid Time Off or Vacation: We provide eligible employees who are scheduled to work 25 hours or more per week with paid vacation for up to 3 weeks to use during your first year of employment. In addition, after being employed for six months, eligible employees begin to accrue vacation for future use. We also recognize eleven paid holidays per year, providing a total of 88 hours along with paid parental leave for all full-time employees.
Weyerhaeuser is an equal opportunity employer. Inclusion is one of our five core values and we strive to maintain a culture where all our people feel a sense of belonging, opportunity and shared purpose. We are committed to recruiting a diverse workforce and supporting an equitable and inclusive environment that inspires people of all backgrounds to join, stay and thrive with our team.
Job Information Technology
Primary LocationUSA-WA-Seattle
Schedule Full-time
Job Level Individual Contributor
Job Type Experienced
Shift Day (1st)"
160,Data Engineer,Apple,"Cupertino, CA",N,"Summary
Posted: Dec 22, 2021
Weekly Hours: 40
Role Number: 200327520
As part of our Video Engineering group, you’ll help deliver creative solutions to various problems that could impact the people all over the world. This Data Engineer will work closely with other members of the Video Engineering group to mine data, implement model evaluation pipeline, analyze large scale data, visualize data, and ensure the delivery is of the highest quality. This position will also require strong coding skills, presentation skills, and collaborating with multiple teams (ex: machine learning, cloud infrastructure support).
Key Qualifications
A curious mind
An obsession for quality
Background in Data science, Data mining, Multivariate statistics, Computer vision, Machine learning
Experience working with large scale data sets
Solid programming skills including:
Python
C/C++
Experience with data visualization and presentation, familiar with data analysis tools such as Tableau
Excellent problem solving and communication skills
Description
The responsibilities of this position includes the following for current and future products: - Implement algorithm evaluation methods - Analyze data and build data analysis tools - Deep-dive failure analysis - Discover new perspectives for old data - Produce / Present meaningful data visualization to higher-ups and across various involved teams
Education & Experience
Masters in Computer Science or relevant experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $104,000 and $190,000 annualized, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program."
161,AWS Data engineer,TekWisen Software Pvt. Ltd,Remote,$60 - $75 an hour,"Job Title: Data Engineer - AWS
Location: Madison WI 53703 - Remote
Duration: 5 Months
Job Type: Contract (c2c and w2)
Work Type: 100% Remote in USA
Job Description
As a Staff Data Engineer on Foundation Analytics, you will be responsible for building many key parts of the foundation data services platform. You’ll work in a collaborative Agile environment using the latest in engineering best practices with involvement in all aspects of the software development lifecycle. You will be responsible for ensuring the team makes sound design & configuration decisions to develop curated data products, apply standard architectural practices, and supports the Data Product Managers in evolving core data products. As a member of the data engineering team, you will help client to deliver impactful reporting products for our customers.
Publishing well written and tested code to production daily using technologies such as Linux, Docker, Kubernetes, AWS, Kafka and Python Drive data architecture and integration design and development discussions with engineering and other teams Investigate production issues and fine-tune our data pipelines Build a platform that will be the foundation for our customer facing reporting features, our machine learning initiatives, and internal product analytics
Perform rapid proto typing Participate in designing, developing key features and functionality of our data platform Continually improve the data platform development for high efficiency, throughput and quality of data Collaborate with team members with researching & brainstorming different solutions for technical challenges facing the team
Develop standard methodologies and mentor other engineers on the team to help make technical decisions on our projects and roadmap.,
Skills
7+ years of software development/data engineering experience 4+ years of hands-on experience of building scalable data platforms and/or reliable data pipelines
Proficiency in at least one of the following programming languages: Java, Python, Scala
Experience with AWS.
Experience in developing and operating high volume, high availability environments Working understanding of Kubernetes’ infrastructure and security best practices
Ability to work effectively in a dynamic, occasionally interrupt driven environment that includes geographically spread teams and customers BS degree in Engineering, CS, or equivalent
Education
Experience writing ETL jobs to help address various data engineering challenges
Strong understanding of Build tools and Deployment tools Familiarity with Kafka, Flink, Spark frameworks with validated understanding of at least one job scheduling tool: Airflow, Celery, AWS Step functions Tech Stack Our data pipelines are written in Java and Python based software stacks
We utilize many open-source technologies, including Spark, Flink, Hudi, Airflow Our software runs on AWS services like EMR and in Kubernetes, and integrates with AWS services S3, Athena, and Glue for data access
Job Type: Contract
Salary: $60.00 - $75.00 per hour
Experience level:
10 years
11+ years
Schedule:
8 hour shift
Experience:
software development: 9 years (Required)
Data Engineer: 9 years (Required)
data platforms: 7 years (Required)
data pipelines: 7 years (Required)
AWS: 9 years (Required)
ETL: 9 years (Required)
EMR systems: 9 years (Required)
Work Location: Remote"
162,Data Engineer,Fastechnowiz Solutions Inc,"Westlake, TX 76262",$57.63 - $65.00 an hour,"Position Details:
Title: Data Engineer
Industry: Banking & Financial
Duration: 12 Months- Long term
Location: Smithfield RI/ Durham, NC/ Westlake, TX
Top Skills: Informatica, SQL, Snowflake, python
Required Qualifications
ETL developer with Informatica
Strong SQL – Snowflake and SQL Server will be a huge plus.
Strong Analysis skills
Working knowledge of Unix OS /Shell scripting
Basic Python knowledge is required.
Good working knowledge of Control-M/Automation tools.
Some experience in DevOps
Production Support will be required – one week every 3 months.
Excellent interpersonal and communication skills
Excellent collaboration skills to work with multiple teams in the organization.
Additional Experience
Experience with Metadata management solutions / Data lineage is a plus
Learn New technologies and evaluate new products, participating in Proof of Concepts (POCs) is a plus
Vendor management is a plus
Some QA/Testing experience is a plus
Some Kubernetes / Docker experience is a plus
Strong communication and presentation skills
Job Type: Contract
Salary: $57.63 - $65.00 per hour
Benefits:
Health insurance
Parental leave
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Westlake, TX 76262: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 2 years (Required)
SQL: 3 years (Required)
Data warehouse: 3 years (Required)
Work Location: One location

Health insurance"
163,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
164,Data Engineer,Violet Ink,"Newark, NJ 07107•Hybrid remote",From $60 an hour,"Key Job Responsibilities
· Analyze data needs and objectives within the broader journey.
· Source, analyze and organize raw data, prepare data for transformation and consumption.
· Identify ways to improve data governance, reliability, efficiency, and quality.
· Build applications ensuring that the code follows latest coding practices and industry standards.
· Build using modern design patterns and architectural principles.
· Ensure developed solutions remain compliant with all applicable Prudential standards.
· Solve complex problems and provides new perspective on existing problems.
· Develop through collaboration and deliver application component solutions.
· Develop high quality, well documented, and efficient code supporting testing and automation.
· Support product owner in defining future stories and tech lead in defining technical designs.
Competencies – Knowledge, Skills, Abilities
Candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. Should have experience using following software/tools:
Big data tools
Relational and NoSQL databases
Data pipeline and workflow management tools
AWS cloud services
Stream processing systems
Object oriented and scripting language
Build processes supporting data transformation, data structure, metadata, dependency, and workload management.
Successful history of manipulating, processing, and extracting value from large, disconnected structured and unstructured datasets.
Advanced working SQL knowledge and experience working with relational databases.
Experience building and optimizing data pipelines, architecture, and data sets.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Strong project management and organization skills.
Experience supporting and working with agile cross functional teams in a dynamic environment
Background in financial services functions strongly desirable.
Job Type: Contract
Pay: From $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newark, NJ 07107: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
No SQL: 1 year (Required)
Work Location: Hybrid remote in Newark, NJ 07107"
165,Data Engineer,Ascent Technologies,Remote,$39.87 - $86.24 an hour,"Job Description: Involved in resource / volume ramp-up plan/ backfilling resources planned for rotation and resignation cases Tracking Performance of individual Projects within the delivery/SIPs. Participate / presenting SD performance during client visit Publish Daily, Weekly and Monthly reports on team productivity Reviewing the weekly / monthly dashboards Publishing dashboards to customer People Management Drive the team to reach the defined targets Ensure training and development of the team members Mentor team members on technical and communication aspects. onsite anni data enginneer-pheonix,az python,charlottee,nc ba-houston tx
Job Types: Full-time, Contract, Permanent
Salary: $39.87 - $86.24 per hour
Benefits:
Dental insurance
Compensation package:
1099 contract
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote
Speak with the employer
+91 7842516676"
166,Data Engineer,Data Ideology,Remote,N,"Data Ideology
At DI, we provide Data & Analytics expertise to drive measurable business outcomes, often solving complex business problems for our clients. Our data analytics advisory services enable our customers to transform data into insights by driving a culture of empowerment and ownership of results. Our team consists of highly motivated individuals passionate about learning, understanding, collaborating, and intellectually curious. For more information about Data Ideology, visit www.dataideology.com
Data Engineer - Contract to Hire (CTH)
We are looking for a Data Engineer to join our growing team. Data Engineer will leverage their business and technical knowledge to develop production-ready data models by integrating multiple data sources while working with business and technical teams to understand business strategy and objectives, gather information, and ensure business requirements are being fulfilled throughout the entire data & analytics lifecycle.
Key Responsibilities
To perform in this position successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions. Other duties may be assigned to meet business needs.
Ability to collect and understand business requirements and translate those requirements into an actionable data warehouse plan.
Knowledge of multi-dimensional and tabular design patterns and ability to identify solutions that leverage these modeling techniques.
Ability to work within the SDLC framework in multiple environments and understand the complexities and dependencies of the data warehouse built within those constraints.
Ability to define and implement best practices across database design and ETL.
Ability to direct the work of others, including but not limited to directing ETL development, demonstrating an understanding of key concepts of ETL/ELT, including best practices for optimization and scheduling.
Supervisory Responsibilities: None
Qualifications
Education and Experience:
Proven understanding of Data Warehousing, Data Architecture, and BI.
Experience with data pipelines and architecture/engineering.
Knowledge of modern apps and data platforms.
Cloud-based project implementation.
Google BigQuery experience
Knowledge, Skills, and Abilities:
BI/Data Warehousing (3+ years) - Google BigQuery
Cloud platforms (1+ years) - Google Cloud Platform
Dimensional Data Modeling (3+ years)
ETL (3+ years)
SQL (3+ years)
Business Intelligence (1+ years) - Power BI
Work Environment:
Remote work from home.
Hours of work and days are generally Monday through Friday. Specific business hours will depend on client needs.
Physical Demands:
Must be able to remain in a stationary position 50% of the time.
The person in this position must occasionally move about inside the office to access file cabinets, library stacks, office machinery, etc.
Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and printer.
The person in this position frequently communicates with clients and coworkers. Must be able to exchange accurate information in these situations.
Benefits:
Unlimited Discretionary Time Off Policy
100% company paid - insurance (medical, dental, vision) for employees
100% company paid - short and long-term disability insurance for employees
100% company paid - life insurance and AD&D insurance for employees
100% company paid – employee assistance program
Retirement plans with company match
Training and Certification Reimbursement annually
Performance-based incentive program
Commission incentive program
Profit Sharing Plan
Referral Bonuses
Data Ideology is an EEO Employer
ut8TrtP2lw"
167,UAM Data Communication Engineer,Supernal,"Fremont, CA","$133,120 - $186,160 a year","Supernal is at the forefront of creating emerging mobility solutions that will foster the development of human-centered cities. We are designing a completely new electric vertical take-off and landing (eVTOL) aircraft tailored to the mobility needs of future cities. This allows passengers a seamless intermodal journey that safely transports them to their final destination. We fuse research in autonomy, robotics, aviation and services to define a new category of mobility for the world's communities. We believe in creative thinking and collaboration to help build a better mobility experience for everyone, improving people's ability to move – whether for work or play.
What we do:
The UAM Data Communication Engineer is responsible for the design, development, and verification of several communication links for aircraft flight automation like real-time telemetry, ground link, vehicle to vehicle communication etc. from flight testing as well, flight simulator software. This role is also responsible for generating both high- and low-level requirements, performing peer reviews, developing unit and integration test plans, and completing verification and validation of several communication requirements for mission critical software.
What you can do:
Develop software and algorithms for aircraft communications, including like real-time telemetry, ground link, vehicle to vehicle communication, lost link, and user interface,
Surveying and contributing to V2V standardizations in various standard meeting.
Build software tools for simulation, log analysis, automated testing, and development prototyping.
Generate both high- and low-level software requirements, implement software algorithms, develop unit and integration test plans, and perform verification and validation of communication software.
Participate in the software architecture and design process of modular systems for embedded, desktop and hosted environments.
Analyze field reports, flight logs, fault analysis and bug reporting to ensure continued improvement of safety critical software.
Work collaboratively with other engineers working on software, algorithms, middleware and simulations.
Other duties as assigned
What you can contribute:
Bachelor's degree in a science, technology, engineering, or mathematics field, or equivalent experience.
A minimum of three (3) years of experience (an equivalent combination of education and experience may be considered)
Experience developing embedded software for aviation, automotive, or robotics applications.
Strong knowledge of wireless communication at physical layer and/or medium access control layers such as radio modulation/demodulation, channel coding, and channel access mechanism
Solid understanding of standardized communication technologies such as PC5-based C-V2X, LTE-V2V PC5, IEEE 802.11p/DSRC, IEEE 802.11ad etc
Experience with MAVLink, ADSB, Protocol development / integration experience (LCM, MAVLink, WiFi or others)
Strong proficiency in C required, Knowledge of C++,Python and ROS preferred
Experience with model based, code generation tools (e.g. MATLAB/Simulink, SCADE)
Experience in simulations and verification methodologies
Experience with ISO 26262, DO-178C, or other safety-related software and product development processes
Proficiency with Git, Linux and embedded RTOS(s)
Strong proficiency in modern software development workflows and practices, including version control, build and test systems, and peer review
You may also be able to contribute:
Master's degree preferred
Any offer of employment is conditioned upon the successful completion of a background check. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, citizenship, sex, gender, gender expression, sexual orientation, age, marital status, veteran status, or disability status. Individuals with disabilities may request to be provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation at: ta-support@supernal.aero
This position may include access to certain technology and/or software source code, that will be subject to U.S. export control laws. If an export license or other export control authorization is required in connection with your employment, your employment is contingent upon Supernal's receipt of such license or authorization and approvals and your continued compliance with all conditions and limitations contained in such license or authorization.
Base pay offered may vary depending on skills, experience, job-related knowledge and location. This position is also eligible for a bonus as part of total compensation.
The pay range for this position is:
$133,120—$186,160 USD
Click HERE or visit: https://jobs.supernal.aero/benefits to view our benefits!"
168,Data Visualization Engineer,Vertex Group,"Columbus, OH",From $50 an hour,"Must be Green Card Holder & US Citizen
Local to Ohio Only (Hybrid)
Experience Architecting and implementing data visualizations that visually present complex data and/or metric relationships to users for exploration, analysis, and action
Experience building wireframes, mock-ups, and ad-hoc visualizations/web pages that present the “art of the possible” for users to interact with and provide direction
Actively collaborates with data engineers, modelers, data scientists, and business leaders to provide design input and rapid prototypes
Strong customer focus and interaction to listen to users, explore the data, and present multiple alternatives to meet solution objectives
Works on and may lead complex activities of large data scope areas, providing solutions that may set a precedent
3+ years developing business-focused visualizations and user interfaces (Tableau, Excel, Looker, etc.)
Experience connecting visualizations to data models in GCP (BigQuery, BigTable, AtScale)
Architecting and implementing data governance and security for data platforms on GCP
Agile development skills and experience
Experience with CI/CD pipelines such as Concourse, Jenkins
Tableau certification is a plus
Job Type: Contract
Pay: From $50.00 per hour
Schedule:
8 hour shift
Experience:
Data visualization: 3 years (Required)
GCP: 2 years (Required)
Data governance: 3 years (Required)
Concourse: 2 years (Required)
Jenkins: 2 years (Required)
Tableau: 2 years (Required)
Work Location: In person"
169,Senior Data Engineer I,Yello,Remote,"$120,000 - $140,000 a year","Company Description
We all remember our first job and can reflect on all the ways it shaped our careers and lives. That’s why we take such great pride in our mission - helping the world’s top employers recruit the future of their workforce. Here at Yello, we’re committed to providing best-in-class campus recruiting software to early talent teams, and helping qualified and diverse students and recent graduates find internships and jobs.
Founded in 2008, Yello is a venture-backed talent acquisition software company that empowers Fortune 500 and fast-growing enterprise companies to find the best new talent for their organizations — and helps people early in their careers find their dream job. In July 2021, Yello merged with WayUp, bringing together Yello’s best-in-class campus recruiting solution and WayUp’s industry-leading DEI sourcing solution, featuring a database of over 7M students and recent graduates. Now, Yello supports the entire early talent recruiting journey for both employers and candidates.


If you’d like to be part of a team that helps some of the world’s largest brands connect with their future employees, come grow your career with Yello.
About The Role
You’ll join an existing data services team charged with ensuring the delivery of business insights from data that Yello collects. You will work closely with the Director of Data, data engineers, product managers, and UX to produce analytics products that drive meaningful change in the recruiting experience.
What will I learn in this role?
Product analytics & the enhancement of your technical skill set
How to implement robust reporting solutions within a multi-tenant environment
About the recruiting industry and challenges companies face in hiring talent
Why is this role important?
Analytics is at the heart of what we do, with our reports inspiring meaningful change in recruiting practices
Internal teams, from CS to B2C marketing, rely on thoughtfully-crafted internal dashboards to better help our customers
As a Yello data subject matter expert, this role will have the opportunity to influence the strategic direction of the company
About our stack:
Scheduling/Orchestration: Airflow, AWS Step functions
ETL: Custom Python, DBT, SQL
Data Warehouse: Snowflake, PostgreSQL
Dashboarding: Sisense/Periscope , Looker
Code versioning: Git
Preferred languages: SQL, Python
How You'll Make An Impact
Work closely with Product, UX, and Design team members to develop analytics solutions (e.g. dashboards, automated reports) for clients and internal personnel
Implement cataloging best practices and lead the definition, collection, documentation of our data sources
Build lasting data modeling solutions, including automating analyses and authoring pipelines via SQL-based ETL frameworks. Develop data models and schemas that enable performant and intuitive analysis in a Data Warehouse setting.
Build complex SQL queries to transform, cleanse, and validate data for the data warehouse based on business rules and requirements
Build SQL queries to deliver ad hoc insights to both internal and external stakeholders
Actively participate in the transition of Yello Enterprise and Yello Sourcing into a new data platform
What We're Looking For

MINIMUM QUALIFICATIONS
A bachelor’s degree or higher in Economics, Computer Science, Statistics, Mathematics, another quantitative discipline or equivalent work experience required
3 to 5 years of experience working in customer-facing analytics reporting
1 to 3 years working experience with advanced reporting tools, such as Tableau, Looker, or Sisense
1+ years of experience with data modeling in dbt (preferably in a daily capacity)
Proficient in SQL - must be able to build and performance tune complex transformation, reporting, and validation queries, preferably in PostgreSQL, to ensure accuracy, scalability, and maintainability of code
Analytical and problem-solving skills to take complex business requests and transform them into clean, simple, data driven solutions
Strong verbal and written communication skills and the ability to effectively collaborate with other engineers as well as internal/external stakeholders
PREFERRED QUALIFICATIONS
Experience generating customer-facing reports (specifically, using tools such as Looker / Sisense)
Experience with dimensional data modeling
Experience with Snowflake or Starburst
Nice to have: experience using git or other version control, JIRA
Additional Information
We are open to remote applicants for this position as long as you live in one of the states we are set up to do business in: AZ, CA, CO, FL, GA, IL, IN, KS, MA, MI, NC, NH, NJ, NY, NV, PA, TN, TX, VA, and WI
Base salary range for this role is $120,000 - $140,000, based on experience, with a 10% annual bonus opportunity
Yello is an Equal Opportunity Employer. All applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status
You must be authorized to work in the United States. Yello offers visa sponsorship for Senior Engineering roles, but not for mid-level Engineering roles
Must be able to sit or stand for continuous periods of time
This role frequently communicates/interacts with individuals, must have strong written and oral communication skills"
170,Azure Data Engineer,zettalogix.Inc,Remote,$50 - $55 an hour,"Azure Data Engineer
Jackson, MS (100% REMOTE)
6+ Months
Job Description:
This position will require qualified T-SQL Developers to take the lead in the following tasks:
Rationalizing and mapping data between transactional and dimensional database
models/systems
Working closely with the database and system administrators to develop stored
procedures and ETL processes related to the data warehouse system.
Working in collaboration with vendors and other agency staff to design, develop and
manage the creation Synapse pipelines.
Ability to work independently and cooperatively as part of a team.
Ability to work under severe time constraints.
Must possess analytical and complex problem-solving skills.
Required Skills:
Extensive experience with development of stored procedures and ETL processes using T-SQL 7
Thorough understanding of data warehouse design hierarchies such as star and snowflake schemas 7
Use of ALM tools for work item management, version control, code analysis, and testing 7
Broad and extensive knowledge of the software development process and its technologies 7
Familiarity with continuous integration 7
Experience creating and scheduling elastic jobs 5
Experience with designing and modeling database structures based on business use cases
Preferred Skills
Experience with student information management systems and K-12 data and reporting
Experience with Common Education Data Standards (CEDS)
Experience with designing and modeling database structures based on business use cases
Azure Data Engineer Associate certification
Experience working with Azure Synapse Pipelines
Job Type: Contract
Pay: $50.00 - $55.00 per hour
Schedule:
Monday to Friday
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Remote"
171,Data Engineer,Middle Tennessee Electric,"555 New Salem Hwy, Murfreesboro, TN 37129","$141,000 a year","Job Summary: To design, develop, and sustain data architectures and data governance to enable deep analysis of enterprise data in support of MTE's strategic objectives.
Job Requirements:
Four-year degree in computer science or related field from an accredited college required.
Master's degree in computer science, software engineering, data science, or a related field preferred.
Must have a minimum of four (4) years' experience in designing, building, maintaining, and testing data infrastructure that enables data analysts, data scientists, and other stakeholders to access and analyze data.
Must have strong proficiency in programming languages, such as Python, Java, and SQL.
AWS Certified Big Data Specialty Certification and/or Azure Data Engineer Associate certifications or equivalent required.
Two years of supervisory experience preferred.
Ability to multi-task, prioritize, and manage time effectively.
Ability to coordinate, communicate and interact effectively with employees across multiple departments.
Willingness to participate in on-the-job training.
Encompass the three virtues of the Ideal Team Player (Humble, Hungry & People Smart).
Benefits Package:
401(k) and ROTH 401(k)
Accrued paid time off for vacation and sick leave
Eleven paid holidays each year
Employee pension program
Medical, dental and vision insurance
Mental health/well-being programs
Discounts from companies like United Communications, Verizon and Nissan
Financial and retirement planning services
Tuition reimbursement
Classification: Exempt

Statement of Nondiscrimination
It is the policy of MTE to ensure compliance with the nondiscrimination obligations of Executive Order 11246, Section 503 of the Rehabilitation Act of 1973, the Vietnam Readjustment Act, and Title VI of the Civil Rights Act of 1964, and each of their amendments and implementing regulations. It is the policy of MTE that no person shall be excluded from participation in or be denied the beneﬁts of or be subjected to discrimination under any program or activity on the grounds of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or status as a protected veteran."
172,Data Engineer,Wawa,"Media, PA 19063•Hybrid remote",N,"Data Engineer
Wawa Corporate - Media, PA
Job Summary:
The Data Engineer role designs and develops scalable data solutions using data integration tools and technologies. The individual utilizes big data computation, data platforms and storage tools to create prototype and data products. Conduct build and testing of data pipelines and solutions. Additionally, Data Engineer integrates, tests data pipelines with Advance Analytics and AI platforms. Must be proficient with multiple data engineering and integration tools such as Scala, Python, Spark, Snowflake etc. in an AWS environment.
Principal Duties:
Responsible for designing and implementing solutions for loading both structured and semi-structured data design into multiple target data systems.
Design, develop, optimize, and maintain data pipelines and processes that adhere to data integration principles and business goals.
Solve complex data problems to deliver insights that helps our business to achieve their goals.
Code, test, and document new or modified data systems to create robust and scalable applications for data analytics.
Ensure that data pipelines are scalable, repeatable, and secure, and can serve multiple users within the company.
Design and implement data ingestion techniques for real time and batch processes for structured and semi-structured data sources into Wawa’s data lake and data warehouse platforms.
Understand complex business requirements and propose end to end and simplified enterprise information architecture solutions.
Develop and implement data design methods, data structures, and modeling standards which work with multiple business intelligence tools.
Work closely with Analytics team and implement their self-service and analytics requirements.
Work with Data Science practitioners and developers to make sure that all data solutions are
Collaborate with Analytics team to build solutions that enable business analytics. Develop quality scalable, tested, and reliable data services using industry best practices.
Manage all activities centered on obtaining data and loading into a data lake environment.
Assess the suitability and quality of candidate data sets for the Data Lake.
Balance business requirements with technical feasibility and set expectations on new projects. Recommend changes in development, maintenance and system standards.
Design and build integration components and interfaces in collaboration with Architects and Infrastructure Engineers as necessary. Perform unit, component, integration testing of software components including the design, implementation, evaluation and execution of unit and assembly test scripts.
Determine if the data received from the upstream systems are of good quality based on the rules and data quality validations defined and in case of any issues with the data quality analyze and come up with a preliminary summary of the root cause/issue.
Assist the Analytics team by leveraging Wawa’s Enterprise Data Platform ecosystem to design, and develop capabilities to deliver our solutions using Spark, Scala, Python and
Follow security standards for all data and tools that are being introduced in the team.
Basic Qualifications:
Bachelor’s degree in Computer Science/Engineering preferred
3-5+ years database, data integration experience
3+ years’ experience with Spark, Scala/Python, SQL and Big Data solutions
Preferred experience with Databricks and Snowflake
3+ years’ experience in designing and implementing the data architecture (conceptual, logical, physical & dimensional models).
Developing Enterprise Business Intelligence solutions on one or more of the following
EDW platforms: Snowflake, Redshift, Google Big Query
Experience implementing Big Data solutions using open source technologies
Strong knowledge of key scripting and programming languages such as Python, Java, Scala, etc
Experience with data integration tools such as Talend would be helpful
Experience designing and implementing various data pipeline patterns and strategies
Hands-on experience with dimensional modeling techniques and creation of logical and physical data models (entity relationship modeling, exposure to data warehouse design)
Strong knowledge of data security principles
Proven track record working with complex, interrelated systems and bringing that data together on Big Data platforms.
Wawa will provide reasonable accommodation to complete an application upon request, consistent with applicable law. If you require an accommodation, please contact our Associate Service Center.
Wawa, Inc. is an equal opportunity employer. Wawa maintains a work environment in which Associates are treated fairly and with respect and in which discrimination of any kind will not be tolerated. In accordance with federal, state and local laws, we recruit, hire, promote and evaluate all applicants and Associates without regard to race, color, religion, sex, age, national origin, ancestry, familial status, marital status, sexual orientation or preference, gender identity or expression, citizenship status, disability, veteran or military status, genetic information, domestic or sexual violence victim status or any other characteristic protected by applicable law. Unlawful discrimination will not be a factor in any employment decision.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Employee stock ownership plan
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
7 years
8 years
9 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Media, PA 19063: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What Big Data Tools/Technologies/Querying Languages do you have experience with (RedShift, Snowflake, Python, SQL, Google BigQuery, Spark, Scala, etc.)?
Experience:
Data Engineering: 7 years (Required)
Leadership: 3 years (Required)
Work Location: Hybrid remote in Media, PA 19063

Health insurance"
173,CAMPUS: Data Engineer I,Capital Group Companies,"San Antonio, TX 78251•Remote","$67,641 - $108,226 a year","“I can be myself at work.”
You define yourself by more than just a job title, and we want you to feel comfortable bringing your true self to work. We value your talents, your traditions and your take on the world ̶ everything that makes you unique. We’re working hard to advance diversity, equity and inclusion in our organization and our communities because we know that what makes us different makes us better.
We want you to feel a strong sense of belonging. We value and welcome your experiences, ideas and identity. Over 40 employee resource groups unite our people and help to develop our collective empathy through unfiltered conversations about race, ethnicity, gender, gender identity, sexual orientation, faith, disabilities, mental health and so much more.
“I can influence my income.”
You want to feel recognized at work. Your performance will be reviewed annually, and your compensation will be designed to motivate and reward the value that you provide. You’ll receive a competitive salary, generous bonuses and premier benefits. Your company-funded retirement contribution will be the equivalent of 15% of your annual pay (including bonuses).
“I can lead a full life.”
You bring unique goals and interests to your job and your life. Whether you’re raising a family, you’re passionate about where you volunteer, or you want to explore different career paths, we’ll give you the resources that can set you up for success.
Enjoy generous time-away and health benefits from day one, with the opportunity for flexible work options
Receive 2-for-1 matching gifts for your charitable contributions and the opportunity to secure annual grants for the organizations you love
Access on-demand professional development resources that allow you to hone existing skills and learn new ones
NOTE: Our recruiting process is 100% virtual, which should help with the logistics of your candidate journey. We have transitioned to a hybrid work model, which means our associates are combining working from home and working from the office. You will work with your manager to determine what that looks like for you.
“I can succeed as an EDGE Early Career Associate at Capital Group.”
As an IT Early Career Associate, you’ll be placed in a selected team and your manager will assign you to work on different technology, systems and projects as needed. As a Direct Placement Associate, you’ll be part of an engaging cohort experience that focuses on learning and development along with making connections through social events and networking during your first year at Capital Group. You’ll learn directly from our leaders through a speaker series, take on interesting business problems in sprint challenges and engage in activities to strengthen your technical skills and business acumen. You will be well supported and will learn how information technology elevates our ability to provide the best technology platforms for our internal customers and how the Capital System has sustained superior results over time.
“I can do meaningful work.”
You will build cloud-based solutions for enterprise data pipelines on AWS and/or Azure using the latest technologies, including but not limited to Cloudera/Hadoop, Databricks, Synapse, Hive, and Spark.
You will build skills in collaboration, requirements gathering, testing, managing ambiguity in a fast paced environment, and influencing outcomes.
You will lead end-to-end delivery of select features in our data platforms.
“I am the person Capital Group is looking for.”
You are a recent grad with a bachelor’s degree in Computer Science, Engineering, Math, Statistics, cognitive science or equivalent.
You have practical experience with one or more of these technologies: SQL, Python, Alteryx, and/or reporting/visualization tools such as Tableau, PowerBI
You have a passion for data and its possibilities, along with a propensity for analysis and logical/critical thinking
You have basic understanding of SDLC and agile methodologies
You express ideas effectively and interact professionally with others
You are not intimidated by the new and unfamiliar and embrace a continuous learning mindset
You are solution driven, accountable for results, and comfortable socializing your thought process
You are highly collaborative and willing to share your point of view
“I can apply in less than 4 minutes.”

You’ve reviewed this job posting and you’re ready to start the candidate journey with us. Apply now to move to the next step in our recruiting process. If this role isn’t what you’re looking for, check out our other opportunities and join our talent community.

“I can learn more about Capital Group.”

At Capital Group, the success of the people who invest with us depends on the people in whom we invest. That’s why we offer a culture, compensation and opportunities that empower our associates to build successful and prosperous careers. Through nine decades, our goal has been to improve people’s lives through successful investing. We know that our history is a testament to the strength of the people we hire. More than 7,800 associates in 30+ offices around the world help our clients and each other grow and thrive every day. Find us on LinkedIn, Glassdoor, FairyGodBoss, DiversityJobs and Instagram.

We are an equal opportunity employer, which means we comply with all federal, state and local laws that prohibit discrimination when making all decisions about employment. As equal opportunity employers, our policies prohibit unlawful discrimination on the basis of race, religion, color, national origin, ancestry, sex (including gender and gender identity), pregnancy, childbirth and related medical conditions, age, physical or mental disability, medical condition, genetic information, marital status, sexual orientation, citizenship status, AIDS/HIV status, political activities or affiliations, military or veteran status, status as a victim of domestic violence, assault or stalking or any other characteristic protected by federal, state or local law.
#LI-Hybrid
San Antonio Base Salary Range: $67,641-$108,226
In addition to a highly competitive base salary, per plan guidelines, restrictions and vesting requirements, you also will be eligible for an individual annual performance bonus, plus Capital’s annual profitability bonus plus a retirement plan where Capital contributes 15% of your eligible earnings.
You can learn more about our compensation and benefits
here
.

We are an equal opportunity employer, which means we comply with all federal, state and local laws that prohibit discrimination when making all decisions about employment. As equal opportunity employers, our policies prohibit unlawful discrimination on the basis of race, religion, color, national origin, ancestry, sex (including gender and gender identity), pregnancy, childbirth and related medical conditions, age, physical or mental disability, medical condition, genetic information, marital status, sexual orientation, citizenship status, AIDS/HIV status, political activities or affiliations, military or veteran status, status as a victim of domestic violence, assault or stalking or any other characteristic protected by federal, state or local law."
174,Data Engineer,Supernal,"Fremont, CA","$121,680 - $186,160 a year","Supernal is at the forefront of creating emerging mobility solutions that will foster the development of human-centered cities. We are designing a completely new electric vertical take-off and landing (eVTOL) aircraft tailored to the mobility needs of future cities. This allows passengers a seamless intermodal journey that safely transports them to their final destination. We fuse research in autonomy, robotics, aviation and services to define a new category of mobility for the world's communities. We believe in creative thinking and collaboration to help build a better mobility experience for everyone, improving people's ability to move – whether for work or play.
What we do:
The Data Engineer is responsible for identifying, implementing, and supporting data use cases and owners throughout the business that can use data engineering and analytics to improve outcomes. Activities include collaboration with business units to identify ways data can improve their outcomes, designing use case ontologies, building, and maintaining start to finish data pipelines across disparate sources, standing up no code dashboards, and designing / guiding standard processes in code, data, and process management. This is a highly collaborative role and will work closely with collaborators and partners across the business and the rest of the Data team. It may also require collaboration with third party technology partners, Corporate IT, and the ERD Digital Tools team.
What you can do:
Transform noisy, real-world data into valuable information that enables the business to quickly and optimally make data driven decisions
Partner with your team and technical stakeholders and deliver internal data and analytics products on critical use cases where data can be used to excavate hidden insights (e.g., battery testing, testing and certification, AI, demand forecasting, cyber security)
Write, maintain, and improve code, pipelines, ontologies, visualizations, interactive tools, and dashboards to enable technical and non-technical users to leverage data
Identify key data sets through deep engagement with use cases and workflows
Design, build, and manage end to end data pipelines in a cohesive data platform
Develop scalable, reliable, manageable data pipelines
Develop and train CI/CD practices for data integrations
Partner with technical team members and collaborators to design solutions for MLOps, data quality verification, anomaly detection, real-time streaming pipelines
Design, integrate, and document technical components for seamless data discoverability and usage
Triage and address support requests (e.g., workflow and product issues)
Actively enable a data driven and innovative culture within Supernal
Communicate insights in a way that resonates with internal and external parties
Design and guide standard methodologies in data, code, and process management
Drive data strategy across the organization including leading training sessions in our data systems
Stay up to date on industry best practices and standards
Other duties as assigned
What you can contribute:
Bachelor's degree in relevant field such as computer science, mathematics, physics, software engineering, data science, or information sciences preferred
A minimum of five (5) years of proven experience with big data platforms across diverse use cases, including data management (an equivalent combination of education and experience may be considered)
Experience and architectural understanding working with one (1) or more cloud data warehouses—Azure Synapse, Snowflake, Google BigQuery, AWS Redshift.
Experience working with highly regulated data in the aerospace, automotive, healthcare, and/or financial industries
Professional experience with / comfortable using: Programming languages, cloud services such as Azure (preferable), databases and data lakes,
Using SQL, Spark (any interface, but PySpark preferred), Git, Command line, CI/CD
Solid expertise and skills in OO programming paradigm and implementation
Must have good understanding of the inner workings and hands-on skills with data technologies: Kafka, Spark, etc.
Knowledge of various ETL techniques and frameworks; experience integrating data from multiple data sources
Solid understanding of distributed system concepts, cloud computing
Robust development abilities and a strong predilection for automation
Ability to develop and handle data pipelines and lineage
Creative and curious problem solver, able to take an ambiguous and complex need and develop data / analytics solutions
Able to work in a rapidly changing and ambiguous environment
Excellent written and verbal communication skills with ability to work with diverse, technical, and non-technical colleagues
Forward-thinking, continuous learner, and strong decision maker
Collaborative teammate with commitment to Diversity, Equity, and Inclusion
You may also be able to contribute:
Master's degree preferred
Any offer of employment is conditioned upon the successful completion of a background check. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, citizenship, sex, gender, gender expression, sexual orientation, age, marital status, veteran status, or disability status. Individuals with disabilities may request to be provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation at: ta-support@supernal.aero
The job responsibilities of this position require access to certain technology and/or software source code subject to U.S. export control laws and certain software that can only be used and accessed by U.S. Persons. Accordingly, this position is limited to applicants that are US Persons, i.e., U.S. citizens, lawful permanent residents as defined by 8 U.S.C. 1101(a)(20),or protected individuals as defined by 8 U.S.C. 1324b(a)(3).
Base pay offered may vary depending on skills, experience, job-related knowledge and location. This position is also eligible for a bonus as part of total compensation.
The pay range for this position is:
$121,680—$186,160 USD
Click HERE or visit: https://jobs.supernal.aero/benefits to view our benefits!"
175,Data Engineer Associate,Carrot Fertility,"San Francisco, CA•Remote","$81,000 - $108,000 a year","About Carrot:
Carrot Fertility is the leading global fertility care platform for women, who are often at the center of fertility care decisions and consequences. Plus, Carrot serves people of every age, race, income, sex, sexual orientation, gender, marital status, and geography. Trusted by hundreds of multinational employers, health plans, and health systems, Carrot's comprehensive clinical program delivers industry-leading cost savings for employers and award-winning experiences for millions of people worldwide. Whether there is a need for care through fertility preservation, male-factor infertility, pre-pregnancy, IVF, pregnancy and postpartum, adoption, gestational surrogacy, or menopause, Carrot supports members and their families through many of the most memorable and meaningful moments of their lives.
The Role:
Carrot is seeking a Data Engineer Associate to join our rapidly growing Business Intelligence function as it scales to meet - and anticipate - the needs of the organization and its clients. As an Associate Data Engineer at Carrot, you will design, program, and test data processing applications to improve our data infrastructure. You will participate in all aspects of data engineering by supporting our existing data integrations and data pipelines, data ingestions, exchanges, and traditional data warehouse and business intelligence systems. You will collaborate with cross-functional teams to support data-driven decision-making and contribute to the development of data architectures and solutions.
This is an exciting opportunity to work with cutting-edge technologies and cloud-based platforms to build scalable and reliable data pipelines and databases. You will have the opportunity to work with a team of talented data engineers, data scientists, and business intelligence analysts to support data-driven initiatives that have a positive impact on people's lives. You will also have the chance to learn from experienced senior data engineers to further develop your technical skills and knowledge in data engineering.
The Team:
The Business Intelligence team at Carrot is a highly cross-functional team that is central to Carrot's long-term success. The growing team is led by our Senior Director of Analytics and Business Intelligence and includes data engineers, data scientists, and business intelligence analysts.
Minimum Qualifications:
Experience writing database queries in SQL
Experience with database relational modeling
Experience with Python development
Experience working with version control systems (Git preferred)
Preferred Qualifications:
Experience with unit and integration testing in Python
Experience with Snowflake, dbt and/or Prefect
Experience working with AWS cloud services
Ability to communicate effectively with technical & non-technical team members across the organization
Quick learner, with ability to quickly grasp new concepts and excel
Compensation:
Carrot offers a holistic Total Rewards package designed to support our employees in all aspects of their life inside and outside of work, including health and wellness benefits, retirement savings plans, short- and long-term incentives, parental leave, family-forming assistance, and a competitive compensation package. The expected base salary for this position will range from $81,000 - $108,000. Actual compensation may vary from posted base salary depending on your confirmed job-related skills and experience.
Why Carrot?
Carrot has received national and international recognition for its pioneering work, including Best Diversity, Equity, & Inclusion Product from the Anthem Awards, Fast Company's Most Innovative Companies, CNBC's 100 Barrier Breaking Startups, and more. Carrot is regularly featured in media reporting on issues related to the future of work, women in leadership, healthcare innovation and diversity, equity, and inclusion, including MSNBC, The Economist, Bloomberg, The Wall Street Journal, CNBC, National Public Radio, Harvard Business Review, and more.
Carrot is fully distributed, with teams in more than 40 states across the United States and dozens of countries around the world. It has received numerous workplace awards, including Fortune's Best Workplaces in Healthcare, Quartz's Best Companies for Remote Workers, and Great Place to Work and Age-Friendly Employer certifications. Learn more at carrotfertility.com."
176,AWS Data engineer,TekWisen Software Pvt. Ltd,Remote,$60 - $75 an hour,"Job Title: Data Engineer - AWS
Location: Madison WI 53703 - Remote
Duration: 5 Months
Job Type: Contract (c2c and w2)
Work Type: 100% Remote in USA
Job Description
As a Staff Data Engineer on Foundation Analytics, you will be responsible for building many key parts of the foundation data services platform. You’ll work in a collaborative Agile environment using the latest in engineering best practices with involvement in all aspects of the software development lifecycle. You will be responsible for ensuring the team makes sound design & configuration decisions to develop curated data products, apply standard architectural practices, and supports the Data Product Managers in evolving core data products. As a member of the data engineering team, you will help client to deliver impactful reporting products for our customers.
Publishing well written and tested code to production daily using technologies such as Linux, Docker, Kubernetes, AWS, Kafka and Python Drive data architecture and integration design and development discussions with engineering and other teams Investigate production issues and fine-tune our data pipelines Build a platform that will be the foundation for our customer facing reporting features, our machine learning initiatives, and internal product analytics
Perform rapid proto typing Participate in designing, developing key features and functionality of our data platform Continually improve the data platform development for high efficiency, throughput and quality of data Collaborate with team members with researching & brainstorming different solutions for technical challenges facing the team
Develop standard methodologies and mentor other engineers on the team to help make technical decisions on our projects and roadmap.,
Skills
7+ years of software development/data engineering experience 4+ years of hands-on experience of building scalable data platforms and/or reliable data pipelines
Proficiency in at least one of the following programming languages: Java, Python, Scala
Experience with AWS.
Experience in developing and operating high volume, high availability environments Working understanding of Kubernetes’ infrastructure and security best practices
Ability to work effectively in a dynamic, occasionally interrupt driven environment that includes geographically spread teams and customers BS degree in Engineering, CS, or equivalent
Education
Experience writing ETL jobs to help address various data engineering challenges
Strong understanding of Build tools and Deployment tools Familiarity with Kafka, Flink, Spark frameworks with validated understanding of at least one job scheduling tool: Airflow, Celery, AWS Step functions Tech Stack Our data pipelines are written in Java and Python based software stacks
We utilize many open-source technologies, including Spark, Flink, Hudi, Airflow Our software runs on AWS services like EMR and in Kubernetes, and integrates with AWS services S3, Athena, and Glue for data access
Job Type: Contract
Salary: $60.00 - $75.00 per hour
Experience level:
10 years
11+ years
Schedule:
8 hour shift
Experience:
software development: 9 years (Required)
Data Engineer: 9 years (Required)
data platforms: 7 years (Required)
data pipelines: 7 years (Required)
AWS: 9 years (Required)
ETL: 9 years (Required)
EMR systems: 9 years (Required)
Work Location: Remote"
177,Data Engineer I,Spruce,"Austin, TX•Remote",N,"Who We Are
At Spruce, our mission is to change the way people live in their homes by making home services more accessible. As the leading provider of lifestyle services to the multifamily industry, we offer daily chores and housekeeping services to more than 2,000 apartment communities across the US, and we work with over 60 of the top apartment managers in the country. Through the Spruce app, apartment residents can easily have their clothes folded, their dishes washed, their bed sheets changed, or their bathroom cleaned.
Venture-backed and headquartered in Austin, Spruce has more than 80 employees and is growing rapidly. We promote a people-first culture where curiosity, ownership, hustle, and boldness are valued and encouraged. Each employee has a personal, measurable impact on the success of the company, and ideas are welcomed from everyone.
About the Role
We're looking for a Data Engineer I who will report to the Director of Business Analytics. You'll be serving an integral role in bridging between our data engineering and business analytics team members. Namely, this person will be owning the maintenance, organization, and improvements made to data in our BI tool, Looker. This will be a great opportunity for someone with a strong technical background who's interested in startups and enjoys tackling unique data challenges with high impact to the business.
What You Get to Do
Designing data models with a deep understanding of how the data will be used in Looker
Re-structuring old data models in LookML to be more efficient and flexible to evolving data needs
Familiarizing yourself with what exists in our databases, data lake and data warehouse that could be pulled through to Looker to answer arising business questions
Translating business analytics needs into technical requirements for engineers
Pulling through new data tables/fields to Looker and ensuring proper setup in LookML (coding in LookML, choosing proper joins, model design/data architecture etc.)
QAing data provided in Looker and creating systems to ensure data quality
Launching new data to the business and maintaining related documentation (data dictionary)
Who You Are
Bachelor's degree, or equivalent, in engineering or data related fields
3-5 years of work experience in a data/analytics oriented role
Self-starter with a desire to continuously learn new skills
Comfortable with ambiguity and enjoys creating structures/processes that improve efficiency with cumulative effect over time (exponential thinking).
Technical Skills
Strong understanding of data modeling
Strong understanding of relational database design and data warehousing concepts
Expert on profiling data and uncovering data quality issues that impact analysis and dashboard
Expert in SQL
Experience with Snowflake or similar cloud warehouse technologies
Expert in building Visualizations in Looker, including strong LookML coding skills essential
Things that can set you apart
Strong understanding of source data optimization for Looker Processing
Experience in designing and developing ETL processes
What We Offer
Competitive salary
Stock options
401K plan
Medical, vision, dental insurance
Unlimited PTO
100% remote work
Spruce-provided WFH setup (laptop, keyboard, monitor(s), mouse)
A huge role in the growth of a company with a meaningful mission
#LI-Remote
We're building a strong, diverse team of curious, creative people who want to find purpose in their work and support each other in the process. If this sounds like you, then let's talk."
178,Data Engineer,Atticus,"Atlanta, GA•Remote","From $165,000 a year","// This position is fully remote but you need to be located in the Southeastern USA (NC to Texas) for in-person meetings when required. //
Atticus helps to match non-profit organizations with major donors that share their mission, vision, and values. This matching process involves collecting and transforming a significant amount of data from a variety of sources. The Data Engineer builds, maintains, and operates the integrations, systems, and datasets that automate this process.
As one of the first Engineering positions in the company, this role will require the candidate to influence and/or lead technology decision-making for a key part of the product. Consequently, Atticus is looking for candidates with significant professional experience and demonstrated leadership as a Data Engineer.
The Data Engineer will take ownership of an existing production system that collects and transforms data from client file uploads, several third-party web APIs, and a few internal databases. The system is hosted in Azure and currently uses Databricks/Spark to automate the data pipeline, ultimately feeding into an Azure Cosmos DB application database. No system is perfect, and the Data Engineer will be expected to review the design of the existing system and recommend improvements.
If this sounds like you, kick off the process by submitting your resume here on Indeed and spending a few minutes (~15) on a work-traits assessment that will give us some insight into how you prefer to work and communicate. https://assessment.predictiveindex.com/bo/849R/datasci
ESSENTIAL DUTIES AND RESPONSIBILITIES
· Ensure that data systems and datasets meet the needs of the business
· Find, analyze, and integrate new data sources, both structured and unstructured
· Build data systems that automatically collect, transform, and combine these data sources into consistent, up-to-date, and high-quality datasets for both human and machine learning use
· Conduct complex statistical analysis of data and report on the results
· Detect, diagnose, and correct data quality and reliability issues
· Optimize data systems’ costs and performance
· Collaborate regularly with software engineers, data scientists, and consumers of the data
Job Type: Full-time
Pay: From $165,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Paid time off
Professional development assistance
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Experience level:
5 years
Schedule:
8 hour shift
Experience:
Relational databases: 3 years (Preferred)
Information Retrieval: 3 years (Preferred)
Work Location: Remote

Health insurance"
179,Senior Data Engineer [USA- Remote],AURA TECHNOLOGIES LLC.,"Chapel Hill, NC 27514",N,"AURA TECHNOLOGIES, LLC (AURA) is an advanced research and development (R&D) and technology company creating game-changing innovations for the US Department of Defense in Artificial Intelligence (AI) and in other systems-level implementation of cutting-edge technology. We are creating advanced intelligent power systems for the US Army; unconventional tactical lights for the US Marines; revolutionary satellite manufacturing for the US Air Force; and a range of AI platforms for DoD implementation. AURA partners with some of the best companies in the world, such as Boeing, Northrop Grumman, and Lockheed Martin. We also collaborate with the best and brightest at our nation’s universities, including Georgia Tech and NC State University.

If you are a smart, capable, and talented individual who possesses high integrity, thrives in a fast-paced environment, wants to chart your own course based on your capabilities, and is willing to be accountable for failures and successes, then continue reading because you may be the ideal candidate to join our growing R&D business.


AURA has an immediate opening for a full-time Senior Data Engineer (remote).

ESSENTIAL DUTIES AND RESPONSIBILITIES:
You will work side-by-side with other team members to provide technical direction and recommendations on how to: standardize, normalize, and format data; extract valuable features from large data sets; develop data pipelines and write preprocessing algorithms; and train others to use and maintain developed tools/applications.

Work on projects involving time-series data, image recognition, computer vision, and geometric shape modeling
Preprocess and extract features from sensor-based, three-dimensional, and visual data
Develop applications to sustain data pipelines, interact with databases, and ensure uptime
Interact with big data applications for visual, video, and geometric information
Collaborate closely with data scientists to ensure that data throughput and preprocessing requirements are met
Work in a team environment to collaborate with coworkers, partners, and clients to produce an integrated solution. Given the demanding, diverse, and fast-paced environment, the Senior Data Engineer must also possess exceptional attention to detail.

WORK EXPERIENCE, EDUCATION & TECHNICAL REQUIREMENTS:

Minimum Years of Work Experience:
Minimum of 3 years of experience in a data engineer, cloud engineer, database management, or similar role (advanced degrees may be substituted for experience in the case of a qualified candidate)

Minimum Education:
Masters degree in Computer Science, Engineering, Data Science, Statistics, or a related technical degree

Minimum Technical Requirements:
At least 3 years of experience (in a professional setting) developing data pipelines and preprocessing algorithms
Expertise in one or more structured database management system types/vendors such as MySQL, Oracle, or SQLite
Expertise in one or more unstructured data management systems
Programming expertise in one or more scripting languages, such as Python or R
Awareness of big data design for cloud applications such as AWS or Azure, including design, provisioning, and tuning

and EITHER:

At least 1-2 years of practical experience with storing and preprocessing visual data, including the development of data pipelines for large image datasets. Past experience working with large open-source visual datasets such as OpenImages, ImageNet, or MS-COCO is preferred.
At least 1-2 years of practical expertise working with three-dimensional geometric data, such as computer-aided design (CAD) files, bounding boxes, and shape data. Past experience working with large open-source 3D datasets such as ShapeNet, ModelNet, or ABC is preferred.

PREFERRED REQUIREMENTS
Prior DoD or military experience
Doctorate degree in Computer Science, Engineering, Data Science, Statistics, or a related technical degree

ADDITIONAL REQUIREMENTS:
US citizenship status is required for this position due to AURA’s contractual obligations to the US Department of Defense (DoD) requiring all employees working in performance of DoD contracts to be US citizens.


BENEFITS:
401(k) Safe Harbor Contribution
Flexible schedule
Health insurance
Discretionary Leave
14 Paid holidays

TO APPLY FOR THIS POSITION:
Submit your resume/CV in PDF format via instructions at the following link: http://aura.company/careers/
No phone calls after submission. We will let candidates know via automated reply that we have received their resumes and will contact them if there is a good fit after the closing date for this job.

AURA Technologies, LLC is an Equal Opportunity Employer and affirmative action employer of veterans protected under the Vietnam Era Veterans’ Readjustment Assistant Act (VEVRAA). We are a Drug Free Workplace and thus, all job offers are contingent on successful criminal background check and drug screen. As a US Federal Contractor, AURA uses the Department of Homeland Security e-Verify system to determine eligibility to legally work in the United States. Most of AURA’s work is for the federal government, and federal regulations may in the future require AURA’s employees to be fully vaccinated against the COVID-19 virus.

Write a carefully crafted, well-written cover letter that elaborates on your interest in this position and why you think you are the best candidate for the job. Submit your cover letter, CV and three professional references in PDF format ONLY via BambooHR.

Any attachments must be in PDF format or will not be opened due to virus concerns. No phone calls after submission. We will let candidates know via automated reply that we have received their resumes and will contact them if there is a good fit after the closing date for this job."
180,"Software Engineer - ETL Data Integration - Contract position (PT/FT, remote)",Artifact,Remote,N,"We are seeking a talented and experienced software engineer, with deep data engineering experience to help us connect our ETL data sources with our company’s databases. The ideal candidate will be responsible for designing and implementing data integration solutions that are efficient, reliable, and scalable. This will include creating connectors for our customers, authentication flows, as well as cleaning and mapping the incoming data.
What you’ll do at Artifact:
Design, develop and maintain ETL workflows that efficiently moves data from source systems to our databases
Rationalize large, complex, heterogeneous data sets that meet functional / non-functional business requirements
Build the infrastructure required for best-in-class data ETL/ELT from a wide variety of vendor-based data sources using SQL and GCP ‘big data’ technologies
Help identify and resolve data quality and integration issues.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
Requirements
Experience building and optimizing ‘big data’ pipelines, architectures and data sets
Ninja-like skills in SQL and Go
Proficient in Python and Typescript
Are comfortable with complex multi-step data pipelines
Have good communication skills, willing and able to collaborate on cross-functional teams
Strong problem-solving skills and attention to detail
Ideal candidate would also
Have experience with Dataform or DBT
Have experience with authentication flows
Worked with microservices
What Artifact does:
Artifact helps teams centralize qualitative data into a single source of truth, and uses advanced but proven AI methods to synthesize actionable insights so they can instantly understand what’s most important to their customers."
181,Data Engineer II,Harvey Nash,Remote,$36.79 - $86.86 an hour,"Summary of the position:
One of our many customer(s) today is urgently looking for an experienced Data Engineer II - South San Francisco, CA
Here are some of the specific details:
Job Title: Data Engineer II
Location- South San Francisco, CA
Duration- 4 Months
MUST HAVE skills:
· At least 5 years of experience in designing and developing ETL solutions using Talend.
· Strong knowledge of Talend Studio, Talend Administration Center, Talend Job Server, and other Talend tools.
· Hands-on experience with data integration, data profiling, and data cleansing techniques.
· Experience with Agile methodologies and project management tools such as Jira.
Let me know if you are interested in this job and/or if you can assist us by referring someone who is interested in this job, since we offer the lucrative referral bonuses.
A reasonable, good faith estimate of the minimum and maximum for this position is $70/hour to $80 /hour.
I am looking forward to speaking with you today.
About us:
Harvey Nash is a national, full-service talent management firm specializing in technology positions. Our company was founded with a mission to serve as the talent partner of choice for the information technology industry.
Our company vision has led us to incredible growth and success in a relatively short period of time and continues to guide us today. We are committed to operating with the highest possible standards of honesty, integrity, and a passionate commitment to our clients, consultants, and employees.
Utilizing our proprietary Predictive Staffingmodel, our company has enjoyed more than a decade of rapid growth, earning our reputation as a client-focused, efficient provider across a broad range of industries. Today, we serve top Fortune 1000 and successful privately held companies all over the country, still operating under the simple idea that great people aligned under a common vision can achieve tremendous results.
We are part of Harvey Nash Group, a global professional services organization with over forty offices worldwide.
For more information, please visit us at https://www.harveynashusa.com/
Job Type: Contract
Salary: $36.79 - $86.86 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
Monday to Friday
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote

Health insurance"
182,Data Engineer,iCapital,"Greenwich, CT","$120,000 - $200,000 a year","iCapital is powering the world's alternative investment marketplace. Our financial technology platform has transformed how advisors, wealth management firms, asset managers, and banks evaluate and recommend bespoke public and private market strategies for their high-net-worth clients. iCapital services approximately $152 billion in global client assets invested in 1,210 funds, as of December 2022.

iCapital has been named to the Forbes Fintech 50 for five consecutive years (2018 – 2022); a back-to-back selection by Forbes to its list of Best Startup Employers; and a two-time winner of MMI/Barron's Solutions Provider award (See link below).

About the Role
iCapital is looking to hire a Data Engineer to join the Data and Analytics team. This role will help build the infrastructure to make data a central part of iCapital. Data driven decisions are very critical for iCapital, as a result, we are looking for engineers that will not only be able to use the data but also understand it and help make actionable decisions. Data Engineers quickly grasp complex and fluid business problems and solve them with robust and creative analyses after having built highly performant and highly scalable infrastructure to warehouse the data. The ideal candidate should enjoy discovering and using the latest technologies, working on creative software projects, and/or thinking about innovative new business plans.
Responsibilities
Leverage open-source technologies and cloud solutions to build elegant features for iCapital Platform.
Develop and automate large scale, high-performance data platform infrastructure to drive iCapital business growth, and enable data-driven organization.
Design and develop reusable components and frameworks for ingestion, cleansing, and data quality.
Streamline the ingestion of raw data from various sources into our Data Lake and Data Warehouse.
Design data models for optimal storage and retrieval that represent the product entities and meet business requirements.
Coordinate closely with sales and product development teams daily to push iCapital's FinTech strategy and improve the overall profitability of the business.
Qualifications
Bachelor's Degree in Computer Science, Data Science, Mathematics, Statistics or other quantitative area or related field
5-8+ years of experience with open-source technologies or object-oriented or functional programming, able to write easy-to-scale, high-quality code
Experienced in at least 1 numeric research framework (python/pandas, R/Splus, Octave/Matlab)
Familiar with OLAP (Redshift, Snowflake) and OLTP (PostgreSQL, MongoDB) databases
Familiar with various database designs (Relational, Columnar, NoSQL)
Some background in probability/statistics
Detail-oriented, able to multitask, and work in a fast-paced environment
Able to work independently while also being a strong team player
Excellent written and verbal communication
Passionate about programming and cutting-edge technologies
Master's in computer science, Data Science or related field
Professional experience with Python and JVM based languages such as Scala, Java, and Kotlin
Experience building data-pipelines, data-lakes, and data warehouses.
Good knowledge of financial markets and financial instruments
Experience with AWS solutions such as Lambda, S3, Kinesis, ElastiCache
Familiar with AWS and infrastructure-as-code (terraform or cloud formation)
Familiar with the use and restrictions of PII data for analysis, research, development, and testing
Full-time or internship experience as a data engineer in the financial technology industry is a plus

Benefits
The base salary range for this role is $120,000 to $200,000 (AVP or VP level - depending on experience). iCapital offers a compensation package which includes salary, equity for all full-time employees, and an annual performance bonus. Employees also receive a comprehensive benefits package that includes an employer matched retirement plan, generously subsidized healthcare with 100% employer paid dental, vision, telemedicine, and virtual mental health counseling, parental leave, and unlimited paid time off (PTO).
We believe the best ideas and innovation happen when we are together. We offer most employees the flexibility to work in the office three or four days. Every department has different needs, and some positions will be designated in-office jobs, based on their function.
For additional information on iCapital, please visit https://www.icapitalnetwork.com/about-us Twitter: @icapitalnetwork | LinkedIn: https://www.linkedin.com/company/icapital-network-inc | Awards Disclaimer: https://www.icapitalnetwork.com/about-us/recognition/

iCapital is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender, sexual orientation, gender identity, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics."
183,Platform Data Engineer,N,"New York, NY•Remote","$130,000 a year","About GiveDirectly
GiveDirectly (GD) provides cash grants directly to people living in extreme poverty. GD has raised over $800M since launching in 2011, delivered cash to more than 1.2 million recipients, and launched offices in 11 countries. We're continuing expansion across the Global South. Over the past decade, GD has also supported large-scale, experimental research, expanding the evidence that cash has a positive short and long-term impact on recipients. As a result, GD has been celebrated as one of the most innovative non-profit companies by Fast Company, while the growing cash transfer movement (and GD's leading role within it) has been featured in the New York Times Magazine, This American Life, Foreign Affairs, and The Economist.
Across our global offices, our culture is candid, analytical, non-hierarchical, and agile. We work alongside 750+ individuals who come from 21 different countries and speak 69 different languages. Team members at GiveDirectly attest that diversity, equity, and inclusion are not just buzzwords, but a fundamental part of our culture and values. We actively seek to recruit individuals from the communities we serve, and use DEI as a lens in our hiring practices, programs, and initiatives. Our goal is to maintain a workplace where everyone can bring their authentic selves to work, and feel valued and respected for who they are. We continue to grapple with balancing inclusivity of all cultures and experiences while maintaining cohesion in our values. While there is much that we are still learning, we take care of one another, have fun, as well as provide flexible working hours wherever possible.
We are proud to be an equal opportunity employer, and we do not discriminate on the basis of race, color, religion, gender, sexual orientation, national origin, age, disability, or any other status protected by law.
About the Central Data team
Effectively leveraging data to improve recipient experience, increase operational efficiency, and maximize fundraising revenue is imperative to successfully scaling to reach millions of people experiencing extreme poverty.
To realize this vision, we are building a best-in-class full stack data team. You will report to the Senior Data Manager and join the Senior Data Architect in owning the core data infrastructure and pipelines required for cash delivery, fundraising, and financial analysis. This is an opportunity to join a fast-paced, early stage team of five staff that applies engineering best practices and a product development approach to data.
Our data infrastructure is built on AWS and Databricks, with dashboards and visualizations in Tableau. We use a configuration-driven data pipeline framework developed by the Senior Data Architect based on SQL and Python.
About this role
Success in this role is determined by meeting these key objectives:
[30%] Build data pipelines within the configuration-driven framework to integrate new data sources into the data lakehouse:
We've onboarded a few key data sources, but have many additional integrations to build. These new pipelines will require augmenting the configuration-driven framework.
[40%] Build stability, security, and data quality features into core infrastructure:
Automate data quality checks, testing, and alerting.
Own pipeline and job monitoring, resolving failures and developing safeguards.
Use Git-based code repository and workflows for code review and deployment.
[20%] Increase transparency of data and platform process:
Support development of a data catalog and maintain platform documentation.
Onboard other members of Central Data to build their own pipelines within the data lakehouse.
[10%] Build data visualizations in Tableau:
End-to-end data support is occasionally required, which will include building data visualizations in addition to pipelines.
This role is fully remote but must overlap with the East Africa timezone by at least 3 hours to take meetings with stakeholders working in our country offices. For candidates in North and South America, this typically means taking some meetings between 9-11am Eastern Standard Time, with occasional meetings as early as 7am EST (meetings this early are rare but do happen from time to time).
Reports to: Senior Manager, Data
About you
You are a highly technical builder who:
Is excited by the opportunity to build data infrastructure from the ground up
Considers the long run implications of design decisions to ensure that systems scale and are resilient to changes over time
You invest in team success by:
Writing generalizable code that others can use
Documenting your process
Training teammates
Conducting thorough code reviews
You are intellectually curious and humble. You are excited about GiveDirectly's mission and the role that data can play in alleviating extreme poverty.
You have:
2+ years of experience as a data engineer
Direct experience with spark and cloud-based data lake(house) solutions
It would be helpful, but we do not require that you have:
Hands-on experience with Databricks
Hands-on experience with Tableau
Level and Compensation
At GiveDirectly, we strive to pay our employees generously and equitably. We use an accredited third party salary aggregator to ensure that staff's total compensation package (base compensation + bonus) falls within the 75th percentile of similar roles, at similar organizations. We also have a no negotiation policy to ensure we are paying staff equitably across roles.
Data Engineer:
The United States base salary for this role is $130,000 with a 10% bonus index.
The Kenya base salary for this role is $72,000.
For exceptionally experienced candidates we are open to considering a Senior Data Engineer role for this posting.
Both positions are independent contributors.
Why work at GiveDirectly?
At GiveDirectly, we work to ensure that you have everything you need to excel in your role and on your team, including:
A positive and supportive team with opportunities for advancement
A demonstrated commitment to helping all staff develop and grow
A competitive salary
A robust health benefits plan (exact details will vary by country)
Unlimited PTO (that we encourage staff to take!)
Desk allowance and flexible work location
Read more about our ongoing diversity, equity, and inclusion efforts here and about our decision to move our central support teams to remote first here.
About the hiring process
Online application. Submit resume and application questionnaire.
Phone screen – 30 minutes. Discuss experience and learn about the role.
Take home exercise – 3 hours, self-directed. Submit code and written responses to solve a problem associated with the role. You will choose a period of 72 hours during which you are available to complete the assignment.
Technical interview – 1.5 hour panel interview. Review the take home exercise and respond to technical questions.
Hiring manager interview – 1.5 hours with Senior Data Manager.
Final interview – 1 hour with Interim CTO.
3-5 Reference checks – 20 minutes each.
Venue: We conduct interviews over Google Meet with camera on (unless communicated otherwise)."
184,Data Platform Engineer,The Hershey Company,"Hershey, PA•Remote",N,"COVID-19 Vaccine Safety Mandate
This position requires that you must be fully vaccinated for COVID-19 (even if working remotely) and present proof of vaccination before beginning employment. If you receive an offer of employment, you will be asked as part of the background check process to upload a copy of your vaccination card. If you believe you require an accommodation for medical or religious reasons related to the vaccine safety mandate, you may submit a request after you receive an offer of employment. Instructions on how to do so will be provided in your offer letter.
Job Title: Data Platform Engineer

Job Location: Hershey, PA

This position is open for 100% remote.

Summary:
The Enterprise Data organization drives value for Hershey by providing high-quality, well governed data to the Enterprise for analytics and decision-making.

As part of a team of highly skilled technologists, the Data Platform Engineer will build monitoring tools to derive actionable insights for informed decision-making as it relates to Hershey’s data platforms. This role will partner with engineering and DevOps teams to develop enhancements and recommendations to optimize data pipelines, jobs, tools, and platforms for cost and performance. This role will have the opportunity to work with the larger team to learn and work on innovative data technologies, PoCs to exercise new ideas and influence the future direction of our data stack to continue to drive business value.

This role is part of the Platform & Operations team, whose focus is to enable Hershey’s business partners to work with data efficiently, securely, and responsibly. In addition, the Data Platform Engineer will be responsible for the management and administration of mixed data platform environments, utilizing tools such as Snowflake, Teradata, Databricks and Informatica.

Major Duties/Responsibilities:

Platform Budget and Monitoring:
Develop and deliver high-quality reports for monitoring of platform performance and cost. Identify opportunities for optimizations.

Platform Maintenance:
Manage platform upgrades, review product releases for enhancements and additional functionality, serve as point of contact for support teams and technical issues.
Platform Domain:
Collaborate with IT and business partners to define, manage and deliver innovative Data Platform solutions to drive growth and adoption of capabilities at Hershey.

Platform Advocacy:
Evangelize future data platform solutions identified by Enterprise Data leadership, including innovations such as: metadata management; data security and governance; cloud-based systems for data storage; multi-environment integration and automation of data tasks and movement.

Specific Job Responsibilities:
Manage SLA’s, performance and optimization opportunities of the cloud platforms will be a responsibility within this job family
Leverage industry standard KPI’s to measure the performance of the data platforms and processes
Collaborate with data governance to ensure adherence and adoption of data policies
Create, maintain, and monitor dashboards and KPI’s of cloud environments (e.g., Azure, Teradata, Snowflake) utilizing various reporting platforms
Monitor and centralize multi-cloud budget, expenses, and user activity to provide recommendations on cost avoidance and optimization of cloud environments
Collaborate with technical engineers to operationalize data solutions and ensure automation through the data value chain
Ensure relevant data and analytics are available to meet business needs by continuously consolidating and modernizing existing solutions and platforms
Strategic thinker with holistic vision, specific focus on the identification for the automation of existing manual processes to drive key business performance
Champion platform standards, tooling, and processes
Develop and/or maintain relationships with 3rd party vendors for appropriate solutions
Routine communication of complex cloud reports and technical concepts to users, clients, development teams, and management
Oversee incidents and enhancement requests within our managed services provider

Minimum knowledge, skills and abilities required to successfully perform major duties/responsibilities:

Ability to manage multiple priorities, meet deadlines and produce quality results under pressure
Demonstrated leadership and managerial skills
Strong problem solving and analytical skills
Strong team player, change agent, and advocate
Excellent customer service skills
High energy self-starter
Excellent verbal and written communication skills
Working knowledge of agile frameworks

Minimum Education and Experience Requirements:

Education:

Bachelor’s in a STEM degree
Master’s degree and/or related equivalent experience preferred

Experience:

Experience working with data, much of which has been focused on working with cross-functional teams and enterprise-wide data management programs
2+ years’ experience with public and private cloud solutions (e.g., Azure, GCP, AWS)
1+ years’ experience with Snowflake, including best practices, development, monitoring, reporting
Advanced working knowledge and experience with relational/non-relational databases e.g., Teradata, Snowflake, Databricks, or Azure Data solutions
Experience building data visualizations or analytics e.g., Power BI, Tableau, SSRS
Experience leading a project team or project function to deliver an enterprise data solution, application and/or ERP solution
Experience building, configuring and consuming APIs
Experience with SAP S/4 a plus
1+ years’ experience with GIT

Nice To Have:

Experience leveraging data integration tools to build data pipelines and microservices e.g., Informatica, Talend, Matillion
Experience working in a high performing agile delivery model, aligning with Scrum Masters, Product Owners, and other data execution team members to deliver rapid and impactful solutions that align to business partner strategy
Excellent problem-solving skills and can help triage operational issues proactively working to eliminate repetitive or manual tasks leveraging scripting and off-the-shelf tools

#LI-CW1
The Hershey Company is an Equal Opportunity Employer. The policy of The Hershey Company is to extend opportunities to qualified applicants and employees on an equal basis regardless of an individual's race, color, gender, age, national origin, religion, citizenship status, marital status, sexual orientation, gender identity, transgender status, physical or mental disability, protected veteran status, genetic information, pregnancy, or any other categories protected by applicable federal, state or local laws.

The Hershey Company is an Equal Opportunity Employer - Minority/Female/Disabled/Protected Veterans
If you require a reasonable accommodation as part of the application process, please contact the HR Service Center (askhr@hersheys.com)."
185,Data Engineer,N,Remote,N,"Drivetrain is on a mission to empower businesses to make better decisions. Our financial planning & decision-making platform helps companies scale and achieve their targets predictably.

Drivetrain is a remote-first company headquartered in the San Francisco Bay Area. Founded in 2021 by a couple of ex-Googlers, Drivetrain is a fast-growing company on a trajectory for success with backing from leading venture capital firms.

Drivetrain provides a great culture for its employees to thrive in and be happy.

Remote-friendly: Drivetrain brings together the best and the brightest, no matter where they are and provides them a great degree of autonomy. We trust our people.
️ Open & transparent: We know that when our creators have access to all the information they need, their best work will emerge.
Idea-friendly: We provide an environment to explore new ideas, to take risks, to make mistakes, and to learn, so you can succeed. Anyone in the company can come up with great ideas and become a catalyst for positive change. We let the best ideas win.
Customer-centric: We follow a product-led growth strategy, continuously learning from our customers and collaborating to build the amazing software that Drivetrain is.

About the role

Drivetrain is looking for a Data Engineer to join our team. The role enables you to lay the foundation of an exceptional data engineering practice.

The ideal candidate will be confident with a programming language of their choice, be able to learn new technologies quickly, have strong software engineering and computer science fundamentals and have extensive experience with common big data workflow frameworks and solutions.
What you’ll be doing
Build and monitor large-scale data pipelines that ingest data from a variety of sources.
Develop and scale our DBT setup for transforming data.
Work with our data platform team to solve customer problems.
Use your advanced SQL & big data skills to craft cutting edge data solutions.
Requirements
1-5+ years in a data engineering role(high-growth startup environments highly preferred).
Expert-level SQL skills.
1 year experience in DBT is required.
At least 1 year experience with a leading data warehouse is required.
Expertise with big data technologies like Hadoop, Spark, Druid preferred.
Track record of success in building new data engineering processes and an ability to work through ambiguity.
Willingness to roll up your sleeves and fix problems in a hands-on manner.
Intellectual curiosity and research abilities.
Sounds exciting? Apply at careers@drivetrain.ai . It may just be the next best decision you’ve ever made!"
186,Data Engineer,NuTechs LLC,"Troy, MI•Hybrid remote","$100,000 - $130,000 a year","Data Engineer
Troy (2-3 Days) / Remote (3-2 Days) - Local candidates preferred

Actively seeking a Data Engineer who possesses a strong passion for designing, optimizing, refactoring, and upgrading complex data solutions. The Insights team is a new team with an exponential growth opportunity – both in terms of technology and personnel, and they are looking for someone to share their mission that will transform their company’s future. With this position, you will have a rare opportunity to use your talents, passions, and expertise to help drive this massive change in how we build, organize, and optimize our backend systems and processes across all product lines. This position offers excellent career growth and promotional opportunities, and stellar compensation.

Required Experience (Must have)
Minimum 5+ years’ data engineering experience working with designing data models in both column store and relational databases, incorporating disparate data to solve complex business needs.
Minimum Bachelor’s degree or Graduate degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field.
Experience building and optimizing data pipelines, architectures, database modeling and data sets that meet business requirements.
Experience in building the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources in ‘big data’ technologies or similar. Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience in building processes supporting data transformation, analytics tools, data structures, metadata, dependency, and workload management. Assist Data Science team that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and optimizing our product into an innovative industry leader.
Experience in using Azure DevOps, or Jira project management/requirements management.
Experience with big data tools: Hadoop, Spark, Kafka, or a related technology.
Proficient in programming languages such as SQL, Python, R, Shell Scripting
Desired Experience (Nice to have)
Experience with NoSQL databases, including Postgres and Cassandra is a plus
Strong working experience with technologies like AWS/Databricks and (minimum 3+ years’ experience) working with big data, including expertise designing data models in both column store and relational databases, incorporating disparate data to solve complex business needs
AWS lambda functions (Python preferred)
AWS databases (Aurora, DynamoDB, or Redshift preferred)
AWS storage services (EC2, S3 preferred)
Data Lake experience or similar (AWS preferred)
Primary Responsibilities
Developing and implementing an overall organizational data strategy that is in line with business processes. The strategy includes data model designs, database development, implementation and management of data warehouses and data analytics systems.
Identifying data sources, both internal and external and work out a plan for data management that is aligned with organizational data strategy.
Coordinating and collaborating with cross-functional teams, stakeholders, and vendors for the smooth functioning of the enterprise data system.
Managing end-to-end data architecture, from selecting the platform, designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.
Identifying, designing, and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition."
187,Sr. Data Engineer,Accrette Information Technology Professional Solutions Inc.,Remote,"$87,409 - $173,190 a year","Job Purpose:
Responsible for visualizing and designing enterprise data management framework. Analyses the data needs of thecompany and creates the data roadmap.
The role demands the deployment of theright data architecture to convert business need into data and system requirements, align business processes with IT systems, and manage the complex flow of data and information within the organization
Work Experience
Must have worked on Data Lake projects and involved in supporting data orchestration activities as part of building and enhancing
Experience of architecting data solution across hybrid (cloud, on premise) data platforms
Exposure to cloud integration/ cloud DWH implementation. (Snowflake, Azure Data Factory, Databricks etc.)
Deep knowledge and experience of the different database technologies - Oracle, SQL Server, PostgreSQL
Experience in large-scale Data Migration Projects
Experience of ETL & ELT Tools such as Informatica IICS.
Experience in Performance Tuning of ETL scripts to reduce the CPU time/load timing
Experience with DevOps and Monitoring on Azure.
Experience of mapping key Enterprise data entities to business capabilities and applications
Principal Accountabilities
Providing technical and data leadership to organization
Providing operation guidance to the application development team, IT and other functions.
Understand the strategic business mission, vision, direction and goals
Understand the future business capabilities (KPI, metrics, goals) etc.
Identify current capabilities and areas of pain
Identify gaps and areas of opportunity
Understand the current data policies and procedure
Gather Data via interviews, workshops, focus group discussion
Baseline and benchmark best PA and common solutions
Bring outside in view on latest tech stack on data management, Data Orchestration, DevOps and Source Code Management insight for Structural as well as semi / unstructured data ( across on-prem as well as on cloud)
Prepare framework to review projects to ensure short term and long terms data strategy are followed by project delivery teams
Build a framework of principles to ensure data integrity across the business
Identify and evaluate new data management technologies
Develop and Integrate technical functionality (e.g. scalability, security, performance, data recovery, reliability, etc.)
Required professsional software development experience
3+ years of programming experience in React.
2+ years of application development using Azure Cloud native architecture
Extensive knowledge in: React.js, Typescript, JSON, Javascript, CSS, UI UX design and development, Agile delivery
Strong proficiency in JavaScript, including the JavaScript object model
Thorough understanding of React.js and its core principles
Experience with popular React.js and React native
Familiarity with RESTful APIs
Familiarity with code versioning tools such as GIT
Should have knowledge about HTML, CSS design that is compatible across multiple browsers, ADA/CATO complaint
Should have knowledge about working in an agile environment & Azure Cloud platform
Job Types: Full-time, Contract
Salary: $87,408.67 - $173,189.72 per year
Compensation package:
Bonus pay
Schedule:
8 hour shift
Experience:
Data modeling: 3 years (Preferred)
Work Location: Remote"
188,Data Engineer - Machine Learning,FanDuel,"Atlanta, GA•Hybrid remote",N,"ABOUT FANDUEL GROUP
FanDuel Group is a world-class team of brands and products all built with one goal in mind — to give fans new and innovative ways to interact with their favorite games, sports, teams, and leagues. That's no easy task, which is why we're so dedicated to building a winning team. And make no mistake, we are here to win, but we believe in winning right. That means we'll never compromise when it comes to looking out for our teammates. From our many opportunities for professional development to our generous insurance and paid leave policies, we're committed to making sure our employees get as much out of FanDuel as we ask them to give.
Our brands include:
FanDuel — A game-changing real-money fantasy sports app
FanDuel Sportsbook — America's #1 sports betting app
FanDuel TV — ""The Bettor Sports Network"" bringing live sports and interactive content to the games fans care about most
FanDuel Racing — A horse racing app built for the average sports fan
FanDuel Casino & Betfair Casino — Fan-favorite online casino apps
FOXBet — A world-class betting platform and affiliate of FanDuel Group
PokerStars — The premier online poker product and affiliate of FanDuel Group
THE POSITION:
Our roster has an opening with your name on it
Data drives our organization. As a Data Engineer at FanDuel, you will help us unlock the full potential of our vast amounts of real-time and relational data, provide our business with insight, and delight our customers with a world-class personalized experience. You will help us turn every click our users make, every bet, every touchdown, every fumble, and every play into a real-time stream of knowledge allowing us to make better and faster decisions – outplaying our competition.
In this role, you will partner with the engineering teams building our online applications, data scientists mining our data and training our models, and our analysts who make the data mean something. You will help us organize, model, and present our data as a coherent product and offer it to our stakeholders, providing a common information framework that allows FanDuel to intelligently react to what is happening on the field and in the marketplace.
THE GAME PLAN:
Everyone on our team has a part to play
Creating and maintain optimal data pipeline architecture
Designing and implementing data pipelines required in the data warehouse and data lake in batch or real-time using data transformation technologies.
Identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability
Designing and deploying data models and views with large datasets that meet functional / non-functional business requirements
Delivering data integration solutions to downstream marketing and campaign software
Delivering quality production-ready code in an agile environment
Delivering test plans, monitoring, debugging and technical documents as a part of development cycle
Creating data tools for analytics and working with stakeholders across all departments to assist with data-related technical issues and supporting their data infrastructure needs
Machine learning and data science
Business intelligence tools (e.g., Tableau, Knime, Looker)
Data security and privacy (e.g. GDPR, CPP)
Data governance and data testing frameworks.
Continuous integration and delivery of production data products
THE STATS:
What we're looking for in our next teammate
Knowledge with languages such as Python, Java, SQL, and/or shell scripts.
Experience as a data engineer in a machine learning environment (recommendation systems, pattern recognition, data mining or artificial intelligence).
Hands on experience with ML frameworks and libraries (Scikit-learn, Pytorch, Tensorflow, LightGBM, Keras), data structures, data modeling, and software architecture.
Modeling and querying data using SQL and DBT.
Processing event-based data using streaming technologies such as AWS Kinesis and Kafka for ML pipelines.
Working with other members of the ML platform team to support delivery of additional project components (API interfaces and microservices).
Developing and contributing to a ML feature store for downstream operational teams.
Designing and building data pipelines for production level ML infrastructure, using tools such as TFX, Kubernetes, Kubeflow Pipelines, TensorFlow.
Deploying ML models under the constraints of scalability, correctness and maintainability.
Implementing open-source data quality frameworks for data profiling and observability.
Experience demonstrating technical leadership working with teams, owning projects, defining and setting technical direction for projects.
THE CONTRACT:
We treat our team right
Competitive compensation is just the beginning. As part of our team, you can expect:
An exciting and fun environment committed to driving real growth
Opportunities to build really cool products that fans love
Mentorship and professional development resources to help you refine your game
Flexible vacation allowance to let you refuel
Hall of Fame benefit programs and platforms
FanDuel Group is an equal opportunities employer. Diversity and inclusion in FanDuel means that we respect and value everyone as individuals. We don't tolerate bias, judgement or harassment. Our focus is on developing employees so that they reach their full potential.
#LI-Hybrid"
189,Sr Big Data/Data Engineer,Infinity Quest,Remote,From $65 an hour,"10 Experience
Job Details:
Must Have Skills
Big data
Informatica
Tableau
Detailed Job Description
Design the architecture of big data platformPerform and oversee tasks such as writing scripts, calling APIs, web scraping, and writing SQL queriesDesign and implement data stores that support the scalable processing and storage of our highfrequency dataMaintain our data pipelineCustomize and oversee integration tools, warehouses, databases, and analytical systemsConfigure and provide availability for dataaccess tools used by all data scientists
Job Type: Contract
Salary: From $65.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 10 years (Preferred)
Tableau: 10 years (Preferred)
Big Data Engineer: 10 years (Preferred)
Work Location: Remote
Speak with the employer
+91 8838059965"
190,Data Analytics Engineer,Ameresco,Remote,N,"Ameresco, Inc. (NYSE:AMRC) is a leading cleantech integrator and renewable energy asset developer, owner and operator. Our comprehensive portfolio includes energy efficiency, infrastructure upgrades, asset sustainability and renewable energy solutions delivered to clients throughout North America and Europe. Our solutions range from upgrades to facility’s energy infrastructure to the development, construction and operation of renewable energy plants combined with tailored financial solutions. We foster an entrepreneurial, collaborative, and forward-thinking culture that thrives with innovation, diversity of thought, and inclusion. We are excited with all that the future holds for our industry, planet, and communities.
Ameresco Asset Sustainability Group (ASG) provides best in class asset management software and consulting services to our customers throughout North America. These customers include Schools, Higher Education, Public Housing, Municipalities, State & Federal Agencies, Commercial and Industrial real asset portfolios. Utilizing our propriety AssetPlanner® software suite, with modules and applications covering Capital Planning, Maintenance Management, Energy Management, Carbon Management and more, ASG is well-positioned to advise, enable, and support Ameresco customers to make better decisions around capital funding appropriation, reducing operating costs, and building towards a more sustainable future.
The successful candidate will work with the AMERESCO’s Asset Sustainability Group to develop and deploy analytics solutions across our portfolio of energy efficiency and distributed energy projects.
Responsibilities:
Initial onboarding of new sites, including creation and validation of point/entity maps, configuration of data pipelines and tuning of fault detection and diagnostic routines;
Develop and maintain data pipelines to capture, process and transform raw data from energy and building automation system for analysis and reporting;
Refine data quality control procedures for energy billing, interval meter, and building automation system data to identify and correct errors and inconsistencies in the data;
Support development and deployment of sensor and system fault detection and diagnostic algorithms.
Develop new visualizations and reports to effectively communicate building energy performance issues and opportunities for optimization to stakeholders;
Support ongoing development of measurement and verification, monitoring based commissioning, and predictive maintenance offerings;
Work with teams through AMERESCO to identify and exploit opportunities to streamline and automate existing processes to increase quality and productivity; and
Perform other duties as required.
Minimum Qualifications:
BS in Computer Science, Mechanical or Electrical Engineering is required.
Demonstrated proficiency using MS Excel and Python for data analysis is required.
Additional Qualifications:
Excellent analytical and problem-solving skills.
Strong communication and collaboration skills to work effectively with stakeholders across organization.
Experience with building automation systems and renewable energy systems is desirable.
AMERESCO challenges the brightest, most talented and creative individuals in the industry by providing an environment that embraces initiative, diversity, and achievement along with comprehensive rewards, including people-oriented insurance, investment, and incentive plans.
Equal Opportunity/Affirmative Action Employer/Women/Minorities/Veteran/Disability."
191,Data Engineers,Artha Solution,"9375 E Shea Blvd, Scottsdale, AZ 85260",N,"Job: Data Engineers / Interns – Entry level
Positions: Multiple
Preferable Locations: Scottsdale AZ, Naperville, IL
We are looking for talented individuals to become part of our growing business and grow with us. The right person should have the interest to learn new tools and adapt to fast paced environment. Experience/knowledge on databases, data management, data integration, Big Data, Data warehouse, reporting, and data analytics is preferable.
The role and compensation will depend upon any prior experience and exposure to technology areas and demonstrated education.
Education, Skills, and Abilities:
BS/BA in Computer Science or related discipline required.
Experience in writing SQL queries and basic functions & Stored procedures.
Fundamental Knowledge of databases, data warehousing, data integration, and BI tools.
Exposure to data management concepts, reporting, and analytics tools is an advantage.
Experience in RDBMS, No SQL and Java is an advantage concepts/ exposure to Cloud Computing
Attention to detail, Analytical, and problem-solving skills.
Ability and willingness to learn new tools and applications.
Effective written and verbal communication skills with the ability to convey complex technical concepts to business users and management."
192,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
193,Data Engineer,N,"New York, NY•Remote",N,"How We’re Different
We’re a lean team so all of your work will have a direct and measurable impact on the business
You’ll have the opportunity interact with some of our amazing beta customers who are constantly providing feedback and helping us make the product better
You’ll have the opportunity to craft elegant, efficient, and (sometimes!) scrappy solutions to hard technical problems using the latest and greatest tools and technologies
Technology
Our application runs on a tech stack that is a mixture of Python and Ruby on Rails + React
For our backend data processing, we use Apache Airflow.
For our web application, we use Ruby on Rails, with a Typescript/React single-page-app frontend, powered by a GraphQL API
What Your Day Would Look Like
Build scalable and fault-tolerant data pipelines in Google Cloud Platform using Apache Airflow
Inspect, analyze, and transform data using SQL based tools like BigQuery and dbt
Design, implement, and test features on our web application
Build integrations with external services
What We Look For
You’re the best at what you do, take ownership for your work and are constantly looking to learn
3+ years of experience in Python and SQL
You have experience with Apache Airflow and familiarity with AWS or GCP"
194,Data Engineer,MIT,"Cambridge, MA 02139•Remote","$95,000 - $108,000 a year","Information on MIT’s COVID-19 vaccination requirement can be found at the bottom of this posting.

DATA ENGINEER, MIT Libraries-Information Technology Services (ITS), to support the Libraries’ vision of building and supporting an open global platform for knowledge creation and sharing through an Infrastructure-as-Code (IaC) and automation-with-code approach. Will continue the build out of the Libraries’ pipeline automation in its cloud infrastructure; optimize, monitor, and scale applications and associated infrastructure; and configure and deploy new workloads in the cloud. Will use Python, Terraform, and other tools together with a DevOps approach to establish automated ETL pipelines, work with immutable infrastructure, and execute repeatable deployments. Responsibilities include building, configuring, documenting, and deploying data-related applications; supporting, monitoring, optimizing, and troubleshooting application automation; designing, building, configuring, documenting, and deploying automated ETL pipelines; partnering on supporting, maintaining, and improving existing data pipelines; solving business and technical problems in collaboration with colleagues from technology teams and functional stakeholders; and establishing and maintaining strong working relationships within the team, among the department, and across the organization.

ITS is committed to continuous learning, failing forward, open communication, shared responsibility, and collaborative problem-solving.

Job Requirements
REQUIRED: bachelor’s degree in a technical field, preferably computer science/technology management; five years’ professional experience in a role where application development and/or data engineering were primary responsibilities; understanding of cloud-native applications, pipeline automation, and serverless deployments; experience with cloud technologies, preferably AWS; excellent verbal and written communication skills; data and/or metadata engineering experience; Python software development skills; and experience building Docker containers. PREFERRED: experience with automated deployments, DevOps, and agile practices/methodologies; and familiarity with multiple metadata content and encoding standards, e.g., MARC/MARCXML, Dublin Core, EAD, PREMIS, VRA Core, FCGD, or CCO. Job #22569-9

Hiring range: $95,000 - 108,000

This role can be filled by a fully remote employee.

Priority will be given to applications received by April 21, 2023."
195,Enterprise Data Warehouse Engineer-(REMOTE),GEICO,"Chevy Chase, MD 20815•Remote","$100,000 - $236,500 a year","The Geico IT Enterprise Data Warehouse is seeking a highly motivated candidate to join our team. The successful candidate will demonstrate their ability to think strategically and be innovative while working in an agile and collaborative team-oriented environment. The candidate must be comfortable working with analysts, developers, architects, and internal customers at all levels across multiple internal teams. Must also possess intellectual curiosity, be detail oriented, and have a desire to learn new skills and technologies.
LEADS design sessions and code reviews to elevate the quality of engineering across the organization.
Responsibilities Include:

The approved candidate will work with other engineers on designing and implementing solutions using ETL frameworks and tools on multiple Cloud platforms using one or more of the following: Ab Initio, Azure Data Factory, Fivetran, HVR, DBT, and a variety of scripting languages, SQL, UNIX, PowerShell, Perl, Python.
Participate in architecture and design sessions to ensure that all business and technical requirements are addressed.
Influence product definition and leverage technical skills to drive towards the right solution
Provide analytical support and triage bugs that originate in both non-production and production and provide resolution as needed.
Assist business or technical users with ad-hoc requests, including data investigation, query and performance-optimization support
.
Meets the requirements specified below

Must be able to communicate in a clear, concise, professional oral or written manner, to be understood by customers, co-workers, and other employees of the organization.
Must be able to concentrate and demonstrate a capacity for learning technical concepts and adapting to new technologies quickly.
Must be able to, with or without accommodation, perform the essential functions which include, but are not limited to seeing, hearing, typing and speaking.
Must be able to follow complex instructions, resolve conflicts or facilitate conflict resolution, and have strong organization/priority setting skills.
Must be able to multi-task.
Must be able to learn and apply large amounts of technical and procedural information and follow processes that have been published.
Must be knowledgeable of software coding and following standards and processes that have been published and the guidelines for the design.
Must be able to provide system training to team members as needed.
Must be able to perform under pressure and stressful situations.

Must have the following experience:

Bachelor’s Degree in Computer Science (or related field)
5+ years of experience in data software development, data engineering with SQL in a Data Warehouse environment, building data pipelines and complex data transformations with one or more integration tools (e.g. Ab Initio, Fivetran, HVR, Azure Data Factory, DBT, Informatica, SSIS, etc.) as well using Relational & NoSQL databases, open data formats, and programming languages such as UNIX scripting, Python, Scala, or other frameworks,
Ability to use one or more of the following tools for data hunting, testing, analytics, and metrics such as SQL IDE’s, and analytics tools such as Power BI or similar tool.
Ability to provide rotational on-call ETL support and as needed 24x7

Additional experience (preferred)

3+ years of experience in Data Warehouse Data Architecture using data modeling tools such as erwin
2+ Experience in Snowflake Data Cloud
2+ years of experience in Microsoft Power BI or other Business Intelligence reporting
2+ years of experience in designing and building solutions for data security, data quality and observability, metadata management, data lineage, and data discovery
3+ years of experience Cloud DevOps Concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework
3+ years of experience designing and building for data quality assurance, reliability, availability and scalability, on existing and new data applications
2+ years of experience building data software in microservices-oriented architecture and extensible REST APIs
2+ years of experience in open-source frameworks
2+ years of experience working in an Agile environment (SAFe/ Scrum/Kanban)
Experience with XML, JSON data schema and data mapping
Experience with monitoring tools: e.g Splunk

Benefits:
At GEICO, we make sure you have the support and resources to leverage and develop your skills, secure your financial future, and take care of your health and well-being. GEICO continually seeks to provide a workplace where everyone can be their authentic self. To help achieve this goal, we support associate-led Employee Resource Groups that foster a true sense of community. Through GEICO’s competitive benefits offerings and various training and development opportunities, we have you covered with our Total Rewards Program* that includes:
Premier Medical, Dental and Vision Insurance with no waiting period**
Paid Vacation, Sick and Parental Leave
401(k) Plan
Tuition Reimbursement
Paid Training and Licensures

Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
**Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
GEICO is proud to be an equal opportunity employer. We are committed to cultivating an environment where equal employment opportunities are available to all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO celebrates diversity and believes it is critical to our success. As such, we are committed to recruit, develop and retain the most talented individuals to join our team.
#AJ19
Annual Salary
$100,000.00 - $236,500.00
The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations."
196,AWS Data Engineer,Oxfaa Pvt.Ltd,"Trenton, NJ",$75 - $80 an hour,"Required Skill:
· Prior experience with writing and debugging python
· Prior experience with building data pipelines.
· Prior experience Data lakes in an AWS environment
· Prior experience with Data warehouse technologies in an AWS environment
· Prior experience with AWS EMR
· Prior experience with pyspark
Job Type: Contract
Salary: $75.00 - $80.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
AWS: 6 years (Required)
Work Location: On the road"
197,Data Engineer,Codazen,"Salt Lake City, UT","$112,000 - $149,000 a year","About Codazen
Want to apply technology in ways that make life more enriching?
We think the digital experiences that make up more and more of our everyday lives should be seamless and engaging—even magical. Using our unique recipe of specialized skills and digital alchemy, we engineer experiences that help businesses engage with users, and users engage with the world.
The source of our magic comes from our people. We continually strive to be better versions of ourselves—and have fun while doing it. If a career creating innovative experiences and digital products sounds exciting, we hope you'll consider joining our amazing team.

Rethink Experience™
Summary
As an engineering consulting agency providing services to large enterprise organizations, Codazen is positioned to support and strongly influence the success of those organizations. As a Data Engineer for Codazen, you will partner with our customers, relevant stakeholders, data scientists, and software engineers to ensure value can be derived from data. You would be responsible for identifying, designing, developing, and delivering infrastructure and solutions that transfers and derives meaningful information from data and allows our customers to make smart decisions quickly and enable their success. You would be involved in understanding business requirements and delivering solutions to support the customer's needs. This would include extracting and translating data into consumable information, utilizing expertise to support the development of extensible data models, and strongly partnering with data scientists to streamline, enhance, and improve relevant work streams. The ideal candidate will be passionate to innovate and deliver impactful solutions within a high-velocity agile environment.

Role Responsibilities
Cross-functional partnership with our Durable Teams and Customers to identify and understand data needs.
Leverage experience and expertise to build enterprise-scale data warehouses.
Design, develop, and deliver effective and reliable pipelines to transfer and translate data.
Securely source new data that can enhance and improve the quality of information.
Smartly design and develop data models that allow for efficient storage and effective retrieval.
Ensure quality and data integrity to support confidence in the insights obtained from the data.
Utilize a security and privacy first mindset in how data is stored, transferred, handled, and accessed.
Manage, maintain, and create data pipelines as needed.
Understand and own the data engineering component within the overall business solution.
Collaborate with software engineers to create and deliver elegant solutions for data consumption.

Qualifications
Bachelor's or Master's degree in Computer Science or related technical field
5+ years of Python or other modern programming language experience
5+ years of relational database experience
3+ years with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
3+ years experience with Data Modeling
Experience working with cloud or on-premises Big Data/MPP analytics platforms (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar)
2+ years experience working with enterprise DE tools and experience learning in-house DE tools

Plus
Solid programming foundation including object-oriented programming, algorithms, and data structures
Experience designing and implementing real-time pipelines
Experience with SQL performance tuning and end-to-end process optimization
Experience with anomaly/outlier detection
Experience with notebook-based Data Science workflow
Experience with Airflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.
Full compensation packages are based on candidate experience and certifications.
Salt Lake City Pay Range
$112,000—$149,000 USD
What Codazen Offers
Competitive salary
Generous paid time off and sick leave
Paid holidays
Health, dental, vision
401k (with company contribution)
Profit sharing
Company bonus
Income protection plans (life, accidental death and dismemberment, short- and long-term disability)
Free drinks and snacks
An incredible leadership team that cares about your development
Fun, smart, diverse colleagues
We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class."
198,Data QA Engineer,First American Financial Corporation,"Santa Ana, CA","$73,700 - $143,000 a year","Company Summary
Come join First American's Digital Title Group, newly formed to re-imagine and digitize the title search and examination process through Big Data, AI, document automation and modern, cloud-native application development. As a market leading title insurance company, powered by the nation's largest and most complete property information, ownership and recorded document database, First American is committed to advancing title automation and removing friction from the real estate closing process. Our modern title decisioning solutions create certainty and speed through data and analytics, delivered to real estate agents, lenders, title agents and homebuyers. Join a team that puts its People First! Since 1889, First American (NYSE: FAF) has held an unwavering belief in its people. They are passionate about what they do, and we are equally passionate about fostering an environment where all feel welcome, supported, and empowered to be innovative and reach their full potential. Our inclusive, people-first culture has earned our company numerous accolades, including being named to the Fortune 100 Best Companies to Work For® list for seven consecutive years. We have also earned awards as a best place to work for women, diversity and LGBTQ+ employees, and have been included on more than 50 regional best places to work lists. First American will always strive to be a great place to work, for all. For more information, please visit www.careers.firstam.com.
Job Summary
Are you passionate about automation and consider yourself an advocate for quality?
Do you enjoy breaking things and finding out how they work?
What you'll do:
Test new product features and enhancements based on functional and design requirements
Collaborate with Engineers and Product Managers in an Agile delivery team to plan, test, and provide feedback and approval for releases
Write and execute test cases for front-end and back-end such as API to data validation
Experience with API testing and parsing JSON files to validate output matches data saved in DB
Collaborate with Product Managers and review product requirements to identify any gaps and ambiguities
Write code to perform automated tests
Document Test Cases, Test Plans, and Results in a shared repository
Perform exploratory testing and thorough regression testing to safeguard against breaking existing features
Knowledge and Skills
3+ years of Software QA experience
Strong knowledge in SQL for data validation
Experience with developing automated test scripts using 3rd party testing tools
Proficient in a variety of interfaces including REST APIs, SQL, and web browser dev tools
Knowledge of at least one programming or scripting language (Python, C#, Java, JavaScript)
Apply strong knowledge of testing methodologies such as black box, white box, smoke, and unit testing
Experience testing cloud migration applications is a plus
Education
Generally requires a BS Degree in IT/Engineering or equivalent work experience
Pay Range: $73,700 - $143,000 annually
This hiring range is a good faith and reasonable estimate of the salary range of possible compensation at the time of the posting, and is subject to change. The actual compensation offered will be determined by various factors, which may include a candidate’s education, training, experience, and geographic location.
#LI-REMOTE
#LI-EL
First American invests in its employees' development and well-being, empowers them to provide superior customer service and encourages them to serve the communities where they live and work. First American is committed to diversity and inclusion. We are an equal opportunity employer.
Based on eligibility, First American offers a comprehensive benefits package including medical, dental, vision, 401k, PTO/paid sick leave and other great benefits like an employee stock purchase plan."
199,Data Engineer - FinTech,EquiLend,"New York, NY•Hybrid remote",N,"Company Overview
At Equilend we pride ourselves on being a leading global provider of trading, post-trade, securities market data, regulatory technology and clearing services in the securities finance industry.
Our journey began in 2000 when 10 of leading global financial institutions including Bank of America, BlackRock, Goldman Sachs, J.P. Morgan, Morgan Stanley, State Street and UBS came together with a view to increasing automation and efficiencies in the Securities Finance space. Since then, we've built our global footprint including offices in New York, Boston, Toronto, London, Dublin, Hong Kong, Tokyo, New Jersey, Pune and Chandigarh.
As a business we're committed to nurturing long-term relationships with our clients, creating an environment where our people can learn and grow, and continuing to foster an innovative and supportive environment.
Role Responsibilities
Design and develop ETL data pipelines using Python on Spark.
You will combine the discipline of software engineering with the knowledge and experience of building data solutions to deliver business value.
As a data engineer, you'll help deploy data pipelines and processes in a production-safe manner
Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility
Implement processes and systems to monitor data quality, ensuring production data is accurate and available.
Work closely with all business units and engineering teams to develop a strategy for long-term data platform architecture.
Required Skills
3+ years of relevant commercial experience
Have excellent knowledge of Python/PySpark or other Object Oriented languages such as Scala or Java
Experience with scheduling and monitoring tools such as Airflow or similar
Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
A background in software development in high-volume environments
Worked with stream processing technologies (such as Kafka, Spark and AWS Kinesis)
Working knowledge of AWS is highly desired, but not required
Thrive in a diverse, open and collaborative agile environment
Role Location Benefits
Hybrid working – you can choose which 100 days per year to work remotely where you prefer!
Generous vacation days each year which increase with your levelling
Discretionary annual bonus
Health Insurance coverage from day 1
401(k) Employer matching contribution plan
Regular team events – both in the office and offsite!
Weekly remote yoga sessions and paid access to a mindfulness-based mental health app
Diversity and Inclusion
We are committed to diversity and inclusion and believe that it is essential to our success as a company. We value and respect all individuals, regardless of their background, and recognize that diversity brings a wealth of perspectives and skills that help us innovate and excel and we welcome applications from individuals of all backgrounds and experiences.
If you require any accommodations during the application process please let us know.
#LI-Hybrid"
200,Big Data Engineer,Start.io,United States,N,"Description
Start.io is a mobile marketing and audience platform. Start.io (formerly StartApp) empowers the mobile app ecosystem and simplifies mobile marketing, audience building, and mobile monetization. Start.io's direct integration with over 500,000 monthly active mobile apps provides access to unprecedented levels of global first-party data, which can be leveraged to understand and predict behaviors, identify new opportunities, and fuel growth.
If you are a data enthusiast and want to participate in real-time data streams of billions of events from billions of users, your place is with us.
Responsibilities:
Develop and deploy real-time and batch data processing infrastructures and pipelines.
Ingesting streams of billions of records per day.
Develop real-time streams of hundreds of thousands of events per second to provide insights to our Advertising platform.
Manage data pipelines from and to all of our data sources, Vertica , Presto / Athena, Spark /Flink apps and streaming.
Develop monitoring over Big Data infrastructure.
Requirements
Proven experience with Java/Scala/Go
Experience in design and development of scalable big data solutions.
Experience with AWS / GCP / Azure.
Experience in Big Data – Spark / Flink / Kafka / Confluent etc.
Experience in Kubernetes and containers."
201,Data Engineer,REPS & Co.,"Arizona City, AZ 85123•Remote",N,"Founded in 2017, REPS & Co. is a leader in the entertainment industry specializing in ticketing for live events. We provide tickets to many events and shows across the nation including music, sports and theatrical performances. We pride ourselves in offering the best experience for the best price to our customers. Our technology is what allows us to outperform our competitors and deliver an unforgettable experience to fans.
We are looking for an organized, experienced, and driven Data Engineer. The Data Engineer is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams.
We are excited to add a Data Engineer to our growing team!
Responsibilities:
Work on data architecture and use a systematic approach to plan, create, and maintain data while also keeping it aligned with business requirements
Collect and obtain data from appropriate sources after formulating a set of dataset processes
Conduct research in the industry to address any issues that can arise while tackling a business problem
Create models and identify patterns
Dive into data and pinpoint tasks where manual participation can be eliminated with automation
Use programming languages and tools
Identify ways to improve data reliability, efficiency, and quality
Use large data sets to address business issues
Prepare data for predictive and prescriptive modeling
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems
Qualifications:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
Experience with relational SQL and NoSQL databases, including RDS, Postgres, MySQL, Mongo / DynamoDB, and Cassandra.
Experience in data pipeline and workflow tools: DBT, Lambda, AirByte, Airflow, etc.
Experience with BigData cloud services: BigQuery, EMR, RDS, Redshift, Glue, Athena
Experience with stream-processing systems: Druid, Storm, Spark-Streaming, etc.
Experience with object-oriented languages: Python, Java, Javascript, etc.
Benefits
Medical, Dental and Vision insurance
Unlimited PTO
Sick Leave
Paid Holidays
Volunteer Time Off
401k with Match
ESOP
Parental Leave
100% Employer Paid Life Insurance & Long Term Disability
EAP Program
Bonus
Gym Membership Reimbursement
Ticket Benefit"
202,"Sr. Software Engineer - Java, Scala and Data Streaming",Caterpillar,Illinois•Remote,"$104,112 - $156,168 a year","Career Area:
Digital
Job Description:
As a Senior Software Engineer, you will build platform notification services that enable Caterpillar customers to be alerted for events that are critical to efficiently operate Caterpillar engines and machines.
JOB DUTIES: As a Senior Software Engineer you will be is responsible for development of services using Scala and data streaming technologies. The responsibilities include the following:
Competent to perform all programming, project management, and development assignments without close supervision; normally assigned the more complex aspects of systems work.
Works directly on complex application/technical problem identification and resolution
Works independently on complex systems or infrastructure components that may be used by one or more applications or systems.
Drives application development focused around delivering business valuable features
Mentors and assists software engineers, providing technical assistance and direction as needed
Maintains high standards of software quality within the team by establishing good practices and habits
Identifies and encourage areas for growth and improvement within the team
Guides the team to develop structured application/interface code, new program documentation, operations documentation and user guides in a casual, flexible environment
Communicates with end users and internal customers to help direct development, debugging, and testing of application software for accuracy, integrity, interoperability, and completeness
Performs integration testing and customer acceptance testing of components that requires careful planning and execution to ensure timely, quality results.
Employee is also responsible for performing other job duties as assigned by Caterpillar management from time to time.
Basic qualifications:
Position requires a four-year degree in Computer Science or related field from an accredited college or university.
5+ years of software development experience using Java 8+
2+ years of Scala programming experience
2+ years of experience with data streaming technologies such as Akka (Streams, Actors, Persistence), Spark Streaming, Flink, Kinesis, etc.
Experience designing well-defined Restful APIs
Deploying software using CI/CD tools, such as Azure DevOps, Jenkins, etc.
Experience with AWS or other Cloud services.
Top candidates will also have:
Strong understanding of data structures and algorithms
Application architectural patterns, such as MVC, Microservices, Event-driven, etc.
Deploying and maintaining software using AWS. Other AWS technologies, such as API Gateway, ALB, NLB, Fargate, Lambda, S3, CloudWatch, IAM, CloudFormation, etc
Developing software applications using relational and NoSQL databases such as PostgreSQL, AWS DynamoDB
Experience with design and implementation of high-availability and reliable solutions
Designing, developing, deploying and maintaining software at scale.
Working within an Agile framework (ideally Scrum)
Compensation & Benefits:
Base salary for this role ranges from $104,112 to $156,168. Actual salary will be based on experience. The total rewards package, beyond base salary includes:
Annual incentive bonus plan*
Medical, dental and vision coverage starting day 1
Paid time off plan (Vacation, Holiday, Volunteer, Etc.)
401(K) Savings Plan including company match
Health savings account (HSA)
Flexible spending accounts (FSA)
Short and long term disability coverage
Life insurance
Paid parental leave
Healthy Lifestyle Programs
Employee Assistance Programs
Voluntary Benefits and Employee Discounts (Ex: Accident, Identity Theft Protection)
Career Development
#LI-Remote
Visa sponsorship available for eligible applicants.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here ."
203,Data Engineer,West Corporation,Remote,N,"Job Description
For this opening we will consider candidates from the following locations: , India |


Essential Duties
Participate in business meetings to identify objectives and develop a system that will meet the company's current and future needs
Develop a comprehensive and logical test plan for systems using appropriate tools to ensure established standards are utilized
Research, design, and develop new or enhanced applications, systems, and programs that meet business or performance improvement requirements
Write design and project requirements which may include both preliminary and firm estimates of system requirements, benefits, personnel, costs, timelines, etc., for review by management
Test and evaluate system requirements and enhancements, documenting findings for management and recommending solutions
Monitor and conduct regression testing in entirety to ensure negative impacts do not result once program alterations are completed; participate in end user acceptance testing, ensuring program alterations and changes meet end user requirements
Code program based on system enhancements, modification requests, or new design specifications
Continually keep up-to-date on new products, development tools, industry trends, and methodologies for incorporation into system platforms
Serve as an escalation point for program support after implementation, which may include additional training, upgrades, or program enhancements
Minimum Qualifications
Experience performing ETL/ELT work
Ability to design table architectures to support downstream analytics/reporting use cases
Google Cloud Platform (GCP) experience preferred but other similar cloud providers acceptable. List of GCP specific components for experience:
Pub Sub
Data Flow – using Python in Apache Beam
Cloud Storage
Big Query
Cloud Function – deploying Python functions
Cloud Build – part of CI/CD
JSON object ingestion and manipulation experience
CI/CD (Terraform, Cloud Build, Github)
Languages – Python and Terraform

ABOUT US
Connecting people with each other and the right information is mission critical. Our Company develops innovative cloud-based technology to make it easier, more effective and more efficient to make the right connections. Our solutions put people in sync with each other and the right information, so they gain the insight needed to reach better decisions on the issues that matter most. We do it with a laser focus on reliability.

The Company is a leading provider of technology-driven, communication services, serving Fortune 1000 companies and other clients in a variety of industries, including telecommunications, retail, financial services, public safety, technology and healthcare. For more than 30 years, we have been leading the way in hosted and cloud-based solutions.

Our solutions connect people with each other and the information needed to gain insights for better decisions on the issues that matter most – Information to Insight.

Our Company has sales and/or operations in the United States, Canada, Europe, the Middle East, Asia Pacific, Latin and South America and is an Equal Opportunity Employer – Veterans/Disabled and Other Protected Categories. Our Company welcomes and encourages applications of individuals with disabilities. Accommodations are available on request for candidates taking part in all aspects of the selection process.


ABOUT THE TEAM
West Cloud Collaboration provides meeting, messaging and collaboration tools that allow businesses to unlock the creativity of their teams and fuel productivity as if everyone was in one room – even when team members are across the globe.

West also offers Cloud PBX systems for globally distributed corporations, Cloud Contact Centers to enable technical support and remote agents supporting businesses and Hosted MPLS Networks to increase business efficiency, execution and security.


With products and services ranging from audio / video conferencing services to meeting management tools, we also provide professional services to ensure support and training at each stage – from implementation of our tools and services throughout the customer lifecycle. West is the #1 global conferencing partner, Cisco Meetings partner and a Microsoft Gold Collaboration Partner.

Job Snapshot
Employee Type:
Full-Time
Location:
Work From Home
Job Type:
Other
Date Posted:
4/14/2023
Job ID:
212992"
204,C++ Software Engineer for Data Analysis and Visualization Software – JMP,SAS,"100 Sas Campus Dr, Cary, NC 27513",N,":
*Eastern time zone candidates preferred*

JMP, a subsidiary of SAS, is committed to empowering scientists and engineers via our world-class family of statistical software products. For over 30 years, JMP has enabled customers to speed new drugs to market, to design better products and processes, and to figure out how to restore ecosystems. Advancements are made when brilliant people use JMP statistical discovery software to see what they’ve not seen before. If you are a problem solver, a connector, and someone who enjoys helping others, then you might just be the next person to join this dynamic, growing, and global team.

What you will do:
As a developer on our Data Discovery team, you’ll enhance JMP within the areas of data cleaning, data visualization, scripting, and core internals. Enhancements can include bug fixes, unit tests, efficiency improvements, user interface improvements, major component redesigns, code refactoring for modern C++, internal libraries, and sometimes entirely new functionality. You’ll eventually be responsible for multiple areas of the code and work independently with considerable leeway to prioritize enhancements. You’ll even have the opportunity to present to customers, if you desire.

What we’re looking for:
Demonstrated C++ expertise; With well over a million lines of C++ code at all levels (from low-level messaging to high-level user interaction), we must be experts in the language, for both writing new code and updating old code.
Software engineering expertise (design, refactoring, debugging); Writing code is not enough. We must do so in the context of quality measures such as unit tests, documentation, localization, and understanding user needs.
Strong analytical and problem-solving skills; We’ve already solved all the easy problems. You’ll need to be able to find creative solutions within constraints such as backward compatibility.
Strong communication skills (wiki editing, presentations)
The Nice to Haves:
Experience with JMP or similar products
Experience with computer graphics
Experience with statistical analysis and data visualization; We have a team of PhD statisticians for the heavy statistics work, but everyone needs some statistics awareness. Much of our work is about making statistical graphs. Both low-level graphics APIs and high-level charting skills are applicable.
Math expertise (geometry, linear algebra)
Online contributions (Stack Overflow, GitHub, blogs, twitter)
Other knowledge, skills, and abilities:
Superior analytical and problem-solving skills.
Ability to work independently and as part of a team.
Excellent verbal, written communication skills.
Ability to effectively communicate with peers and management; Developers sometimes work closely with other teams, such as marketing and product management, where it’s helpful to be able to communicate effectively on all levels.
Demonstrated ability to manage time across multiple projects.

Additional Information:
JMP is an Equal Opportunity/Affirmative Action employer. All qualified applicants are considered for employment without regard to race, color, religion, gender, sex, sexual orientation, gender identity, age, national origin, disability status, protected veteran status or any other characteristic protected by law. Read more: Know Your Rights. Also view the Pay Transparency notice.

Equivalent combination of education, training and experience may be considered in place of the above qualifications. The level of this position will be determined based on the applicant's education, skills and experience. Resumes may be considered in the order they are received. JMP employees performing certain job functions may require access to technology or software subject to export or import regulations. To comply with these regulations, JMP may obtain nationality or citizenship information from applicants for employment. JMP collects this information solely for trade law compliance purposes and does not use it to discriminate unfairly in the hiring process.

To qualify, applicants must be legally authorized to work in the United States, and should not require, now or in the future, sponsorship for employment visa status.

All valid JMP job openings are located on the Careers page at www.jmp.com . JMP only sends emails from verified “jmp.com” email addresses and never asks for sensitive, personal information or money. Should you have any doubts about the authenticity of any type of communication from, for, or on behalf of JMP, please contact us at Recruitingsupport@sas.com before taking any further action."
205,Data Engineer,ASCENDING,"Rockville, MD 20850•Remote",N,"Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a true Mid level Big Data Engineer to contribute to join their team of technologists to build and contribute to large-scale, innovative projects. Technological and career growth opportunities are a natural and every day part of the working environment.
This role is only available for W2 or individual contracts. Please no C2C.
100% Remote Work.

Responsibilities:
Analyze system requirements and design responsive algorithms and solutions.
Use big data and cloud technologies to produce production quality code.
Engage in performance tuning and scalability engineering.
Work with team, peers and management to identify objectives and set priorities.
Perform related SDLC engineering activities like sprint planning and estimation.
Work effectively in small agile teams.
Provide creative solutions to problems.
Identify opportunities for improvement and execute.

Requirements:
Minimum 5 years of proven professional experience working in the IT industry.
Degree in Computer Science or related domains.
Experience with cloud based Big Data technologies.
Experience with big data technologies like Hadoop, Spark and Hive.
AWS experience is a big plus.
Proficiency in Hive / Spark SQL / SQL. Experience with Spark.
Experience with one or more programming languages like Scala & Python & Java.
Ability to push the frontier of technology and independently pursue better alternatives.
Kubernetes or AWS EKS experience will be a plus.

Thanks for applying!
U3GJMKlbkr"
206,Software/Data Engineer,"TeamWorx Security, LLC","Columbia, MD•Remote","$100,000 - $125,000 a year","Title: Software/Data Engineer
Location: Remote
Position Type: Full-Time
Salary Range: $100k-$125k
Experience: 3-6 Years
U.S. Citizenship is required. Must be willing to submit for a security clearance.

The Opportunity
TeamWorx Security is looking for a highly motivated and adaptable Software Engineer with a drive to succeed in a fast-paced environment! The Software Engineer will work closely with both our technical team and our government customers to build capabilities and solutions to assess program effectiveness across a range of processes, projects, and systems. The candidate will apply a wide range of qualitative and/or quantitative methods to recommend performance improvements by producing thorough management reports and automation tools tailored to the specific needs of each program component.

What it Takes!
We are product-focused and love innovation. We look for leadership qualities in all of our teammates. You must be comfortable exploring, learning, and building cool technologies, making mistakes, and learning along the way. You should be self-driven and self-motivated with a great attitude! You should take ownership of projects and drive them to completion. We aim to challenge and grow our employees, and in return, their contributions guide our efforts to deliver maximum value to our customers. You must be comfortable working remotely, directly with our customers, and in our team environment.

What You'll Be Doing
The perfect candidate should have detailed knowledge of JavaScript, Google Apps Script, Google Workspace, data structures, analysis, and visualization. Knowledge of web-app development and programming languages such as CSS, HTML, and Python are desired. Strong attention-to-detail and the ability to produce clear and concise documentation are also essential. The Software Engineer is expected to:

Develop and maintain web applications using Google Apps Script and JavaScript
Extract and analyze data to provide unique insights and patterns specific to the necessary task
Validate the work of others by performing confirmatory data analysis
Collaborate with other team members to design, develop, and implement data-driven solutions
Create and maintain data pipelines and database architectures
Develop and implement data quality checks and processes
Apply knowledge and experience using industry-standard web development tools and programming languages
Utilize software packages such as Oracle, MySQL, Tableau, Google Sheets, and Excel at an advanced level to manipulate and display data

What We Think You'll Need
Bachelor’s in Computer Science, Data Science, Information Systems, or a related field (or equivalent experience)
3-6 years experience as Software Engineer, Data Engineer, Business Intelligence Engineer, or similar role.
Proficiency in JavaScript, Knowledge of other programming languages such as CSS, HTML, and Python is a plus
Experience with Google Apps Script, Google Workspace, and web application development
Ability to perform exploratory and confirmatory data analysis
Knowledge of descriptive analytics, trends, business intelligence, and business analytics
Strong problem-solving and analytical skills
Excellent communication and teamwork abilities
Self-motivation, adaptability, and drive to succeed in a fast-paced environment.
US citizenship required and must willing to submit for a security clearance

What’s in it for you?

No more bullet points for starters. Also, these awesome perks: Comprehensive Health, Dental, & Vision Insurance, Short-Term Disability & Life Insurance, Matching 401(k) Contributions, Paid Time Off including 11 Holidays and your Birthday, Professional Development, Fitness Membership, and Home Office allocations, Performance Bonuses & Profit Sharing, & Employee Referral Bonuses. But wait, there's more: TeamWorx Security is committed to our teammates and their professional goals. Being a small but growing company affords us the opportunity to work WITH YOU to reach your full potential.

A Little About Us and Our Team
We love to be inspired. We started TeamWorx Security to build, create, and dream about what inspires us, and because of this, our Maryland startup is quickly growing. We are a team of data scientists, software engineers, researchers, product developers, and operation-minded folks who specialize in developing applications and products that our customers love. Our mission is to reinvent the way technical and non-technical people work with technology. We believe in creating harmonious, automated solutions designed to improve our customers’ daily workflow, allowing them to act quickly and decisively about what matters most. We are looking for critical thinkers who excel at solving problems and have a passion for what they do to help us move our mission forward. This is our story; come tell us your story.

At TeamWorx Security, Inc., we hold our values as guiding principles in all that we do and encourage our community to embrace and embody them. Our core values are putting our employees first, being curious, being authentic, being scrappy, and honoring those we serve. These values drive our company culture, decision-making, and operations.

All qualified applicants will receive consideration for employment without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, or any other basis prohibited by law. TeamWorx Security will also consider the employment of qualified applicants with criminal histories, consistent with relevant laws."
207,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
208,Data Engineer,Violet Ink,"Newark, NJ 07107•Hybrid remote",From $60 an hour,"Key Job Responsibilities
· Analyze data needs and objectives within the broader journey.
· Source, analyze and organize raw data, prepare data for transformation and consumption.
· Identify ways to improve data governance, reliability, efficiency, and quality.
· Build applications ensuring that the code follows latest coding practices and industry standards.
· Build using modern design patterns and architectural principles.
· Ensure developed solutions remain compliant with all applicable Prudential standards.
· Solve complex problems and provides new perspective on existing problems.
· Develop through collaboration and deliver application component solutions.
· Develop high quality, well documented, and efficient code supporting testing and automation.
· Support product owner in defining future stories and tech lead in defining technical designs.
Competencies – Knowledge, Skills, Abilities
Candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. Should have experience using following software/tools:
Big data tools
Relational and NoSQL databases
Data pipeline and workflow management tools
AWS cloud services
Stream processing systems
Object oriented and scripting language
Build processes supporting data transformation, data structure, metadata, dependency, and workload management.
Successful history of manipulating, processing, and extracting value from large, disconnected structured and unstructured datasets.
Advanced working SQL knowledge and experience working with relational databases.
Experience building and optimizing data pipelines, architecture, and data sets.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Strong project management and organization skills.
Experience supporting and working with agile cross functional teams in a dynamic environment
Background in financial services functions strongly desirable.
Job Type: Contract
Pay: From $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newark, NJ 07107: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
No SQL: 1 year (Required)
Work Location: Hybrid remote in Newark, NJ 07107"
209,Data Engineer - Full time,Shirley Ryan Ability Lab,"355 East Erie Street, Chicago, IL 60611",N,"General Summary
The Data Engineer is a member of the Data Solutions Team, working in a dynamic cross-functional environment to assist in developing the strategic direction and the meaningful use of data within the organization. Assists with development or selection of data models to be implemented in support of clinical, quality improvement, and research objectives. Creates pipelines for data from various sources into our EDW consistent with existing data models. Adapts database systems and architecture as necessary to meet the needs of the organization.
The Data Engineer will consistently demonstrate support of the Shirley Ryan AbilityLab statement of Vision, Mission and Core Values by striving for excellence, contributing to the team efforts and showing respect and compassion for patients and their families, fellow employees, and all others with whom there is contact at or in the interest of the institute.
The Data Engineer will demonstrate Shirley Ryan AbilityLab Core Attributes: Communication, Accountability, Flexibility/Adaptability, Judgment/Problem Solving, Customer Service and Core Values (Hope, Compassion, Discovery, Collaboration, & Commitment to Excellence) while fulfilling job duties.
Principal Responsibilities
The Data Engineer will:
Collaborate within the Data Solutions Team, other Information Services (IS) teams, executive leadership, external consultants, and stakeholders from various departments within the hospital to:
Primarily utilizing existing resources, assist in developing secure, stable, scalable long-term plans for the flow of the hospital’s data.
Assist in the development of scalable data infrastructure and platforms to collect and process large amounts of data, including structured and unstructured data for analysis by clinical, research, and business resources. Data sources may be internally or externally sourced.
Establish methods and procedures for tracking data quality, completeness, redundancy, compliance and improvement.
Create strategies and plans for data capacity planning, data security, life cycle data management, scalability, backup and archiving.
Oversee the mapping of data sources, data movement, interfaces, and analytics, with the goal of ensuring data quality.
Define standards for naming, describing, governing, managing, modeling, cleaning, enriching, transforming, moving, storing, searching, and delivering data securely within the Data Solutions Team.
Create short-term tactical solutions to achieve long-term objectives and an overall data management roadmap.
Participate in data and information model design across multiple stakeholders.
Develop or adapt a data model for representation of clinical data for our EDW.
Acquire and maintain data from primary or secondary data sources.
Identify and evaluate opportunities for enhancement of current data architecture and data management technologies.
Monitor, update, and ensure consistent uptime and availability of data systems.
Reporting Relationships
Reports directly to the Manager – Data Solutions
Knowledge, Skills & Abilities Required
Bachelor’s or master’s degree in computer science or computer engineering, or in a related area (data science, mathematics, statistics, economics, etc.) with demonstrated proficiency in the skills below.
Early career candidates are welcome to apply, such as current Data Analysts looking to advance their career into a Data Engineering role.
Fluency in SQL (5 years total experience, including school) with experience with PostgreSQL and PostgreSQL derivatives
Knowledge and understanding of HIPAA and ability to implement data security standards in line with those requirements.
Experience with Java and/or Python (at least 2 years)
Experience with enterprise technologies like Hadoop and Talend
Demonstrated experience in handling large data sets and relational databases.
Knowledge and experience in developing ETL processes.
Experience with AWS data utilities (Aurora, API gateway, S3, etc.) is a plus.
Strong communication skills, particularly those relevant to translating highly technical information for non-technical audiences.
Demonstrated ability to manage and see through long-term projects.
Proactive work habits such as proposing and developing new projects and initiatives or advocating and promoting incremental adoption of new technologies.

Working Conditions
Normal office environment with little or no exposure to dust or extreme temperature.

The above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not intended to be construed as an exhaustive list of all responsibilities, duties and skills required of personnel so classified.

Required Skills

Required Experience"
210,Senior Data Engineer,Discord,"San Francisco, CA 94107•Hybrid remote",N,"Discord is looking for experienced and passionate data engineers to join our Data team! As one of the early data engineers at Discord, you’ll have an outsized impact on our data foundations. Data Engineers at Discord collaborate with data science and engineering teams to design, build, and scale high-leverage datasets that enable analytics, modeling, and experimentation. These critical datasets directly inform how we identify opportunities, measure success, and drive decision making. If this sounds exciting to you and you’re passionate about data, impact, and working on an amazing team, read on! Check out our blog here! What you'll be doing Create and maintain data pipelines and foundational datasets to support analytics, modeling, experimentation, and product/business needs Design and build database architectures with massive and complex data, balancing ergonomic benefits with computational load and cost Collaborate closely with data science and engineering teams to improve the coverage, accuracy, and reliability of instrumentation Develop audits for data quality at scale, implementing alerting and anomaly detection as necessary Create scalable dashboards and reports to support business objectives and enable data-driven decision making Partner with data scientists, engineers, and product teams to accomplish all of the above! You'll Thrive In This Role If (not hard qualifications - if you're not sure, please apply!) 4+ years of experience building data pipelines in production with deep knowledge of performant scalable patterns 4+ years of experience in designing, developing, and maintaining robust data models from structured and unstructured sources 4+ years of experience writing accurate and effective code in SQL and Python Experience implementing and monitoring audits for data quality with massive data sets (e.g. billions of rows) Experience proactively identifying opportunities to improve ETL &amp; dashboard performance and cost Experience leveraging your excellent communication skills to thrive in ambiguous environments where problems are not well-defined and evolve quickly A desire to work with amazing, passionate people who care deeply about solving challenging problems to improve Discord. Last but not least - a collaborative attitude and a healthy dose of natural curiosity! Bonus Points Passion for Discord or online communities Experience owning and proactively improving the data models for a functional area Experience collaborating directly with data science and product engineering teams Experience with modern data storage and processing technologies (i.e. BigQuery SQL, Looker, Airflow, and DBT or similar) Experience with designing data architecture to power a variety of use cases, including experimentation The US base salary range for this full-time position is $190,000 to $204,000 + equity + benefits. Our salary ranges are determined by role and level. Within the range, individual pay is determined by additional factors, including job-related skills, experience, and relevant education or training. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include equity, or benefits. #buildbelonging #LI-JF1Benefits and Perks Comprehensive medical insurance including Health, Dental and Vision (plus up to $20,000 for gender affirmation procedures) Mental health resources and quarterly wellness stipends 16+ paid holidays, 4 weeks of PTO + use-what-you-need sick days Paid parental leave (plus fertility, adoption and other family planning benefits) Flexible long-term work options (remote and hybrid) Volunteer time off A diverse slate of Employee Resource Groups Plus commuter contributions and other perks for office-based employees About Us Discord is a voice, video and text app that helps friends and communities come together to hang out and explore their interests — from artists and activists, to study groups, sneakerheads, plant parents, and more. With 150 million monthly users across 19 million active communities, called servers, Discord has grown to become one of the most popular communications services in the world. Discord was built without selling ads or user data and instead, offers a premium subscription called Nitro that gives users special perks like higher quality streams and fun customizations. We’re working toward an inclusive world where no one feels like an outsider, where genuine human connection is a click, text chat, or voice call away. A place where everyone can find belonging. Challenging? Heck yes. Rewarding? Double heck yes. It’s a mission that gives us the chance to positively impact millions of people all over the world. So if this strikes a chord with you, come build belonging with us!"
211,Data Engineer,Edmunds.com,"Santa Monica, CA 90404•Remote","$113,000 - $169,000 a year","Edmunds offers flexibility to work fully remote, from our Edquarters, or a combination of both

At Edmunds we’re driven to make car buying easier. Ever since we began publishing printed car guides in the 60’s, the company has been in the business of trust, innovating ways to empower and support car shoppers. When Edmunds launched the car industry’s first Internet site in 1994, we established a leadership position online and have never looked back. Now, as one of the most trusted review sites on the Internet, millions of visitors use our research, shopping and buying tools every month to make an easy and informed decision on their next car. For consumers, we bring peace of mind. For dealers, we make tools to help them solve their problems and sell more cars. How do we do it, you ask? The key ingredients are our enthusiastic employees, progressive company culture and cutting-edge technology. Want to join the team? Read on to find out how!

What You’re Applying For:
The analytics group at Edmunds.com is looking for an Analytics Development/ Data Engineer who will work with Business Intelligence tools in collaboration with product owners, analysts, marketing and engineering to design and implement Analytics Solutions for Edmunds.
Edmunds.com is all about data, big data! We are fast becoming the de facto standard by which automotive information is defined in the United States. We collect detailed, user behavior for over 18 million unique visitors a month. The BI Engineer is responsible for building BI Solutions, analyzing data, maintaining the platform and answering questions in the analytics realm.
We are looking for a person that is smart, creative, ambitious and most of all passionate about data wrangling and management. Large-scale data analytics should excite you. Edmunds is very much about helping its people grow—meaningfully, effectively and prosperously. We care about what we do and we care about our people. A demonstrated passion for data and analytics will provide incredible growth opportunities and will make this the best job you ever had.

What You’ll Do:
Work with business users directly to gather requirements and provide solutions and insights
Follow Agile project management methodology
Translate functional requirements into technical designs and choose the appropriate BI solutions
Build Data Pipelines to cleanse and store data in databricks
Support infrastructure on AWS
Document functional requirements, functional designs, and all technical designs according to Edmunds technical documentation standards
Apply Minimum Viable Product (MVP) approach to test the solution before building the entire application

What You Need:
Bachelor’s degree in Engineering, Mathematics, Statistics
Minimum 5 Years of Professional development experience
Minimum 2 years experience in deploying & troubleshooting AWS infrastructure. AWS Certification Is preferred
Working Knowledge in Scripting Languages like Linux (Shell Script), Python
Proficient data analysis experience utilizing SQL
Working knowledge in any one of the RDBMS such as Postgres, Redshift
Experience in Business Intelligence tools such Tableau, Microstrategy or equivalent
Experience in Big Data analysis using SparkSQL HDFS, Hive and Scala
Working knowledge in any two of the programming languages Java, C#, or Perl
Working knowledge of statistical analysis using tools such as R
Experience in interacting with cross functional teams
Advanced analytical thinking and problem solving skills with deep technical knowledge of data infrastructure practices and tools
Ability to create and present status reports to senior executives
Experience in configuring and using DevOps tools JIRA, Stash, Confluence

The compensation range for this position is $113,000 - $169,000 per year. The base pay will take into account internal equity as well as job-related knowledge, skills, and experience among other factors. In addition, Edmunds offers full-time employees a comprehensive total rewards package including the benefits listed below.
Edmunds Perks:
Flexible time off
13 Paid Holidays
Comprehensive Health Benefits (medical, dental, vision, life and disability)
Flexible Spending Accounts (Employees) and Health Savings Accounts (Employee and Employer Contributions)
401K Plan with company matching at 100%, up to 6% of eligible salary with immediate vesting
Stock purchase program
CarMax vehicle discount
Up to 4 months Paid Parental Leave
HeartCash matches employee donations to the causes that are important to them
2 Days of Paid Time Off for time to dedicate to social impact causes
FitCash covers a portion of gym or fitness activity fees
Well being sessions and events such as yoga, meditation and walking challenges
On-going career development sessions and an annual learning event
Pet insurance
Sabbatical leave
Education Reimbursement
Pre-tax spending accounts for qualified transportation expenses
Plus a coffee bar, frozen yogurt and more!

Working @ Edmunds.com:
Employees think it’s a pretty great place to work and some pretty impressive publications think it is too: we have been recognized as one of the best places to work by the Fortune Magazine and Great Places to Work, LA Business Journal (for the last 6 years!), Computerworld, Built in LA and Inc. Magazine. We've also been identified as one of the best workplaces specifically in Technology and also for Diversity and Asian Americans. If you’re interested in learning more and joining our mission, we’d love to hear from you!

Edmunds will consider for employment qualified candidates with criminal histories in a manner consistent with the requirements of all applicable laws.
#BI-Remote

This is a remote position."
212,Sr Software Engineer - Data Integrations,N,Remote,"$130,000 - $155,000 a year","Senior Software Engineer - Data Integrations
Pluto Health is a fast-moving and quickly growing healthcare technology company. Pluto is receiving significant attention and interest across the healthcare industry as a technology solution for multiple stakeholder segments. We are seeking a Senior Software Engineer - Data Integrations for one of our data integration team. This is an opportunity to start on the ground floor with one of the fastest-growing healthtech companies and to help shape its long-term success. You will be joining a people-focused organization, and will be a key member of our Engineering team.
About the Role
The Senior Software Engineer - Data Integrations is primarily responsible for healthcare data integrations involving electronic medical records. This role partners closely with the Head of Interoperability to ensure we have strong, scalable data integration pipelines that exceed security, compliance and interoperability best practices. The person in this role is an expert on Healthcare Interoperability standards and is well versed in working with the FHIR data resources and FHIR interoperability platforms.
Primary Responsibilities:
Designing, developing and deploying complex data interoperability solutions using Healthcare industry-standard data formats/specifications (CDA, FHIR, HL7, etc.)
Integrate with well known health data sharing networks
Build scalable, reliable, high quality, and well-architected code in a timely manner
Contribute to the ongoing evolution of the existing code base and services.
Design, develop, modify, implement, and support existing and new software components.
Determine root cause for the most complex software issues and develop practical, efficient, and permanent technical solutions.
Assist in task planning and estimation for new features.
Work in close partnership with cross-functional teams and management



Qualifications:
Bachelor's Degree in Computer Science, Computer Engineering or related equivalent
3+ years experience with one or more programming languages (Typescript, Javascript, C#, Python, GO)
2+ years experience working directly with FHIR data formats including exponent with FHIR interoperability platforms
Expert in working with CCD, C-CDA documents, and FHIR R4
Experienced with object-oriented design and programming concepts
Excellent interpersonal skills
We are open to full-time, contract, or contract-to-hire for this position.
What You Get:
Join one of the fastest-growing, venture-backed health tech companies
Work alongside a world-class team
Shape how leading health systems and plans think about their patient outreach and engagement strategies
Be part of the solution: improve the way care is delivered for millions of patients
Gain deep expertise about healthcare transformation by building technology with the country's most innovative health systems and payers
Competitive salary and equity compensation with benefits including health, dental, and vision coverage, flexible work hours, paid maternity/paternity leave, bi-annual retreats, personal Macbook, and a matched 401(k) plan
About Pluto Health, Inc.
Our Why: We believe that every person has a chance to thrive.
About Us: Pluto Health strives to coordinate care and increase access to needed resources for people by leveraging quick access to data and insights in minutes, and then bringing together care coordination activities. Our team brings together siloed medical data, evaluates it, and works to address gaps in care for patients. Pluto has access to most major EMRs, labs and health centers (about 90% of the U.S. market), alongside multiple social determinants of health data sets, and insurance information. What is unique about Pluto is that it applies automated processes on behalf of patients so recall is minimized, no need to remember names of doctors, health centers, or recall portal log-ins to get needed information, then, our team evaluates health opportunities and works to address health gaps. We empower to optimize care and access.
Operating Ethos: We are one of the few companies linking siloed data across health sectors and bridging care. We are committed to patient privacy, promising to never monetize and share patient data without not only their consent, but also transparency with whom that data is shared. Our job is to connect people to the resources they need, not monetize data"
213,AWS Data Engineer,Oxfaa Pvt.Ltd,"Trenton, NJ",$75 - $80 an hour,"Required Skill:
· Prior experience with writing and debugging python
· Prior experience with building data pipelines.
· Prior experience Data lakes in an AWS environment
· Prior experience with Data warehouse technologies in an AWS environment
· Prior experience with AWS EMR
· Prior experience with pyspark
Job Type: Contract
Salary: $75.00 - $80.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
AWS: 6 years (Required)
Work Location: On the road"
214,Data Engineer,West Corporation,Remote,N,"Job Description
For this opening we will consider candidates from the following locations: , India |


Essential Duties
Participate in business meetings to identify objectives and develop a system that will meet the company's current and future needs
Develop a comprehensive and logical test plan for systems using appropriate tools to ensure established standards are utilized
Research, design, and develop new or enhanced applications, systems, and programs that meet business or performance improvement requirements
Write design and project requirements which may include both preliminary and firm estimates of system requirements, benefits, personnel, costs, timelines, etc., for review by management
Test and evaluate system requirements and enhancements, documenting findings for management and recommending solutions
Monitor and conduct regression testing in entirety to ensure negative impacts do not result once program alterations are completed; participate in end user acceptance testing, ensuring program alterations and changes meet end user requirements
Code program based on system enhancements, modification requests, or new design specifications
Continually keep up-to-date on new products, development tools, industry trends, and methodologies for incorporation into system platforms
Serve as an escalation point for program support after implementation, which may include additional training, upgrades, or program enhancements
Minimum Qualifications
Experience performing ETL/ELT work
Ability to design table architectures to support downstream analytics/reporting use cases
Google Cloud Platform (GCP) experience preferred but other similar cloud providers acceptable. List of GCP specific components for experience:
Pub Sub
Data Flow – using Python in Apache Beam
Cloud Storage
Big Query
Cloud Function – deploying Python functions
Cloud Build – part of CI/CD
JSON object ingestion and manipulation experience
CI/CD (Terraform, Cloud Build, Github)
Languages – Python and Terraform

ABOUT US
Connecting people with each other and the right information is mission critical. Our Company develops innovative cloud-based technology to make it easier, more effective and more efficient to make the right connections. Our solutions put people in sync with each other and the right information, so they gain the insight needed to reach better decisions on the issues that matter most. We do it with a laser focus on reliability.

The Company is a leading provider of technology-driven, communication services, serving Fortune 1000 companies and other clients in a variety of industries, including telecommunications, retail, financial services, public safety, technology and healthcare. For more than 30 years, we have been leading the way in hosted and cloud-based solutions.

Our solutions connect people with each other and the information needed to gain insights for better decisions on the issues that matter most – Information to Insight.

Our Company has sales and/or operations in the United States, Canada, Europe, the Middle East, Asia Pacific, Latin and South America and is an Equal Opportunity Employer – Veterans/Disabled and Other Protected Categories. Our Company welcomes and encourages applications of individuals with disabilities. Accommodations are available on request for candidates taking part in all aspects of the selection process.


ABOUT THE TEAM
West Cloud Collaboration provides meeting, messaging and collaboration tools that allow businesses to unlock the creativity of their teams and fuel productivity as if everyone was in one room – even when team members are across the globe.

West also offers Cloud PBX systems for globally distributed corporations, Cloud Contact Centers to enable technical support and remote agents supporting businesses and Hosted MPLS Networks to increase business efficiency, execution and security.


With products and services ranging from audio / video conferencing services to meeting management tools, we also provide professional services to ensure support and training at each stage – from implementation of our tools and services throughout the customer lifecycle. West is the #1 global conferencing partner, Cisco Meetings partner and a Microsoft Gold Collaboration Partner.

Job Snapshot
Employee Type:
Full-Time
Location:
Work From Home
Job Type:
Other
Date Posted:
4/14/2023
Job ID:
212992"
215,Data Engineer,Solera Health,"Chicago, IL•Remote",N,"About Solera
Solera Health is committed to changing lives by guiding people seamlessly to better health solutions, while providing payers and employers the tools to providers and outcomes across conditions. Solera's platform provides a marketplace of curated networks of digital and community point solutions focused on intensive, evidence-based lifestyle, behavioral, and social interventions to impact the most prevalent and costly chronic conditions. Solera strategically matches consumers to their best-fit solution and helps keep them engaged for successful health outcomes.
About the Position
As a member of the Solera Data Team, the Data Engineer is responsible for contributing to the analysis, implementation, and maintenance of Solera's enterprise data assets. The Data Engineer will support the performance of the enterprise data ecosystem, working within the Solera Data team and with internal stakeholders to ensure data exchanges are adhering to established quality standards and operational specifications. This person should be able to apply hands-on support to monitoring and maintaining data intake, processing, and management capabilities for the organization in concert with engineering, data architecture, reporting, finance, and business teams.
In addition, this role will be expected to partner with the Business Insights and Analytics team in defining operational reporting around the enterprise's data management ecosystem and leverage these in supporting an enterprise data operations and governance capability. This role will also contribute to the long-term data strategy and technical data architecture development.
Who You Are
You are intellectually curious and always up for a good challenge. You are flexible and adaptable to change. You work well independently, and with a team, and are comfortable working in a remote environment. You are a self-starter who is excited to provide new ideas and opinions for sustainable solutions. You can consume and transform requirements into efficient and effective solutions that meet the highest quality and performance standards. You believe that unit and integration tests are an integral part of the development process. You are a problem solver.
What You'll Do
Build and deliver high quality data architecture and pipelines to support business analysts, data scientists, and customer reporting needs
Support day-to-day monitoring of operational performance and initiating issue resolution processes
Support and contribute to data processing improvement initiatives including the documentation of current state, development of end state solutions, and designs for management of solution in sustainment
Analyze, implement, and maintain ETL/ELT processes to enrich and govern enterprise data assets
Identify ways to improve the reliability, efficiency, observability and quality of data operations
Support the gathering and documentation of business requirements for data governance and/or data quality enhancement initiatives
Work with technical teams to support data acquisition from a wide variety of sources
Ongoing data analysis to support timely and accurate delivery of custom analysis to stakeholders regarding our data assets and the quality thereof
What You'll Need
Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering) or relevant experience
3+ years of experience as a Data Engineer
3+ years of hands-on experience in writing complex, highly optimized SQL queries across large data sets
3+ years of experience in application development with Python, SQL, Scala, Java, or JavaScript
3+ years of experience with SQL Server, Google BigQuery, NoSQL, Snowflake, etc
2+ years of experience with a public cloud (AWS, Microsoft Azure, or Google Cloud)
2+ years of experience with Agile engineering practices
1+ year(s) of experience working with healthcare and regulatory data
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
FHIR experience a plus
Demonstrated strength in data modeling and data warehousing a plus
Experience with health plan systems, benefits, and eligibility a plus
Experience with Master Data Management, Data Governance and Data Quality tools is a plus
Experience with Matillion ETL a plus
Where you are located
This is a remote position within the North America time zone(s) and will require travel to other corporate locations as needed.
Benefits
Remote first culture
Competitive salary
Unlimited paid time off
Paid pregnancy and parental leave
Adoption assistance program
Medical, dental and vision insurance
13 company holidays
401k + company match
Free FitBit, Sworkit and Headspace (meditation & mindfulness app) account
Casual and fun atmosphere
Disclaimer
The information contained herein is not intended to be an all-inclusive list of the duties and responsibilities of the job, nor are they intended to be an all-inclusive list of the skills and abilities required to do the job. Management may, at its discretion, assign or reassign duties and responsibilities to this job at any time.
EEO
Solera Health provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Solera Health complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training."
216,Data Engineer,Wawa,"Media, PA 19063•Hybrid remote",N,"Data Engineer
Wawa Corporate - Media, PA
Job Summary:
The Data Engineer role designs and develops scalable data solutions using data integration tools and technologies. The individual utilizes big data computation, data platforms and storage tools to create prototype and data products. Conduct build and testing of data pipelines and solutions. Additionally, Data Engineer integrates, tests data pipelines with Advance Analytics and AI platforms. Must be proficient with multiple data engineering and integration tools such as Scala, Python, Spark, Snowflake etc. in an AWS environment.
Principal Duties:
Responsible for designing and implementing solutions for loading both structured and semi-structured data design into multiple target data systems.
Design, develop, optimize, and maintain data pipelines and processes that adhere to data integration principles and business goals.
Solve complex data problems to deliver insights that helps our business to achieve their goals.
Code, test, and document new or modified data systems to create robust and scalable applications for data analytics.
Ensure that data pipelines are scalable, repeatable, and secure, and can serve multiple users within the company.
Design and implement data ingestion techniques for real time and batch processes for structured and semi-structured data sources into Wawa’s data lake and data warehouse platforms.
Understand complex business requirements and propose end to end and simplified enterprise information architecture solutions.
Develop and implement data design methods, data structures, and modeling standards which work with multiple business intelligence tools.
Work closely with Analytics team and implement their self-service and analytics requirements.
Work with Data Science practitioners and developers to make sure that all data solutions are
Collaborate with Analytics team to build solutions that enable business analytics. Develop quality scalable, tested, and reliable data services using industry best practices.
Manage all activities centered on obtaining data and loading into a data lake environment.
Assess the suitability and quality of candidate data sets for the Data Lake.
Balance business requirements with technical feasibility and set expectations on new projects. Recommend changes in development, maintenance and system standards.
Design and build integration components and interfaces in collaboration with Architects and Infrastructure Engineers as necessary. Perform unit, component, integration testing of software components including the design, implementation, evaluation and execution of unit and assembly test scripts.
Determine if the data received from the upstream systems are of good quality based on the rules and data quality validations defined and in case of any issues with the data quality analyze and come up with a preliminary summary of the root cause/issue.
Assist the Analytics team by leveraging Wawa’s Enterprise Data Platform ecosystem to design, and develop capabilities to deliver our solutions using Spark, Scala, Python and
Follow security standards for all data and tools that are being introduced in the team.
Basic Qualifications:
Bachelor’s degree in Computer Science/Engineering preferred
3-5+ years database, data integration experience
3+ years’ experience with Spark, Scala/Python, SQL and Big Data solutions
Preferred experience with Databricks and Snowflake
3+ years’ experience in designing and implementing the data architecture (conceptual, logical, physical & dimensional models).
Developing Enterprise Business Intelligence solutions on one or more of the following
EDW platforms: Snowflake, Redshift, Google Big Query
Experience implementing Big Data solutions using open source technologies
Strong knowledge of key scripting and programming languages such as Python, Java, Scala, etc
Experience with data integration tools such as Talend would be helpful
Experience designing and implementing various data pipeline patterns and strategies
Hands-on experience with dimensional modeling techniques and creation of logical and physical data models (entity relationship modeling, exposure to data warehouse design)
Strong knowledge of data security principles
Proven track record working with complex, interrelated systems and bringing that data together on Big Data platforms.
Wawa will provide reasonable accommodation to complete an application upon request, consistent with applicable law. If you require an accommodation, please contact our Associate Service Center.
Wawa, Inc. is an equal opportunity employer. Wawa maintains a work environment in which Associates are treated fairly and with respect and in which discrimination of any kind will not be tolerated. In accordance with federal, state and local laws, we recruit, hire, promote and evaluate all applicants and Associates without regard to race, color, religion, sex, age, national origin, ancestry, familial status, marital status, sexual orientation or preference, gender identity or expression, citizenship status, disability, veteran or military status, genetic information, domestic or sexual violence victim status or any other characteristic protected by applicable law. Unlawful discrimination will not be a factor in any employment decision.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Employee stock ownership plan
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
7 years
8 years
9 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Media, PA 19063: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What Big Data Tools/Technologies/Querying Languages do you have experience with (RedShift, Snowflake, Python, SQL, Google BigQuery, Spark, Scala, etc.)?
Experience:
Data Engineering: 7 years (Required)
Leadership: 3 years (Required)
Work Location: Hybrid remote in Media, PA 19063

Health insurance"
217,Data Engineer,Norwin,"1 Dna Way, South San Francisco, CA 94080",Up to $90 an hour,"Title: Data Engineer or Data Scientist
Location: South San Francisco, CA 94080(Candidate need to be local and be onsite 3 days a week)
Duration: 6+months(W2 Contract)
Client: Genentech
Description
The systems specialist role will be part of the Workplace Digital Insights team which supports making key data-driven decisions for the Real Estate and Workplace Effectiveness(RE&WE) organization. We help this organization’s forward planning strategies for South San Francisco ranging from developing real estate strategies, to campus planning, building investments, occupancy planning and move management.
Our team designs, builds, integrates and operationalizes technology applications that are customized to meet those organizational business needs. We are responsible for the end to end lifecycle of key applications and data streams that feed our Analytics team and Business Operations teams with high quality, well defined data. These data and data streams are critical to those applications and reporting platforms. Important business data originates from real time sensors, Wi-Fi, other databases, and people, then pipelines into storage, validation and then it is combined with other data in cloud storage, processed by software applications, and ends in visualization and decision support.
This specialized role will have to use a wide range of skills to understand business requests, identify the problem and break it into their business process, functional and system components. A clear and critical thinker is required to then bring new potential solutions to fruition, manage and analyze existing pipelines and data.
This role presents a unique opportunity to work with systems developed and maintained by the group as well as within corporate IT environments, multiple vendor systems, our AWS data lake, and all the pipelines in between. This includes working with data from various Internet of Things (IOT) networks and a great opportunity to collaborate and develop new streams and strategies on our horizon. The position must be filled with someone who has sharp critical thinking, combined with excellent project management and coordination skills, having a strong ability to work with both people representing the business people and those supporting roles within the IT organization. Technical skills are also extremely important, especially skills that support understanding SQL queries, database concepts, API’s, AWS/Cloud services, and developing Tableau as well. You’ll be responsible for understanding how real time data from sensors integrates with corporate systems, combined to cloud based data lake, funnels into data science analytics and will end in visualizations.
Responsibilities
● Understands and writes SQL code in Oracle and Redshift databases to support existing data structures, joins, views. Write and optimize new SQL to support new storage needs, reporting needs, and better performance and efficiency
● Makes decisions and solves problems using sound, inclusive reasoning and judgment. Gathers and analyzes information to fully understand a problem and proactively anticipates needs and prioritize action steps.
● Partners and communicates well with internal stakeholders, business partners, and Global IT to understand, define and help deliver technology needs
● Implements vetted strategies developed by the business to achieve technical roadmaps
● Manages project delivery end-to-end, in other cases helping our IT partner stay on project delivery milestones
● Wrangling structured, unstructured and poorly structured data into appropriate data structures.
● Auditing data in databases and/or reports with a goal of improving data quality from systems or data owners
● Identify opportunities to further build out our IoT strategy
● Develops new and supports existing Tableau dashboard platform, including how to improve sql queries and data management for optimal performance in reporting
Experience/Skills/Abilities:
● At least 3 years of experience working as a Systems Integrator, Data Engineer, Software Engineer or similar position demonstrating the ability to design and implement automation, data modeling, data wrangling, data analysis, and data vision solutions to complex problems, processes, and scenarios. Familiarity with common data structures and languages.
● BS Computer Science, Computer/Electrical Engineering, or Math Degree or relevant experience.
● Experience with IoT, cloud computing, distributed data systems
● Strong capabilities in SQL and Tableau, Excel/Google Suite
● Understands AWS platform (Amazon Redshift, S3, EC2, Glue jobs, etc.)
● Friendly and approachable, with strong communication and presentation skills
● Desire to keep current with a challenging and evolving environment
● Team focused and self-motivated. Able to work as part of a coordinated team, yet independently when necessary
● Proven abilities to take initiative and to be innovative; have an analytical mind with a problem-solving aptitude
● Demonstrated ability in leading technology projects
● Demonstrated experience in bridging business requirements and technical development
● Strong communications skills maintaining ties to product developers and stakeholders
● Demonstrated experience with software lifecycle management
Desirable Experience (but not required):
● Experience working with statistical teams and/or data scientists
● Tools/Programming Experience: Python, R, web services, other languages(e.g. Java, C++, scripting languages)
● AWS Certifications and working experience
● Experience with LEAN/KANBAN/SCRUM development methodologies is desirable
Job Type: Contract
Salary: Up to $90.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
South San Francisco, CA 94080: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 4 years (Required)
SQL: 4 years (Required)
Tableau: 2 years (Required)
AWS: 2 years (Preferred)
Work Location: One location"
218,Lead Data Platform Engineer II - Java / J2EE / Python,CVS Health,"Alpharetta, GA•Remote","$115,000 - $230,000 a year","CVS Health Digital is looking for a dynamic and driven Senior Digital Engineer to lead engineering teams tasked with creating a best in class Customer Data Platform. In this role, you will be leading the implementation of the Adobe Customer Data Platform (CDP) working with a cross functional team of product managers, architects, and business partners to enable the foundational component of our personalization and orchestration engine.

The platform focused on helping people live healthier lives, will enable our marketing partners to serve the right content to the right customers in the right channel. You will use your engineering skills to deliver robust, scalable, and secure solutions on the Adobe Experience Platform. You should have the ability to prioritize well, communicate clearly, have a consistent track record of delivery and excellent software engineering and people management skills. A successful candidate will be a highly motivated, collaborative individual; motivated to achieve results in a fast-paced environment.

In this role, you work with the product owner and architects to define and implement features and functionality based on the product roadmap.

In this role you will be:

Leading the implementation of the Adobe Experience Platform
Providing leadership for Digital initiatives/digital transformation for ecommerce leveraging Microservices, open-source frameworks, Cloud technologies
Collaborating with various areas of the organization including business partners, product owners, architecture, security mavens, digital and IT engineering teams
Promoting proper implementation of SAFe process techniques
Supporting all phases of SDLC and ensure to deliver high-quality products
Developing recommendations for continuous improvement and coordinating multiple resources to solve complex problems
Encouraging a collaborative, open and dynamic team culture where individuals and teams can realize their full potential

Pay Range
The typical pay range for this role is:
Minimum: 115,000
Maximum: 230,000

Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will take into account a wide range of factors, including location.

Required Qualifications
6+ years development experience on enterprise class applications
6+ years of experience building data pipelines and extracting, transforming, and loading marketing and customer data
4+ years of experience with programming languages/frameworks: Java, J2EE, python
4+ years of experience with CI/CD tools like Jenkins, Ansible, Git, GitHub, Bitbucket, Jira, Confluence
2+ years of experience with cloud-based services like GCP or AWS
2+ years of experience in a lead engineer role
2+ years of experience in partnering with architecture, product and program management teams to influence product development assisting or improving products

Preferred Qualifications
Adobe Experience Platform especially CDP or similar platforms
Data warehouse/cloud solutions like Amazon Redshift, Google BigQuery, Snowflake, or similar
Database testing using SQL and UNIX, including: designing and manipulating test data, validating stored procedures, jobs, triggers and replication
Providing self-service to engineers through Infrastructure as Code (IaC) and cloud automation principles.
IT Security best practices and other compliance standards such as ADA, HIPAA, PCI DSS
SAFe Agile work environments; certification of SAFe for leaders would be a plus
Excellent interpersonal and communication skills; thrives working in a highly iterative, agile, and open team environment

Education
Bachelors degree or equivalent years of experience

Business Overview
Bring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities."
219,Senior Data Engineer,Stellar Inc.,"8000 Dominion Pkwy, Plano, TX 75024",$75 an hour,"***This is a hybrid job at below locations : Plano TX, Richmond VA, Mclean VA, SF CA, NY NY***
MUST HAVE SKILLS (Most Important): 9+ years of Experience in designing, building, and deploying enterprise-level large data streaming event based application using Spark, Kafka, Snowflake, Java, HBase 7+ years of programming experience in java Strong experience with AWS and scaled agile based product development methodology is preferred 3+ years of proven experience in collaborating with data architects, data scientists and enterprise platform team in building, deploying and managing models in production Knowledge on Jenkins, Jira, Git etc.
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application
Job Type: Contract
Pay: $75.00 per hour
Schedule:
8 hour shift
Work Location: In person"
220,Data Engineer,"Second Wave Delivery Systems, LLC",Remote,N,"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build and maintain scalable, high-performance data processing systems (batch and/or streaming)
Lead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable
Design our data models for optimal storage and retrieval and to meet critical product and business requirements
Contribute to tooling & standards to improve the productivity and quality of output across the company
Work and consult with various non-technical stakeholders
Participate in interviewing and onboarding of new team members

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
5+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks
Expert knowledge of relational databases and query authoring (SQL).
Strong experience developing in one of the following - Python, Java, Scala
Experience with building and managing data pipelines
Experience with big data technology, e.g: Airflow, Spark, Google Cloud Composer

Preferred Qualifications
Demonstrated experience developing machine learning algorithms in the healthcare field
Experience extracting and correlating medical findings from unstructured data such as PDF charts
Experience with big data analysis tools such as BigQuery and Looker"
221,Data Engineer - Remote,ITEOM,Remote,"$130,000 - $160,000 a year","Our client is seeking a Data Engineer who would be responsible for design of data warehouse schemas as well as end-to-end design and implementation of fault tolerant and scalable data processing pipelines using a variety of technologies for orchestrating data movement, primarily Snowflake , DBT and Airflow . You'll work with a team of data engineers to design and implement solutions that provide business-critical insights.
Salary : Mid-Level - 130K to160K+ bonuses and equity options
Location: REMOTE WORK 100% is an option: (locations limited to CO, AZ, TX, VA, CA, GA, HI, MN, MI, NJ, NY, NC, OR, PA, WA)
How You’ll Do It
Working collaboratively with a team of other data engineers and developers of varying experience levels and areas of expertise, you'll be responsible for:
Responsibilities
Design and implement data warehouse schemas to store large data sets that can be easily and efficiently queried and joined together for reporting and analytical purposes.
Design and develop ETL/ELT pipelines to efficiently move and aggregate data, while optimizing for maximum code and data reuse.
Collaborate with other team members on improvements to existing systems
Investigate data anomalies and provide quick resolutions.
Provide technical support to business users and analysts.
What We’re Looking For
TOP SKILLS: ETL, Data Warehousing, SQL, Jinga Scripting, Python and Snowflake.
3+ years of working experience with large scale data warehouse systems.
Very strong knowledge of SQL and data manipulation best practices
Experience in building efficient and fault tolerant ELT/ETL data pipelines
Very strong knowledge of working with large scale datasets and data modeling and data warehouse design
Experience with Snowflake is preferable
Knowledge of DBT, Jinja scripting and Airflow is a big plus
Nice to Have
Experience working cross functionally with product and engineering teams
Desire to wear many hats and work your tail off for a bit - great earning potential and equity available.
Benefits: A diverse benefit package, sign on bonus, options, etc. is offered through our client.
About ITEOM: ITEOM Digital Technology Talent Partners collaborates with top technologists and top employers to align purpose and passion and re-incorporate belonging to make the job search and work experience more human. We are an equal opportunity employer; however, this role is not eligible for sponsorship at this time. Employees eligible for full time W2 employment are encouraged to apply."
222,Principal Engineer I - Data (REMOTE),GEICO,"Chevy Chase, MD 20815•Remote","$100,000 - $204,500 a year","The Data Engineering – Data Management Services department is seeking highly motivated Principle Engineer I to help lead and grow GEICO’s data lineage and discovery capabilities. The mission is to build a program where data lineage and discovery is embedded in all aspects of the data lifecycle, in the least intrusive way possible.

Qualified candidate must have a passion for development, mentoring and leading teams. Qualified candidates must have the ability to help develop roadmap, plan, prioritize, execute, and manage complex projects and initiatives. He or she must also be able to coach, train and mentor a team of talented team of associates to ensure associate growth and program development. The role demands project management skills, great interpersonal skills, and strong oral and written communication skills. A successful candidate must also be able to effectively communicate technical and non-technical concepts across all levels of the organization.

As a Principal Engineer I, you will provide technical leadership for a team of junior engineers, developers, and analysts responsible for the build out, enhancement and maintenance of our data lineage and data discovery dashboard. You will also partner and work closely with the team (Enterprise Data Governance team, Cybersecurity, Data Engineering, Data Governance and Data Architecture) to ensure that the platform support the needs of our data governance initiatives.

Well-qualified candidates must possess excellent interpersonal, verbal, and written communication skills and demonstrate ability to coordinate efforts across functional areas.

Responsibilities include:
Demonstrate strategic thinking and strong planning skills to establish long term roadmap and business plan
Strong relationship building to establish and foster great relationships with Tech teams and business partners
Able to work independently while functioning as part of a cohesive cross-functional management team
Work with stakeholders to establish and meet data quality requirements, SLAs and SLOs for data ingestion
Mentors and shares experiences with other junior full stack engineers to ensure team members are keeping pace with changes in the technology landscape
Proficient in micro-services-orientedvarchitecture and extensible REST APIs
Troubleshoot and evaluate root cause of systems errors and faults by analyzing symptoms, determining root cause of failure, researching solutions to resolve failure along with planning, testing and deployment of resolution.
Research best practices and offer recommendations for proactively improving the reliability, scalability and performance of data management systems and processes.
Development and automate system and processes to support data quality automation for logical application components to serve specific requirements within the GEICO’s Data Quality portfolio.
Certified or commitment to be Azure cloud certified within 6 months of joining the team

Required Qualifications:
6+ years of professional software development experience. Candidate should have fluency and specialization with at least two modern languages such as Java, C++, Python or C# including object-oriented design
Proficient in micro-services oriented architecture and extensible REST APIs
3+ years of experience with architecture and design (architecture, design patterns, reliability, and scaling) of new and current systems
3+ years of experience with AWS, GCP, Azure, or another cloud service.
Experienced in DevOps Concepts. Familiarity with Azure DevOps Operational Framework and Azure Networking is preferred
4+ years of experience in open source frameworks. Candidate should have 2+ years of experience specifically with React (Clean Architecture), Angular & GIT
Understanding of infrastructure concepts related to Hosting, Networks, IP Address Management, Firewalls, Certificates, Load Balancing and Reverse Proxy.
Experienced with PowerShell scripting
Experienced with Monitoring tools such as Splunk, Thousand Eyes, Dynatrace or Application Insights
Understanding of CI/CD tooling
Security Protocols & Products: Understanding of Active Directory, Windows Authentication, SAML, OAuth
Experience working in Agile environment (Scrum/Kanban/SAFe)
1+ years of people management experience

Desired Qualifications:
Object oriented programming skills and awareness SOLID design patterns a strong plus.
Understanding of microservices a strong plus
Understanding or expertise in Containers a strong plus.
Knowledge of Unix/Linux and the Java platform a plus
Benefits:
At GEICO, we make sure you have the support and resources to leverage and develop your skills, secure your financial future, and take care of your health and well-being. GEICO continually seeks to provide a workplace where everyone can be their authentic self. To help achieve this goal, we support associate-led Employee Resource Groups that foster a true sense of community. Through GEICO’s competitive benefits offerings and various training and development opportunities, we have you covered with our Total Rewards Program* that includes:
Premier Medical, Dental and Vision Insurance with no waiting period**
Paid Vacation, Sick and Parental Leave
401(k) Plan
Tuition Reimbursement
Paid Training and Licensures
Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
**Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
GEICO is proud to be an equal opportunity employer. We are committed to cultivating an environment where equal employment opportunities are available to all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO celebrates diversity and believes it is critical to our success. As such, we are committed to recruit, develop and retain the most talented individuals to join our team.
#LI-SS3
Annual Salary
$100,000.00 - $204,500.00
The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations."
223,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
224,AVP Data Engineer Analyst,Citi,"480 Washington Blvd, Jersey City, NJ 07310","$117,200 - $175,800 a year","Data Engineering Analyst
Job Description Summary: The position is within ICG Technology, and we are seeking a Data Engineer with strong hands-on experience in Data Engineering, Scripting and Modeling. Candidate must have experience in developing large scale data solutions that fits multiple lines of business. The work will be developing strategic state-of-the-art solutions using agile methodologies, micro services, and integration with many application/services.
Introduction
Citi's Institutional Clients Group (ICG, visit us at icg.citi.com) comprised of diverse, talented professionals located in more than 100 countries, jurisdictions and territories globally. ICG Operations and Technology develops innovative solutions and provide exceptional service to our clients in full partnership with our product teams. ICG O&T features a diverse, inclusive team of professionals in approximately 90 countries around the globe. Institutional operations group is responsible for the management and execution of transactions for Markets, Credit Risk, Security Services, Information Services, Private Bank, Treasury and Trade Solutions and Operation Controls and reporting. The technology arm of ICG engaged in the application development and support for various lines of businesses. We are focused on building a cross-functional team of talented individuals, creating a unique platform to deliver products that will pioneer the industry through technical innovation and creativity. Our mission as a business focused technology organization is to provide best-in-class products and services to our global clients.
Why Citi?
Over 200 years in business with a great team of creative, visionary and entrepreneurial thinkers.
Tremendous opportunities to advance; open to everyone across all levels. Learn and grow simultaneously.
We offer a unique journey of personal and professional development, accomplishment and satisfaction.
We value intelligence and a global mindset. There is a strong culture of empowerment here – we embrace diversity.
We do our best to be flexible and may differ depending on the business or area.
Citi India supports would be and returning mothers with various childcare benefits.
Citi offers a robust training program. We continually reevaluate our training model to ensure that we are always offering the most relevant and best in class training.
At Citi, we value internal mobility. We want you to build a long-term career with us, so you can expect opportunities to move around the organization.
Description:
Citi Institutional Clients Group Operations and Technology (ICG O&T) develops innovative solutions and provide exceptional service to our clients in full partnership with our product teams. Institutional operations group is responsible for the management and execution of transactions for Markets, Credit Risk, Security Services, Information Services, Private Bank, Treasury and Trade Solutions and Operation Controls and reporting. The technology arm of ICG engaged in the application development and support for various lines of businesses.
Responsibilities:
Identify data and information needs for wholesale lending business of ICG
Deliver solutions to automate data model creation and transformations and partner with model engineering team to deliver Data Driven solutions
Automate the build model generation Software Development Lifecycle to merge, build and release models as code
Partner with Data Architects to produce JSON, AVRO and Java versions of Models produced by Data Architects using Magic Draw
Define processes and rationalize target state architecture, socialize with key stakeholders and conduct walkthrough
Merge data model changes from different workstreams and generate model change report
Automate data model deployment and manage version control in bit bucket or central model repository
Upload the data model to data catalog services
Develop scripts to reduce manual work and automate the model delivery process
Participate in requirement engineering and working groups and collaborate with business leads, operations, SMEs, technologists and governance teams
Collaborate with Enterprise, Sector and Federated architecture team and adopt recommendations
Incorporate data standards and implement governance model
Develop data flows, ownership matrixes and data lineage
Qualifications:
MUST Have:
3-5 years of demonstratable hands-on experience in Python, and Java Coding
Test driven development by setting up automated testing
Continuous Integration and Continuous Deployment Practice
Experience in modelling and transforming data to meet best-practice data standards for Operational Data Stores, Big Data platforms, and Reporting & Visualization
Analytical and problem solver, excellent verbal, written and presentation skills
Excellent influencing, meeting organization, facilitation skill
Self-Starter with keen interest in learning new products and skills
NICE to Have:
Experience in building complex API driven distributed IT systems.
Product knowledge in commercial banking, capital and investment banking and markets
Expertise in Data Management methodologies involving architecture, modeling, storage, security
Prior experience in data integration, interoperability, and data quality solutions
Knowledge of modeling and architecture tools such as Magic Draw, Erwin, Enterprise Architect etc.
Education:
Bachelor’s degree/University degree or equivalent experience
Master’s degree preferred
This job description provides a high-level review of the types of work performed. Other job-related duties may be assigned as required.
-
Job Family Group:
Technology
-
Job Family:
Architecture
-
Time Type:
Full time
-
Primary Location:
Jersey City New Jersey United States
-
Primary Location Salary Range:
$117,200.00 - $175,800.00
-
Citi is an equal opportunity and affirmative action employer.
Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Citigroup Inc. and its subsidiaries (""Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi.
View the ""EEO is the Law"" poster. View the EEO is the Law Supplement.
View the EEO Policy Statement.
View the Pay Transparency Posting"
225,GCP Data Engineer (Remote),Cognizant,"Deerfield, IL 60015•Remote","$95,000 - $120,000 a year","Job Title - GCP Data Engineer (Remote)
We are Cognizant Artificial Intelligence
Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. However, clients need new business models built from analyzing customers and business operations at every angle to really understand them.
With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks
*You must be legally authorized to work in United States or Canada without the need of employer sponsorship, now or at any time in the future *

Roles and Responsibilities
Interface directly with clients to solve broad business goals with database solutions
Responsible for gathering requirements, designing solutions, and overseeing the development and execution of projects
Build pilot solutions and proofs of concept with minimal direction
Provide support to project manager through developing tasks, estimates, and dependencies to meet expectations
Design and develop transformative and modern enterprise data solutions on GCP using both native GCP services and 3rd party data tech
Support in designing GCP architectures and environments
Design native cloud application architectures
Develop conversational omni bot and design the user experience flow.
Configure and deploy Google Cloud infrastructure consisting of network architecture, application security, identity and access management, logging, monitoring, and more.
Host the application by using Compute Engine, App Engine, Cloud SQL, Kubernetes Engine, Cloud Storage.

Qualifications
GCP Certification
Minimum 5 years in either system engineering or another development background
Hands-on experience across a broad range of Google services, with deep-dive knowledge and skills in specific services
Familiar with the software development lifecycle and agile methodologies
Experience with application architecture patterns and best practice
Working with IT and Business stakeholders
Deep understanding of using data and analytics services to solve enterprise data challenges
Adept at SQL and Python
Experienced in the GCP platform and other Google offerings centered around Data and Analytics (BigQuery, Dataproc, Dataflow, Dataprep, Composer, Looker, Data Studio, etc.)

Salary and Other Compensation:
The annual salary for this position is between $95,000.00 - $120,000.00 depending on experience and other qualifications of the successful candidate.
This position is also eligible for Cognizant’s discretionary annual incentive program, based on performance and subject to the terms of Cognizant’s applicable plans.

Benefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements:
Medical/Dental/Vision/Life Insurance
Paid holidays plus Paid Time Off
401(k) plan and contributions
Long-term/Short-term Disability
Paid Parental Leave
Employee Stock Purchase Plan

Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.
#LI-DC1 #MA #Ind123
Employee Status : Full Time Employee
Shift : Day Job
Travel : No
Job Posting : Apr 13 2023
About Cognizant
Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.
Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.
Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.
If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information."
226,Data Engineer (TS/SCI) REMOTE,aqua IT,"Reston, VA•Remote",N,"The Company:
We are aqua IT and we win together! The vision for aqua IT centers around the phrase 'we win together.' This is because we realize we can accomplish significantly more as a team instead of as individuals. Our mission is to provide best solutions and services to our clients while also creating an exceptional employee experience for everyone that works at aqua IT.
Our core values are empathy, accountability, passion, and teamwork. It is important that your values also align with these values.
We are currently prepared for large scale growth, and in a few years, possibly world domination!
About The Position:
We are seeking a data-focused software engineer to join our team! We are deeply involved in the customization of a data governance tool and the associated automation of metadata tagging/management - we are the driving force behind strategy and approach to all things data. This position requires a TS/SCI clearance and daily work with classified data; meaning there is some onsite work, but there is significant remote work as well. The onsite work can be done out of Reston, VA or Washington, DC.
You can expect to receive a customized care package on your first day with us. In addition, we invite you to be a part of our Fun-gineering committee - a group that meets multiple times a quarter to discuss fun ideas for the company to do; all paid by the company. There are also additional incentives for your performance and company success!
It's time to celebrate your career! Apply with us today!
Requirements
TS/SCI clearance
Bachelor's degree and 5 years of IT experience with at least 1 year of dev and data engineering experience (less experience is acceptable for graduate level degrees)
Strong with APIs in Python, Java, JavaScript, JSON, and HTML, and database formats/structures like JSON, PostgresSQL, and Oracle
Experience working with creating an encrypted link between a web server and a web browser (tokens, etc)
Consultative approach to explaining concepts to team members and clients

All applicants are considered for all positions without regard to race, religion, color, sex, gender, sexual orientation, pregnancy, age, national origin, ancestry, physical/mental disability, medical condition, military/veteran status, genetic information, marital status, ethnicity, citizenship or immigration status or any other protected classification, in accordance with applicable federal, state, and local laws. By completing this application, you are seeking to join a team of passionate professionals dedicated to consistently delivering outstanding service to our customers and contributing. Equal access to programs, services, and employment is available to all qualified persons. Those applicants requiring accommodation to complete the application and/or interview process should contact a company representative."
227,Data Engineer,Jetty,"New York, NY•Remote",N,"Welcome to Jetty, the financial services platform on a mission to make renting a home more affordable and flexible. We've built multiple financial products that benefit both renters and property managers - and we're just getting started.

As a member of the Jetty engineering team, you're passionate about building fintech products that provide value to our customers and to Jetty. You are motivated by designing engineering systems around complex business problems. You love to learn, take on challenges, and are empowered in a fast-paced and transparent culture. You're comfortable finding the right tool or pattern for the job, and advocating for improvements to the way we work.

As a Data Engineer, your goal is to cultivate a data-informed culture and create insights that will be leveraged across the entire organization. You have experience executing at a high level, solving complex problems, and delivering solutions with real business impact - and you're excited by the opportunity to apply those principles to a new, best in class function.

Role & Responsibilities
Build / Support our modern data stack (Snowflake / Fivetran / DBT / Tableau)
Implement the Five Pillars of Data Observability
Write ELT code using modern software engineering practices (Git, automated testing and deployments)
Build and maintain data pipelines to support various business processes and reporting (Fivetran / AWS Lambdas)
Document our data models in a user friendly way for our business stakeholders
Partner with the Product Engineering team to ensure we are capturing the data we need from our applications for analytics and to iterate on our development practices for the data analytics team.
Be an enthusiastic evangelist of our modern data stack (Fivetran / DBT / Snowflake / Tablea)
Be the resident resource on building standard reports and BI dashboards
Experience & Qualifications
4+ years of experience working in a data / analytics engineering role
High proficiency in Snowflake / Fivetran / dbt / Tableau
High proficiency in SQL and Python
Ability to collect, interpret, and synthesize inputs from various parts of the business into data model requirements
Ability to simplify without being simplistic - ability to communicate complex topics and actionable insights in a compelling way that can be understood by a variety of audiences
Inherent curiosity and analytical follow-through — you can't help but ask ""why?"" and love using data and logic to explore potential solutions
Ability to balance ""Rigor"" and ""Scrappiness"" — you know the difference between 80/20 and giving something 110%; as well as when each is appropriate.
Deep understanding of the first and second order effects of reporting — you know the power of presenting the right data to the right people at the right time
Experience in a data/analytics function at a high-growth startup managing multiple stakeholders and delivering actionable insights
About Jetty

At Jetty, we know renting a home can be a financial challenge. That's why we're on a mission to make renting accessible to everyone. Jetty offers four financial products designed to help our members every step of the renting process: Jetty Deposit, a low-cost security deposit product that dramatically reduces move-in costs; Jetty Rent, a flexible rent payment program to eliminate pricey late rent fees; Jetty Credit, a credit building service that helps renters build credit just by paying rent; and Jetty Protect, an affordable renters insurance product that provides comprehensive coverage in just a few clicks.

Jetty has raised multiple rounds of venture capital from investors including Khosla Ventures, Ribbit Capital, Citi, Valar, and strategic investors. We've built a highly collaborative team working remotely around the country, and we believe in finding the best talent—regardless of where they live. To learn more about life at Jetty, visit jetty.com/careers.

Jetty is firmly committed to building a team as diverse as our Members. We are proud to provide equal employment opportunities for all candidates regardless of race, ancestry, citizenship, sex, gender identity or expression, religion, sexual orientation, marital status, age, disability, or veteran status.

Benefits & Perks
Health (with HSA and FSA options), dental, and vision insurance through Aetna & MetLife
401(k) retirement savings program
Optional life and disability coverage
20 days of PTO + 12 holidays, ""Jetty Winter Break,"" and flexible sick days
Generous parental leave policy
Flexible remote work in any US location (keeping east coast hours)
Stipends to cover WFH set-up, childcare, phone/internet bill, and optional co-working space"
228,Azure Data Engineer,"DevCare Solutions Pvt., Ltd.",Remote,$80 - $85 an hour,"This position will require qualified T-SQL Developers to take thelead in thefollowing tasks:
Rationalizing and mapping data between transactional and dimensional database models/systems
Working closely with the database and system administrators to develop stored procedures and ETLprocesses related to thedata warehouse system.
Working in collaboration with vendors and other agencystaff to design,develop and managethe creation Synapse pipelines.
Ability to work independently and cooperatively as part of a team.
Ability to work under severetime constraints.
Must possess analytical and complex problem-solving skills.
REQUIRED SKILLS/EXPERIENCE YEARS
Extensive experience with development of stored procedures and ETL processes using T-SQL-7 years
Thorough understanding of data warehouse design hierarchies such as star and snowflake schemas-7 years
Use of ALM tools for work item management, version control, code analysis, and testing-7 years
Broad and extensive knowledge of the software development process and its technologies-7 years
Familiarity with continuous integration-7 years
Experience creating and scheduling elastic jobs-5 years
Experience with designing and modelling database structures based on business use cases-5 years
Job Type: Contract
Salary: $80.00 - $85.00 per hour
Experience level:
10 years
9 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote"
229,Data Warehouse Engineer - Remote,UnitedHealth Group,"Minnetonka, MN 55345•Remote","$67,800 - $133,100 a year","At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us and start doing your life's best work.(sm)
Join us as a Data Warehouse Engineer where your in-depth technical skills in data and data structures and building sustainable and modernized data pipelines and data asset solutions, will be instrumental in delivering on key analytical and reporting initiatives that impact our member’s health and wellbeing at both the market and national levels.
You’ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.
Primary Responsibilities:
Design, develop, enhance, and integrate data warehouse and analytical data infrastructure solutions
Gather, analyze, and interpret business requirements
Design, build and test processes to ensure a high level of data quality and optimal system performance for large volumes of data
Analyze new data sources for medical claims, member, clinical, call system, and financial data for integration into existing data models
Lead initiatives to architect and design Data Warehouse infrastructure and ETL/ELT (extract, transform and load) solutions and maintain existing jobs using technologies such as Oracle PL/SQL, Unix, Python, and cloud technologies
Develop, enhance and assist team in transitioning to solutions which leverage modernized technology such as Snowflake, Azure, ADF, Databricks, Kafka streaming
Create complex data extracts from the Data Warehouse
Develop technical specifications and design documents
Guide, train, instruct and assist team members with technical issues and designs
Understand the business and how various data elements and subject areas are utilized
Assist as needed to monitor data warehouse daily or weekly ETL batch processing
You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
Required Qualifications:
2+ years of experience in Data warehouse development and design
2+ years of SQL and/or PL/SQL experience
Healthcare data experience (experience with claims, clinical or call systems)
Experience with mid - high volume ETL/ELT
Experience with formal System Development Lifecycle and Data Quality processes
Solid Data Analysis experience
Preferred Qualifications:
Experience with data movement processes and tools, ETL/ELT
Experience with Snowflake or Azure cloud data warehouse technologies
Experience with Unix/Linux Shell Scripting
Experience with Agile development methodology
Proven detail oriented
Proven team oriented, Communicative, Proactive
Careers with UnitedHealthcare. Work with a Fortune 5 organization that’s serving millions of people as we transform health care with bold ideas. Bring your energy for driving change for the better. Help us improve health access and outcomes for everyone, as we work to advance health equity, connecting people with the care they need to feel their best. As an industry leader, our commitment to improving lives is second to none.
California, Colorado, Connecticut, Nevada, New York, Rhode Island, or Washington Residents Only: The salary range for California, Colorado, Connecticut, Nevada, New York, Rhode Island or Washington residents is $67,800 to $133,100. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.
*All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy
At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.
Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.
UnitedHealth Group is a drug-free workplace. Candidates are required to pass a drug test before beginning employment."
230,Data Engineer - FinTech,EquiLend,"New York, NY•Hybrid remote",N,"Company Overview
At Equilend we pride ourselves on being a leading global provider of trading, post-trade, securities market data, regulatory technology and clearing services in the securities finance industry.
Our journey began in 2000 when 10 of leading global financial institutions including Bank of America, BlackRock, Goldman Sachs, J.P. Morgan, Morgan Stanley, State Street and UBS came together with a view to increasing automation and efficiencies in the Securities Finance space. Since then, we've built our global footprint including offices in New York, Boston, Toronto, London, Dublin, Hong Kong, Tokyo, New Jersey, Pune and Chandigarh.
As a business we're committed to nurturing long-term relationships with our clients, creating an environment where our people can learn and grow, and continuing to foster an innovative and supportive environment.
Role Responsibilities
Design and develop ETL data pipelines using Python on Spark.
You will combine the discipline of software engineering with the knowledge and experience of building data solutions to deliver business value.
As a data engineer, you'll help deploy data pipelines and processes in a production-safe manner
Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility
Implement processes and systems to monitor data quality, ensuring production data is accurate and available.
Work closely with all business units and engineering teams to develop a strategy for long-term data platform architecture.
Required Skills
3+ years of relevant commercial experience
Have excellent knowledge of Python/PySpark or other Object Oriented languages such as Scala or Java
Experience with scheduling and monitoring tools such as Airflow or similar
Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
A background in software development in high-volume environments
Worked with stream processing technologies (such as Kafka, Spark and AWS Kinesis)
Working knowledge of AWS is highly desired, but not required
Thrive in a diverse, open and collaborative agile environment
Role Location Benefits
Hybrid working – you can choose which 100 days per year to work remotely where you prefer!
Generous vacation days each year which increase with your levelling
Discretionary annual bonus
Health Insurance coverage from day 1
401(k) Employer matching contribution plan
Regular team events – both in the office and offsite!
Weekly remote yoga sessions and paid access to a mindfulness-based mental health app
Diversity and Inclusion
We are committed to diversity and inclusion and believe that it is essential to our success as a company. We value and respect all individuals, regardless of their background, and recognize that diversity brings a wealth of perspectives and skills that help us innovate and excel and we welcome applications from individuals of all backgrounds and experiences.
If you require any accommodations during the application process please let us know.
#LI-Hybrid"
231,Data Engineer,Codazen,"Salt Lake City, UT","$112,000 - $149,000 a year","About Codazen
Want to apply technology in ways that make life more enriching?
We think the digital experiences that make up more and more of our everyday lives should be seamless and engaging—even magical. Using our unique recipe of specialized skills and digital alchemy, we engineer experiences that help businesses engage with users, and users engage with the world.
The source of our magic comes from our people. We continually strive to be better versions of ourselves—and have fun while doing it. If a career creating innovative experiences and digital products sounds exciting, we hope you'll consider joining our amazing team.

Rethink Experience™
Summary
As an engineering consulting agency providing services to large enterprise organizations, Codazen is positioned to support and strongly influence the success of those organizations. As a Data Engineer for Codazen, you will partner with our customers, relevant stakeholders, data scientists, and software engineers to ensure value can be derived from data. You would be responsible for identifying, designing, developing, and delivering infrastructure and solutions that transfers and derives meaningful information from data and allows our customers to make smart decisions quickly and enable their success. You would be involved in understanding business requirements and delivering solutions to support the customer's needs. This would include extracting and translating data into consumable information, utilizing expertise to support the development of extensible data models, and strongly partnering with data scientists to streamline, enhance, and improve relevant work streams. The ideal candidate will be passionate to innovate and deliver impactful solutions within a high-velocity agile environment.

Role Responsibilities
Cross-functional partnership with our Durable Teams and Customers to identify and understand data needs.
Leverage experience and expertise to build enterprise-scale data warehouses.
Design, develop, and deliver effective and reliable pipelines to transfer and translate data.
Securely source new data that can enhance and improve the quality of information.
Smartly design and develop data models that allow for efficient storage and effective retrieval.
Ensure quality and data integrity to support confidence in the insights obtained from the data.
Utilize a security and privacy first mindset in how data is stored, transferred, handled, and accessed.
Manage, maintain, and create data pipelines as needed.
Understand and own the data engineering component within the overall business solution.
Collaborate with software engineers to create and deliver elegant solutions for data consumption.

Qualifications
Bachelor's or Master's degree in Computer Science or related technical field
5+ years of Python or other modern programming language experience
5+ years of relational database experience
3+ years with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
3+ years experience with Data Modeling
Experience working with cloud or on-premises Big Data/MPP analytics platforms (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar)
2+ years experience working with enterprise DE tools and experience learning in-house DE tools

Plus
Solid programming foundation including object-oriented programming, algorithms, and data structures
Experience designing and implementing real-time pipelines
Experience with SQL performance tuning and end-to-end process optimization
Experience with anomaly/outlier detection
Experience with notebook-based Data Science workflow
Experience with Airflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.
Full compensation packages are based on candidate experience and certifications.
Salt Lake City Pay Range
$112,000—$149,000 USD
What Codazen Offers
Competitive salary
Generous paid time off and sick leave
Paid holidays
Health, dental, vision
401k (with company contribution)
Profit sharing
Company bonus
Income protection plans (life, accidental death and dismemberment, short- and long-term disability)
Free drinks and snacks
An incredible leadership team that cares about your development
Fun, smart, diverse colleagues
We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class."
232,Senior Software Engineer- Data Semantic layer,Apple,"Seattle, WA",N,"Summary
Posted: Feb 10, 2023
Weekly Hours: 40
Role Number: 200461722
The Apple Media Products Engineering team is one of the most exciting examples of Apple’s long-held passion for combining art and technology. These are the people who power the App Store, Apple TV, Apple Music, Apple Podcasts, and Apple Books. And they do it on a massive scale, meeting Apple’s high expectations with dedication to deliver a huge variety of entertainment in over 35 languages to more than 150 countries. These engineers build secure, end-to-end solutions. They develop the custom software used to process all the creative work, the tools that providers use to deliver that media, all the server-side systems, and the APIs for many Apple services. Thanks to Apple’s outstanding integration of hardware, software, and services, engineers here partner to get behind a single unified vision. That vision always includes a deep dedication to strengthening Apple’s privacy policy, one of Apple’s core values. Although services are a bigger part of Apple’s business than ever before, these teams remain small, nimble, and multi-functional, offering greater exposure to the array of opportunities here. We are seeking a highly experienced Senior Engineer to join our team and own the development of a semantic layer for our analytics data. In this role, you will be responsible for crafting and building a comprehensive data architecture that will enable seamless data integration and enable the delivery of high-quality insights to our customers.
Key Qualifications
Proven experience in data engineering, data architecture, or a related field
Experience in building and deploying semantic layers for analytics data is a plus
Strong understanding of data modeling, data warehousing, and ETL concepts
Proficiency in SQL and experience with at least one major data analytics platform, such as Hadoop, Spark, or Snowflake
Experience with data integration and data governance tools, such as Talend, Informatica, or Collibra
Excellent problem-solving and analytical skills, and the ability to work well under tight deadlines
Excellent interpersonal skills and the ability to collaborate effectively with multi-functional teams
Description
Design and implement a semantic layer that integrates analytics data from multiple sources in an efficient and effective manner. Develop data models and mapping rules to transform raw data into actionable insights and reports. Collaborate with the analytics and data science teams to understand their requirements and deliver solutions that meet their needs. Ensure data quality and accuracy by developing data validation and reconciliation processes. Play an active role in the development and maintenance of user documentation, including data models, mapping rules, and data dictionaries. Collaborate with multi-functional teams to define and implement data governance policies and standards. Stay informed about the latest developments in data analytics and data management technologies and recommend new tools and methodologies to improve the semantic layer.
Education & Experience
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field Apple is an equal opportunity employer and value diversity at our company. Apple does not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $115,000 and $217,500, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program."
233,Data Platform Engineer,Holman,"Mount Laurel, NJ",From $110 an hour,"At Holman, we exist to provide rewarding careers and better lives for employees and their families. We hire, train, empower, and reward exceptional people. Our journey is guided by our desire to get it right every time and the acknowledgement that we have an opportunity to be better. To be better, we have to do better, and to do better we must know better. That’s why we are listening, open to learning new things – about ourselves and each other. We will never stop striving for improved diversity, equity, and inclusion because we are successful together when we feel trusted and supported. It’s The Holman Way.

At Holman, your total compensation goes beyond your paycheck. To position you for success and provide a rewarding career and better life for you and your family, Holman is proud to offer you the benefits you deserve; including protection against illness, disability, loss of work, or preparation for retirement. Below is a brief overview of these programs:

Health Insurance

Dental Insurance

Life and Disability Insurance

Flexible Spending and Health Savings Accounts

Employee Assistance Program

401(k) with Employer Match

Paid Time Off

Tuition Reimbursement

Exclusive pricing and concierge sales support on new and used vehicles

Holman is currently accepting applications for the role of Data Platform Engineer

Principal Purpose of Position:
Design, develop, document and execute data solutions, tools and practices
Analysis of requirements at sufficient level of detail to allow ETL solution to be developed
Development of ETL job flows according to company standards for naming, performance, restartability and performance.
Support testing and remediation of defects in newly-developed/modified ETL workflows
Promote ETL workflows to PROD and provide ongoing support in PRODUCTION, including monitoring and troubleshooting
Ability to create Power BI Datasets to support the Analytic Delivery team
Evaluate emerging data platform technologies
Lead technology implementations
Follow and contribute to best practices for data management and governance
Collaborate with the Data Architecture team to understand and implement load processes for reporting and analytic data structures (data warehouses, data marts and data lakes)
Performance tune and troubleshoot processes under development and in production as necessary.
Work with the Data Architects to augment ERD’s as changes are developed
Develop, maintain, and extend reusable data components
Provide timely project and task updates to all concerned parties
Monitor production data integration jobs and correct failures in a timely manner
Create and manage incident reports as they pertain to data integration processes
Perform all other duties and special projects as assigned.

Required Experience/Skills

2+ years Azure exposure (Any Resources: Databases, Data Factory, Synapse Studio, Storage Account, Power Platform)
2+ years ANSI SQL experience
1+ years data modeling exposure
Advanced problem solving/Critical thinking mindset

Preferred Experience/Skills

Azure connectivity/authentication (service principals, managed identities, certificates)
Power BI Dataset creation/maintenance
Azure Resources: DevOps, Logic Apps, Gen 2 Storage, Purview
SQL Server, Oracle, Python, Spark

Education and/or Training:
Bachelor’s degree in Computer Science or equivalent work experience

Compensation: Starting at $110,00 USD

Holman is a global automotive leader that serves both commercial and consumer clients The Holman Way by always doing the right thing for our people, our customers, and the community since 1924. The Holman story began nearly a century ago as a single Ford dealership in New Jersey. Today, Holman, headquartered in Mount Laurel, New Jersey, is one of the largest family-owned automotive service organizations in North America with more than 6,500 employees across North America, the UK, and Germany.

Holman delivers a unique range of automotive-centric services including industry-leading fleet management and leasing; vehicle fabrication and upfitting; component manufacturing and productivity solutions; powertrain distribution and logistics services; commercial and personal insurance and risk management; and retail automotive sales as one of the largest privately owned dealership groups in the United States. Guided by its deeply rooted core values and principles, Holman is continuously Driving What’s Right.

Holman provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training."
234,Data Engineer,Whereoware,United States•Remote,N,"Description
Leading digital experince agency for +20 years, Whereoware drives smart growth through digital strategy and activation. We specialize in successfully guiding brands through the ever-changing digital landscape, through customer acquisition, retention, and maximization; marketing optimization; and e-commerce solutions. Pioneering online personalization and holistic digital experiences, we design and build, award-winning websites and campaigns, generating impactful results with data integrations, analytics, and digital advertising.
Whereoware acquired a user experience consultancy in 2022 accelerating our growth and advancing our capabilities to help our clients achieve their goals.
Whereoware is hiring a Data Engineer to join our analytics team. You will work alongside project managers, developers, strategists, solution architects and analytics team members. Your main tasks will include designing, building, launching, optimizing business intelligence solutions. To succeed in this role, you should be able to work in big data environments providing solutions that enable efficiencies in data architecture and data connection.
Whereoware is a digital agency with clients across industries and verticals. Day-to-day projects for the Data Engineer will vary, but may look like:

You Are Great At:
Working closely with Digital Strategists and Data Analysts to apply advanced technical experience and knowledge to develop, maintain, and monitor complex and critical BI data sources and integration processes
Developing data flow diagrams and communicating those diagrams to cross-functional teams and clients
Reporting and data source standardization including data requirements gathering, wireframing, and documentation
Supporting dashboard builds that answer key business questions across Sales, Marketing, Customer Success, Recruiting, Product, and Engineering
Working across multiple planned analytics projects and ad-hoc data requests for internal and external stakeholders

Must Have:
Minimum three years of recent experience working as a Data Engineer or related role
Bachelor’s degree from an accredited college/university in computer science, information systems, statistics, science, or equivalent practical experience
Direct experience within data visualization tools such as DOMO, Tableau, Power BI and/or Looker supporting data connection, transform, and dashboard builds
Fluency in scripting languages (e.g., SQL, Python, etc.)
Experience in working in data warehouses such as Snowflake, Redshift, Google BigQuery, etc.
Hands-on experience building ETL processes and data flows
Excellent communication skills and are able to communicate to a variety of non-technical and technical stakeholders

Nice To Have:
Knowledge of emerging technologies in database design, database management, data science, and machine learning
Experience working in an agency

#LI-Remote
Whereoware, Inc is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status, or any other protected characteristic, and will not be discriminated against on the basis of disability."
235,Senior Data Engineer - Top Secret Clearance,Marathon TS,"Chantilly, VA","$140,000 - $170,000 a year","Position Overview:
Marathon TS is looking for a Senior Data Engineer to join our team supporting our Federal customer out of Chantilly, VA. This position will provide the ability to make a significant impact to the mission while also allowing the candidate to grow their skills and career. Successful candidate will need to be able to maintain existing software as the transition from server-based to cloud-based occurs. The candidate will work with a GOTS-based Extract, Transform and Load (ETL) system that loads information into Postgres database with billions of rows and deals with a file repository that is in a Petabyte range. In addition, the candidate will be highly involved in the engineering planning and transition of the existing system software / data into a Cloud-based (currently AWS) solution.
**Hybrid Work Model**
Requirements:
Active Top Secret Clearance with ability to get an SCI
Bachelor’s degree in Engineering, Computer Science, or other related analytical, scientific, or technical discipline
At least 6 years’ experience with ETL.
Database ETL engineers should have experience with Oracle 11g/12c, Linux (CentOS, Red Hat), and Windows environments.
Software/scripting engineers should have experience performing software and scripting engineering for data ingest with Java or Python for ETL.
Experience with web services and/or microservices.
Experience using software repository tools such as GIT or SVN.
Preferred Education, Experience, & Skills
ETL development, extraction, parsing, etc.
AWS Cloud development
Java, Python, Springboot
Job Types: Full-time, Contract
Pay: $140,000.00 - $170,000.00 per year
Benefits:
401(k)
Health insurance
Paid time off
Schedule:
8 hour shift
Ability to commute/relocate:
Chantilly, VA: Reliably commute or planning to relocate before starting work (Required)
Education:
Associate (Required)
Experience:
ETL: 6 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location

Health insurance"
236,Data Engineer,Nursa,"Murray, UT•Remote",N,"Nursa is revolutionizing the temporary healthcare staffing industry by delivering an unparalleled level of transparency and accessibility to healthcare providers, and living out our mission to enrich the life of every clinician with innovation and opportunity. We’re committed to helping professionals such as registered nurses (RNs), licensed practical nurses (LPNs), and certified nursing assistants (CNAs) advance their careers by giving them the power to choose from available shifts provided by facilities in their area. In turn, Nursa’s marketplace platform technology empowers healthcare facilities to bring in the talent they need without spending time and resources on traditional nursing recruitment processes. By connecting healthcare clinicians and healthcare facilities to fill per diem and on-call shifts, the Nursa marketplace increases access and transparency to the $17 billion temporary clinician staffing industry. We’re a profitable, venture backed startup founded in March 2020 and are rapidly expanding. Your impact will not only be on the business, but also on the daily lives of healthcare providers and the quality of care everyone experiences.
Role overview:
Our data team is growing and we are expanding the Data Engineering pod within the Data Team. You will function as a Data Engineer and ETL Developer and will report directly to the Data Engineering Manager. You will help to expand our tech stack by implementing processes for ETL, data pipeline, and build monitoring tools. You will help manage our data warehouse, Segment (CDP / session tracking tool), and data integrations for third-party platforms (Salesforce, Iterable, etc). This Data Engineering pod works closely with Business Intelligence, Data Science, Development, as well as many other business teams. The ideal working situation would be hybrid, but highly qualified remote candidates will be considered.
This position requires for someone to be onsite in Murray, UT.
Responsibilities:
Take a proactive role in expanding tech stack and building data engineering team
Develop pipelines between systems using Rest or similar APIs
Integrate and manage ETL for systems such as Salesforce, Iterable, RedShift, NetSuite
Work continually to monitor and improve data accuracy and reliability
Collaborate with and mentor others on data team and across the organization
Qualifications:
3+ year experience in Data Engineering or ETL Development
Proficiency in Python scripting, SQL, and database management (AWS Redshift)
Must have experience creating data pipelines to send and pull data with REST or similar API
Experience using third-party integrations (CRM or CDP)
AWS experience would be beneficial
What you get in return:
Opportunity to revolutionize healthcare industry and build both relationships and teams that make a tangible impact
We empower team members to act intelligently and be owners, believing that execution is everything, and have designed a learning-focused environment where you get ongoing support and regular feedback to help you grow
An opportunity to join an international team with a work culture that is based on trust, flexibility, and curiosity
Competitive salary and benefits
Closing:
Nursa is an equal opportunity employer. We aim to build a workforce of individuals from different backgrounds, with different abilities, identities, and mindsets. Even if you do not meet all of the qualifications listed above, we encourage you to apply!"
237,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
238,Data Engineer,Enterpryze Consulting Ltd.,"Norfolk, MA",N,"Data Engineer


Working Location: Norfolk, USA
Security Clearance: NATO Secret
Language: High proficiency level in English language

DESCRIPTION:
Data science, data analytics and Artificial Intelligence (AI) are increasingly gaining momentum in NATO touching all military and political domains and functional areas. In response to HQ SACT’s understanding of the disruptive potential of data science and AI, and recognizing the strategic value of data, the Data Science & Artificial Intelligence section, established in 2020 in the Federated Interoperability Branch, is focusing on data science and AI as cross-cutting and enabling capabilities for HQ SACT and the NATO Enterprise. The section provides a broad spectrum from strategy and policy development and support to technical delivery and implementation to HQ SACT and the NATO Enterprise. In addition to serving as the centre of gravity for HQ SACT’s efforts in advancing data centricity and integrating rapidly changing technology related to data exploitation, the section has developed a substantial reputation inside NATO and is regularly invited to offer policy and technical expertise.


EXPERIENCE AND EDUCATION:


Essential Qualifications/Experience:
A Bachelor of Science degree from a recognized university in computer science, IT, software or computer engineering, data science, applied math, physics, statistics, or a related field
A Master’s degree or higher from a recognized university in computer science, IT, software or computer engineering, data science, applied math, physics, statistics, or a related field
Experience with advanced level SQL, including query optimization, complex joins, development of stored procedures, user-defined functions and working with Analytic Functions in the last 3 years
Proficient in at least one data manipulation language such as Python, Scala, R, etc.
Ability to develop ETL processes for batch and streaming data, with proficiency in tools and technologies such as Apache Spark, Apache Airflow, Pentaho Data Integration, SQL Server Integration Service
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases
Experience working with at least one Data Warehouse schemas – such as Star or Snowflake
Ability to work with large datasets
Working experience in an international environment with both military and civilian elements
Understanding of the NATO organization and its functions
Desirable Qualifications/Experience:
Knowledge of NoSQL databases such as MongoDB, Cosmo DB
Ability to work in cloud environments to develop scalable data pipelines
Skills in Cloud infrastructure and technologies such as Google Cloud Compute, AWS, Azure Data Factory, distributed computing
Working experience with geospatial data structures such as raster and vector-based data
Ability to collect and document project requirements, and to translate the requirements to technical solutions, including working in an agile environment to implement complex database projects

DUTIES/ROLE:
Contribute to the development and implementation of an enabling data science and AI capability at HQ SACT and for the NATO
Contribute to ML/AI initiatives across HQ SACT and the NATO Enterprise with a particular focus on the data engineering side
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, proposes how to re-design infrastructure for greater scalability
Develop, construct, test and maintain data pipelines and architectures such as databases and large-scale processing systems, within the constraints of existing but evolving processes and technologies
Transform data into formats that can be easily analyzed by developing, maintaining, and testing infrastructures for data generation
Prepare data for prescriptive and predictive modelling
Provide subject matter expertise to (military and civilian) staff within HQ SACT or the NATO Enterprise and develop proofs of concept, as directed
Work in tandem with data scientists and software engineers
Select from existing data sources and prepare data to be used by data science models
Improve data quality and efficiency
Support evaluation of operational requirements and objectives
Interpret trends and patterns and support building of algorithms and prototypes
Support educational efforts and training development related to data, AI or digital literacy
Remain up-to-date with new developments in data engineering and data architectures to bring innovative ideas into implementation
Support building a data-driven culture that uses data and analytics to generate insights, improve decision making at all levels, inform strategy and policy decisions, and improve performance
Perform additional tasks as required, related to the LABOR category"
239,Data Engineer/SW Developer (TS/SCI FSP),IBM,"15036 Conference Center Dr, Chantilly, VA 20151","$103,000 - $225,000 a year","Introduction
As a Data Scientist at IBM, you will help transform our clients’ data into tangible business value by analyzing information, communicating outcomes and collaborating on product development. Work with Best in Class open source and visual tools, along with the most flexible and scalable deployment options. Whether it’s investigating patient trends or weather patterns, you will work to solve real world problems for the industries transforming how we live.

Your Role and Responsibilities
IBM is committed to providing National Security leaders with cutting edge data exploitation and analysis capabilities. Seeking a talented, versatile Data Engineer / Back End Developer with TS/SCI and Full Scope Polygraph.

The successful candidate will be part of a small agile team building cutting edge analytic capabilities including a data pipeline, data lake, and associated analytic and AI/ML capabilities for US Government customers. The team has is fast-paced, collaborative, and cohesive, and depends on team members to communicate openly and to design solutions and deliver quality code on a regular, aggressive clip.

The Data Engineer / Back End Developer will be a hands-on software developer who will design, develop, integrate, test, and deploy software using a combination of custom, open source, and off-the-shelf (GOTS/COTS) software packages and a variety of data types (structured, unstructured, non-text). The Data Engineer / Back End Developer will work as part of an Agile scrum team to analyze business requirements and translate them into technical tasks. They will enhance software functionality based on rapidly evolving mission needs and technology opportunities.

Level (entry, journeyman, senior, master) will be determined based on education and years of experience.

#CLEARED22

Required Technical and Professional Expertise
REQUIRED ON DAY 1: Active TS/SCI security clearance + Full Scope Polygraph
At least 2 years demonstrated hands-on programming experience with one or more of the following: Java, Node, Python, AWS
At least 2 years of experience architecting, designing and programming applications in hybrid, cloud or on-prem environments
Experience with Git/GitHub
Experience integrating with JSON-based REST APIs
Experience with Agile software development, Scrum, and software development lifecycle (SDLC)
Self-starter with an ability to work independently and in a team environment
Strong problem-solving, critical thinking, and analysis skills.Ability to articulate complex analytic problems, work effectively without detailed instructions, collaborate and ask questions
Ability to generate, follow, and update technical procedures and evaluate results

Preferred Technical and Professional Expertise
Bachelors or Maser's Degree in Computer Science, Computer Information Systems, Computer Engineering
At least 1 year of classified software engineering experience on IC systems and architectures
At least 1 year classified software engineering experience

About Business Unit
IBM Consulting is IBM’s consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients’ businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.
This job requires you to provide your COVID-19 vaccination status with supporting documentation, where legally permissible.

Your Life @ IBM
In a world where technology never stands still, we understand that, dedication to our clients success, innovation that matters, and trust and personal responsibility in all our relationships, lives in what we do as IBMers as we strive to be the catalyst that makes the world work better.
Being an IBMer means you’ll be able to learn and develop yourself and your career, you’ll be encouraged to be courageous and experiment everyday, all whilst having continuous trust and support in an environment where everyone can thrive whatever their personal or professional background.
Our IBMers are growth minded, always staying curious, open to feedback and learning new information and skills to constantly transform themselves and our company. They are trusted to provide on-going feedback to help other IBMers grow, as well as collaborate with colleagues keeping in mind a team focused approach to include different perspectives to drive exceptional outcomes for our customers. The courage our IBMers have to make critical decisions everyday is essential to IBM becoming the catalyst for progress, always embracing challenges with resources they have to hand, a can-do attitude and always striving for an outcome focused approach within everything that they do.
Are you ready to be an IBMer?

About IBM
IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement
IBM offers a competitive and comprehensive benefits program. Eligible employees may have access to:

Healthcare benefits including medical & prescription drug coverage, dental, vision, and mental health & well being
- Financial programs such as 401(k), the IBM Employee Stock Purchase Plan, financial counseling, life insurance, short & long- term disability coverage, and opportunities for performance based salary incentive programs
Generous paid time off including 12 holidays, minimum 56 hours sick time, 120 hours vacation, 12 weeks parental bonding leave in accordance with IBM Policy, and other Paid Care Leave programs. IBM also offers paid family leave benefits to eligible employees where required by applicable law
Training and educational resources on our personalized, AI-driven learning platform where IBMers can grow skills and obtain industry-recognized certifications to achieve their career goals
Diverse and inclusive employee resource groups, giving & volunteer opportunities, and discounts on retail products, services & experiences

The compensation range and benefits for this position are based on a full-time schedule for a full calendar year. The salary will vary depending on your job-related skills, experience and location. Pay increment and frequency of pay will be in accordance with employment classification and applicable laws. For part time roles, your compensation and benefits will be adjusted to reflect your hours. Benefits may be pro-rated for those who start working during the calendar year.

We consider qualified applicants with criminal histories, consistent with applicable law.

US Citizenship Required.

Being You @ IBM
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
240,azure Data Engineer - Direct Hire,Formac Inc,Remote,"$83,947 - $110,732 a year","he Data Engineer is a key position within the GetInsuredBusiness Intelligence and Reporting unit and will work closely with the Data Visualization Team Lead, clients, and various teams to develop, optimize, and oversee the conceptual, logical, and physical data systems, data warehouses, ETL, and analytics environments. This is a hands-on role which requires analyzing new data system requirements, design implementation, and optimization of the data architecture. The ideal candidate will have excellent analytical and problem-solving abilities, great communication skills, and be effective in working cross-functionally.
Responsibilities:
Work with partners across different practice areas to understand and evaluate business needs.
Consult with analytics team members to design and develop data engineering pipelines for analytic and dashboard projects.
Develop streamlined and governed processes for data ingestion into data lake
Understand and integrate data quality and data governance processes into data engineering pipelines
Develop data validation process for transforming and curating raw data into business ready datasets
Optimize query performance of large data sets for ad hoc exploration, analysis, and reporting
Communicate complex technical information to business customers and project teams in an effective and concise manner
Participate as a technical resource for a variety of information projects that will be dictated by current business needs and technological developments
Work with a variety of external IT consulting partners on developing optimal data solutions and provide effective direction to contracted resources
Requirements:
Bachelor's degree in Computer Science, Information Systems, Engineering or related fields.
5+ years of experience working with data, 3+ years’ experience working on data projects with a focus on data engineering
Experience with cloud platforms preferably Microsoft Azure and Databricks
Experience with large-scale data collection and analysis
Experience with batch, streaming, or API-based data integration patterns
Strong knowledge of programming languages, such as SQL and Python
Strong organizational skills and understanding of agile project management methodology
Strong written and oral communication skills with demonstrated ability to communicate with technical and non-technical partners
Documentation of data engineering processes
Experience optimizing query performance of large data sets
Developing and maintaining database schemas
Additional required qualifications:
C# programming, object-oriented programming, Python
PostgreSQL
Visual studio
Microsoft Azure
.Net core
Preferred qualifications:
Entity framework, multi-threading, Azure functions
Job Types: Full-time, Permanent
Pay: $83,947.48 - $110,731.54 per year
Benefits:
Dental insurance
Health insurance
Vision insurance
Experience level:
7 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Application Question(s):
your Expected Salary for this job
Education:
High school or equivalent (Required)
Experience:
data enginner: 6 years (Required)
Microsoft Azure: 1 year (Required)
Databricks: 1 year (Preferred)
API-based data integration: 1 year (Preferred)
C# programming, object-oriented programming, Python: 1 year (Preferred)
Work Location: Remote

Health insurance"
241,Data Engineer,HP,"Spring, TX 77389",N,"Applies developed subject matter knowledge to solve common and complex business issues within established guidelines and recommends appropriate alternatives. Works on problems of diverse complexity and scope. May act as a team or project leader providing direction to team activities and facilitates information validation and team decision making process. Exercises independent judgment within generally defined policies and practices to identify and select a solution. Ability to handle most unique situations. May seek advice in order to make decisions on complex business issues.
Responsibilities
Designs and establishes secure and performant data architectures, enhancements, updates, and programming changes for portions and subsystems of data pipelines, repositories or models for structured/unstructured data.
Analyzes design and determines coding, programming, and integration activities required based on general objectives and knowledge of overall architecture of product or solution.
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies and debugs, and creates solutions for issues with code and integration into data system architecture.
Leads a project team of other data engineers to develop reliable, cost effective and high-quality solutions for assigned data system, model, or component.
Collaborates and communicates with project team regarding project progress and issue resolution.
Represents the data engineering team for all phases of larger and more-complex development projects.
Provides guidance and mentoring to less experienced staff members.
Knowledge & Skills
Using data engineering tools, languages, frameworks to mine, cleanse and explore data.
Fluent in NoSQL & relational based systems.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Designing data systems/solutions to manage complex data.
Strong understanding of database technologies and management systems.
Strong understanding of cloud-based systems/services.
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Excellent written and verbal communication skills; mastery in English and local language.
Ability to effectively communicate product architectures, design proposals and negotiate options at management levels.
Scope & Impact
Collaborates with peers, junior engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams.
Leads a project requiring data engineering solutions development.
Education & Experience
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
Typically 4-6 years’ experience.
#LI-POST

About HP

You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.
So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.

HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.

Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.
From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!"
242,Software Engineer - Data Campaign,Torc Robotics,"405 Partnership Dr, Blacksburg, VA 24060","$114,400 - $137,300 a year","About the Company
At Torc, we have always believed that autonomous vehicle technology will transform how we travel, move freight, and do business.
A leader in autonomous driving since 2007, Torc has spent over a decade commercializing our solutions with experienced partners. Now a part of the Daimler family, we are focused solely on developing software for automated trucks to transform how the world moves freight.
Join us and catapult your career with the company that helped pioneer autonomous technology, and the first AV software company with the vision to partner directly with a truck manufacturer.

What you'll do:
We are seeking experienced Full Stack Engineers to build software that enables access to our cloud-based data lake as well as remote and in-vehicle visualizations throughout the testing process. The ideal candidate has extensive experience with current web technologies and proficiencies working with cloud technologies at scale.
Here are some of the technologies we're looking for:
Managed services powered by AWS (Lambda, SFN, SQS, API Gateway, Greengrass)
Terraform
Python
Javascript
Vue.js
On-Call Tooling (PagerDuty, Datadog, AWS Cloudwatch)
It's nice to have experience with some of our tooling choices, but not required. What we do need is the ability to learn new tools and technologies, as we are constantly finding new and better ways of doing things.
How We Work:
We collaborate frequently in many different forms including daily stand-ups, planning meetings, and many ad-hoc discussions/brainstorming/troubleshooting/pairing sessions over zoom or slack. Our team is operationally responsible for the services we own, so we do have an on-call rotation in place where each member will take a turn serving as the front-line to incidents for our services. Rotations last a week each, during business hours. We value maintaining a healthy work/life balance and prefer sustainable development over heroic efforts.
What you'll need to Succeed:
BS/MS Degree in Computer Engineering, Computer Science, or related field
A strong commitment to test-driven development patterns, continuous integration and delivery, and infrastructure as code
Working knowledge of front-end web technologies and/or microservices
HTML5/CSS/JavaScript/JSON
Agile methodologies (Scrum, Kanban)
Experience building multi-tiered, web hosted software using modern patterns and practices
Basic knowledge of AWS serverless architecture
A strong willingness to learn and work with new tools
Bonus Points!
Strong organizational, time management, and communication skills working with a team orientation and collaborative style
Expert level proficiency with AWS serverless architectures
3D visualization tools
Perks of Being a Full-time Torc'r
Torc cares about our team members and we strive to provide benefits and resources to support their health, work/life balance, and future. Our culture is collaborative, energetic, and team focused. Torc offers:
A competitive compensation package that includes a bonus component and stock options
100% paid medical, dental, and vision premiums for full-time employees
401K plan with a 6% employer match
Flexibility in schedule and generous paid vacation (available immediately after start date)
Company-wide holiday office closures
AD+D and Life Insurance
Hiring Range for Job Opening
US Pay Range
$114,400—$137,300 USD
At Torc, we're committed to building a diverse and inclusive workplace. We celebrate the uniqueness of our Torc'rs and do not discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, veteran status, or disabilities.
Even if you don't meet 100% of the qualifications listed for this opportunity, we encourage you to apply. We're always looking for those that are hungry, humble, and people smart and your unique experience may be a great fit for this role or others."
243,Data Engineer,Gateway Professional Network,Remote,"$95,000 - $145,000 a year","Apply Directly: https://gatewaypn.applicantstack.com/x/detail/a2bwypr26k74
Data Engineer
Job Description for Full-Time Position
Location: US Remote (must reside in US and we are unable to sponsor Visas at this time)
Pay Scale: $95,000 – $145,000 per Annum (based on experience)
Reports to: Ramon Navarro
Job Description Summary:
Individual to contribute to GPN Tech transformation assuming accountability for data engineering lifecycle including research, proof of concepts, design, development, test, deployment, and maintenance of enterprise-scale data integration solutions leveraging Microsoft Azure PaaS offerings. Extensive experience working on Agile Scrum/DevOps teams employing the latest CI/CD cloud-first best practices. Potential to become an SME on Platform capabilities (ingestion, storage, processing, and presentation patterns) and extend future-state strategic roadmap features.
What You’ll Do: Responsibilities
Provide technical direction to delivery scrum teams, extending patterns and establishing new ones as required
Create and review technical designs of data integrations across various systems within the enterprise and with external business partners over multiple transmission channels, data formats, and processing patterns
Extend/implement CICD pipelines and containerization strategy
Responsible for ETL and SQL development using Domain Architecture and Interface based programming
Build processes and diagnostics tools to troubleshoot, maintain and optimize solutions and respond to customer and production issues
Experience You’ll Need: Qualifications
Data Engineer with in-depth knowledge of developing data-driven solutions using Azure App Services, Function Apps, API Service Management, Active Directory, Azure Storage, Data Factory, Databricks, RedisCache, SQL DB, Key Vault, Service Bus, Event Hub and Application Insights
Bachelor’s degree in computer science or related discipline; r Equivalent Work Experience
5+ years of experience leveraging cloud data storage technologies specifically Microsoft Azure offerings
5+ years of experience developing on modern data ingestion tools/platforms/protocols (Kafka, APIs, EventHubs, ServiceBus, etc.)
5+ Experience with data integration best practices (ETL/ELT patterns, Data Factory, Streams Analytics, etc.)
5+ years of experience in ETL and SQL Programming
2+ years of experience in C# and T-SQL
3+ years of experience working with Microsoft Azure DevOps and CI/CD best practices
3+ years of experience working in an Agile Scrum environment
Experience with various data structure optimization techniques on a Data Streaming Platform
Experience with Power BI or other similar data visualization tools
Bonus Points: Additional qualifications if you have them!
Experience with Machine Learning and Artificial Intelligence based solutions
Knowledge of data regulations and compliance policies (e.g., HIPAA)
Data Visualization experience, or strong affinity for leveraging data to help transform/optimize business functions
Big Data experience
IT healthcare experience
IoT experience
Who We Are:
GPN Technologieswas founded in 2007 with the mission of providing “big business” infrastructure to independent practitioners in the ophthalmic industry. The company’s driving initiative has been empowering independents to be profitable and competitive in today’s market by making high-tech, business-critical tools accessible and meaningful in small business settings.
Our analytics platforms have revolutionized the way practitioners view, understand, and act on their business data. We have continued to expand and hone those platforms to help those practitioners succeed and thrive in an increasingly competitive, fast-paced marketplace.
Thousands of practitioners across the country are serving their patients, their team members, and their bottom lines more efficiently with EDGEPro.
Job Type: Full-time
Pay: $95,000.00 - $145,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Paid time off
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
Do you require Visa sponsorship?
Experience:
Relational databases: 2 years (Preferred)
Azure: 2 years (Preferred)
Big data: 2 years (Preferred)
SQL: 2 years (Required)
Microsoft SQL Server: 2 years (Required)
C#: 2 years (Required)
.NET: 3 years (Required)
Work Location: Remote"
244,"Senior Software Engineer, Azure Data Platform",Microsoft,"One Microsoft Way, Redmond, WA 98052","$112,000 - $218,400 a year","Microsoft is a company where passionate innovators come to collaborate, envision what can be and take their careers to levels they cannot achieve anywhere else. This is a world of more possibilities, more innovation, more openness, and the sky is the limit thinking a cloud-enabled world.

Microsoft’s Intelligence Platform engineering team is leading the transformation of analytics in the world of data with products like Power BI, Synapse Analytics, Azure Data Factory, Azure Data Explorer. We will bring the world’s data to the Microsoft Cloud, power a new class of data first applications, and empower everyone on the planet to make better decisions with data.

We do not just value differences or different perspectives. We seek them out and invite them in so we can tap into the collective power of everyone in the company. As a result, our ideas are better, our products are better, and our customers are better served.

Azure Data Platform is quite possibly one of the most exciting Microsoft services today, frequently in the news, growing its user base very rapidly, and making waves across the industry. The Admin & Governance team delivers scalable and data driven products and features that are core for Azure data. This work is designed to ensure that customers experience exceptionally fast service interactions and can efficiently manage any azure data service for their organization.

We rely on numerous Azure technologies like Service Fabric, SQL Azure, REDIS cache, Service Bus, API Management, blob storage, Document DB, Azure Data Lake, Kusto, containers, etc. We are building and expanding world-class services and platform that combines big data, mission critical services, and real-time insights. This combination makes the area among the most cutting-edge across the entire industry!

We are looking for passionate people with experiences working with all service aspects of high throughput and multi-tenant services, ability to design components carefully, properly handle errors, write clean and well-factored code with good tests and good maintainability.
Responsibilities
Design high quality components in C# using clean and tested code.
Integrate Azure technologies to achieve high scale and reliability.
Lead architecture, design, and develop features and solutions with high quality
Design, implement, and refine chosen solutions in close partnership with Product Management and partner teams
Review and contribute to the specifications and implementations written by other team members
Provide technical leadership across features, projects, service fundamentals and tools
Mentor new and junior engineers to bring them up to speed in software development environment
Provide support to online services by investigating and mitigating issues
Qualifications
Required Qualifications
Bachelor's Degree in Computer Science, or related technical discipline AND 4+ years technical engineering experience with coding in languages including, but not limited to: C, C++, C#, Java, JavaScript, or Python
OR equivalent experience.
Experience with cloud service development
Technical problem solving and debugging skills.
SQL experience required
Ability to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings: Microsoft Cloud Background Check. This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter.
Preferred Qualifications
4+ years of experience of C# (preferred), Java or C++.
Experience building highly performant scalable services.
Experience designing and implementing automated tests, including unit tests and integration tests.
Effective communication skills.
Familiarity with big data, machine learning and data analytics would be a bonus.

Software Engineering IC4 - The typical base pay range for this role across the U.S. is USD $112,000 - $218,400 per year. There is a different range applicable to specific work locations, within the San Francisco Bay area and New York City metropolitan area, and the base pay range for this role in those locations is USD $145,800 - $238,600 per year.

Certain roles may be eligible for benefits and other compensation. Find additional benefits and pay information here: https://careers.microsoft.com/us/en/us-corporate-pay

Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.

Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.

#azdat"
245,Data Engineer,"Sancorp Consulting, LLC","Arlington, VA","From $175,000 a year","Position Description:
Senior Data Warehouse Architect
Location: In-Person- National Capital Region
Employment Category: Full Time / Exempt
Travel: Minimal, if any.
Deployment: No
Drug screening: Yes
Security Clearance / Citizenship: TS/SCI; Must be US Citizen
Required Education:
Bachelor’s degree in Computer Science or Software Engineering
Preferred Education:
Master’s degree
Salary: Starting at $175,000
Required Qualifications:
Active TS security clearance with immediate SCI eligibility upon award
Senior level operations management, administrative management, or project management experience within the past five years
Ability to work in a fast-paced, dynamic environment with tight deadlines and competing requirements. Strong organizational, planning, and time management skills.
5-10 years of professional and technical experience as a Data Architect, Data Engineer, or related role developing data models and architectural solutions.
Knowledge of developing and using data standards comprising common formats, representation, definition, structuring, manipulation, tagging, transmission, use, and management of data
5-10 years of experience designing, developing, and implementing business applications from business concept through to production
5-10 years of experience working with data programming languages (Python, Java, SQL, etc.) and data orchestration and integration pipelines
Knowledge of cloud computing platforms similar to Amazon Web Services, Azure or equivalent.
Understanding of software development lifecycle including Agile and traditional project management and delivery methodologies
Strong written and oral communications skills. Have the ability to identify stakeholders, foster collaboration, lead groups, develop and advocate positions, and negotiate compromises in a multifunctional environment.
Strong math and analytical skills.
Preferred Qualifications:
10 + of professional and technical experience as a Data Architect, Data Engineer, or related role developing data models and architectural solutions at the enterprise level.
Ten or more years of experience designing, developing, and implementing business applications from business concept through to production.
Ten or more years of experience designing and implementing large-scale solutions that empower business data and enterprise information and data solutions
Expert level knowledge of developing and using data standards comprising common formats, representation, definition, structuring, manipulation, tagging, transmission, use, and management of data.
Ten or more years of experience working with data programming languages (Python, Java, SQL, etc.) and data orchestration and integration pipelines.
Ten or more years of experience with cloud computing platforms similar to Amazon Web Services, Azure or equivalent.
Demonstrated math and analytical skills.
Strong consulting and communication skills.
Ten or more years of experience with software development lifecycle including Agile and traditional project management and delivery methodologies.
Ability to define, maintain, and manage architecture models and artifacts (e.g. current and future state application architecture diagrams, data models, etc.).
Knowledge of DoD, Intelligence Community (IC), and interagency organizations and their functional relationships, programs, and policies.
Specific experience leading groups, negotiating compromises, and influencing key leaders, customers and stakeholders involving problems or public policy issues that have great sensitivity, including national or international consequence.
Responsibilities:
Sancorp Consulting, LLC is seeking a Senior Data Warehouse Architect to support the Office of the Under Secretary of Defense for Intelligence and Security (OUSD(I&S)). The following are examples of responsibilities:
Serve as the data architecture, knowledge management focal point, and Senior advisor responsible for creating, coordinating, and managing diverse information systems up to and including information requiring compartment and special access protection.
Provide technical expertise, advice, and support for the development and delivery of enterprise data architecture strategies including data modeling, design, data engineering and implementation solutions to meet enterprise level data architecture challenges.
Provide technical expertise, advice, and support for the design, coordination, management and implementation, and oversight of a common analytical platform capable of using compartmented and special access information, to include DoD-wide efforts to gain system accreditation.
Provide technical expertise, advice, and support for the design, coordination, management, implementation, operation, and oversight of a common operational reporting system.
Provide technical expertise, advice, and support required to ensure that incident reporting is appropriately cataloged, stored, curated and maintained, to include sensor data from multiple platforms and data at multiple classification levels.
Provide technical expertise, advice, and support required to design, coordinate, develop, deliver, manage, and assess data analysis and visualization tools to support tracking, evaluation, analysis and reporting of incidents.
Review, evaluate and recommend to the OUSD (I&S) Director best-of-breed, off-the-shelf and other government-procured information technology solutions to meet OUSD (I&S) technical challenges.
Support the technical oversight of OUSD (I&S) sponsored RDT&E projects and activities.
Support the identification of disparate data-sources and data sharing requirements necessary to coordinate data import and exchange with DoD Components, the Intelligence Community, and NDFDAs
Provides technical expertise, advice, and support required to identify, recommend, and implement Artificial Intelligence/Machine Learning (AI/ML) technologies to improve data management and analysis support for the OUSD (I&S) mission.
Develop and establish positive working relationships with the OUSD(I&S) Special Access Program Central Office, the Special Security Office, the Office of the Director of National Intelligence, and the Combat Support Agencies.
Develop and coordinate OSD staff packages, including for time-sensitive requirements, for Senior Leader support and approval.
Sancorp Consulting LLC shall, in its discretion, modify or adjust the position to meet Sancorp’s changing needs. This job description is not a contract and may be adjusted as deemed appropriate in Sancorp’s sole discretion.
Sancorp Consulting, LLC, is an SDVOSB and SBA 8(a) company seeking highly motivated and qualified professionals. We offer an attractive salary and benefits package that includes: Medical, Dental, life and Disability Insurance; a matching 401K retirement program; and paid time off including federal holidays. Please visit our website for more information at www.sancorpconsulting.com.
Job Types: Full-time, Contract
Pay: From $175,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Signing bonus
Yearly pay
Experience level:
8 years
Schedule:
8 hour shift
Application Question(s):
5-10 years of professional and technical experience as a Data Architect, Data Engineer, or related role developing data models and architectural solutions.
Education:
Bachelor's (Required)
Security clearance:
Top Secret (Required)
Work Location: In person

Health insurance"
246,Data Engineer III,ApolloMed,Remote,"$80,000 - $160,000 a year","Job Title: Data Engineer III
Department: Data
About the Role:
We are currently seeking a highly motivated Data Engineer III. This role will report to the Analytics Manager and work closely with data analysts and clinical leaders to produce deliverables for internal and external clients. With over a million managed lives across the country and terabytes of data generated, our teams need to be continuously equipped with the tools and insights to drive strategy and innovation to further our core values of improving patient outcomes and empowering our providers.
What You'll Do:
Design and develop a data reporting environment across organizational data systems
Identify and promote best practices and patterns for data modeling and provide oversight for activities to report migration and data consolidation
Design, code, manage data & analytics product backlog
Identify technical solutions that achieve efficiency and effectiveness and enable data usage as a strategic business asset
Review team members’ codes for quality and correctness
Collaborate with data analysts and other team members to architect data products and services
Integrate the data platform with analytical tools such as Tableau and PowerBI
Analyze and document processes to create a large shared knowledge base
Minimum qualifications:
Bachelor's degree required in healthcare, analytics, statistics, finance, business, or related field; Master’s degree (MBA, MPH) preferred
Experience with SQL Server or similar relational databases.
Strong understanding of database structures, theories, principles, and practices
Working knowledge with programming or scripting languages such as Python, Spark, and SQL
Experience with data profiling.
Familiarity with normalized, dimensional, star schema and snow flake schematic models
Familiarity with business intelligence exploratory or visualization tools (e.g., Tableau, PowerBI.)
Knowledge of professional software engineering practices and best practices for the full software development life cycle (SDLC), including documentation, coding standards, code reviews, source control management, build processes, testing, and operations.
Strong written and oral communication skills.
Experience with Excel.
You're a great for this role if:
5+ years’ experience developing data pipelines or ETLs
3+ years' experience working with SQL Server Integration Services
3+ years’ experience in managed care or other healthcare data field preferred
Who We Are:
ApolloMed (NASDAQ: AMEH) is a physician-centric, technology-powered healthcare management company. We are building and operating a novel, integrated, value-based healthcare delivery platform to empower our physicians to provide the highest quality of end-to-end care for their patients in a cost-effective manner. Our mission is to combine our clinical experience, best-in-class delivery network, and technological expertise in order to improve patient outcomes, increase access to healthcare, and make the US healthcare system more efficient.
Our platform currently empowers over 10,000 physicians to provide care for over 1.2 million patients nationwide. Our rapid growth and unique position at the intersection of all major healthcare stakeholders (payer, provider, and patient) gives us an unparalleled opportunity to combine clinical and technological expertise in order to improve patient outcomes, increase access to quality healthcare, and reduce the waste in the US healthcare system.
Our Values:
Patients First
Empowering the Independent Provider
Be Innovative
Operate with Integrity & Deliver Excellence
Team of One
Environmental Job Requirements and Working Conditions:
This position is based remotely in the U.S.
The target pay range for this role is: $80,000 - $160,000. The salary range represents our national target range for this role


ApolloMed is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. All employment is decided on the basis of qualifications, merit, and business need. If you require assistance in applying for open positions due to a disability, please email us at humanresourcesdept@networkmedicalmanagement.com to request an accommodation.
Additional Information:
The job description does not constitute an employment agreement between the employer and employee and is subject to change by the employer as the needs of the employer and requirements of the job change."
247,Engineer I (Data Engineering),Ross Stores,"Dublin, CA 94568•Remote","$81,400 - $148,700 a year","Welcome to Ross Stores, Inc., where our differences make us stronger… At Ross and dd’s, inclusion is a way of life. We care about our Associates and the communities we serve and we value their differences. We are committed to building diverse teams and an inclusive culture. We respect and celebrate the diversity of backgrounds, identities, and ideas of those who work and shop with us. Come join us as we continue our diversity, equality and inclusion journey!
GENERAL PURPOSE:
The Data Engineer I plays a critical role in engineering of data solutions that support Ross reporting and analytic needs. As a key member of the Data engineering team, will work on diverse data technologies such as Steramsets, dbt, data ops and others to build insightful, scalable, and robust data pipelines that feed our various analytics platforms.

The base salary range for this role is $81,400 - $148,700. The base salary range is dependent on factors including, but not limited to, experience, skills, qualifications, relevant education, certifications, seniority, and location. The range listed is just one component of the total compensation package for employees. Other rewards vary by position and location.

Benefits offered to all Associates include Accident, Critical Illness, Vision, Auto, Home, and Pet insurance programs; Associate Discount, Identity Protection, Associate Purchase Program, Benefit Hub Discount Mall, Employee Assistance Program and Commuter Benefits, 401K (service requirements), Employee Stock Purchase Program, Ross Cares Fund, College Discounts, Sick Pay (where legally required) and Referral Bonuses. In addition, all Full-Time Associates, including FT Retail Associate, Area Supervisor, Assistant Store Manager and Store Manager, are eligible for extended benefits offered including Medical/Dental/Vision Health Insurance, Legal Insurance, Vacation Buy, Flexible Spending Account, Health Savings Account, Life/ADD Insurance, Long- Term Disability, Enhanced Maternity and Parental Leave Benefit, Vacation Pay accrued at a rate of 15 days/year, 15 Personal and Company Holidays. AS, ASM and SM Associates in Stores and Exempt Corporate and Merchandising roles are also eligible to receive a Bonus based on individual and business performance.

ESSENTIAL FUNCTIONS:

Develop data engineering pipelines that support Ross reporting and analytic needs.

Engineer efficient, adaptable, and scalable data pipelines for moving data from different sources into our Cloud Lakehouse

Understand and analyze business requirements and translate into data pipelines and transformations

Design, build and manage data objects across the data analytics platform.

Develop and deploy performance optimization methodologies

Drive timely and proactive issue identification, escalation & resolution

Collaborate effectively within Data Technology teams, Business Information teams to design and build optimized data flows from source to Data visualization

COMPETENCIES:
People

Building Effect Teams (for managers of People and Projects)

Developing Talent (for managers of people only, N/A for this role/level)

Collaboration
Self

Leading by Example

Communicates Effectively

Ensures Accountability and Execution

Manages Conflict
Business

Business Acumen

Plans, Aligns and Prioritizes

Organizational Agility
With particular emphasis on the following specific position-related competencies:

Problem Solving

Written Communications

Dealing with Ambiguity

Time Management

Composure

Integrity & Trust

Customer Focus

Listening

QUALIFICATIONS AND SPECIAL SKILLS REQUIRED:

5+ years in-depth, data engineering experience and execution of data pipelines, data ops, scripting and SQL queries

1-2 years' experience in modern data architecture that support advanced analytics including Snowflake, Azure, etc. Experience with Snowflake and other Cloud Data Warehousing / Data Lake preferred

Bachelor's Degree in Computer Science, Information Systems, Engineering, Business Analytics, Business Management

Experience in engineering data pipelines using various data technologies - ETL/ELT, big data technologies (Hive, Spark) on large-scale data sets demonstrated through years of experience

Proficient in at least one of these programming languages: Java, Python

Experience with adding data lineage, technical glossary from data pipelines to data catalog tools

Experience in Data analysis - analyzing SQL, Python scripts, ETL/ELT transformation scripts

Experience in data orchestration with experience in tools like Ctrl-M, Apache Airflow. Hands on DevOps/Data Ops experience required

Knowledge/working experience in reporting tools such as MicroStrategy, Power BI would be a plus

Self-driven individual with the ability to work independently or as part of a project team

Experience working in an Agile Environment preferred, Familiarity with Retail domain preferred

Experience with Steramsets, dbt preferred

Strong communication skills are required with the ability to give and receive information, explain complex information in simple terms and maintain a strong customer service approach to all users

Ability to work independently, creatively problem solve complex technical problems

Ability to provide accurate estimates of timeframes necessary to complete Data engineering activities

PHYSICAL REQUIREMENTS/ADA:
This position requires the ability to work in an office environment, including using a computer, attending meetings, working as part of a team, and the ability to communicate with team members and others. Regular attendance also is a requirement of the position.
Vision requirements: Ability to see information in print and/or electronically.
This position may be performed remotely anywhere within the United States. #LI-Remote

SUPERVISORY RESPONSIBILITIES:
N/A

DISCLAIMER:
This job description is a summary of the primary duties and responsibilities of the job and position. It is not intended to be a comprehensive or all-inclusive listing of duties and responsibilities. Contents are subject to change at management's discretion.

Ross is an equal employment opportunity employer. We consider individuals for employment or promotion according to their skills, abilities and experience. We believe that it is an essential part of the Company's overall commitment to attract, hire and develop a strong, talented and diverse workforce. Ross is committed to complying with all applicable laws prohibiting discrimination based on race, color, religious creed, age, national origin, ancestry, physical, mental or developmental disability, sex (which includes pregnancy, childbirth, breastfeeding and medical conditions related to pregnancy, childbirth or breastfeeding), veteran status, military status, marital or registered domestic partnership status, medical condition (including cancer or genetic characteristics), genetic information, gender, gender identity, gender expression, sexual orientation, as well as any other category protected by federal, state or local laws."
248,Data Engineer,PowerReviews,"Chicago, IL 60602•Remote",N,"WHO WE ARE
PowerReviews delivers software that more than 1,000 brands and retailers utilize to collect, display, and syndicate customer ratings and reviews on 5,000+ websites. Now more than ever before, ratings and reviews are an essential resource for consumers as they search and shop online and in-store. Reviews drive traffic, increase sales, and create actionable insights to improve products and services for our clients. With a syndication network that reaches more than 500 million in-market shoppers each month, PowerReviews delivers more reviews to more consumers. And we make it easier than anyone else.
ABOUT THE ROLE
At PowerReviews, we leverage data engineering and machine learning techniques to power automated decisions around our data and publications processes and to help deliver key insights to customers and internal stakeholders. We are looking for a Data Engineer to join our talented engineering team to help support our current data pipelines, platform and solutions as well as unlock new capabilities and constantly look for ways to innovate and improve.
This role will be on an integrated application team where you will be surrounded by talented engineers, data scientists, analysts and a product manager. Collectively the team will prioritize, design and deliver solutions that are both customer facing and for internal stakeholders.
Our Tech Stack
At PowerReviews we use lots of open source software and use Amazon Web Services (almost) exclusively. Our current stack consists of Java, Spring-Boot, Python, CircleCi, PostgreSQL, Neo4J, Snowflake, ElasticSearch, Sumologic/DataDog React.js, Docker, and Ruby on Rails. In AWS we make use of EC2, DynamoDB, RDS, Redshift, S3, Elasticache, Elastic Map Reduce, Lambda, Cloudfront, and others.
WHAT YOU WILL DO
Participate in product and engineering discussions, influence the roadmap and user experience, take ownership and responsibility over new projects and features and turn those ideas into a reality
Design, develop, and support existing and new data pipelines leveraging cloud-based tools (SageMaker, ECS, Glue, DMS, EMR, Lambdas, Snowflake, etc.) to scalably and efficiently process data and a variety of cloud based tooling to supply any given needed metrics (Tableau, DataDog, CloudWatch, etc)
Research, design, and implement strategic automation to assist in big data processing, machine learning predictions, or other customer needs.
Build and improve automation tooling around machine learning and data engineering including (CircleCI, SageMaker, EMR, SnowFlake, DBT, etc)
Become an integral part of a team participating in agile processes, peer review, planning, testing, etc.
WHAT YOU BRING
3+ years of experience building systems that collect and process data in analytics and/or software development in an agile environment
Primary programming experience in a JVM Language (Java, Kotlin, Scala) and/or Python
Experience building, deploying too and supporting cloud solutions in AWS (preferred), GCP, Azure, etc.
Experience building software leveraging big data and/or high traffic and are knowledgeable about data techniques and processes (ETLs, DBT, AWS Glue, Apache Spark, Apache Flume, AWS Athena Pandas, Spacy, etc)
Experience with business intelligence platforms such as Tableau, Looker, PowerBI, AWS QuickSight and the data warehouse's that power them (AWS Redshift, Google BigQuery, SnowFlake, etc)
Experience with the use of SQL or NoSQL for ad hoc analysis.
Excellent communication and effectively communicate with team members as well as non-technical stakeholders and consumers of your work
You have with source tools a such as Git
You have experience with containerization of applications (Docker, ECS, Docker Swarm, Kubernetes, etc)
OUR VALUES
PowerValues represent who we are, what we stand for and how we work with each other and our customers. They are: Accountability, Communication, Collaboration, Continuous Improvement and Customer Focus.
WHAT WE OFFER
Remote Friendly: Do your best work from our downtown Chicago office or from home; it's your choice. In addition to Illinois, you could also live in AL, AZ, CT, FL, GA, IN, KS, MA, MI, MO, NC, NJ, NY, OH, PA, SC, TN, TX, WI.
Real Career Growth Opportunities – At PowerReviews, you have the opportunity to learn a lot and progress quickly. We love to promote our PowerPeople and filled over 30% of our open roles with internal applicants in the past year.
Work With Nice People: At every level - warm, friendly, collaborative, humble.
Work-Life Balance: We believe in taking the time you need, when you need it with unlimited PTO, 10 Company Holidays, Paid Parental Leave, extra bonus days off at the holidays, Summer Fridays
Great Benefits: We offer affordable, comprehensive medical, dental and vision coverage plus many additional benefits to best fit the needs of you and your family.
Company-Matched 401(k)
PowerReviews is an Equal Opportunity Employer (EOE) that welcomes and encourages all applicants to apply regardless of age, race, color, religion, sex, sexual orientation, gender identify and/or expression, national origin, disability, veteran status, marital or parental status, ancestry, citizenship status, pregnancy or other reasons prohibited by law."
249,Data Engineer,Sezzle,"Minneapolis, MN•Remote",N,"About the Role:
We are looking for a Data Engineer who will assist us building, running and improving the data infrastructure that data and engineering teams use to power their services. Your duties will include the development, testing, and maintenance of data tooling and services, using a combination of cloud products, open source tools and internal applications. You should be able to build high-quality, scalable solutions for a variety of problems.
Our Company:
Sezzle is a cutting-edge fintech company whose mission is to financially empower young consumers. Only one in three millennials own a credit card, and the vast majority of millennials possess a subprime credit score or no score at all. To address these problems, Sezzle has built a payment platform that increases purchasing power for consumers by offering interest-free installment plans at online stores. This increase in purchasing power for consumers leads to increased sales and basket sizes for the thousands of eCommerce merchants that currently work with Sezzle.
What Makes Working at Sezzle Awesome?
At Sezzle, we are more than just brilliant engineers, passionate data enthusiasts, out-of-the-box thinkers, and determined innovators; we are skilled musicians, yogis, cyclists, chefs, golfers, dog-lovers, and rock-climbers. We believe in surrounding ourselves with not only the best and the brightest individuals, but those that are unique and purpose-driven in all that they do. Our culture is not defined by a certain set of perks designed to give the illusion of the traditional startup culture, but rather, it is the visible example living in every employee that we hire.
Key Responsibilities Include:
Plan, design and build tools and services that improve our internal data infrastructure platform, using Go, Python, AWS, Terraform, and Kubernetes.
Be on a Pagerduty on-call rotation to respond to production incidents.
Develop monitoring and alerting of our data infrastructure to detect problems.
Document the actions you take and produce both runbooks and automation to reduce day to day toil.
Perform ongoing maintenance of our data infrastructure, such as apply upgrades.
Take part in the postmortem reviews, suggesting ways we can improve the reliability of our platform.
Assist product developers in debugging and triaging production issues.
Minimum Requirements:
Bachelor's in Computer Science (preferred) or equivalent tech-related experience
Preferred Knowledge and Skills:
Excellent knowledge of Relational Databases, SQL and ORM technologies
Experience with AWS, Redshift, and S3
Experience with Python and Golang
Basic knowledge of Docker and Kubernetes
Basic knowledge of a Microservice Architecture
Familiarity with deployment/provisioning tools like Terraform, Helm, Ansible
Close familiarity with software engineering tools, software development methodology, and release processes
Experience documenting requirements and specification
About You:
A+ character. We are team-first here at Sezzle.
A hard-working mentality. It's early and there is still a lot to build.
An excellent communicator.
A fun attitude. Life's too short. We can have fun while we work hard on cool things.
Smarts. We need people that are smart enough to make decisions on their own and also smart enough to know when they need input from others.
Perks & Benefits:
Competitive salary and benefits
Generous stock options
Medical, dental and vision insurance
Life and long term disability insurance
401k with 100% match
Collaborative workspace, commuter benefits, full-stocked kitchen, weekly lunches and much more!
The opportunity to join Minneapolis's fastest growing startup alongside a team of motivated and driven individuals
Sezzle provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, creed, gender, sex (including pregnancy, childbirth, and related medical conditions), sexual orientation, gender identity, national origin, age, disability, genetic information or characteristics, marital status, familial status, veteran or military status, status regarding public assistance, membership or activity in a local commission, or any other protected status in accordance with applicable federal, state and local laws.
#LI-Remote"
250,Data Engineer,DrivenIQ,"Towson, MD 21204•Remote",N,"Company Description

DrivenIQ is a data intelligence disruptor! Using localized data and geo-based intel, we know who is in market, ready to buy, and we can ensure that organizations and/or agency partners align their media spend and advertising platforms with true data to drive effective ROI for the client.
In this hand’s-on position, the Data Engineer will work very closely with the support and development team and apply knowledge of data engineering to design and develop enterprise analytical solutions. As a Data Engineer, you will be responsible for designing, building, and maintaining the data pipelines and architectures that support our data intelligence products and services. The individual filling this role will be expected to hit the ground running on the delivery of owning and running different data requests. In this role, you will be expected to design and develop proof-of-concept solutions in support of presales activities as well as support the development of standardized analytical offerings.
This position is 100% remote.

Job Description

What you’ll do:
Design and implement data pipelines to ingest, transform, and store large volumes of structured and unstructured data from various sources
Own, handle, and run data requests with an understanding of data matrix’s
Collaborate with data scientists and product owners to understand their data needs and develop solutions to support their work
Build and maintain data lakes and data warehouses to support data analysis and reporting
Optimize data pipelines for performance and scalability.
Develop and maintain documentation for data pipelines and architectures
Stay up-to-date with the latest data engineering technologies and best practices
Work closely with the support team to ensure understanding of all client data
Define data engineering strategies to meet the demands of business requirements
Define the technical requirements of the data engineering solutions
Define the data requirements of the data engineering solutions
Conduct sophisticated analyses and build models, as required
Translate data engineering results into clear business focused deliverables for decision makers
Lead project plans and works with development team to deploy models into operational systems
Other job duties may be assigned by supervisor

Qualifications

What you’ll need:
Bachelor's or Master's degree in Computer Science, Data Science, or a related field
3+ years of experience in data engineering
3+ years of experience in Python
3+ years of experience in SQL
3+ years of experience in Athena
Experience with cloud technologies such as AWS
Experience with Firehorse/Kinesis
Experience with machine learning
Experience with big data technologies such as Hadoop, Spark, or Flink
Experience with data storage and processing technologies such as SQL, NoSQL, and columnar databases
Strong problem-solving, communication, and presentation skills
Strong business focus with the ability to excel at connecting business requirements to data engineering objectives
Extensive experience working with IT Development teams to implement analytical application/solution development
Must be able to successfully pass a background screen check

Additional Information

What We Offer:
Competitive Pay
Holidays + Unlimited PTO. It’s all about balance and we trust you will get your work done!
Medical, Dental, Vision plans available
Short-Term and Long-Term Disability Plans, Company Sponsored
Complimentary $25k Basic Life Insurance and AD&D
Learning and growth opportunities
Remote work - work comfortably from your home
This job advertisement in no way states or implies that these are the only duties and responsibilities to be performed by this employee. A full job description will be provided if and when a job offer is presented. The employee will be required to follow any other instructions and to perform any other duties and responsibilities upon the request of a supervisor.
DrivenIQ is an Equal Opportunity Employer. Minorities, women, veterans, and individuals with disabilities are encouraged to apply."
251,Data Operations Engineer,Hudson River Trading,"1 Financial Sq, New York, NY 10005","$150,000 - $250,000 a year","Data is at the core of everything we do here at HRT. We excel at deriving deep insights from all types of data, allowing us to achieve consistent success in a competitive market. This is a challenging and exciting role that provides opportunities to contribute to the success of HRT.
The Data Operations Engineer will write software, explore data, work closely with our research and trading teams, and interact with a variety of external partners such as data providers, brokers, and exchanges.
The role requires strong data engineering skills, experience wrangling data, and the ability to provide exceptional support in a production trading environment.
Responsibilities
Data Engineering: Write tools to classify, onboard, and reconcile data. Onboard datasets using Airflow, explore data, and automate tasks using a modern Python data stack.
Production Support: Provide proactive oversight of our data pipeline, handle inquiries from internal customers, and manage issues through resolution.
Vendor Technical Specialist: Work closely with financial and alternative data vendors to acquire data and solve issues, including Bloomberg, Refinitiv, and many others. Be the internal expert on their various delivery platforms (FTP, APIs, etc) and databases.
Data Analysis: Parse, analyze, and understand data sets. Perform data reconciliations, validation, and quality checks. Identify and develop new processes within the data request process to enrich data.

Skills
Strong working knowledge of Python
Comfortable with the Linux command line
Experienced in at least one SQL dialect (PostgreSQL, MSSQL, MYSQL) and able to use others as needed
Able to provide technical support in a production trading environment
Track record of being detail-oriented (you love when everything lines up!)
Excellent communication skills, oral and written
Profile
A minimum of 2-4 years of experience wrangling data is preferred
Strong programming experience in Python
Experience managing ETL pipelines
Experience with financial datasets (e.g. Refinitiv, S&P, Bloomberg) is a plus
You enjoy problem solving and researching large datasets to resolve complex issues
You thrive when collaborating with people who motivate you and make you better
Annual base salary range of $150,000 to $250,000. Pay (base and bonus) may vary depending on job-related skills and experience. A sign-on and discretionary performance bonus may be provided as part of the total compensation package, in addition to company-paid medical and/or other benefits.
Culture
Hudson River Trading (HRT) brings a scientific approach to trading financial products. We have built one of the world's most sophisticated computing environments for research and development. Our researchers are at the forefront of innovation in the world of algorithmic trading.

At HRT we welcome a variety of expertise: mathematics and computer science, physics and engineering, media and tech. We're a community of self-starters who are motivated by the excitement of being at the cutting edge of automation in every part of our organization—from trading, to business operations, to recruiting and beyond. We value openness and transparency, and celebrate great ideas from HRT veterans and new hires alike. At HRT we're friends and colleagues – whether we are sharing a meal, playing the latest board game, or writing elegant code. We embrace a culture of togetherness that extends far beyond the walls of our office.

Feel like you belong at HRT? Our goal is to find the best people and bring them together to do great work in a place where everyone is valued. HRT is proud of our diverse staff; we have offices all over the globe and benefit from our varied and unique perspectives. HRT is an equal opportunity employer; so whoever you are we'd love to get to know you."
252,Software Engineer I- Big Data (Remote),Vericast,"Austin, TX•Remote","$80,000 - $90,000 a year","Company Description

Vericast is a premier marketing solutions company that accelerates profitable revenue growth for thousands of businesses businesses it serves directly by influencing consumer purchasing and transaction behavior at scale while engaging with over 120 million households daily. We are recognized as leading providers of incentives, advertising, marketing services, transaction solutions, customer data and cross-channel campaign management, and intelligent media delivery that create millions of customer touch points annually for their clients. For more information, visit http://www.vericast.com or follow Vericast on LinkedIn.

Job Description

JOB SUMMARY
A Software Engineer I is energized by the thought of developing new system stacks and tools for big data ingestion, processing, and analytics on a multi-petabyte infrastructure. This individual will work on a team of talented engineers responsible for reporting on the delivery of our campaigns and handling technical integrations and data ingestion. They will develop products that help validate our media buys, ingest data into our platform, and generate datasets and reporting for our teams to analyze and monitor delivery and performance of our campaigns.

The Big Data Platform Team owns and operates the world-class big data processing infrastructure that over a dozen engineering teams use to power their 24/7 technical marketing products. We own and operate a large hadoop+spark processing cloud on which we store 6PB of data, and run 30 thousand jobs every day. We write a fabric of java microservices and bots to manage all those jobs, extend hadoop's functionality, and help us operate and optimize our investment. We write custom data streaming services that ingest and curate 100TB of data each day that drives the entire advertising data ecosystem. We also participate in all our users' groundbreaking workflow projects so that we can constantly stay abreast of our developer's tooling needs.
What you're like:
This position is perfect for a far-sighted engineer who always wants to be the first to apply cutting-edge technologies to solve complex business and engineering problems. You work throughout the software lifecycle including requirements analysis, development, testing and operations. If you are energized by the thought of developing new system stacks and tools for big data processing and analytics we want to talk to you. If you have worked on big data engineering, cloud migration, or infrastructure tooling projects, we want to talk to you. If you have ever worked on a service mesh or a collection of data workflows and thought 'I could make this better', we want to talk to you!
KEY DUTIES/RESPONSIBILITIES
Proficiency in Java/Scala/Python programming; experience with microservices; exposure to Spring Boot
Curiosity to learn and apply new technologies
Excellent problem-solving abilities
Excellent verbal and written communication skills
Experience with agile development methodologies
Process and analyze high volume data to mine valuable chunks of information using a combination of Scala, Python and Spark
Analyze input and output data to discover interesting signals, validate outputs, gain understanding of systems
Build powerful systems from simple building blocks while managing the complexity
Contribute to our team's continuous efforts to improve quality and efficiency of development platforms, tools, and processes
Strong in Java, Scala and/or Python programming language.
Exposure to data process with Hadoop, Spark, Kafka and/or Spring Boot micro services

Qualifications

EDUCATION
Bachelor's Degree in Computer Science or other technical discipline (e.g., Engineering, Mathematics, or Physics) (Required)
In lieu of the above education requirements, a combination of experience and education will be considered.
Two-year degree in Computer Science with relevant and high-performing work experience would be considered.
EXPERIENCE
0 - 2 years Relevant Experience (Required)
KNOWLEDGE/SKILLS/ABILITIES
0-2 years Skills required: programming, design, testing, standard platform technologies (e.g. Microsoft, Java, Python, etc), SQL databases, independent thought, and methodical work habits.
0-2 years Skills desired: big data techniques; high scalability computing techniques; ability to program in both web technologies (web-based UI; web services; etc.) and/or back-end (Java, C#, or C++) services.
Learning team service architecture

Additional Information

Salary: $80,000 - $90,000
Position is eligible for an annual bonus incentive program

The ultimate compensation offered for the position will depend upon several factors such as skill level, cost of living, experience, and responsibilities.
All team members are responsible for demonstrating the company's Core Values at all times and for using Performance Excellence principles to continuously improve effectiveness, efficiency, products, and services. This includes, but is not limited to, participating on improvement teams, recommending, and implementing improvement ideas, and participating in training and other activities to keep up to date on processes, information, etc.
All team members are responsible for supporting and complying with internal and external audits, to include providing information, performing assigned tasks to ensure compliance, and preparing and maintaining evidence that key duties identified as internal controls have been performed.
All team members are responsible for supporting and complying with safety and security policies to promote a healthy working environment.
Vericast offers a generous total rewards benefits package that includes medical, dental and vision coverage, 401K and flexible PTO. A wide variety of additional benefits like life insurance, employee assistance and pet insurance are also available, not to mention smart and friendly coworkers!
At Vericast, we don’t just accept differences - we celebrate them, we support them, and we thrive on them for the benefit of our employees, our clients, and our community. As an Equal Opportunity employer, Vericast considers applicants for all positions without regard to race, color, creed, religion, national origin or ancestry, sex, sexual orientation, gender identity, age, disability, genetic information, veteran status, or any other classifications protected by law. Applicants who have disabilities may request that accommodations be made in order to complete the selection process by contacting our Talent Acquisition team at talentacquisition@vericast.com. EEO is the law. To review your rights under Equal Employment Opportunity please visit: www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf."
253,Senior Data Engineer,Marketing Samurai,"New York, NY•Remote","$125,000 - $180,000 a year","Marketing Samurai helps brands amplify their stories, grow their businesses and delight their customers through a suite of digital marketing services.
The word Samurai originally meant “one who serves” and referred to men of noble birth assigned to guard members of the Imperial Court in old world Japan. The best were fiercely loyal to their masters and true to the unwritten code of chivalrous behavior known today as Bushido.
Virtuous or villainous, the samurai emerged as the colorful central figures of Japanese history: a romantic archetype akin to Europe’s medieval knights or the American cowboy of the Wild West. After civil society was at peace, their role as professional fighters disappeared, and they became less preoccupied with martial training and more concerned with spiritual development, teaching, and the arts.
It is these practices that have helped give Marketing Samurai its identity. An agency that strives to be kind, be useful, be helpful, to educate, to treat everybody with respect.
With a wealth of data collected on our clients and their customers, we are seeking a senior data engineer to build and drive data engineering initiatives. Specifically, we need someone who can:
- analyze and integrate several silos of data into meaningful, insightful reports
- work across ad platforms to contribute to the improvement of many different campaigns
- build new distributed databases across ad experiences
- help drive optimization, testing and tooling to improve reliability and data quality
- apply a data centric approach to all platforms, pipelines and engineering activities
- immerse themselves in engineering for better data visibility and unearth viable channels for improvement
Who You Are
You love Data Engineering
You have a strong understanding of data systems and databases
You are knowledgeable and passionate about improving and building new distributed data pipelines
You are knowledgeable about data modeling, data access, and data storage techniques
You are familiar with current engineering practices, and are curious about new technologies that help derive insights and value from data
You have experience working on and building distributed data pipelines that ingest huge amounts of data across multiple sources and brands
You have experience working with SQL
You have experience working with Python, Java or Scala
The salary range encompasses multiple levels dependent on experience and work history. This is determined during the interview process.
Marketing Samurai is an equal opportunity employer. You are welcome for who you are, no matter where you come from, what you look like, or which sports team you support.
The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be forward-thinking! So bring us your personal experience, your perspectives, and your background.
To apply, please send your latest resume and cover letter via this site or to info@marketing-samurai.com.
We encourage applicants to visit https://marketing-samurai.com/ to learn more about our company, clients and culture.
Job Type: Full-time
Salary: $125,000.00 - $180,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Compensation package:
Yearly pay
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
SQL: 5 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Remote

Health insurance"
254,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
255,Data Engineer |Training|,PCS GLOBAL TECH,"San Diego, CA","$60,000 - $80,000 a year","Responsibilities
Analyze and organize raw data
Build data systems and pipelines
Evaluate business needs and objectives
Interpret trends and patterns
Conduct complex data analysis and report on results
Prepare data for prescriptive and predictive modeling
Build algorithms and prototypes
Combine raw information from different sources
Explore ways to enhance data quality and reliability
Identify opportunities for data acquisition
Develop analytical tools and programs
Collaborate with data scientists and architects on several projects
Requirements
Previous experience as a data engineer or in a similar role (such as internships)
Technical expertise with data models, data mining, and segmentation techniques
Knowledge of programming languages (e.g. Java and Python)
Hands-on experience with SQL database design (MUST HAVE)
Great numerical and analytical skills
Bachelor’s degree in Computer Science, IT, or similar field
Job Types: Full-time, Contract
Pay: $60,000.00 - $80,000.00 per year
Experience level:
1 year
Under 1 year
Experience:
SQL: 1 year (Preferred)
Work Location: On the road"
256,Data Engineer II,Swire Coca-Cola,"Draper, UT",N,"Who is Swire Coca-Cola? We are one of the largest bottlers of Coca-Cola and other beverage brands in America. Our 7,200 hardworking and passionate employees produce and distribute more than 50 brands and flavors across 13 states. We have as many careers as we have flavors, and regardless of which role you choose, you have a direct impact on Swire Coca-Cola, our products, and the communities we call home.
At Swire Coca-Cola, our commitment to excellence is what guides our actions with our employees. We offer a comprehensive benefits package for full-time associates which includes: Medical, Dental, Vision, 401K, Paid Vacation, Paid Holidays and Company Discounts.

As part of Swire’s Enterprise Data Warehouse Team, the Data Engineer will help business users and analysts throughout the organization access the data they need to operate and grow the business. The Data Engineer will work with end users and IT staff to gather requirements, build and maintain secure, compliant data processing pipelines using different services, tools and techniques. They will ensure that data pipelines and data objects are high-performing, efficient, organized, and reliable.

Responsibilities:
Build and maintain secure, compliant data processing pipelines, tables, views, procedures and datasets
Integrate, transform, and consolidate data from various structured and unstructured data systems into structures that are suitable for building analytics solutions and/or performing analysis
Ensure that data pipelines tables, views, procedures, and datasets are high-performing, efficient, organized, and reliable
Design, develop, operate and tune data & analytics services (various)
Monitor the data pipelines and data stores responding to issues in a timely manner
Monitor, troubleshoot and resolve issues in production environments
Help business users and analysts understand what data are available and how to interpret them.
Coordinate with CONA BI team to ensure Swire’s solution integrates with CONA’s solution.
Attend regular meetings with and participating in CONA BI data warehouse design and development activities and adapting Swire’s solution to be compatible.
Collaborate with the CONA data warehouse team, third-party consultants and third-party data/service providers during the development and maintenance of Swire’s data warehouse.
Requirements:
Bachelor’s degree in Computer Science or 6+ years of directly-related work experience
Preferred: Master’s degree in Computer Science (or similar) or 4+ years of directly-related work experience
4+ years in designing, developing, operating and maintaining various data pipelines and data stores following best practices for the given environment
4+ years of strong SQL scripting knowledge
4+ years working with data processing languages, such as SQL, Scala or Python (or Similar)
2+ years working with Cloud-based Data Solutions like Azure SQL Database/Azure Synapse Analytics/Snowflake
2+ years working with ETL tools like Azure Data Factory, DBT, SSIS (or similar)
Good to have knowledge with Power BI or other similar reporting tool(s)
Knowledge of NoSQL databases
Experience with parallel processing and data architecture patterns
Experience with relational and dimensional data modelling
Onsite standard work week to ensure face-to-face interaction with team members and Swire management.
Team coordination skills and a good team player
Swire Coca-Cola is an equal employment opportunity and affirmative action employer that participates in the E-Verify program as required by law. All qualified applicants will receive consideration for employment without regard race, color, religion, sex, sexual orientation, gender identity, national origin, disability, Veteran status or other legally protected characteristics."
257,Data Engineer II,Apptrics LLC,"Seattle, WA",$58 - $60 an hour,"Job title : Data Engineer II
Location : San Francisco, CA AND Seattle ,WA
Duration : 12 Months
Day 1 onsite with Hybrid
Qualifications :
Bachelor or Master’s degree in Computer Science, Engineering, Information Systems or relevant degree is required
1-3 years of professional work experience designing and implementing data pipelines in on-prem and cloud environments ( S3, EKS )
Experience with SQL/Relational databases.
Experience with manipulating structured and unstructured data.
Experience with distributed data systems such as Hadoop and related technologies (Spark, Trino, etc.).
Background in both programming languages (Python & Scala ).
Experience working with databases that power APIs for front-end applications.
Experience with modern schedulers ( Airflow )
Responsibilities
Support, Design, develop, test, deploy, maintain and improve data pipeline
Designing and developing data processing techniques: automating manual processes, data delivery, data validation, data quality and integrity
Support, test, deploy, maintain the AWS ecosystem from an infrastructure standpoint ( Hybrid Cloud SDK upgrade, security fixes etc)
Communicate effectively with customers / team members & help with site up challenges.
Must have skills:
Python
SQL
Dashboarding tools ( Superset, Tableau )
Spark
Scala
AWS ecosystem ( S3, EKS(not a requirement)
Trino
Airflow
Kafka
Hive
Nice to have skills:
Statistics Background
Job Type: Contract
Pay: $58.00 - $60.00 per hour
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
San Francisco, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
designing and implementing data pipelines: 5 years (Required)
SQL: 7 years (Required)
cloud environments ( S3, EKS ): 5 years (Required)
manipulating structured and unstructured data.: 6 years (Required)
data systems such as Hadoop: 5 years (Required)
programming languages (Python & Scala: 4 years (Required)
databases that power APIs for front-end applications.: 5 years (Required)
• Spark • Scala • AWS ecosystem: 5 years (Required)
• Trino • Airflow • Kafka • Hive: 5 years (Required)
Work Location: In person"
258,Data Engineer,PowerReviews,"Chicago, IL 60602•Remote",N,"WHO WE ARE
PowerReviews delivers software that more than 1,000 brands and retailers utilize to collect, display, and syndicate customer ratings and reviews on 5,000+ websites. Now more than ever before, ratings and reviews are an essential resource for consumers as they search and shop online and in-store. Reviews drive traffic, increase sales, and create actionable insights to improve products and services for our clients. With a syndication network that reaches more than 500 million in-market shoppers each month, PowerReviews delivers more reviews to more consumers. And we make it easier than anyone else.
ABOUT THE ROLE
At PowerReviews, we leverage data engineering and machine learning techniques to power automated decisions around our data and publications processes and to help deliver key insights to customers and internal stakeholders. We are looking for a Data Engineer to join our talented engineering team to help support our current data pipelines, platform and solutions as well as unlock new capabilities and constantly look for ways to innovate and improve.
This role will be on an integrated application team where you will be surrounded by talented engineers, data scientists, analysts and a product manager. Collectively the team will prioritize, design and deliver solutions that are both customer facing and for internal stakeholders.
Our Tech Stack
At PowerReviews we use lots of open source software and use Amazon Web Services (almost) exclusively. Our current stack consists of Java, Spring-Boot, Python, CircleCi, PostgreSQL, Neo4J, Snowflake, ElasticSearch, Sumologic/DataDog React.js, Docker, and Ruby on Rails. In AWS we make use of EC2, DynamoDB, RDS, Redshift, S3, Elasticache, Elastic Map Reduce, Lambda, Cloudfront, and others.
WHAT YOU WILL DO
Participate in product and engineering discussions, influence the roadmap and user experience, take ownership and responsibility over new projects and features and turn those ideas into a reality
Design, develop, and support existing and new data pipelines leveraging cloud-based tools (SageMaker, ECS, Glue, DMS, EMR, Lambdas, Snowflake, etc.) to scalably and efficiently process data and a variety of cloud based tooling to supply any given needed metrics (Tableau, DataDog, CloudWatch, etc)
Research, design, and implement strategic automation to assist in big data processing, machine learning predictions, or other customer needs.
Build and improve automation tooling around machine learning and data engineering including (CircleCI, SageMaker, EMR, SnowFlake, DBT, etc)
Become an integral part of a team participating in agile processes, peer review, planning, testing, etc.
WHAT YOU BRING
3+ years of experience building systems that collect and process data in analytics and/or software development in an agile environment
Primary programming experience in a JVM Language (Java, Kotlin, Scala) and/or Python
Experience building, deploying too and supporting cloud solutions in AWS (preferred), GCP, Azure, etc.
Experience building software leveraging big data and/or high traffic and are knowledgeable about data techniques and processes (ETLs, DBT, AWS Glue, Apache Spark, Apache Flume, AWS Athena Pandas, Spacy, etc)
Experience with business intelligence platforms such as Tableau, Looker, PowerBI, AWS QuickSight and the data warehouse's that power them (AWS Redshift, Google BigQuery, SnowFlake, etc)
Experience with the use of SQL or NoSQL for ad hoc analysis.
Excellent communication and effectively communicate with team members as well as non-technical stakeholders and consumers of your work
You have with source tools a such as Git
You have experience with containerization of applications (Docker, ECS, Docker Swarm, Kubernetes, etc)
OUR VALUES
PowerValues represent who we are, what we stand for and how we work with each other and our customers. They are: Accountability, Communication, Collaboration, Continuous Improvement and Customer Focus.
WHAT WE OFFER
Remote Friendly: Do your best work from our downtown Chicago office or from home; it's your choice. In addition to Illinois, you could also live in AL, AZ, CT, FL, GA, IN, KS, MA, MI, MO, NC, NJ, NY, OH, PA, SC, TN, TX, WI.
Real Career Growth Opportunities – At PowerReviews, you have the opportunity to learn a lot and progress quickly. We love to promote our PowerPeople and filled over 30% of our open roles with internal applicants in the past year.
Work With Nice People: At every level - warm, friendly, collaborative, humble.
Work-Life Balance: We believe in taking the time you need, when you need it with unlimited PTO, 10 Company Holidays, Paid Parental Leave, extra bonus days off at the holidays, Summer Fridays
Great Benefits: We offer affordable, comprehensive medical, dental and vision coverage plus many additional benefits to best fit the needs of you and your family.
Company-Matched 401(k)
PowerReviews is an Equal Opportunity Employer (EOE) that welcomes and encourages all applicants to apply regardless of age, race, color, religion, sex, sexual orientation, gender identify and/or expression, national origin, disability, veteran status, marital or parental status, ancestry, citizenship status, pregnancy or other reasons prohibited by law."
259,Sr. Data Analytics Engineer,Avalara,North Carolina•Remote,"$157,300 - $259,500 a year","Overview:
We are building cloud-based tax compliance solutions to handle every transaction in the world. Imagine every transaction you make - every tank of gas, cup of coffee, or pair of sneakers, every movie ticket, or streamed song, every sensor-to-sensor ping. Nearly every time you make a purchase, physical or digital, there is an accompanying unique and nuanced tax compliance calculation.
Responsibilities:
The Senior Data Analytics Engineer will be responsible for gathering business requirements, understanding complex product, business and engineering challenges, compose and prioritize research projects and then execute them in partnership with other data scientists, data analysts as well as leverage the work of our data engineering team.

The individual will have direct ownership of corporate metrics, ensure accuracy, timeliness and intent of all published data products within area of ownership by our team and all other teams within the company. Will also have executive visibility and influence capability.

Learn the tax business domain knowledge and data.
Work with the product managers, DBA teams and broader engineering teams build scalable data orchestration, transformation and reporting streams that can capture and prepare billions of transactions per day for customer reporting.
Build strong and captivating data visualizations in Dash/PowerBI/Tableau/R.
Develop and manage end-to-end project plans and ensure on-time delivery.
Communicate status and big picture to the project team and management.
Work with business and engineering teams to identify scope, constraints, dependencies, and risks.
Identify risks and opportunities across the business and drive solutions.
Qualifications:
Minimum of 5 years combined work experience in data engineer, business intelligence and in-app embedded analytics
Strong experience with cloud data engineering teams
Advanced SQL, scripting (Python or R) and advanced data visualization experience
Implement streaming and batch ETL pipelines supporting both internal and external data sources
Data warehouse modeling and architecture with strong requirements gathering and technical analysis
Technical expertise with AWS, DBT, and Airflow
Bachelor's degree in Computer Science or Engineering
Experience with requirements of data security, data privacy and legal / jurisdictional compliance and governance
Experience managing efforts in distributed systems and/or developing large scale web applications
Proven ability to combine business acumen, technical acumen and process expertise to define client (internal/external) engagement and program execution
Ability to communicate effectively with technical and non-technical stakeholders across multiple business units
Strong communication skills, data driven product and corporate excellence culture
Excellent problem-solving skills

Preferred Skills
Minimum of 5 years work experience with AWS data pipeline and Snowflake cloud DW
Experience building powerful dashboards which are embedded in web and mobile applications

Pay Range Details
The base pay range(s) below are provided in compliance with state specific laws. Pay ranges may be
different in other locations.

Colorado: $157,300-$259,500 (annually)
Washington: $157,300-$286,900 (annually)
California: $157,300-$314,200 (annually)
NYC: $173,900-$314,200 (annually)

The pay range above is the general base pay range for a successful candidate in the state listed. The
successful candidate’s actual salary/wage may be based on various factors, such as geographic
location, candidate experience and qualifications, as well as market and business considerations.
This role is eligible for an annual bonus based on individual and company performance, depending on
the terms of the applicable plan and the employee’s role.

Benefits
Avalara’s benefits for eligible employees includes company benefits such as medical, dental, and
vision coverage, life, AD&D, and disability insurance, a 401(k) retirement plan, 17 days of paid
time off annually, 12 paid holidays, paid parental leave, an employee assistance program, and
subsidized transportation options for commuters.

All benefits are subject to eligibility requirements and Avalara reserves the right to modify or change
these benefits programs at any time, with or without notice, unless otherwise required by law.
About Avalara:
About Avalara:
We’re building cloud-based tax compliance solutions to handle every transaction in the world. Imagine every transaction you make — every tank of gas, cup of coffee, or pair of sneakers, every movie ticket, meal kit, or streamed song, every sensor-to-sensor ping. Nearly every time you make a purchase, physical or digital, there’s an accompanying unique and nuanced tax compliance calculation. The logic behind calculating taxes — the rules, rates, and boundaries is a global, layered, three-dimensional mess of complexity, with compliance dictated by governments and applied by every business, every day.

Avalara works with businesses of all sizes, all over the world — from corner stores to gigantic global retailers — to calculate tax accurately and automatically, at speeds measured in milliseconds. That’s a massive technical challenge, in terms of scale, reliability, and complexity, and we do it better than anyone. That’s why we’re growing fast. Headquartered in Seattle, Avalara has offices across the U.S. and around the world, in Brazil, Canada, India, U.K, Belgium and across Europe.

Equal Opportunities:
Avalara is an Equal Opportunity Employer. All qualified candidates will receive consideration for employment without regard to race, colour, creed, religion, age, gender, national orientation, disability, sexual orientation, US Veteran status, or any other factor protected by law."
260,Data Science Engineer,PrizePicks,"Atlanta, GA 30309•Remote",N,"PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on becoming the most exciting compliment to your sports viewing experience. We believe establishing a strong culture and investing in our people are paramount to our team continuing to achieve our goals and reach new heights.
The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data streaming infrastructures - you will enable PrizePicks to offer real-time priced markets within the product.
If this resonates with you, join the hundreds of us building a shared vision for the future of sports entertainment.
What you'll do:
Create and maintain optimal sport data stream pipeline architecture, ensuring data reliability
Enable the creation and distribution of real-time, in-game micro market offerings to the product
Lead the design and implementation of the data stack required real-time pricing models
What you have:
Strong organizational skills.
Experience building and optimizing data streaming pipelines and infrastructure.
Experience supporting real-time data science modeling workflows.
Experience exposing real-time predictive model outputs to a production service.
5+ years of experience in a data analytics or data engineering role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Experience using the following software/tools:
Relational SQL databases like Postgres
Object-oriented/object function scripting languages like Python
Data streaming technologies: Kafka, Redis, etc...
Cloud Technologies: AWS, EC2, EMR, RDS, Redshift, GCP
Data modeling tools like DBT
Preferred experience with big data tools: Hadoop, Spark, BQ, etc..
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect
Where you’ll live:
We are based in Atlanta, GA but we are a remote organization so live where you prefer (just in the U.S.)
Benefits you’ll receive:
In addition to your competitive salary, medical/dental/vision coverage plans and matching 401(k), we’ll shower you with perks including:
Break room with ping pong, end endless snacks and in-office lunch once a week
Unlimited PTO to encourage a healthy work/life balance
A modern work schedule focused on getting the job done well, not hours clocked
Company and team outings, we encourage a tight-knit workplace
Open office layout promotes collaboration, conversation and teamwork to drive results
Significant opportunity for growth and career development
Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or takeover sponsorship or an employment Visa at this time.
PrizePicks is an Equal Opportunity Employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.

#LI-remote"
261,Data Engineer Coop,Entegris,United States,N,"Req Id: 23215
Location(s): US
Job Type: Temp/Intern/Apprent
Job Function: Corporate
Shift:
Company Overview and Values
Why work at Entegris?

Lead. Inspire. Innovate. Define Your Future.

Not everyone who works for a global company shares the same background, experiences and perspectives. We leverage the differences of our employees to bring new ideas to the table. Every employee throughout the company is encouraged to share input on projects and initiatives. Our decision making process is truly a collaborative effort as we realize there are leaders at every level of the organization. We put our values at the core of how we operate as an organization — not just when it’s convenient, but in a lasting and meaningful way. We want the time and energy you spend here to have a positive impact on your life inside and outside of the office.
Entegris is a values-driven culture and our employees rally around our core PACE values:

People
Accountability
Creativity
Excellence

The Role:
The Data Engineer Coop is responsible for designing, implementing and supporting systems for collecting, storing and analyzing data for our big data analytics cloud platform to meet short, medium and long-term digital transformation and enterprise initiatives
The Data Engineer Coop specializes in working in a variety of settings building solutions to collect, manage and convert raw data into usable information for data science and analytical purposes
In this role you will serve as a technical engineer utilizing tools (e.g., Python), data ingestion, testing, and deployment of data to the Google Cloud Platform to enable our digital transformation strategy and harness the value of data systems strategy, needed to advance manufacturing operational excellence, customer engagement

In This Role You Will:
Work with business experts and IT Business Analysts to understand functional requirements and interact with other cross-functional teams to design, develop, test, and release features
Analyze and organize raw data into datasets for business needs
Build data systems and pipelines
Develop algorithms to transform data into useful, actionable information Interpret trends and patterns
Help provide analytic support with BI and Data Science technologies
Ensure compliance with data governance and security policies
Be a member of Agile Scrum teams with planning, scoping and creation of technical solutions for the new product capabilities, through to continuous delivery to production
Engage with business stakeholders to understand required capabilities, integrating business knowledge with technical solutions.

Traits We Believe Make a Strong Candidate:
Desired Major: Bachelor’s degree in Data Engineering, Computer Science or Business discipline or equivalent work experience
Familiar with relevant languages such as Python, R, XML, JSON, and SQL
Familiar with APIs such as REST
Experience with SAP Dataservices would be beneficial
Fundamental understanding of data models, data mining and segmentation techniques
Great numerical and analytical skills.

Your Succes Will Be Measured By:
Sets an example for others following our PACE values; People and Teamwork, Accountability, Integrity, and Trust, Creativity and Innovation, and Dedication to Excellence
Demonstrates and promotes the capability and willingness to adapt to new, different, and changing requirements and assignments
Self-motivated, curious, resourceful and creative problem solver
Implementing sustainable processes and change in the organization
Collaborate cross functionally in a matrixed organization and help develop internal competencies
Build trusted advisor relationships that helps develop the culture of excellence

Entegris reserves the right to make changes to its plans at any time. Our offer of employment is subject to your successful completion of the company paid pre-employment drug screen and background check. Your employment with Entegris is at-will and either party can terminate the employment relationship at any time with or without cause and with or without notice.

At Entegris we are committed to providing equal opportunity to all employees and applicants. Our policy is to recruit, hire, train, and reward employees for their individual abilities, achievements and experience without regard to race, color, religion, sexual orientation, age, national origin, disability, marital or military status.

Entegris strongly encourages all of its employees to be vaccinated against COVID-19. At Entegris, COVID-19 vaccination is preferred but not required at this time."
262,Senior Data Science QA Engineer,Ancestry,Remote,"$68,000 - $191,000 a year","About Ancestry:
When you join Ancestry, you join a human-centered company where every person’s story is important. We believe that by discovering the struggles and triumphs of our past, we can foster deeper bonds and more meaningful connections among families and communities. With more than 30+ billion digitized global historical records, 125+ million family trees, and 22+ million people in our growing AncestryDNA database, Ancestry helps customers discover their family story and gain a new level of understanding about their lives.

We are committed to our location flexible work approach, allowing you to work from where you want — in an office or from home or a hybrid of both (subject to location restrictions and roles that are required to be in the office). We will continue to hire and promote beyond the boundaries of our office locations, to enable broadened possibilities for employee diversity.

Together, we work every day to foster a work environment that's inclusive as well as diverse, and where our people can be themselves. Every idea and perspective is valued so that our products and services reflect the global and diverse clients we serve.

Ancestry encourages applications from minorities, women, the disabled, protected veterans and all other qualified applicants. Passionate about dedicating your work to enriching people’s lives? Join the curious.
We are seeking a Senior Data Science QA Engineer with strong Java and big data skills who will be responsible for ensuring the quality and reliability of our data science projects through the creation and execution of automated tests. The ideal candidate should have experience working in a data science or analytics environment as well as a deep understanding of JSON. Experience with AWS or other cloud providers is also preferred. Additionally, the ideal candidate should have a proven track record of troubleshooting and debugging complex systems using all available tools and techniques to identify and resolve issues.
This is a highly rewarding opportunity for the right candidate, as you will be working with some of the best and brightest engineers and data scientists around, produce code that is vital to the success of these massive projects and have a real impact on the quality of the final product. Artificial intelligence is not just the future, it is the present, and you will have a front row seat as this disruptive technology transforms our world.
Responsibilities:
Create and execute automated integration tests for our data science products, ensuring that they are accurate and reliable
Work with the data science engineering team to identify areas for testing and develop a testing strategy that aligns with the team's goals
Understand the projects in depth, ensuring that tests are covering all required scenarios and edge cases
Collaborate with the development team to create and maintain testing frameworks and infrastructure
Investigate and troubleshoot complex issues found during testing, working with developers to identify and resolve the root cause and using all available tools and techniques to identify and resolve issues
Continuously improve our testing processes and methodologies, keeping up with the latest industry trends and best practices
Work with cloud infrastructure to create and execute integration tests, ensuring that they are accurate and complete
Requirements:
5-8 years of experience in software testing, with a focus on back-end automation.
Strong programming skills in Java with experience creating integration tests, not just UI/Selenium automation.
Good working knowledge of a variety of data formats such as JSON, CSV, TSV, etc.
Ability to read and understand data flow and sequence diagrams
Producing high quality, maintainable code is vital. Knowledge of and experience with standard Java patterns, including the Java Streams API, is a must
Experience working with JSON datasets. Ability to programmatically transform data between different formats and extract specific values for validation.
Experience working with AWS or other cloud providers highly desirable but not required
Familiarity with Java linting tools and other code quality tools
Strong troubleshooting and debugging skills, with the ability to use all available tools and techniques to identify and resolve issues
Basic familiarity with data science concepts and techniques, with the ability to understand and test data pipelines and algorithms
Strong communication and collaboration skills, with the ability to work effectively with developers, data scientists, and other stakeholders
This Senior Data Science QA Engineer role is a key position in our organization, and we are looking for a highly skilled and motivated candidate to join our team. If you have experience in software testing, a strong understanding of Java, and are familiar with AWS and data science concepts, we encourage you to apply for this position.
As a signatory of the ParityPledge in Support of Women and the ParityPledge in Support of People of Color, Ancestry values pay transparency and pay equity. We are pleased to share the base salary range for this position: $68,000 - $191,000 with eligibility for bonus, equity and comprehensive benefits including health, dental and vision. The actual salary will vary by geographic region and job experience. We will share detailed compensation data for a specific location during the recruiting process. Read more about our benefits here: https://www.ancestrybenefits.com/.

Note: Disclosure as required by sb19-085(8-5-20) and sb1162(1-1-23)

Additional Information:
Ancestry is an Equal Opportunity Employer that makes employment decisions without regard to race, color, religious creed, national origin, ancestry, sex, pregnancy, sexual orientation, gender, gender identity, gender expression, age, mental or physical disability, medical condition, military or veteran status, citizenship, marital status, genetic information, or any other characteristic protected by applicable law. In addition, Ancestry will provide reasonable accommodations for qualified individuals with disabilities.
All job offers are contingent on a background check screen that complies with applicable law. For San Francisco office candidates, pursuant to the San Francisco Fair Chance Ordinance, Ancestry will consider for employment qualified applicants with arrest and conviction records.
Ancestry is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at Ancestry via-email, the Internet or in any form and/or method without a valid written search agreement in place for this position will be deemed the sole property of Ancestry. No fee will be paid in the event the candidate is hired by Ancestry as a result of the referral or through other means."
263,Data Fabric Front-End Design Engineer,"Advanced Micro Devices, Inc","7171 Southwest Pkwy, Austin, TX 78735",N,"Overview:
WHAT YOU DO AT AMD CHANGES EVERYTHING

We care deeply about transforming lives with AMD technology to enrich our industry, our communities, and the world. Our mission is to build great products that accelerate next-generation computing experiences – the building blocks for the data center, artificial intelligence, PCs, gaming and embedded. Underpinning our mission is the AMD culture. We push the limits of innovation to solve the world’s most important challenges. We strive for execution excellence while being direct, humble, collaborative, and inclusive of diverse perspectives. This is who we are at our best. One Company. One Team.

AMD together we advance_
Responsibilities:
MTS Data Fabric Front-End Design Engineer
THE ROLE:
This is a front-end design engineering position on AMD’s Data Fabric IP. The Data Fabric is the high-bandwidth, high performance, fabric network logic that ties together all the IPs on an SOC. Every product that AMD sells has its own custom-designed Data Fabric, so this role gives an engineer the opportunity to work on a broad array of products that address a variety of markets, including traditional servers, high performance computing, client desktop and laptop PCs, machine intelligence, graphics, console gaming, embedded, and customer-specific applications. It is a challenging position that involves working at a fast pace of innovation on the cutting edge of technology. Come join the AMD team!
THE PERSON:
The candidate should be able to work cooperatively with a talented global team and use his or her engineering skills to solve novel problems and optimize designs and flows.
KEY RESPONSIBILITIES:
Work with the Data Fabric architects and the chip floorplanning team to develop a custom Data Fabric topology
Synthesize Data Fabric IP using Synopsys tools and work with the micro-architects to ensure the design components meet the project’s area, power, and performance goals
Provide feedback to RTL team to resolve timing, power, area, LINT, DFT, and cross-clock-domain issues.
Provide interface to integrate IP blocks into the SOC and resolve the same types of power, timing, area, and formal equivalence checking at the chip level.
Analyze design power and devise improvements through architectural or flow optimizations
PREFERRED EXPERIENCE:
Extensive working knowledge, gained through multiple tapeouts, of the latest generation of Synopsys design tools, including Design Compiler NXT, Formality, Power Compiler, PrimeTime, Fusion Compiler, and IC Compiler II
A proven understanding of computer architecture
Knowledge of high performance interconnection logic and networks is a plus
ACADEMIC CREDENTIALS:
MS degree in Computer Engineering preferred
LOCATION:
Austin, TX

#LI-G11
Qualifications:
At AMD, your base pay is one part of your total rewards package. Your base pay will depend on where your skills, qualifications, experience, and location fit into the hiring range for the position. You may be eligible for incentives based upon your role such as either an annual bonus or sales incentive. Many AMD employees have the opportunity to own shares of AMD stock, as well as a discount when purchasing AMD stock if voluntarily participating in AMD’s Employee Stock Purchase Plan. You’ll also be eligible for competitive benefits described in more detail here.

AMD does not accept unsolicited resumes from headhunters, recruitment agencies, or fee-based recruitment services. AMD and its subsidiaries are equal opportunity, inclusive employers and will consider all applicants without regard to age, ancestry, color, marital status, medical condition, mental or physical disability, national origin, race, religion, political and/or third-party affiliation, sex, pregnancy, sexual orientation, gender identity, military or veteran status, or any other characteristic protected by law. We encourage applications from all qualified candidates and will accommodate applicants’ needs under the respective laws throughout all stages of the recruitment and selection process."
264,Data Analytics Engineer,Child Mind Institute,"445 Park Ave, New York, NY 10022","$120,000 - $177,000 a year","Summary:
The Child Mind Institute (CMI) seeks an individual with exceptional product acumen and intuition to join our team as a Data Analytics Engineer. This 3-year position will help CMI, through its partnership with California’s Department of Health Care Services (DHCS), with data and analytics for the no-code mental health delivery platform, MindLogger. MindLogger (by CMI) is empowering clinicians, researchers, and educators to deliver a fully customizable assessment and intervention platform for children’s mental health. Our mission is to transform children’s lives with technology that fits into where children eat, study and play.
As the first Data Analytics Engineer, this individual will play a critical leadership role, spanning data engineering and product analytics, with ownership over end-to-end product data and analytics strategy, from data creation and storage to self-service reporting and insight. As the data and analytics leader, you will partner with engineering, design, product, and research (and other cross-functional teams) and drive a culture of self-service analytics, measurement, and accountability, ensuring the product team has a clear line of sight into product engagement, growth and eventually monetization.
The individual must be equally adept at working with technical teams to define product measurement frameworks, data storage solutions, and various reporting platforms while working in partnership with business stakeholders to help run complex analyses and deliver results to inform product development decisions.
Reporting to the Program Director, this is a Full-Time, Exempt position at the Child Mind Institute headquarters in Midtown, New York (part-time remote work is possible).
The Child Mind Institute is proud to be named a Great Place to Work-Certified company! Our competitive compensation and benefits include medical insurance, 401(k) with match, flexible work schedules, paid parental leave, dependent care and discounted tickets and entertainment perks program. For more information about our benefits, please visit our employee benefits website.

Responsibilities will include, but are not limited to:
Leadership (25%):
Manage teams of engineers, data scientists, and business analysts, with the ability to hire, mentor, motivate, and hold team members accountable for delivering high-quality results.
Support with 3rd party vendor management, including vendor selection and term negotiations, in the data and analytics space.
Understand the value of data and proactively support senior management in making critical data-driven decisions.
Independently analyze, interpret, and derive insights into business decisions.
Technical (50%):
Implement and own data workflow management technologies (e.g., dbt, databricks).
Utilize data orchestration tools (e.g., Dagster, airflow).
Ability to operate in cloud data platforms (AWS) and analytics environments (e.g., Snowflake).
Guide the build-out of our back-end data infrastructure using knowledge of data architecture and various data storage and management frameworks (data lake, enterprise data warehouse, etc.).
Query data (SQL) and develop reports using BI reporting tools (i.e., Tableau, Amplitude, Google Analytics, Looker, QlikView, Periscope, PowerBI, etc.).
Set up and manage product experimentation and related tools (e.g., Otimizely, Google Analytics, etc.).
Communication and Collaboration with Strategic Focus (25%):
Derive insight from data and effectively socialize key takeaways to business partners and influence business decisions and priorities.
Develop collaborative relationships with program leadership and external stakeholders.

Qualifications:
Minimum of 7 years in Analytics / Data Science experience; at least 3 years in product and data analytics.
Minimum 3 years of experience building and developing analytics teams.
Expert-level understanding of SQL.
Expert-level knowledge of data and analytics infrastructure.
Expert-level development using BI tools such as Tableau, Google Analytics, Looker, QlikView, Periscope, and PowerBI.
Experience in a start-up environment defining and delivering the end-to-end product data and analytics roadmap, from back-end data infrastructure to reporting platforms.
Strong project management skills, with the ability to help business stakeholders and team members negotiate competing priorities.
Highly developed communication skills and the ability to story-tell with data to influence stakeholders and bridge the gap between technical and non-technical audiences.
Solid working knowledge of probability and statistics, with experience in programming languages used in data science (R, MATLAB, Python) a plus.
A passion for the Child Mind Institute mission.

Special Considerations:
Please upload your CV during the application process.
All new hires must be vaccinated and must stay up to date with vaccines against the COVID-19 virus unless they have been granted a reasonable accommodation for religion or disability. If you are offered employment with CMI, this requirement must be met by your date of hire, unless a reasonable accommodation for exemption is received and approved by CMI.
The Child Mind Institute is an equal opportunity employer and does not discriminate in employment based on race, religion (including religious dress and grooming practices), color, sex/gender (including pregnancy, childbirth, breastfeeding or related medical conditions), sex stereotype, gender identity/gender expression/transgender (including whether or not you are transitioning or have transitioned) and sexual orientation; national origin (including language use restrictions and possession of a driver's license issued to persons unable to prove their presence in the United States is authorized under federal law [Vehicle Code section 12801.9]); ancestry, physical or mental disability, medical condition, genetic information/characteristics, marital status/registered domestic partner status, age (40 and over), sexual orientation, military or veteran status, or any other basis protected by federal, state or local law or ordinance or regulation."
265,Lead Data Engineer (contract),Wells Fargo,"Charlotte, NC",N,"Title: Lead Data Engineer
Location:
Charlotte, NC
New York, NY
Summit, NJ
Duration: 12 months
Benefits on offer for this contract position: Health Insurance, Life insurance, 401K and Voluntary Benefits

Summary:
Lead complex initiatives on selected domains. Ensure systems are monitored to increase operational efficiency and managed to mitigate risk. Define opportunities to maximize resource utilization and improve processes while reducing cost. Lead, design, develop, test and implement applications and system components, tools and utilities, models, simulation, and analytics to manage complex business functions using sophisticated technologies. Resolve coding, testing and escalated platform issues of a technically challenging nature. Lead team to ensure compliance and risk management requirements for supported area are met and work with other stakeholders to implement key risk initiatives. Mentor less experienced software engineers. Collaborate and influence all levels of professionals including managers. Lead team to achieve objectives. Partner with production support and platform engineering teams effectively.

Qualifications:
Application development and implementation experience using SQL Server, SSIS and other ETL tool
Strong Experience in Microsoft SQL Server T-SQL Development such as complex stored procedures, data load, views.
Experience in SQL Server performance tuning and query monitoring
Design SSIS ETL packages to move data from various sources preferably from Flat files, JSON, Oracle and experience with FTP, NDM
Experience in software development life cycle
Deployment of Database components from lower environment to UAT/Production environment
Experience with Waterfall and Agile project methodologies and Agile Tools
Cloud experience such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or MS Azure
Experience in writing AUTOSYS (or something similar) JIL (Job Information Language) Commands (preferred)"
266,Senior Data Engineer,Kensho,Remote,"$150,000 - $225,000 a year","At Kensho, we hire talented people and give them the autonomy, support, and resources needed to build cutting edge technology and products. We produce a suite of AI-powered solutions that solves the challenges of the largest, most successful businesses and institutions, helping them make sense out of a world full of messy data.

We are seeking an experienced Software Engineer with a specialization in data engineering to join our Machine Learning Team. You will be responsible for implementing data ingestion pipelines to further our large language model efforts. You will work with stakeholders across Kensho and S&P to proactively understand the data landscape, triage data requirements, and ultimately incorporate the necessary data sets into our training corpus.

Our ideal candidate has 5+ years of experience, has been recognized as a technical leader, and has demonstrated success:
Navigating between project management, technical leadership, and individual contributor roles
Negotiating between new requirements, legacy systems, technical debt, and best practices
Mentoring colleagues on data engineering best practices and systems design
What You'll Do:
Work with industry standard and/or open source software such as s3, PostgreSQL and Airflow
Support machine learning and application teams by setting up custom data solutions
Build event and batch driven ingestion systems for machine learning and R&D as needed
Develop and administer databases, knowledge bases, and distributed data stores
Create and use systems to clean, integrate, or fuse datasets to produce data products
Establish and monitor data integrity and value through visualization, profiling, and statistical tools
Perform updates, migrations, and administration tasks for data systems
Translate data governance strategy into implementation
What You'll Need:
B.S. or M.S. in Computer Science (or a related field) or equivalent work experience
Experience with various data-stores, distributed messaging platforms, or data processing frameworks
Experience designing and building reliable systems and data pipelines
Experience working with large structured and unstructured data sets
Effective coding, documentation, and communication habits
Proficient understanding of distributed computing principles
Experience integrating/fusing data from multiple data sources and domains
Knowledge of various ETL techniques, frameworks, and best practices
Strong communication, project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
Proficiency with Python
(Bonus) Experience with Machine Learning workflows and operations
$150,000 - $225,000 a year
Kensho states that the anticipated base salary range for the position is 150k-225k. In addition, this role is eligible for an annual incentive bonus and equity plans. At Kensho, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case.
At Kensho, we pride ourselves on providing top-of-market benefits, including:
Medical, Dental, and Vision insurance
100% company paid premiums
Unlimited Paid Time Off
26 weeks of 100% paid Parental Leave (paternity and maternity)
401(k) plan with 6% employer matching
Generous company matching on donations to non-profit charities
Up to $20,000 tuition assistance toward degree programs, plus up to $4,000/year for ongoing professional education such as industry conferences
Plentiful snacks, drinks, and regularly catered lunches
Dog-friendly office (CAM office)
In-office gyms and showers (CAM, DC)
Bike sharing program memberships
Compassion leave and elder care leave
Mentoring and additional learning opportunities
Opportunity to expand professional network and participate in conferences and events
About Kensho
Kensho is an Artificial Intelligence company that builds solutions to uncover insights in messy and unstructured data that enable critical workflows and empower businesses to make decisions with conviction.
Kensho was founded in 2013 and was acquired by S&P Global in 2018. Kensho continues to operate as a startup in order to maintain our distinct, independent brand and to promote our breakthrough, innovative culture. Our team of Kenshins enjoy a dynamic and collaborative work environment that runs autonomously from S&P Global, while leveraging the unparalleled breadth and depth of data and resources available as part of S&P Global. As Kenshins, we pride ourselves on maintaining an innovative culture that depends on diversity and inclusion.
We are an equal opportunity employer that welcomes future Kenshins with all experiences and perspectives. Kensho is headquartered in Cambridge, MA, with offices in New York City, and Washington D.C. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin."
267,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
268,Data Engineer,On-Demand Group,"On-Demand Group in Minneapolis, MN 55402","Up to $100,000 a year","Data Engineer
Direct hire
This role develops enterprise data solutions that support the organization in achieving its strategic goals. Your work on cloud data pipelines will advance our enterprise data capabilities, while you get immersed in our collaborative, fun, and engaging culture. The position reports to our IT Data Engineering department and is a part of projects that align with key strategic initiatives to meet our business objectives. In your role, you will be part of a team that is responsible for the design, development, testing, deployment and support of cloud-based (GCP), data, analytical, and reporting applications.
Essential Duties & Responsibilities:
Develop and maintain custom ELT data pipelines with Python and SQL-based transformations running on the Google Cloud Platform.
Collaborate and implement event and batch based data science scoring pipelines.
Develop data access APIs to facilitate cross application data sharing.
Conduct and/or participate in requirements analysis sessions with internal customers, external vendors, and project teams.
Translate business requirements into technical designs.
Follow engineering best practice to ensure robust, tested, and reliable data pipelines.
Support data governance and security practices.
Follow agile development methodologies and actively participate in sprint planning sessions.
Support downstream users and resolve production issues with excellent customer service.
Job Skills:
Hands-on experience in creating API based data ingestion pipelines.
Good design skills in data pipeline, enrichment, and API patterns.
Good understanding of object oriented software engineering patterns.
Good relationship-building, customer service, and problem resolution skills.
Knowledge of software engineering, version control, and testing practices.
Knowledge of Agile software development methodologies.
Works effectively in a dynamic work environment with competing priorities.
Work Experience (6+ months experience each)
Python object oriented programming. Multi-language experience preferred.
API based custom data ingestion, particularly working with 3rd party vendor API's including reading API documentation, authentication, and bulk data staging strategies.
Cloud-based development. GCP preferred.
SQL
Experience with development in a version control, CI/CD environment.
Education:
From an accredited institution, Bachelor’s degree is required.
Job Type: Full-time
Salary: Up to $100,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Minneapolis, MN 55402: Reliably commute or planning to relocate before starting work (Required)
Experience:
Google Cloud Platform: 1 year (Preferred)
SQL: 1 year (Preferred)
Python: 1 year (Preferred)
Work Location: In person"
269,Azure Data Engineer,N,Remote,$80 - $85 an hour,"This position will require qualified T-SQL Developers to take thelead in thefollowing tasks:
Rationalizing and mapping data between transactional and dimensional database models/systems
Working closely with the database and system administrators to develop stored procedures and ETLprocesses related to thedata warehouse system.
Working in collaboration with vendors and other agencystaff to design,develop and managethe creation Synapse pipelines.
Ability to work independently and cooperatively as part of a team.
Ability to work under severetime constraints.
Must possess analytical and complex problem-solving skills.
REQUIRED SKILLS/EXPERIENCE YEARS
Extensive experience with development of stored procedures and ETL processes using T-SQL-7 years
Thorough understanding of data warehouse design hierarchies such as star and snowflake schemas-7 years
Use of ALM tools for work item management, version control, code analysis, and testing-7 years
Broad and extensive knowledge of the software development process and its technologies-7 years
Familiarity with continuous integration-7 years
Experience creating and scheduling elastic jobs-5 years
Experience with designing and modelling database structures based on business use cases-5 years
Job Type: Contract
Salary: $80.00 - $85.00 per hour
Experience level:
10 years
9 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote"
270,Sr. Data Analytics Engineer,INPOSIA Solutions GmbH,Remote,"$157,300 - $259,500 a year","Job Description
Overview
We are building cloud-based tax compliance solutions to handle every transaction in the world. Imagine every transaction you make - every tank of gas, cup of coffee, or pair of sneakers, every movie ticket, or streamed song, every sensor-to-sensor ping. Nearly every time you make a purchase, physical or digital, there is an accompanying unique and nuanced tax compliance calculation.
Responsibilities
The Senior Data Analytics Engineer will be responsible for gathering business requirements, understanding complex product, business and engineering challenges, compose and prioritize research projects and then execute them in partnership with other data scientists, data analysts as well as leverage the work of our data engineering team.
The individual will have direct ownership of corporate metrics, ensure accuracy, timeliness and intent of all published data products within area of ownership by our team and all other teams within the company. Will also have executive visibility and influence capability.
*
Learn the tax business domain knowledge and data.
Work with the product managers, DBA teams and broader engineering teams build scalable data orchestration, transformation and reporting streams that can capture and prepare billions of transactions per day for customer reporting.
Build strong and captivating data visualizations in Dash/PowerBI/Tableau/R.
Develop and manage end-to-end project plans and ensure on-time delivery.
Communicate status and big picture to the project team and management.
Work with business and engineering teams to identify scope, constraints, dependencies, and risks.
Identify risks and opportunities across the business and drive solutions.
Qualifications
Minimum of 5 years combined work experience in data engineer, business intelligence and in-app embedded analytics
Strong experience with cloud data engineering teams
Advanced SQL, scripting (Python or R) and advanced data visualization experience
Implement streaming and batch ETL pipelines supporting both internal and external data sources
Data warehouse modeling and architecture with strong requirements gathering and technical analysis
Technical expertise with AWS, DBT, and Airflow
Bachelor's degree in Computer Science or Engineering
Experience with requirements of data security, data privacy and legal / jurisdictional compliance and governance
Experience managing efforts in distributed systems and/or developing large scale web applications
Proven ability to combine business acumen, technical acumen and process expertise to define client (internal/external) engagement and program execution
Ability to communicate effectively with technical and non-technical stakeholders across multiple business units
Strong communication skills, data driven product and corporate excellence culture
Excellent problem-solving skills
Preferred Skills
Minimum of 5 years work experience with AWS data pipeline and Snowflake cloud DW
Experience building powerful dashboards which are embedded in web and mobile applications
Pay Range Details
The base pay range(s) below are provided in compliance with state specific laws. Pay ranges may be
different in other locations.
Colorado: $157,300-$259,500 (annually)
Washington: $157,300-$286,900 (annually)
California: $157,300-$314,200 (annually)
NYC: $173,900-$314,200 (annually)
The pay range above is the general base pay range for a successful candidate in the state listed. The
successful candidate’s actual salary/wage may be based on various factors, such as geographic
location, candidate experience and qualifications, as well as market and business considerations.
This role is eligible for an annual bonus based on individual and company performance, depending on
the terms of the applicable plan and the employee’s role.
Benefits
Avalara’s benefits for eligible employees includes company benefits such as medical, dental, and
vision coverage, life, AD&D, and disability insurance, a 401(k) retirement plan, 17 days of paid
time off annually, 12 paid holidays, paid parental leave, an employee assistance program, and
subsidized transportation options for commuters.
All benefits are subject to eligibility requirements and Avalara reserves the right to modify or change
these benefits programs at any time, with or without notice, unless otherwise required by law.
About Avalara*
About Avalara:*
We’re building cloud-based tax compliance solutions to handle every transaction in the world. Imagine every transaction you make &mdash; every tank of gas, cup of coffee, or pair of sneakers, every movie ticket, meal kit, or streamed song, every sensor-to-sensor ping. Nearly every time you make a purchase, physical or digital, there’s an accompanying unique and nuanced tax compliance calculation. The logic behind calculating taxes &mdash; the rules, rates, and boundaries is a global, layered, three-dimensional mess of complexity, with compliance dictated by governments and applied by every business, every day.
Avalara works with businesses of all sizes, all over the world &mdash; from corner stores to gigantic global retailers &mdash; to calculate tax accurately and automatically, at speeds measured in milliseconds. That’s a massive technical challenge, in terms of scale, reliability, and complexity, and we do it better than anyone. That’s why we’re growing fast. Headquartered in Seattle, Avalara has offices across the U.S. and around the world, in Brazil, Canada, India, U.K, Belgium and across Europe.
*
Equal Opportunities:*
Avalara is an Equal Opportunity Employer. All qualified candidates will receive consideration for employment without regard to race, colour, creed, religion, age, gender, national orientation, disability, sexual orientation, US Veteran status, or any other factor protected by law.
Salary: $157,300.00 - $259,500.00 per year"
271,"Sr. Software Engineer II, Data Infrastructure",HubSpot,"Cambridge, MA 02141•Remote","$150,000 - $210,000 a year","About the team
The Data Infrastructure teams at HubSpot are responsible for building and maintaining data storage technologies across the product. Teams work with a variety of open source technologies like MySQL, Vitess, Hadoop HBase, Kafka, Spark, and Elastic Search all running on AWS.
One of the primary objectives is to automate the deployments, configuration, and recoverability of data stores to efficiently scale storage systems. Teams also take on projects to improve the client-side interaction with the data stores with analysis tools and wrapper client interfaces. A number of our teams are active contributors to the Open Source projects that they work on.
We're operationally responsible for a huge volume of traffic to and from these data stores. Our HBase clusters serve over 3 million requests/second across 220+ tables, while our ElasticSearch clusters serve over 20k searches/second and 50k indexes/second to 90+ billion documents.
Streaming that data to and from applications amounts to more than 3 GB/sec of data through our Kafka clusters, with hundreds of producers and consumers.
What we're looking for
We're looking for talented software engineers to help us build the vision of making our database access simple, intuitive, highly performant and highly reliable to our customers and HubSpot application developers.
The goal of the team is to implement a comprehensive set of intelligent features that will drive the productivity of using any of our data stores at scale and with the highest possible reliability.
Bonus points for being:
Someone with experience developing automation or is a power user of one or more of the above-mentioned data storage technologies.
Willingness and ability to work with the user base, in this case, other developers, to define and build tools that make the lives of application developers easier and their code less error-prone through automation.
Interested in working with and supporting infrastructure at scale.
Someone who has shown that they can solve complicated technical problems and analyze tradeoffs with empathy for the developers, yet building creative solutions.
Able to demonstrate pragmatic decision making and problem-solving abilities.

Cash compensation range: 150000-210000 USD Annually

This resource will help guide how we recommend thinking about the range you see. Learn more about HubSpot's compensation philosophy from Katie Burke, HubSpot's Chief People Officer.

The cash compensation above includes base salary, on-target commission for employees in eligible roles, and annual bonus targets under HubSpot's bonus plan for eligible roles. In addition to cash compensation, all HubSpotters are eligible to participate in HubSpot's equity plan to receive restricted stock units (RSUs). Some roles may also be eligible for overtime pay. Individual compensation packages are based on a few different factors unique to each candidate, including their skills, experience, qualifications and other job-related reasons.

We know that benefits are also an important piece of your total compensation package. To learn more about what's included in total compensation, check out some of the benefits and perks HubSpot offers to help employees grow better.

At HubSpot, fair compensation practices isn't just about checking off the box for legal compliance. It's about living out our value of transparency with our employees, candidates, and community.


We know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates, so please don't hesitate to apply — we'd love to hear from you.

If you need assistance or an accommodation due to a disability, please email us at interviewaccommodation@hubspot.com. This information will be treated as confidential and used only for the purpose of determining an appropriate accommodation for the interview process.

Germany Applicants: (m/f/d) - link to HubSpot's Career Diversity page here.

About HubSpot
HubSpot (NYSE: HUBS) is a leading customer relationship management (CRM) platform that provides software and support to help businesses grow better. We build marketing, sales, service, and website management products that start free and scale to meet our customers' needs at any stage of growth. We're also building a company culture that empowers people to do their best work. If that sounds like something you'd like to be part of, we'd love to hear from you.
You can find out more about our company culture in the HubSpot Culture Code, which has more than 5M views, and learn about our commitment to creating a diverse and inclusive workplace, too. Thanks to the work of every employee globally, HubSpot was named the #2 Best Place to Work on Glassdoor in 2022, and has been recognized for award-winning culture by Great Place to Work, Comparably, Fortune, Entrepreneur, Inc., and more.
Headquartered in Cambridge, Massachusetts, HubSpot was founded in 2006. Today, thousands of employees work across the globe in HubSpot offices and remotely. Visit our careers website to learn more about culture and opportunities at HubSpot.
By submitting your application, you agree that HubSpot may collect your personal data for recruiting, global organization planning, and related purposes. HubSpot's Privacy Notice explains what personal information we may process, where we may process your personal information, our purposes for processing your personal information, and the rights you can exercise over HubSpot's use of your personal information."
272,Junior Data Engineer,"Link Technologies, Inc.","Las Vegas, NV","$64,134 - $178,786 a year","Link Technologies (LinkTechConsulting.com), is currently seeking a Junior Data Engineer for a position in Las Vegas, NV. This will be an onsite/contract opportunity. This role will need to support on-call activities including over the weekend.
QUALIFICATIONS
Minimum three (3) years of data warehouse experience.
Strong understanding of SQL Server database security is required.
Ability to design and evaluate store procedure for business logic is required.
Ability to create and manage reports in SSRS 2019 and Power BI is required.
Bachelor’s degree or equivalent in related field.
MUST be able to acquire and retain a Nevada Gaming Control Board registration and other certification or license, as required by law or policy.
Understanding of Windows 2019 with SQL Server 2016 or higher.
Experience with SQL ETL construction with support of SSIS versions 2016 and 2019.
Experience with the management of transactional SQL replication.
Previous experience fulfilling ServiceNow and data warehouse incident tickets.
MUST have a strong grasp of relational and dimensional database modeling.
MUST have expert knowledge of the Microsoft BI Stack including SSIS, SSRS, SSAS, SharePoint, PowerPivot, etc.
Previously worked with automated file handling with ETL tools via FTP and other means.
Administrative experience with SQL Server 2008 – 2012.
Clear understanding of Windows, Enterprise Systems, Networking, and Enterprise Security.
MUST adhere to SOPs and methodologies and be open to updating them as needed.
Effective communication skills with the ability to work with all backgrounds and levels of experience.
MUST be available to work varied shifts, including nights, weekends, and holidays.
Willing to learn and adapt to new technologies and paradigms, i.e., Hadoop, Hive, etc.
Experience with Java or .NET development; C#, ASP.NET, scripting languages, and web site operations is a plus.
Data preparation for data science experience a plus.
Ability to understand big data, advanced analytic techniques, and real time data a plus.
Confidence with analytic tools, i.e., Spotfire, Tableau, SAS, Cognos a plus.
PREFERRED
Gaming and hospitality experience.
Experience with the following:Python, C#, and PowerShell
GCP; Big Query and dataflows
Azure; Synapse and data lake
IBMI DB2
Tableau and PowerBI
Azure DevOps, Jenkins, and JIRA
Informatica and Precisely; CDC Connect/DTShare
Kafka
TFS and Agile/SCRUM experience.
Previously worked with key concepts of Data Management, i.e., Data Quality and MDM.
DUTIES AND RESPONSIBILITIES
Create, develop, administer, and maintain Data Warehouse and ETL structures and solutions.
Be a point-of-contact and help with architecture, vision, problem anticipation, and problem solving for the assigned project, and become a subject matter expert for data and analytic users.
Offer L3 application and on-call support for data and integration solutions.
Contribute and be prepared to lead project teams within an agile environment.
Multitask and prioritize responsibilities including application and on call support, major and minor projects, functional conditions, and systems specifications.
Provide efficient documentation for delivered solutions and processes, incorporating documentation with the business knowledgebase.
Support subject matter expertise for establishing business requirements to address business opportunities or issues across business functions for data centric solutions in partnership with the Product Management team.
Keep up-to-date with technological trends and innovations in the data warehousing and data management domains.
Understand and fulfill business needs by designing and enhancing systems to transform, cleanse, and provision corporate data assets in a controlled, secure, and high-performance manner.
Take part in the ongoing data management maturation process.
Safety is an essential function of this job.
Consistent and regular attendance including on-call availability on a rotational basis is an essential function of this job.
Performs other related duties as assigned.
Link Technologies is an equal opportunity employer. All qualified applicants will receive consideration for employment without discrimination because of race, color, religion, sex, gender identity/expression, sexual orientation, national origin, protected veteran status, disability, or any other factors protected by law.
Job Type: Contract
Pay: $64,134.31 - $178,785.55 per year
Benefits:
Dental insurance
Health insurance
Vision insurance
Experience level:
3 years
Schedule:
Day shift
Evening shift
Night shift
On call
Overnight shift
Weekend availability
Ability to commute/relocate:
Las Vegas, NV: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data warehouse: 3 years (Required)
SQL Server Database Security: 3 years (Required)
SSRS 2019: 3 years (Required)
Power BI: 3 years (Required)
Windows 2019: 3 years (Required)
ServiceNow: 3 years (Required)
Work Location: One location

Health insurance"
273,Data Engineer,Gateway Professional Network,Remote,"$95,000 - $145,000 a year","Apply Directly: https://gatewaypn.applicantstack.com/x/detail/a2bwypr26k74
Data Engineer
Job Description for Full-Time Position
Location: US Remote (must reside in US and we are unable to sponsor Visas at this time)
Pay Scale: $95,000 – $145,000 per Annum (based on experience)
Reports to: Ramon Navarro
Job Description Summary:
Individual to contribute to GPN Tech transformation assuming accountability for data engineering lifecycle including research, proof of concepts, design, development, test, deployment, and maintenance of enterprise-scale data integration solutions leveraging Microsoft Azure PaaS offerings. Extensive experience working on Agile Scrum/DevOps teams employing the latest CI/CD cloud-first best practices. Potential to become an SME on Platform capabilities (ingestion, storage, processing, and presentation patterns) and extend future-state strategic roadmap features.
What You’ll Do: Responsibilities
Provide technical direction to delivery scrum teams, extending patterns and establishing new ones as required
Create and review technical designs of data integrations across various systems within the enterprise and with external business partners over multiple transmission channels, data formats, and processing patterns
Extend/implement CICD pipelines and containerization strategy
Responsible for ETL and SQL development using Domain Architecture and Interface based programming
Build processes and diagnostics tools to troubleshoot, maintain and optimize solutions and respond to customer and production issues
Experience You’ll Need: Qualifications
Data Engineer with in-depth knowledge of developing data-driven solutions using Azure App Services, Function Apps, API Service Management, Active Directory, Azure Storage, Data Factory, Databricks, RedisCache, SQL DB, Key Vault, Service Bus, Event Hub and Application Insights
Bachelor’s degree in computer science or related discipline; r Equivalent Work Experience
5+ years of experience leveraging cloud data storage technologies specifically Microsoft Azure offerings
5+ years of experience developing on modern data ingestion tools/platforms/protocols (Kafka, APIs, EventHubs, ServiceBus, etc.)
5+ Experience with data integration best practices (ETL/ELT patterns, Data Factory, Streams Analytics, etc.)
5+ years of experience in ETL and SQL Programming
2+ years of experience in C# and T-SQL
3+ years of experience working with Microsoft Azure DevOps and CI/CD best practices
3+ years of experience working in an Agile Scrum environment
Experience with various data structure optimization techniques on a Data Streaming Platform
Experience with Power BI or other similar data visualization tools
Bonus Points: Additional qualifications if you have them!
Experience with Machine Learning and Artificial Intelligence based solutions
Knowledge of data regulations and compliance policies (e.g., HIPAA)
Data Visualization experience, or strong affinity for leveraging data to help transform/optimize business functions
Big Data experience
IT healthcare experience
IoT experience
Who We Are:
GPN Technologieswas founded in 2007 with the mission of providing “big business” infrastructure to independent practitioners in the ophthalmic industry. The company’s driving initiative has been empowering independents to be profitable and competitive in today’s market by making high-tech, business-critical tools accessible and meaningful in small business settings.
Our analytics platforms have revolutionized the way practitioners view, understand, and act on their business data. We have continued to expand and hone those platforms to help those practitioners succeed and thrive in an increasingly competitive, fast-paced marketplace.
Thousands of practitioners across the country are serving their patients, their team members, and their bottom lines more efficiently with EDGEPro.
Job Type: Full-time
Pay: $95,000.00 - $145,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Paid time off
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
Do you require Visa sponsorship?
Experience:
Relational databases: 2 years (Preferred)
Azure: 2 years (Preferred)
Big data: 2 years (Preferred)
SQL: 2 years (Required)
Microsoft SQL Server: 2 years (Required)
C#: 2 years (Required)
.NET: 3 years (Required)
Work Location: Remote"
274,Senior Principal Engineer - Health Data and Analytics,Oracle,United States,N,"At Oracle Health Data & Analytics Platform, we're making a difference in healthcare by combining the strengths of OCI with the leading healthcare technology platforms from Oracle Cerner. As part of our team, you'll have the opportunity to directly impact the health experiences of hundreds of millions of people.
We're looking for a highly skilled experienced engineers to design and build high-scale, cloud-based data processing systems that can handle massive amounts of data with low latency. You'll work with a team of smart, motivated, and diverse people and be given the autonomy and support to do your best work. This is a rare opportunity to make a meaningful impact in society while working in a dynamic and flexible workplace where you'll belong and be encouraged.

An Oracle career can span industries, roles, Countries and cultures, giving you the opportunity to flourish in new roles and innovate, while blending work life in. Oracle has thrived through 40+ years of change by innovating and operating with integrity while delivering for the top companies in almost every industry.
In order to nurture the talent that makes this happen, we are committed to an inclusive culture that celebrates and values diverse insights and perspectives, a workforce that inspires thought leadership and innovation.
Oracle offers a highly competitive suite of Employee Benefits designed on the principles of parity, consistency, and affordability. The overall package includes certain core elements such as Medical, Life Insurance, access to Retirement Planning, and much more. We also encourage our employees to engage in the culture of giving back to the communities where we live and do business.
At Oracle, we believe that innovation starts with diversity and inclusion and to create the future we need talent from various backgrounds, perspectives, and abilities. We ensure that individuals with disabilities are provided reasonable accommodation to successfully participate in the job application, interview process, and in potential roles. to perform crucial job functions.
That’s why we’re committed to creating a workforce where all individuals can do their best work. It’s when everyone’s voice is heard and valued that we’re inspired to go beyond what’s been done before.
Oracle is an Equal Employment Opportunity Employer * . All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans’ status, or any other characteristic protected by law. Oracle will consider for employment qualified applicants with arrest and conviction records pursuant to applicable law.
* Which includes being a United States Affirmative Action Employer

Responsibilities:
Design and build distributed, scalable, and fault-tolerant software systems.
Build cloud services on top of the modern OCI infrastructure.
Participate in the entire software lifecycle, from design to development, to quality assurance, and to production.
Invest in the best engineering and operational practices upfront to ensure our software quality bar is high.
Optimize data processing pipelines for orders of magnitude higher throughput and faster latencies.
Leverage a plethora of internal tooling at OCI to develop, build, deploy, and troubleshoot software.
Qualifications:
Bachelor's Degree required
Significant experience with distributed systems.
Experience with modern programming languages such as Java, C#, C/C++, or Ruby.
Fluency in technologies and design concepts around Big Data processing and relational databases, such as the Hadoop ecosystem, Map/Reduce, stream processing, etc.
Experience with production operations and good practices for putting quality code into production and troubleshooting issues when they arise.
Effective communication of technical ideas verbally and in writing, including technical proposals, design specs, architecture diagrams, and presentations.
Ability to collaborate effectively with the team and other stakeholders.
Preferably, production experience with Cloud and data processing technologies.
10+ years of experience of relevant experience
US Citizenship is required and Fed clearance preferred
#LI-LL7

Responsibilities:
Design and build distributed, scalable, and fault-tolerant software systems.
Build cloud services on top of the modern OCI infrastructure.
Participate in the entire software lifecycle, from design to development, to quality assurance, and to production.
Invest in the best engineering and operational practices upfront to ensure our software quality bar is high.
Optimize data processing pipelines for orders of magnitude higher throughput and faster latencies.
Leverage a plethora of internal tooling at OCI to develop, build, deploy, and troubleshoot software.
Qualifications:
Bachelor's Degree required
Significant experience with distributed systems.
Experience with modern programming languages such as Java, C#, C/C++, or Ruby.
Fluency in technologies and design concepts around Big Data processing and relational databases, such as the Hadoop ecosystem, Map/Reduce, stream processing, etc.
Experience with production operations and good practices for putting quality code into production and troubleshooting issues when they arise.
Effective communication of technical ideas verbally and in writing, including technical proposals, design specs, architecture diagrams, and presentations.
Ability to collaborate effectively with the team and other stakeholders.
Preferably, production experience with Cloud and data processing technologies.
10+ years of experience of relevant experience
US Citizenship is required and Fed clearance preferred
#LI-LL7"
275,Data Engineer/SW Developer (TS/SCI FSP),IBM,"15036 Conference Center Dr, Chantilly, VA 20151","$103,000 - $225,000 a year","Introduction
As a Data Scientist at IBM, you will help transform our clients’ data into tangible business value by analyzing information, communicating outcomes and collaborating on product development. Work with Best in Class open source and visual tools, along with the most flexible and scalable deployment options. Whether it’s investigating patient trends or weather patterns, you will work to solve real world problems for the industries transforming how we live.

Your Role and Responsibilities
IBM is committed to providing National Security leaders with cutting edge data exploitation and analysis capabilities. Seeking a talented, versatile Data Engineer / Back End Developer with TS/SCI and Full Scope Polygraph.

The successful candidate will be part of a small agile team building cutting edge analytic capabilities including a data pipeline, data lake, and associated analytic and AI/ML capabilities for US Government customers. The team has is fast-paced, collaborative, and cohesive, and depends on team members to communicate openly and to design solutions and deliver quality code on a regular, aggressive clip.

The Data Engineer / Back End Developer will be a hands-on software developer who will design, develop, integrate, test, and deploy software using a combination of custom, open source, and off-the-shelf (GOTS/COTS) software packages and a variety of data types (structured, unstructured, non-text). The Data Engineer / Back End Developer will work as part of an Agile scrum team to analyze business requirements and translate them into technical tasks. They will enhance software functionality based on rapidly evolving mission needs and technology opportunities.

Level (entry, journeyman, senior, master) will be determined based on education and years of experience.

#CLEARED22

Required Technical and Professional Expertise
REQUIRED ON DAY 1: Active TS/SCI security clearance + Full Scope Polygraph
At least 2 years demonstrated hands-on programming experience with one or more of the following: Java, Node, Python, AWS
At least 2 years of experience architecting, designing and programming applications in hybrid, cloud or on-prem environments
Experience with Git/GitHub
Experience integrating with JSON-based REST APIs
Experience with Agile software development, Scrum, and software development lifecycle (SDLC)
Self-starter with an ability to work independently and in a team environment
Strong problem-solving, critical thinking, and analysis skills.Ability to articulate complex analytic problems, work effectively without detailed instructions, collaborate and ask questions
Ability to generate, follow, and update technical procedures and evaluate results

Preferred Technical and Professional Expertise
Bachelors or Maser's Degree in Computer Science, Computer Information Systems, Computer Engineering
At least 1 year of classified software engineering experience on IC systems and architectures
At least 1 year classified software engineering experience

About Business Unit
IBM Consulting is IBM’s consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients’ businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.
This job requires you to provide your COVID-19 vaccination status with supporting documentation, where legally permissible.

Your Life @ IBM
In a world where technology never stands still, we understand that, dedication to our clients success, innovation that matters, and trust and personal responsibility in all our relationships, lives in what we do as IBMers as we strive to be the catalyst that makes the world work better.
Being an IBMer means you’ll be able to learn and develop yourself and your career, you’ll be encouraged to be courageous and experiment everyday, all whilst having continuous trust and support in an environment where everyone can thrive whatever their personal or professional background.
Our IBMers are growth minded, always staying curious, open to feedback and learning new information and skills to constantly transform themselves and our company. They are trusted to provide on-going feedback to help other IBMers grow, as well as collaborate with colleagues keeping in mind a team focused approach to include different perspectives to drive exceptional outcomes for our customers. The courage our IBMers have to make critical decisions everyday is essential to IBM becoming the catalyst for progress, always embracing challenges with resources they have to hand, a can-do attitude and always striving for an outcome focused approach within everything that they do.
Are you ready to be an IBMer?

About IBM
IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement
IBM offers a competitive and comprehensive benefits program. Eligible employees may have access to:

Healthcare benefits including medical & prescription drug coverage, dental, vision, and mental health & well being
- Financial programs such as 401(k), the IBM Employee Stock Purchase Plan, financial counseling, life insurance, short & long- term disability coverage, and opportunities for performance based salary incentive programs
Generous paid time off including 12 holidays, minimum 56 hours sick time, 120 hours vacation, 12 weeks parental bonding leave in accordance with IBM Policy, and other Paid Care Leave programs. IBM also offers paid family leave benefits to eligible employees where required by applicable law
Training and educational resources on our personalized, AI-driven learning platform where IBMers can grow skills and obtain industry-recognized certifications to achieve their career goals
Diverse and inclusive employee resource groups, giving & volunteer opportunities, and discounts on retail products, services & experiences

The compensation range and benefits for this position are based on a full-time schedule for a full calendar year. The salary will vary depending on your job-related skills, experience and location. Pay increment and frequency of pay will be in accordance with employment classification and applicable laws. For part time roles, your compensation and benefits will be adjusted to reflect your hours. Benefits may be pro-rated for those who start working during the calendar year.

We consider qualified applicants with criminal histories, consistent with applicable law.

US Citizenship Required.

Being You @ IBM
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
276,"Senior Frontend Software Engineer, Global E-Commerce Data Intelligence",TikTok,"Mountain View, CA","$187,040 - $280,000 a year","Responsibilities
TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Mountain View, Los Angeles, New York, London, Paris, Berlin, Dubai, Mumbai, Singapore, Jakarta, Seoul, and Tokyo.

Why Join Us

At TikTok, our people are humble, intelligent, compassionate and creative. We create to inspire - for you, for us, and for more than 1 billion users on our platform. We lead with curiosity and aim for the highest, never shying away from taking calculated risks and embracing ambiguity as it comes. Here, the opportunities are limitless for those who dare to pursue bold ideas that exist just beyond the boundary of possibility. Join us and make impact happen with a career at TikTok.

Global e-commerce is content e-commerce business with international short video product as the carrier. It is committed to becoming the first choice for users to discover and purchase good products at affordable prices. Global e-commerce business team hopes to provide users with more tailored, active and efficient consumption experience, enabling merchants to receive stable and reliable platform services in different scenarios such as live e-commerce, short video content e-commerce, so as to make more affordable and high-quality products sell easily and a better life within reach.

The data insights team is responsible for development of data analytics & data-empowered platform capabilities across global e-commerce. Our mission is to empower our users to leverage and extract actionable insights from data to maximize their potential and efficiency on the global e-commerce platform. In essence, we want to extract facts, attribute causes and predict the future from oceans of data; and our fundamental goals are to reflect business impact, leverage data to support key decisions by lowering decision making complexity and optimising decision making efficacy and efficiency.

Responsibilities:
Architect and develop efficient and highly reusable front-end systems that drive complex web applications for e-commerce products.
Code optimisation to improve scalability, reliability, security and performance of web applications.
Collaborate with product design, product management and software engineering teams to deliver best in class user experience.
Qualifications
B. Sc or higher degree in Computer Science or related fields from accredited and reputable institutions.
5+ years experience in frontend engineering, with demonstrable experience with JavaScript/HTML/CSS, React/Vue/Angular and packaging frameworks like Webpack/Rollup/BaBel/AST/Gulp. Qualified fresh graduates would also be considered.
Familiar with key concepts like functional and asynchronous programming, closures and types, layouts, specificity, animation, cross browser compatibility, data security and accessibility.
Good understanding of multi-tier application architecture and protocols, familiarity with product and software development lifecycle process.
Demonstrable experience in developing data visualisation or data insights web/mobile applications is advantageous
Even Better if:
Agile, quick self-learner, highly self-motivated with a strong sense of product ownership, and creative problem solver.
Deeply passionate about software coding/development and building great mobile/web applications.
Ability to lead independent research to solve complex technical problems.
Good collaborator and team player, comfortable working in a fast-moving, culturally diverse, and globally distributed team environment.
TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.

TikTok is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at allen.chen@bytedance.com.
Job Information
The base salary range for this position in the selected city is $187040 - $280000 annually.



Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.



At ByteDance/TikTok our benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support ByteDancers to give their best in both work and life. We offer the following benefits to eligible employees:



We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care.



Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off(PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability.



We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice."
277,Data Engineer,Fidelity Investments,"Boston, MA 02210","$96,000 - $144,000 a year","Job Description:
Job Title: Data Engineer
Job Description:
We are seeking a highly skilled and motivated Data Engineer to join our team in Fidelity Labs. The Data Engineer will be responsible for crafting, building, and maintaining the data pipelines and infrastructure that support AI-enabled solutions in Fidelity Labs’ technology incubator. The ideal candidate will have a strong background in data engineering, with experience in designing and implementing data pipelines, warehousing, and modeling, while applying modern cloud tools.

Key Responsibilities:
Design and implement data pipelines to support the collection, storage, and processing of large datasets
Develop and maintain data warehousing solutions
Design and implement data models to support business requirements
Collaborate with data scientists and analysts to support the development and deployment of machine learning models
Ensure data quality and integrity
Stay up to date with emerging technologies and industry trends in data engineering
Qualifications:
Bachelor's degree in Computer Science, Information Systems, or a related field
3+ years of professional experience in data engineering
Strong experience with SQL and NoSQL databases
Experience with cloud-based data warehousing and data lake solutions such as AWS Redshift, Snowflake, and Google BigQuery
Experience with distributed computing frameworks such as Apache Spark and Dask
Experience with distributed training and fine-tuning of language models using libraries like TensorFlow, PyTorch and HuggingFace.
Experience with data storage and data management for large datasets like parquet, HDF5, etc.
Experience with data integration and ETL tools such as Apache NiFi, Apache Airflow, and Talend
Experience with programming languages such as Python, Java, and Scala
Experience with natural language processing (NLP) techniques and tools, such as tokenization, stemming, lemmatization, and sentiment analysis
Familiarity with large language models such as GPT-3, BERT, and RoBERTa
Experience working with unstructured data, such as text and audio
Experience with data pre-processing for language models like tokenization, vocabulary creation and crafting input features for language models.
Strong problem-solving and analytical skills
The Value You Deliver:
A qualified candidate will be bright, highly motivated and self-starting, able to work independently within a small dynamic team
Communicate issues and status to all levels of the organization, including senior management
Provide high quality work under tight deadlines
Ability to deal with ambiguity
Good communication skills, both written and verbal
Ability to focus on delivering customer value in a highly sophisticated and fast paced environment
Ability to work collaboratively and efficiently in a virtual team environment across multiple time zones
The Team
This opportunity is brought to you by Fidelity Labs, Fidelity Investments’ in-house software incubator and digital studio. Founded in 2005, Fidelity Labs has played a critical role in driving growth and innovation for the firm. The Fidelity Labs organization has a portfolio of new businesses and is constantly prototyping concepts for Fidelity’s next new ventures.
Fidelity Labs is a dynamic workplace that combines the best parts of startup life—building from scratch, adapting quickly, and moon-shot ambition—with the scale and stability of an industry leader. Learn more at labs.fidelity.com.
Please see below for the salary range for work locations in Colorado only:
N/A
Please see below for the salary range for work locations in New York City, Westchester County, NY and Jersey City, NJ only:
$96,000 - $144,000 per year
This position is eligible for incentive compensation or an annual bonus opportunity.
Please see below for the salary range for work locations in California only:
N/A
Please see below for the salary range for work locations in Washington only:
N/A
Certifications:
Company Overview
Fidelity Investments is a privately held company with a mission to strengthen the financial well-being of our clients. We help people invest and plan for their future. We assist companies and non-profit organizations in delivering benefits to their employees. And we provide institutions and independent advisors with investment and technology solutions to help invest their own clients' money.

Join Us
At Fidelity, you'll find endless opportunities to build a meaningful career that positively impacts peoples' lives, including yours. You can take advantage of flexible benefits that support you through every stage of your career, empowering you to thrive at work and at home. Honored with a Glassdoor Employees' Choice Award, we have been recognized by our employees as a Best Place to Work in 2023. And you don't need a finance background to succeed at Fidelity—we offer a range of opportunities for learning so you can build the career you've always imagined.
At Fidelity, our goal is for most people to work flexibly in a way that balances both personal and business needs with time onsite and offsite through what we’re calling “Dynamic Working”. Most associates will have a hybrid schedule with a requirement to work onsite at a Fidelity work location for at least one week, 5 consecutive days, every four weeks. These requirements are subject to change.
We invite you to Find Your Fidelity at fidelitycareers.com.

Fidelity Investments is an equal opportunity employer. We believe that the most effective way to attract, develop and retain a diverse workforce is to build an enduring culture of inclusion and belonging.
Fidelity will reasonably accommodate applicants with disabilities who need adjustments to participate in the application or interview process. To initiate a request for an accommodation, contact the HR Accommodation Team by sending an email to accommodations @fmr.com, or by calling 800-835-5099, prompt 2, option 3."
278,Data Analytics Engineer,Intelligent Medical Objects,Remote,N,"Research shows that women and underrepresented groups only apply to jobs only if they think they meet 100% of the qualifications on a job description. IMO is committed to considering all candidates even if you don’t think you meet 100% of the qualifications listed. We look forward to receiving your application!

Work that is meaningful. A job that has impact. Colleagues that inspire. That’s what you’ll find at Intelligent Medical Objects (IMO), a growing health IT company creating clinical terminology and insights solutions that are used by more than 740,000 US physicians and 4,500 US hospitals to power better patient care and support meaningful analytics.

The Data Analytics Engineer will support our software developers, database architects, data analysts, and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Join our growing Software Engineering department as a Data Analytics Engineer to help design, create, and support high quality solutions that support 80% of US clinicians and build the application of Data Engineering within IMO!
Responsibilities
Build complex visualizations that tell a story about data.
Embed analytical solutions into web and mobile applications using embedded analytics.
Design, develop and maintain business intelligence solutions using PowerBI/Tableau, SQL, and other tools
Maintain analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Develop proficiency in our tech stack for optimal ETL from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Deliver quality products leveraging the values of transparency, inspection, and adaptation in an agile way. Takes ownership to proactively anticipate the implications and consequences of situations and acts appropriately to make decisions.
Requirements
A relevant technical BA/ BS Degree and one year of experience, or three years of relevant professional experience implementing well-architected data pipelines that are dynamically scalable, highly available, fault-tolerant, and reliable for analytics and platform solutions.
Strong experience with SQL and data warehousing techniques. Work with relational and NoSQL databases such as PostgreSQL, Dynambo DB, MongoDB, and Elasticsearch.
Expertise in data visualization using tools like Power BI, Tableau, or angular/Kendo.
Experience with embedded analytics, including integrating BI dashboards into web and mobile applications.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
Experience with manipulating, processing, and extracting value from disconnected datasets.
Experience with AWS cloud services, such as EC2, EMR, RDS, Redshift.
Experience with object-oriented/functional scripting languages, such as Python, Java, C++, Scala, etc.
#LI-JI1
#LI-Remote

At IMO, we celebrate diversity and are committed to creating an inclusive environment for all employees. IMO is proud to be an equal opportunity workplace and is an affirmative action employer.

IMO also provides visa sponsorship opportunities. Please don't hesitate to apply if you meet all the qualifications for this position and require visa sponsorship."
279,Data Engineer,"Second Wave Delivery Systems, LLC",Remote,N,"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build and maintain scalable, high-performance data processing systems (batch and/or streaming)
Lead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable
Design our data models for optimal storage and retrieval and to meet critical product and business requirements
Contribute to tooling & standards to improve the productivity and quality of output across the company
Work and consult with various non-technical stakeholders
Participate in interviewing and onboarding of new team members

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
5+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks
Expert knowledge of relational databases and query authoring (SQL).
Strong experience developing in one of the following - Python, Java, Scala
Experience with building and managing data pipelines
Experience with big data technology, e.g: Airflow, Spark, Google Cloud Composer

Preferred Qualifications
Demonstrated experience developing machine learning algorithms in the healthcare field
Experience extracting and correlating medical findings from unstructured data such as PDF charts
Experience with big data analysis tools such as BigQuery and Looker"
280,Model-e Data Engineer,Ford Motor Company,"Dearborn, MI",N,"Job Description & Qualifications
At Ford Motor Company, we believe freedom of movement drives human progress. We also believe in providing you with the freedom to define and realize your dreams. With our incredible plans for the future of mobility, we have a wide variety of opportunities for you to accelerate your career potential as you help us define tomorrow’s transportation.

As a data engineer in the Data Platforms and Analytics Team you will communicate and collaborate with architects, product managers and feature teams to provide core-platform components that support the creation, pipelining and consumption of data and telemetry. You will be designing, coding, and testing data pipelines and models, as well as the production processes around them, such as deployment and health-monitoring.

In this role, you will design, build, and scale data pipelines that transform billions of records of data to measurable insights in a brand new data ecosystem on GCP. You will collaborate with multiple organizations (Model e, Product Development, GDI&A) to convert business goals into data pipeline solutions. This role will work in a dynamic team and embrace lean and agile practices, software best practices, automated testing, and CI/CD to build and support the Vehicle Data Communications Center.

What you'll be able to do:
Design, implement and build end-to-end reusable data pipelines that ingest, transform and load data into data marts on-prem and on GCP.
Perform exhaustive testing to create best-in-class pipelines with maximum performance.
Collaborate with a dynamic team to deliver work in an Agile model.
Participate in daily standups and other agile ceremonies.
Deliver quality products on time with CI/CD capabilities following standard data engineering methodologies.
Partner with product teams, business and enterprise teams to build foundational data sets for analytics.

The minimum requirements we seek:
Bachelors degree in Computer Science, Electrical Engineering, or a related field.
3+ years of experience in SQL, Datawarehousing and working in a Big Data Environment.
2+ years of experience crafting ETL pipelines.
1+ years of experience with Java or Python.

Our preferred qualifications:
Paired programming experience.
Experience with BitQuery, Cloud SQL, or other experience in Cloud Computing.

What you will receive in return:
As part of the Ford family, you will enjoy excellent compensation and a comprehensive benefits package that includes generous PTO, retirement, savings and stock investment plans, incentive compensation, and much more. You will also experience exciting opportunities for professional and personal growth and recognition.

If you have what it takes to help us redefine the future of mobility, we would love to have you join us.
Candidates for positions with Ford Motor Company must be legally authorized to work in the United States. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is available for this position.

We are an Equal Opportunity Employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status.
https://corporate.ford.com/content/dam/corporate/us/en-us/documents/careers/2022-benefits-and-comp-GSR-sal-plan-2.pdf

At Ford, the health and safety of our employees is our top priority. Vaccination has been proven to play a critical role in combating COVID-19. As a result, Ford has made the decision to require U.S. salaried employees to be fully vaccinated against COVID-19, unless employees require an accommodation for religious or medical reasons. Being fully vaccinated means that an individual is at least two weeks past their final dose of an authorized COVID-19 vaccine regimen. As a condition of employment, newly hired employees will be required to provide proof of their COVID-19 vaccination or an approved medical or religious exemption.
Auto req ID
63419BR
State
Michigan
Skill Team
Product Development (PD)
Sub-Component
Model e - Software Development"
281,Software Engineer I- Big Data (Remote),Vericast,"Austin, TX•Remote","$80,000 - $90,000 a year","Company Description

Vericast is a premier marketing solutions company that accelerates profitable revenue growth for thousands of businesses businesses it serves directly by influencing consumer purchasing and transaction behavior at scale while engaging with over 120 million households daily. We are recognized as leading providers of incentives, advertising, marketing services, transaction solutions, customer data and cross-channel campaign management, and intelligent media delivery that create millions of customer touch points annually for their clients. For more information, visit http://www.vericast.com or follow Vericast on LinkedIn.

Job Description

JOB SUMMARY
A Software Engineer I is energized by the thought of developing new system stacks and tools for big data ingestion, processing, and analytics on a multi-petabyte infrastructure. This individual will work on a team of talented engineers responsible for reporting on the delivery of our campaigns and handling technical integrations and data ingestion. They will develop products that help validate our media buys, ingest data into our platform, and generate datasets and reporting for our teams to analyze and monitor delivery and performance of our campaigns.

The Big Data Platform Team owns and operates the world-class big data processing infrastructure that over a dozen engineering teams use to power their 24/7 technical marketing products. We own and operate a large hadoop+spark processing cloud on which we store 6PB of data, and run 30 thousand jobs every day. We write a fabric of java microservices and bots to manage all those jobs, extend hadoop's functionality, and help us operate and optimize our investment. We write custom data streaming services that ingest and curate 100TB of data each day that drives the entire advertising data ecosystem. We also participate in all our users' groundbreaking workflow projects so that we can constantly stay abreast of our developer's tooling needs.
What you're like:
This position is perfect for a far-sighted engineer who always wants to be the first to apply cutting-edge technologies to solve complex business and engineering problems. You work throughout the software lifecycle including requirements analysis, development, testing and operations. If you are energized by the thought of developing new system stacks and tools for big data processing and analytics we want to talk to you. If you have worked on big data engineering, cloud migration, or infrastructure tooling projects, we want to talk to you. If you have ever worked on a service mesh or a collection of data workflows and thought 'I could make this better', we want to talk to you!
KEY DUTIES/RESPONSIBILITIES
Proficiency in Java/Scala/Python programming; experience with microservices; exposure to Spring Boot
Curiosity to learn and apply new technologies
Excellent problem-solving abilities
Excellent verbal and written communication skills
Experience with agile development methodologies
Process and analyze high volume data to mine valuable chunks of information using a combination of Scala, Python and Spark
Analyze input and output data to discover interesting signals, validate outputs, gain understanding of systems
Build powerful systems from simple building blocks while managing the complexity
Contribute to our team's continuous efforts to improve quality and efficiency of development platforms, tools, and processes
Strong in Java, Scala and/or Python programming language.
Exposure to data process with Hadoop, Spark, Kafka and/or Spring Boot micro services

Qualifications

EDUCATION
Bachelor's Degree in Computer Science or other technical discipline (e.g., Engineering, Mathematics, or Physics) (Required)
In lieu of the above education requirements, a combination of experience and education will be considered.
Two-year degree in Computer Science with relevant and high-performing work experience would be considered.
EXPERIENCE
0 - 2 years Relevant Experience (Required)
KNOWLEDGE/SKILLS/ABILITIES
0-2 years Skills required: programming, design, testing, standard platform technologies (e.g. Microsoft, Java, Python, etc), SQL databases, independent thought, and methodical work habits.
0-2 years Skills desired: big data techniques; high scalability computing techniques; ability to program in both web technologies (web-based UI; web services; etc.) and/or back-end (Java, C#, or C++) services.
Learning team service architecture

Additional Information

Salary: $80,000 - $90,000
Position is eligible for an annual bonus incentive program

The ultimate compensation offered for the position will depend upon several factors such as skill level, cost of living, experience, and responsibilities.
All team members are responsible for demonstrating the company's Core Values at all times and for using Performance Excellence principles to continuously improve effectiveness, efficiency, products, and services. This includes, but is not limited to, participating on improvement teams, recommending, and implementing improvement ideas, and participating in training and other activities to keep up to date on processes, information, etc.
All team members are responsible for supporting and complying with internal and external audits, to include providing information, performing assigned tasks to ensure compliance, and preparing and maintaining evidence that key duties identified as internal controls have been performed.
All team members are responsible for supporting and complying with safety and security policies to promote a healthy working environment.
Vericast offers a generous total rewards benefits package that includes medical, dental and vision coverage, 401K and flexible PTO. A wide variety of additional benefits like life insurance, employee assistance and pet insurance are also available, not to mention smart and friendly coworkers!
At Vericast, we don’t just accept differences - we celebrate them, we support them, and we thrive on them for the benefit of our employees, our clients, and our community. As an Equal Opportunity employer, Vericast considers applicants for all positions without regard to race, color, creed, religion, national origin or ancestry, sex, sexual orientation, gender identity, age, disability, genetic information, veteran status, or any other classifications protected by law. Applicants who have disabilities may request that accommodations be made in order to complete the selection process by contacting our Talent Acquisition team at talentacquisition@vericast.com. EEO is the law. To review your rights under Equal Employment Opportunity please visit: www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf."
282,Senior Data Engineer (USA Remote),Blue Orange Digital,Remote,"$130,000 - $160,000 a year","Company Overview:
Blue Orange Digital is a cloud-based data transformation and predictive analytics development firm with offices in NYC and Washington, DC. From startups to Fortune 500's, we help companies make sense of their business challenges by applying modern data analytics techniques, visualizations, and AI/ML. Founded by engineers, we love passionate technologists and data analysts. Our startup DNA means everyone on the team directly contributes to the company's growth.
Position Summary:
Blue Orange is seeking a Senior Data Engineer to join our talented multi-disciplinary team. The ideal candidate will have a passion for data analytics and quality. The ideal candidate will be well versed in SQL and modern data technologies, understand how to drive the extraction of business requirements for data visualizations & reporting, build and maintain robust data pipelines, assess data quality, and have excellent communications skills. This candidate will work on analyzing and extracting information from source data, help interpret and visualize data, define data quality needs, and help make sure they are met.
Responsibilities:
Work with the team and stakeholders to define requirements for the Data Warehouse and BI reports
Build and maintain robust end-to-end data pipelines
Work with source data systems to extract and analyze data to assist in defining the requirements for the data platform
Develop and maintain dashboards and reports, and model data based on client requirements
Work with the data team to implement the data quality standards and ensure ongoing quality assurance of that data process
Build robust and reusable codebases in Python
Support analytical and statistical reporting requirements and work with the data teams and subject matter experts to visualize the data in dashboards and reports
Work within an Agile environment to constantly deliver value for our clients.
Basic Qualifications:
5+ years of experience with data engineering, and data analysis, including data cleansing, and data visualization
Expert-level skills with SQL and statistical methods to analyze and model behavioral data
Expert-level skills in Python
Experience with dimensional modeling
BA or BS degree in a technical or quantitative field (e.g.: statistics, economics, computer science)
Excellent verbal and written English communication
Interacts with others using sound judgment, good humor, and consistent fairness in a fast-paced environment
Preferred Qualifications:
Hands-on experience with SAS
Experience in the Finance or Real estate industries
Our Benefits Include:
401k Matching
Unlimited PTO
100% remote role with an option for hybrid
Healthcare, Dental, Vision, and Life Insurance
Home office stipend
Salary: 130,000 - 160,000 annual salary (USD $)

Background checks may be required for certain positions/projects.
Blue Orange Digital is an equal-opportunity employer."
283,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
284,Data Engineer,N,Remote,N,"JOB SUMMARY
The Data Engineer is responsible for modeling complex problems, building pipelines, maintaining ETL processes and troubleshooting issues within a cloud environment. They utilize cloud databases, Databricks and databases to support a robust infrastructure which drives large, revenue generating data strategies.

ESSENTIAL DUTIES
Participate in use case feasibility discussions and translate business idea / business problems into use cases.
Provide support as needed to maintain and update models running in production environment.
Develop and maintain complex ETL processes and algorithms
Own and enhance ETL processes to move data from PMS to cloud databases
Monitor and troubleshoot processes on a daily basis
Document new and existing processes
Proactively and independently identify performance issues and recommend enhancements
Modernize legacy data models and pipelines using Databricks and cloud database capabilities.
QA and validate data moving between various parts of the company to ensure accuracy
Understand business problems/needs and provide proposed solutions.
KNOWLEDGE & REQUIREMENTS
REQUIRED QUALIFICATIONS
Experience working with healthcare professionals in a clinical setting
2+ years of data analysis experience
Understanding of algorithm design, machine learning, and applied statistics
Proven track record in use of SQL specifically in cloud databases and working with data including extracting information, validating data, creating and maintaining custom data structures.
Bachelor’s degree in Business, Computer Science, Information Systems or equivalent combination of education and experience.
Be able to work well with people of various backgrounds and education levels and establish cooperative working relationships with all coworkers.
Timely and effectively communicate information to and consult with others in order to complete work assignments.
Act in a responsible, trustworthy and ethical manner that considers the impact and consequences of one’s actions or decisions.
Communicate ideas, thoughts, and facts in writing through the use of proper grammar, spelling, document formatting and sentence structure.
Identify and respond to current and future clients’ needs; provide excellent client service.
Evaluate and analyze problems or tasks from multiple perspectives; adaptively employ problem solving methods to find creative or novel solutions; use logical, systematic and sequential processes to solve problems.
Complete assigned job tasks in an accurate and timely manner.
Carefully prepare for meetings and presentations; follow up with others to ensure that agreements, tasks or commitments have been fulfilled.
Demonstrate commitment to achieving Company’s core business objectives of increasing the role of pharmacy and improving patient health in America.

DESIRABLE QUALIFICATIONS
Experience working with healthcare professionals in a clinical setting.
Experience resolving issues that do not have clear answers.
Experience with Databricks (preferred), Talend, Informatica or other ETL tools.
Experience working in cloud databases (BigQuery preferred)
Highly motivated and possessed excellent interpersonal, problem solving, and technical skills.
High sense of urgency and accountability
Adaptable, friendly, and ability to work with a team.
Excellent attendance
Passion for data and digging into the minutia of datasets.
Take calculated risks based on data-driven analytics
Be a self-starter
Enjoy working in a fast-paced environment."
285,Data Platform Engineer,Holman,"Mount Laurel, NJ",From $110 an hour,"At Holman, we exist to provide rewarding careers and better lives for employees and their families. We hire, train, empower, and reward exceptional people. Our journey is guided by our desire to get it right every time and the acknowledgement that we have an opportunity to be better. To be better, we have to do better, and to do better we must know better. That’s why we are listening, open to learning new things – about ourselves and each other. We will never stop striving for improved diversity, equity, and inclusion because we are successful together when we feel trusted and supported. It’s The Holman Way.

At Holman, your total compensation goes beyond your paycheck. To position you for success and provide a rewarding career and better life for you and your family, Holman is proud to offer you the benefits you deserve; including protection against illness, disability, loss of work, or preparation for retirement. Below is a brief overview of these programs:

Health Insurance

Dental Insurance

Life and Disability Insurance

Flexible Spending and Health Savings Accounts

Employee Assistance Program

401(k) with Employer Match

Paid Time Off

Tuition Reimbursement

Exclusive pricing and concierge sales support on new and used vehicles

Holman is currently accepting applications for the role of Data Platform Engineer

Principal Purpose of Position:
Design, develop, document and execute data solutions, tools and practices
Analysis of requirements at sufficient level of detail to allow ETL solution to be developed
Development of ETL job flows according to company standards for naming, performance, restartability and performance.
Support testing and remediation of defects in newly-developed/modified ETL workflows
Promote ETL workflows to PROD and provide ongoing support in PRODUCTION, including monitoring and troubleshooting
Ability to create Power BI Datasets to support the Analytic Delivery team
Evaluate emerging data platform technologies
Lead technology implementations
Follow and contribute to best practices for data management and governance
Collaborate with the Data Architecture team to understand and implement load processes for reporting and analytic data structures (data warehouses, data marts and data lakes)
Performance tune and troubleshoot processes under development and in production as necessary.
Work with the Data Architects to augment ERD’s as changes are developed
Develop, maintain, and extend reusable data components
Provide timely project and task updates to all concerned parties
Monitor production data integration jobs and correct failures in a timely manner
Create and manage incident reports as they pertain to data integration processes
Perform all other duties and special projects as assigned.

Required Experience/Skills

2+ years Azure exposure (Any Resources: Databases, Data Factory, Synapse Studio, Storage Account, Power Platform)
2+ years ANSI SQL experience
1+ years data modeling exposure
Advanced problem solving/Critical thinking mindset

Preferred Experience/Skills

Azure connectivity/authentication (service principals, managed identities, certificates)
Power BI Dataset creation/maintenance
Azure Resources: DevOps, Logic Apps, Gen 2 Storage, Purview
SQL Server, Oracle, Python, Spark

Education and/or Training:
Bachelor’s degree in Computer Science or equivalent work experience

Compensation: Starting at $110,00 USD

Holman is a global automotive leader that serves both commercial and consumer clients The Holman Way by always doing the right thing for our people, our customers, and the community since 1924. The Holman story began nearly a century ago as a single Ford dealership in New Jersey. Today, Holman, headquartered in Mount Laurel, New Jersey, is one of the largest family-owned automotive service organizations in North America with more than 6,500 employees across North America, the UK, and Germany.

Holman delivers a unique range of automotive-centric services including industry-leading fleet management and leasing; vehicle fabrication and upfitting; component manufacturing and productivity solutions; powertrain distribution and logistics services; commercial and personal insurance and risk management; and retail automotive sales as one of the largest privately owned dealership groups in the United States. Guided by its deeply rooted core values and principles, Holman is continuously Driving What’s Right.

Holman provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training."
286,Data Engineer - Remote,ITEOM,Remote,"$130,000 - $160,000 a year","Our client is seeking a Data Engineer who would be responsible for design of data warehouse schemas as well as end-to-end design and implementation of fault tolerant and scalable data processing pipelines using a variety of technologies for orchestrating data movement, primarily Snowflake , DBT and Airflow . You'll work with a team of data engineers to design and implement solutions that provide business-critical insights.
Salary : Mid-Level - 130K to160K+ bonuses and equity options
Location: REMOTE WORK 100% is an option: (locations limited to CO, AZ, TX, VA, CA, GA, HI, MN, MI, NJ, NY, NC, OR, PA, WA)
How You’ll Do It
Working collaboratively with a team of other data engineers and developers of varying experience levels and areas of expertise, you'll be responsible for:
Responsibilities
Design and implement data warehouse schemas to store large data sets that can be easily and efficiently queried and joined together for reporting and analytical purposes.
Design and develop ETL/ELT pipelines to efficiently move and aggregate data, while optimizing for maximum code and data reuse.
Collaborate with other team members on improvements to existing systems
Investigate data anomalies and provide quick resolutions.
Provide technical support to business users and analysts.
What We’re Looking For
TOP SKILLS: ETL, Data Warehousing, SQL, Jinga Scripting, Python and Snowflake.
3+ years of working experience with large scale data warehouse systems.
Very strong knowledge of SQL and data manipulation best practices
Experience in building efficient and fault tolerant ELT/ETL data pipelines
Very strong knowledge of working with large scale datasets and data modeling and data warehouse design
Experience with Snowflake is preferable
Knowledge of DBT, Jinja scripting and Airflow is a big plus
Nice to Have
Experience working cross functionally with product and engineering teams
Desire to wear many hats and work your tail off for a bit - great earning potential and equity available.
Benefits: A diverse benefit package, sign on bonus, options, etc. is offered through our client.
About ITEOM: ITEOM Digital Technology Talent Partners collaborates with top technologists and top employers to align purpose and passion and re-incorporate belonging to make the job search and work experience more human. We are an equal opportunity employer; however, this role is not eligible for sponsorship at this time. Employees eligible for full time W2 employment are encouraged to apply."
287,Azure Data Engineer,"DevCare Solutions Pvt., Ltd.",Remote,$80 - $85 an hour,"This position will require qualified T-SQL Developers to take thelead in thefollowing tasks:
Rationalizing and mapping data between transactional and dimensional database models/systems
Working closely with the database and system administrators to develop stored procedures and ETLprocesses related to thedata warehouse system.
Working in collaboration with vendors and other agencystaff to design,develop and managethe creation Synapse pipelines.
Ability to work independently and cooperatively as part of a team.
Ability to work under severetime constraints.
Must possess analytical and complex problem-solving skills.
REQUIRED SKILLS/EXPERIENCE YEARS
Extensive experience with development of stored procedures and ETL processes using T-SQL-7 years
Thorough understanding of data warehouse design hierarchies such as star and snowflake schemas-7 years
Use of ALM tools for work item management, version control, code analysis, and testing-7 years
Broad and extensive knowledge of the software development process and its technologies-7 years
Familiarity with continuous integration-7 years
Experience creating and scheduling elastic jobs-5 years
Experience with designing and modelling database structures based on business use cases-5 years
Job Type: Contract
Salary: $80.00 - $85.00 per hour
Experience level:
10 years
9 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote"
288,Data Communications Engineer - III,GTA (Global Technology Associates),"San Francisco, CA","$91,144 - $180,999 a year","Looking for a Data Communications Engineer - III
What you will be doing as a Data Communications Engineer – III
This member of technical staff will be responsible for managing the standalone IT network and 4G/5G infrastructure and mobile edge cloud environments for a test and development lab program.
This is a technical position focused on supporting the development of technology solutions testing and incubation of new platforms and the product engineering of greenlit projects.
In this role the candidate will work with internal technology and product groups and external partners to support the development of new solutions.
He/She will be responsible for the architecture design and deployment of new 4G/5G and MEC platforms and features and will work very closely with Device testing Product engineering Business groups and peer TPD teams.
The candidate will be a member of a small team of Sr. Network engineers who manage remote lab access configure and manage virtual environments to support projects monitor and troubleshoot problems and make recommendations on how to streamline development with internal and external partners.
What you will bring to the table as a Data Communications Engineer - III
BA in IT/EE or Computer Engineering
The ideal candidate needs to have a strong technical background with experience in wireless and wireline networks hands on integration experience with MEC/Cloud/IT
Experience working in a customer facing environment Expert understanding of computer technologies and architectures including O/S Database NFV Cloud and SDN.
Passion for working with cutting edge technology Ability to work well in a team environment.
What you didn’t know about us:
Competitive salary
Health, Dental and Vision Benefits
Short/Long Term Disability and Critical Care/Illness Protection
Life Insurance and Retirement Plans
Employee Assistance Program
With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: $91,144.17 - $180,998.90 per year
Work Location: In person"
289,AWS Data Engineer - Evernorth Health Services - Remote,Cigna,"Hartford, CT 06152•Remote","$100,500 - $167,500 a year","Responsibilities:
Closely work with technical leadership to understand requirements.

Should have strong hands-on cloud experience with AWS suite.

Should have strong development experience on Databricks components.

Strong hands-on experience on Spark is must (Python or Scala)

Should be able to demonstrate work experience in Hadoop.

Will be asked to demonstrate coding.

Participate in sprint planning sessions and help TPOs.

Strong experience on DevOps pipeline implementation

Troubleshooting the issues, provide effective solutions and application monitoring in productions environment.

Willing to take more responsibilities as needed.

Moderate to senior Terraform development experience needed.

Health care domain knowledge is a plus.

Skills:
Strong development experience in Spark, Python, Shell scripting, and Hadoop

Development experience on AWS services like S3, EC2, SNS, SQS, Lambda, ECS, Glue, IAM, and CloudWatch etc.

Development experience using Terraforms to manage AWS resources/ deployments.

Development experience on Databricks components like Delta-lake tables, Notebooks, Pipelines, cluster management, AWS integration

ETL development experience using AWS services.

Development experience on AWS data pipelines (on-prem to AWS or AWS-AWS)

In depth knowledge on Hadoop and its underlying components

Strong experience in writing complex and effective SQLs, and Stored Procedures

Strong query optimizations skills to improve perforce of ETL process.

Experience on job orchestration and management

If you will be working at home occasionally or permanently, the internet connection must be obtained through a cable broadband or fiber optic internet service provider with speeds of at least 10Mbps download/5Mbps upload.

For this position, we anticipate offering an annual salary of 100,500 - 167,500 USD / yearly, depending on relevant factors, including experience and geographic location.

This role is also anticipated to be eligible to participate in an annual bonus plan.

We want you to be healthy, balanced, and feel secure. That’s why you’ll enjoy a comprehensive range of benefits, with a focus on supporting your whole health. Starting on day one of your employment, you’ll be offered several health-related benefits including medical, vision, dental, and well-being and behavioral health programs. We also offer 401(k) with company match, company paid life insurance, tuition reimbursement, a minimum of 18 days of paid time off per year and paid holidays. For more details on our employee benefits programs, visit Life at Cigna Group .

About The Cigna Group
Doing something meaningful starts with a simple decision, a commitment to changing lives. At The Cigna Group, we’re dedicated to improving the health and vitality of those we serve. Through our divisions Cigna Healthcare and Evernorth Health Services, we are committed to enhancing the lives of our clients, customers and patients. Join us in driving growth and improving lives.

Qualified applicants will be considered without regard to race, color, age, disability, sex, childbirth (including pregnancy) or related medical conditions including but not limited to lactation, sexual orientation, gender identity or expression, veteran or military status, religion, national origin, ancestry, marital or familial status, genetic information, status with regard to public assistance, citizenship status or any other characteristic protected by applicable equal employment opportunity laws.

If you require reasonable accommodation in completing the online application process, please email: SeeYourself@cigna.com for support. Do not email SeeYourself@cigna.com for an update on your application or to provide your resume as you will not receive a response.

The Cigna Group has a tobacco-free policy and reserves the right not to hire tobacco/nicotine users in states where that is legally permissible. Candidates in such states who use tobacco/nicotine will not be considered for employment unless they enter a qualifying smoking cessation program prior to the start of their employment. These states include: Alabama, Alaska, Arizona, Arkansas, Delaware, Florida, Georgia, Hawaii, Idaho, Iowa, Kansas, Maryland, Massachusetts, Michigan, Nebraska, Ohio, Pennsylvania, Texas, Utah, Vermont, and Washington State."
290,DATA ENGINEER,Dollar General,"100 Mission Ridge, Goodlettsville, TN 37072",N,"Company Overview:
Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations. Dollar General operates more than 18,000 stores in 47 states, and we’re still growing. Learn more about Dollar General at www.dollargeneral.com.
Job Details:
General Summary:
Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations. Dollar General operates more than 18,000 stores in 47 states, and we’re still growing. Learn more about Dollar General at www.dollargeneral.com.

Duties & Responsibilities:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud technologies.
Build analytics tools that utilize the data pipelines to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Qualifications:
Knowledge, Skills and Abilities:
Knowledge of programming languages (e.g. Java and Python)
Hands-on experience with SQL database design
Great numerical and analytical skills
Degree in Computer Science, IT, or similar field; a Master’s is a plus
Data engineering certification (e.g IBM Certified Data Engineer) is a plus
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with Snowflake/Azure cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc
Work Experience &/or Education:
Degree in information technology or computer science with additional vendor-specific certification.
BS or MS degree in Computer Science or a related technical field
4+ years of Python or Java development experience
4+ years of SQL experience (No-SQL experience is a plus)
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining data processing systems
Experience working with a cloud platform such as Snowflake / Azure or Databricks"
291,Data Engineer,"The Knowlton Group, LLC",Remote,"$80,000 - $120,000 a year","Help credit union get control of their data!
Description
Designs and implements SQL data models for our clients in the credit union industry. This role will involve developing new integrations with data sources our clients leverage, while also deploying existing code into new client environments.

Creative problem solving is essential in this position to quickly address changing client needs and adapt to an evolving dataset we have at our disposal.

Responsibilities:
Develop and manage the movement of data from source through to the data warehouse environment.
Design, test, and implement stored procedures to move data from a staging environment and/or operational data store into the dimensional data model.
Extend existing tables to add new columns and calculations based on client feedback.
Leverages existing data infrastructure to fulfill all data-related requests, perform necessary data housekeeping, data cleansing, normalization, hashing, and implementation of required data model changes.
Work with clients/analysts to define requirements for data desired to be provisioned updating defined requirement document templates.
Work with the technical manager to transform business requirements into appropriate schema and data model.
Troubleshoots problems, identifies possible solutions, and resolves them accordingly.

Daily tasks can include:
Consulting with credit union clients to understand their data requirements.
Use Azure tools to transform data within the data warehouse.
Tune databases and ETL for maximum performance.
Data Profiling, Data Cleansing, and Data Auditing.
Loading large volumes of data.
Decoding and writing complex SQL queries.
Performance tuning of queries and data loading process.
Modeling data into dimensional model structures.

Experience:
Extensive experience with Jack Henry Symitar Episys
Extensive experience with SQL
Experience with credit union and bank vendors (MeridianLink, Temenos, nCino, Fiserv, FICS etc.)
Experience developing with cloud services (Azure or AWS)
Bonus if you have experience in Power BI, Azure Databricks, Azure Data Factory, and Azure SQL.
Must be legally authorized to work in the United States.

Salary
$80,000 - $120,000 per year"
292,"Data Engineer - Cupertino, CA - W2",Techprism.inc,"20644 Acadia Court, Cupertino, CA 95014","$115,875 - $207,192 a year","Greetings,
I have a job opportunity for you, please find the Job Description below. If you are interested, please share your resume along with your contact details.
Looking only w2
Job Title: Data Engineer
Durations: 12 + Months contract Role.
Location: Cupertino, CA/ Remote
Roles & Responsibilities:
Remotely Will Work with Clients To Develop the new components for data pipelines and data collection software.
Drive operational efficiency by maintaining their data ecosystems and providing As-a-Service offerings for continuous data collection improvements
Implement large-scale data ecosystems including data management, governance, and the integration of structured and unstructured data
Report to client on daily basis. Work you’ll do
Localize an in-house software to adapt the French speaking environment.
Implement at-scale data collection pipeline, include but not limit to developing the pipeline of data from raw to curation layers, such as cleansing, transformation, derivation, and aggregation of data.
Support in the development of technical solutions to business problems, such as design, develop new databases, leverage the dev-ops platforms such as Kubernetes, etc.
Communicate effectively (written and spoken) and work with the multi-location development teams and self-manage own work.
Key Qualifications & Skills required
Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
Fluent in French and English
5+years of hands-on experience as a Software Developer or Data Engineer
5+ years of experience in Python and SQL, foundational to advanced knowledge including relational database experience
3+ years of experience in Spark / Unix Shell Scripting, including performance tuning, working with Data frames, and code optimization
Experience defining, analyzing, and documenting functional and technical requirements
Ability to communicate effectively (written and spoken) and work with the multi-location development teams and self-manage own work
Strong customer facing skills with the ability to articulate business value and communicate features of the end-product Additional Requirements
Complex software development, test, maintenance experience is a big plus.
Hands on big data/ Hadoop performance tuning and optimization experience
Developed or applied Dev-Ops platform experience
Job Types: Full-time, Contract
Pay: $115,874.56 - $207,192.05 per year
Schedule:
Monday to Friday
Ability to commute/relocate:
Cupertino, CA 95014: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person"
293,Business Analytics Engineer– Clinical Data Products,CVS Health,"Hartford, CT","$70,000 - $140,000 a year","CVS Health, a healthcare innovation company, is committed to providing Individuals, Employers, Healthcare professionals, producers, and others with innovative benefits, products, and services. The industry is evolving and the opportunities to drive real change and improve the health of millions are endless. If you are passionate about making a difference in the work you do, being recognized for your talent and expertise in solving complex and challenging clinical data problems while contributing to the success of key strategic initiatives, consider growing your career with a Fortune 5 healthcare leader!

Position Summary:
As a member of the Data and Analytics organization, you will be responsible for building and delivering best-in-class clinical data initiatives aimed at driving best-in-class solutions. You will collaborate with analytic partners and business partners from product strategy, program management, IT, data strategy, and predictive analytics teams to develop effective solutions for our partners.

Job responsibilities:
Perform Health care Data Analysis, Data profiling of Raw/source data to derive meaningful insights, and document data requirements to support the new data source onboarding process
Determines approaches to Interoperability of Clinical/Healthcare data, including mapping of clinical content to data standards to support use case requirements.
Analyze and Validate Data to ensure the appropriate data modeling, vocabulary, terminology, and code-set standard for representing and harmonizing Healthcare data
Facilitate problem-solving sessions with business stakeholders bringing in all relevant stakeholders.
Influence/guide stakeholders as clinical data expert and shaping up business requirements.
Collaborates with technical resources to perform root cause analysis and complete remediation of data quality issues
Understand and examine healthcare data received from different applications/vendors & build the data modeling/mapping for converting the raw Healthcare data into FHIR format
Perform Business acceptance testing on FHIR data
Interact with different business units to translate their data needs into requirements

Pay Range
The typical pay range for this role is:
Minimum: 70,000
Maximum: 140,000

Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will take into account a wide range of factors, including location.

Required Qualifications
1 to 3 years of experience with FHIR/HL7 platforms & technologies and knowledge of Healthcare Industry standards and requirements
1 to 3 years’ experience in Continuity of Care Documents (CCD), Consolidated Clinical Document Architectures (CCDA), HL7 V2 and HIE ITI transactions data handling
1 to 3 years of experience in Interoperability and FHIR R4 concepts.
Strong understanding of Healthcare standards (HL7, FHIR, ICD, SNOMED, CPT, LOINC)
Strong knowledge of FHIR APIs and FHIR resources
Ability to develop source to target maps, mapping to convert HL7 v2 and FHIR
Perform functional and business acceptance testing of FHIR outputs
Strong SQL skills (preferably Big Query)
Proficient in utilizing MS Excel for reporting and documentation needs
Excellent communication and workflow documentation/diagrams skills are a must

Preferred Qualifications
Rich experience with Interoperability of clinical data formats and standards (HL7)
Strong understanding and knowledge of HL7 V2 messages, HL7 C-CDA, HL7 FHIR, and HIE ITI transactions
Working experience with the following HL7 interfaces/messages: ADT, ORM, ORU, RDE, and MDM with a good understanding of the principles of asynchronous information exchange interface
Working in a fluid environment, defining, and owning priorities that adapt to our larger goals.
Should have a strong understanding of healthcare data, including clinical data in proprietary and industry standard formats.
Understand the dirtiness of clinical data and seek out ways to improve conformance and quality.
Knowledge of patient privacy, patient consent, deidentification, data encryption

Education
Required: Bachelor’s degree in one of the following fields Business Information Systems, Computer Science, Engineering, Business Statistics.
Preferred: Master’s degrees in one of the fields above

Business Overview
Bring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities."
294,Remote Data Engineer,Edgesource,"Alexandria, VA 22314•Remote",N,"EOE Statement
We are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status or any other characteristic protected by law.
Category
Information Technology
Description
Experience:
**Fully Remote** Must be US Citizen and able to obtain a TS/SCI
Develop standardized data architecture that includes data structure and transfer protocols to facilitate sensor integration and dynamically share information to improve situational awareness.
Develop a standardized architecture that supports a centralized data repository that advances all data analytics, and AIML capabilities enhance command and control decisions.
Design and build end-to-end data pipeline solutions (esp. streaming and batch processing, machine learning model training and updating).
Develop strategies for data acquisitions, archive recovery, and implementation of a database.
Define, design, and build dimensional databases.
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
Writes unit/integration tests, contributes to engineering wiki, and documents work.
Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Works closely with a team of frontend and backend engineers, product managers, and analysts.
Designs data integrations and data quality framework.
Works closely with all engineering teams to develop strategy for long term data platform architecture.

Qualifications / Skills:
SQL/MySQL/PostgreSQL
Microservice deployment, cloud certification (AWS architect)
Education, Experience, and Licensing Requirements:
BS or MS degree in Computer Science or a related technical field
2-4+ years of experience as a data engineer
U.S. Citizenship with ability to obtain government security clearance

Position Requirements

Full-Time/Part-Time: Full-Time

Shift: -not applicable-

Position: Data Architect

Number of Openings: 1

Req Number: INF-20-00012

Open Date: 8/24/2020
Location: Mountain View
About the Organization: Edgesource Corporation is a leading small business providing information technology and business consulting services to the federal government. We are a stable, growing business offering excellent growth potential for business minded professionals."
295,Senior Data Engineer | Remote | Contract to HIre,Spartan Technologies,"Dallas, TX•Remote",N,"Spartan Technologies, Inc. - Dallas, TX, United States
We seek an experienced Senior Data Engineer in the United States to work remotely in the Central US or Eastern US time zones.
Prefer local candidates to Austin, TX that will convert fulltime after 6 months. Must be a GC Holder or US Citizen in order to be considered for this role.
Your Job
As a Senior Data Engineer, you will take on a technical leadership role in a dynamic and collaborative team environment. You will have the opportunity to work on complex business problems, design and develop data pipelines, data warehousing, and data transformations while applying your knowledge of modern software design and best practices. You will work alongside engineers and technical leads in a team environment that encourages collaborative programming and mentoring.
Most import technology:
SQL skills
Snowflake
AWS (Lambda, step functions, cloudwatch, glue (haven't used but nice to have), cloudformation, sns, sqs)
Python (+pandas)
Oracle

The Work
As a Senior Data Engineer, you will be responsible for:
Constructing and managing services to extract data from disparate databases, transform the data, and load it into a Snowflake data warehouse.
Developing high-performing, scalable, and secure solutions to complex business problems.
Collaborating with team members on best practices, code reviews, internal tools, and process improvements.
Influencing technical solutions while coaching newer or less experienced members on your Scrum team.
Planning and delivering core technology upgrades.
Diagnosing, designing, and implementing solutions to key technology or application problems.
Evangelizing new ideas within your team as well as across teams.
Building patterns to support data pipelines, data warehousing, and data transformations.
Qualifications
To be considered for the position of Senior Data Engineer, you must have:
6+ years of professional software development experience.
A degree in Computer Science or a related field (or equivalent work experience in lieu of education).
Experience with ETL and ELT processes.
Data Warehousing experience, including Snowflake or other data warehousing tools.
AWS/Cloud experience.
Programming experience with data sets in Python, Scala, Java, or C#.
Understanding of SQL, relational databases, columnar data warehouses, and data modeling.
Experience with automated infrastructure tooling.
A history of taking applications from conception/design to implementation/support.
Experience designing and implementing applications with highly optimized and scalable architectures."
296,Senior Data Engineer (No C2C),NOVUS Professional Services Inc.,"Parker, CO•Hybrid remote",$60 - $90 an hour,"Senior Data Engineer (No C2C accepted)

Please read: We are unable to accept Corp to Corp candidates.

Hybrid work environment (3 days in the office) at Parker, CO,
NOVUS Professional Services is seeking a Data Engineer, with at least 6 years of experience for our Denver based client. The successful candidate will perform data collection, analysis, validation, and reporting. Design, test, document processes, and write SQL queries. Extract and analyze data from various sources, including databases, manual files, external websites, and MS Excel. Responds to data inquiries from various groups within an organization. Document reporting requirements and processes. Validates data components as required. Requires experience with relational databases and knowledge of query tools and/or statistical software. Strong analytical and organizational skills are also essential. 6+ years of experience.
Skill Requirements:
Advanced SQL skills.
Good working knowledge of MS Excel, PowerPoint, and Microsoft office products.
Engineer datasets using Python to support reporting in Tableau.
Identify and recommend continuous improvement opportunities.
Integrate into an Agile (Scrum-Ban) team within a SAFe framework.
Actively participate in collaborative design and review sessions; pushing innovation while observing best practices.
Observe the organization, identify pain points, and think of creative solutions that leverage our data.
Data Privacy Engineer Soft Skills:
Excellent written and verbal communication skills with the ability to capture and articulate technical and non-technical details, as well as elaborate process flows.
A strong, team-oriented spirit, a mindset focused on learning and achieving objectives, and the discipline to work rigorously while unmonitored.
An appetite for working in a fast-paced, quick-changing, environment and spending time on multiple projects.
Ability to present analysis and findings to a large group.
Technical Expertise:
2+ years of Python development experience.
2+ years experience with Snowflake.
2+ years of experience with Tableau.
2+ years of experience with Sigma.
Experience with AWS data stores. (exporting data)
Experience working with structured and unstructured data.
Experience with JavaScript is a plus.
Experience with Linux is a plus.



About NOVUS Professional Services Inc.:

Founded in 1997 in the shadows of Colorado’s Rocky Mountains, NOVUS Professional Services was built from the beginning on three cornerstones: Focus on Clients Focus on Consultants Focus on Technology"
297,Senior Data Engineer,Princeton University,"Princeton, NJ 08542",N,"Overview:
Reporting to the Associate Director, Data Management, the Senior Data Systems Engineer is a member of the Analytics & Data Management Team within the Data Strategy & Innovation unit of Princeton University Advancement.

The senior systems engineer performs a variety of tasks to deliver technology improvements to the Data Warehouse environment. The specific responsibilities of this position include developing and testing extraction, transformation, and load (ETL) processes; defining and capturing metadata and rules associated with ETL processes; adapting ETL processes to accommodate changes in source systems and new business user requirements; working with both technical staff and business constituents to translate business requirements into technical solutions; maintaining technical and user documentation.
Responsibilities:
Data Warehouse Development and Strategy
Develop a Business Intelligence solution framework according to architectural specifications to integrate the following components: operational data store (ODS), APIs, ETLs, as well as complementary tools
Creation and maintenance of the logical and physical data models for the Business Intelligence data warehouse
Design and build Cognos metadata via Framework Manager
Create detailed documentation of the ODS and report specifications as part of project solutions and general production support

Cross-Team and Departmental Collaboration
Work closely with business intelligence analysts to gather, analyze, and document business and technical requirements
Work with other Advancement partners to analyze, and resolve, data quality issues
Provide business analysis support to University Advancement on production issues
Participate with cross-departmental teams in support of the priorities and information needs of the Office
Serve as a solid contributor and driver of the data management team and act as a mentor to other staff

Best Practices and Process Improvements
Responsible for quality management surrounding production support and project initiatives
Review existing business processes and analyze to provide and implement solutions for process improvements
Develop solutions using Lean and Agile software development principles and processes
Serve in a consultative capacity to identify and recommend system improvements and next generation applications of our CRM system
Qualifications:
Bachelor’s degree or equivalent work related experience
5+ years of related experience
Strong experience designing relational and multi-dimensional models
Good analysis and problem-solving skills to understand business information requirements and resolve data quality issues in delivered BI solutions
Experience performing data selection and SQL development using PL/SQL and Oracle Stored Procedures
Strong understanding of data architecture requirements for BI reporting tools (Cognos, MS Reporting, and Tableau preferred)
Experience developing solutions using Excel and MS Access
Experience with office tools such as PowerPoint and Word
Familiarity with project management
Strong customer service orientation
Organizational skills to handle several projects simultaneously to accommodate shifting priorities and meet deadlines
Effective communication, presentation and facilitation skills
Strong initiative, self-motivation and the ability to work both independently and in teams
Demonstrated ability to work effectively and collaboratively with others
High degree of professionalism, a positive ‘can do’ attitude and strong work ethic
Commitment to University Advancement’s mission to inform, involve, and inspire Princeton’s global community of alumni and friends, and adhering to its guiding principles of High Performance, Innovation, Civility, and Collaboration.

Preferred
Knowledge of Princeton’s mission
Experience in higher education
Familiar with the applications in use by the Office of Development including SunGard Advance and OnBase is preferred.

Princeton University is an Equal Opportunity/Affirmative Action Employer and all qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity or expression, national origin, disability status, protected veteran status, or any other characteristic protected by law. KNOW YOUR RIGHTS
Standard Weekly Hours: 36.25 Eligible for Overtime: No Benefits Eligible: Yes Probationary Period: 180 days Essential Services Personnel (see policy for detail): No Physical Capacity Exam Required: No Valid Driver’s License Required: No Experience Level: Mid-Senior Level : #LI-JJ1"
298,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
299,Data Engineer,Jetty,"New York, NY•Remote",N,"Welcome to Jetty, the financial services platform on a mission to make renting a home more affordable and flexible. We've built multiple financial products that benefit both renters and property managers - and we're just getting started.

As a member of the Jetty engineering team, you're passionate about building fintech products that provide value to our customers and to Jetty. You are motivated by designing engineering systems around complex business problems. You love to learn, take on challenges, and are empowered in a fast-paced and transparent culture. You're comfortable finding the right tool or pattern for the job, and advocating for improvements to the way we work.

As a Data Engineer, your goal is to cultivate a data-informed culture and create insights that will be leveraged across the entire organization. You have experience executing at a high level, solving complex problems, and delivering solutions with real business impact - and you're excited by the opportunity to apply those principles to a new, best in class function.

Role & Responsibilities
Build / Support our modern data stack (Snowflake / Fivetran / DBT / Tableau)
Implement the Five Pillars of Data Observability
Write ELT code using modern software engineering practices (Git, automated testing and deployments)
Build and maintain data pipelines to support various business processes and reporting (Fivetran / AWS Lambdas)
Document our data models in a user friendly way for our business stakeholders
Partner with the Product Engineering team to ensure we are capturing the data we need from our applications for analytics and to iterate on our development practices for the data analytics team.
Be an enthusiastic evangelist of our modern data stack (Fivetran / DBT / Snowflake / Tablea)
Be the resident resource on building standard reports and BI dashboards
Experience & Qualifications
4+ years of experience working in a data / analytics engineering role
High proficiency in Snowflake / Fivetran / dbt / Tableau
High proficiency in SQL and Python
Ability to collect, interpret, and synthesize inputs from various parts of the business into data model requirements
Ability to simplify without being simplistic - ability to communicate complex topics and actionable insights in a compelling way that can be understood by a variety of audiences
Inherent curiosity and analytical follow-through — you can't help but ask ""why?"" and love using data and logic to explore potential solutions
Ability to balance ""Rigor"" and ""Scrappiness"" — you know the difference between 80/20 and giving something 110%; as well as when each is appropriate.
Deep understanding of the first and second order effects of reporting — you know the power of presenting the right data to the right people at the right time
Experience in a data/analytics function at a high-growth startup managing multiple stakeholders and delivering actionable insights
About Jetty

At Jetty, we know renting a home can be a financial challenge. That's why we're on a mission to make renting accessible to everyone. Jetty offers four financial products designed to help our members every step of the renting process: Jetty Deposit, a low-cost security deposit product that dramatically reduces move-in costs; Jetty Rent, a flexible rent payment program to eliminate pricey late rent fees; Jetty Credit, a credit building service that helps renters build credit just by paying rent; and Jetty Protect, an affordable renters insurance product that provides comprehensive coverage in just a few clicks.

Jetty has raised multiple rounds of venture capital from investors including Khosla Ventures, Ribbit Capital, Citi, Valar, and strategic investors. We've built a highly collaborative team working remotely around the country, and we believe in finding the best talent—regardless of where they live. To learn more about life at Jetty, visit jetty.com/careers.

Jetty is firmly committed to building a team as diverse as our Members. We are proud to provide equal employment opportunities for all candidates regardless of race, ancestry, citizenship, sex, gender identity or expression, religion, sexual orientation, marital status, age, disability, or veteran status.

Benefits & Perks
Health (with HSA and FSA options), dental, and vision insurance through Aetna & MetLife
401(k) retirement savings program
Optional life and disability coverage
20 days of PTO + 12 holidays, ""Jetty Winter Break,"" and flexible sick days
Generous parental leave policy
Flexible remote work in any US location (keeping east coast hours)
Stipends to cover WFH set-up, childcare, phone/internet bill, and optional co-working space"
300,Data Engineer,Avid Technology,"Burlington, MA 01803•Remote",N,"It's fun to work in a company where people truly BELIEVE in what they're doing!
We're committed to bringing passion and customer focus to the business.
ABOUT AVID
Avid makes technology and collaborative tools so creators can entertain, inform, educate and enlighten the world. Our customers are the visionaries behind the most inspiring feature films, television programs, news broadcasts, televised sporting events, music recording and live concerts. To learn how Avid powers greater creators or for more information, visit
www.avid.com
.

JOB SUMMARY
The Data Engineer role will focus primarily on designing and implementing the data pipeline and common data models that will create the new data foundation for our enterprise analytics function. This will include building the ETL/ELT processes and orchestration layer that will drive the development of our Enterprise Data Warehouse and deliver data for analytics and systems integrations.
RESPONSIBILITIES AND DUTIES
Design, develop, implement, and support overall data eco-system which consolidates into an Enterprise Data Warehouse (EDW)
Maintain constantly evolving Data Vault data model which will serve the business data needs
Build and support ETL/ELT and data architectures within the data eco-system
Contribute to the constantly evolving Data Vault data model which will serve the business data needs
Execute and maintain standards and practices for adding new data sources into ecosystem, development, data quality, and support within the EDW
Ensure proper documentation, versioning, and data cataloging is performed for all data consumed & modelled within the EDW
Implementing comprehensive testing and monitoring processes to ensure data quality and timely error detection
QUALIFICATIONS & SKILLS
Bachelor’s degree in computer science or related field
A minimum of 3-5 years EDW design/development experience or similar type role
Extensive experience with data related programming languages (SQL, python, redshift, etc) a MUST.
Experience developing and maintaining a data warehouse environment within Snowflake.
Understanding of data warehouse architecture techniques (Data Vault 2.0 preferred)
Strong experience with ELT/ETL development and tools (DBT experience preferred)
Familiarity with Bulk Data Integrations (FiveTran experience a plus)
Understanding of data modeling practices and methodologies to deliver data for business use.
Strong analytical, troubleshooting, problem-solving and follow-through abilities, ability to navigate through ambiguous requirements
Have demonstrated verbal and written communication skills, and ability to interface with Business, Analytics, and IT organizations
Ability to work within a team and individually in a fast-paced agile environment
WHAT WE OFFER:
Here are some of the things you can count on when working for Avid:
Being part of one of the most recognized brands in the movie, music and broadcast business (when we perform, Earth shakes and superheroes fly!).
A competitive salary, bonuses and benefits (such as private medical insurance, life insurance and sport package subsidies, depending on location).
Full employment contract.
Flexible working hours.
A modern work from home policy.
Visa sponsorship and relocation support (depending on the position).
Avid is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
#LI-Remote
#LI-MK2
US Pay Transparency Range $105,000.00 - $132,000.00
If you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us !"
301,Associate Data Engineer,MedSpeed,"Elmhurst, IL 60126",N,"Associate Data Engineer
About Us
Come join MedSpeed to help us deliver health! MedSpeed is a healthcare logistics company that partners with healthcare organizations throughout your communities to transport a wide range of medical supplies, specimens, and materials. At Medspeed we work as a team, keep our promises and strive to get better every day. We are looking for individuals who believe in and represent those values.
Today, we have over 140 locations in 32 states but still have kept that small business, entrepreneurial feel and remain committed to the same culture established day 1!
Our people are at the heart of what we do and how we support our customers.
Why become a MedSpeeder? Take a look at what MedSpeed offers:
Medical, Dental, Vision and Flexible Spending Account - We offer plans that help you and your family take care of your whole self.
401(K) with Company Match - Helping you make good financial decisions today and for the future.
Paid Time Off - We value well-being and encourage work life balance.
Opportunities for Career Advancement – Over 50% of our market managers have been promoted into their roles.
Training Provided – Our Blue Shirt Certified program ensures you excel in your role.
What you will be doing as an Associate Data Engineer with MedSpeed:
Gather data from multiple sources and import into our enterprise data warehouse
Maintain, troubleshoot, and work to improve robustness of our data pipelines
Work with stakeholders to address data related issues and support data needs
Implement internal process improvements: automating manual processes, optimizing data delivery, etc.
Create tools for the Analyst and Field Operations teams to enable them to effectively use our data to optimize our business, provide clarity, and improve efficiency
Work closely with the development team on initiatives
Create reports to meet immediate business needs
Build and nurture a collaborative MedSpeed team culture
All other job duties as assigned
What you need to bring to be a MedSpeed Associate Data Engineer:
Bachelor’s degree in Computer Science Statistics, Mathematics, Business Analytics or related field
Strong problem solving and analytical skills
Exposure to R, Python, and C#
Familiar with continuous integration environments (e.g. Azure DevOps, Git, etc.)
Knowledge of Microsoft data tools including SSIS, SSMS, SSRS, Power BI, and T-SQL
Knowledge of ETL and exposure to Type 1 and 2 slow changing dimensions
Knowledge of AWS and Azure
Experience with Salesforce"
302,"Data Engineer - Cupertino, CA - W2",Techprism.inc,"20644 Acadia Court, Cupertino, CA 95014","$115,875 - $207,192 a year","Greetings,
I have a job opportunity for you, please find the Job Description below. If you are interested, please share your resume along with your contact details.
Looking only w2
Job Title: Data Engineer
Durations: 12 + Months contract Role.
Location: Cupertino, CA/ Remote
Roles & Responsibilities:
Remotely Will Work with Clients To Develop the new components for data pipelines and data collection software.
Drive operational efficiency by maintaining their data ecosystems and providing As-a-Service offerings for continuous data collection improvements
Implement large-scale data ecosystems including data management, governance, and the integration of structured and unstructured data
Report to client on daily basis. Work you’ll do
Localize an in-house software to adapt the French speaking environment.
Implement at-scale data collection pipeline, include but not limit to developing the pipeline of data from raw to curation layers, such as cleansing, transformation, derivation, and aggregation of data.
Support in the development of technical solutions to business problems, such as design, develop new databases, leverage the dev-ops platforms such as Kubernetes, etc.
Communicate effectively (written and spoken) and work with the multi-location development teams and self-manage own work.
Key Qualifications & Skills required
Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
Fluent in French and English
5+years of hands-on experience as a Software Developer or Data Engineer
5+ years of experience in Python and SQL, foundational to advanced knowledge including relational database experience
3+ years of experience in Spark / Unix Shell Scripting, including performance tuning, working with Data frames, and code optimization
Experience defining, analyzing, and documenting functional and technical requirements
Ability to communicate effectively (written and spoken) and work with the multi-location development teams and self-manage own work
Strong customer facing skills with the ability to articulate business value and communicate features of the end-product Additional Requirements
Complex software development, test, maintenance experience is a big plus.
Hands on big data/ Hadoop performance tuning and optimization experience
Developed or applied Dev-Ops platform experience
Job Types: Full-time, Contract
Pay: $115,874.56 - $207,192.05 per year
Schedule:
Monday to Friday
Ability to commute/relocate:
Cupertino, CA 95014: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person"
303,DATA ENGINEER,Dollar General,"100 Mission Ridge, Goodlettsville, TN 37072",N,"Company Overview:
Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations. Dollar General operates more than 18,000 stores in 47 states, and we’re still growing. Learn more about Dollar General at www.dollargeneral.com.
Job Details:
General Summary:
Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations. Dollar General operates more than 18,000 stores in 47 states, and we’re still growing. Learn more about Dollar General at www.dollargeneral.com.

Duties & Responsibilities:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud technologies.
Build analytics tools that utilize the data pipelines to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Qualifications:
Knowledge, Skills and Abilities:
Knowledge of programming languages (e.g. Java and Python)
Hands-on experience with SQL database design
Great numerical and analytical skills
Degree in Computer Science, IT, or similar field; a Master’s is a plus
Data engineering certification (e.g IBM Certified Data Engineer) is a plus
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with Snowflake/Azure cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc
Work Experience &/or Education:
Degree in information technology or computer science with additional vendor-specific certification.
BS or MS degree in Computer Science or a related technical field
4+ years of Python or Java development experience
4+ years of SQL experience (No-SQL experience is a plus)
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining data processing systems
Experience working with a cloud platform such as Snowflake / Azure or Databricks"
304,Principal-Big Data Engineer,AT&T,"Alpharetta, GA","$127,100 - $211,900 a year","DUTIES: Create, design, and build core frameworks for cloud-based system platforms and end to end solutions. Build cloud platform and performance components. Enable data sharing across business units in support of BI and analytics related use cases. Collaborate with critical stakeholders to understand business needs, priority, and the quality of services to be created or enhanced. Drive solution to be compliant with target state architecture. Discover and identify bottleneck gaps including manual flows that can hinder services and impact cycle time. Resolve bottleneck gaps early during the solution, design, and construction phase. Drive research and standardization of platform components. Ensure integration of nonfunctional requirements such as security, scalability, resiliency, and performance in all solutions. Define and design functional, technical, performance, and reliability solutions in line with client inferred requirements. Create integrated solutions between legacy and target state solutions. Create vendor guidelines and best practices. Apply advanced knowledge of Linux and Unix. Utilize AWS. Display knowledge of Azure infrastructure including implementing, monitoring, and maintaining Azure compute, storage, network, and security services. Analyze large sets of data using Cloudera HDF NiFi, ADF, Ambari, Nifi, Kafka, Ranger, Kubernetes, and Docker. Manage data using Databricks and Hadoop. Apply knowledge of data domains of Telecomm including location, browsing, billing, and network.

REQUIREMENTS: Requires a Master’s degree, or foreign equivalent degree in Computer Science, Computer Engineering, or Electrical Engineering and Computers and two (2) years of experience in the job offered or two (2) years of experience in a related occupation building cloud platform and performance components; resolving bottleneck gaps early during the solution, design, and construction phase; creating vendor guidelines and best practices; applying advanced knowledge of Linux and Unix; utilizing AWS; displaying knowledge of Azure infrastructure including implementing, monitoring, and maintaining Azure compute, storage, network, and security services; analyzing large sets of data using Cloudera HDF NiFi, ADF, Ambari, Nifi, Kafka, Ranger, Kubernetes, and Docker; managing data using Databricks and Hadoop; and applying knowledge of data domains of Telecomm including location, browsing, billing, and network.

Our Principal Big Data Engineers earn between $127,100 - $211,900 yearly. Not to mention all the other amazing rewards that working at AT&T offers. From health insurance to tuition reimbursement and paid time off to discounts on products and services just to name a few. There is a lot to be excited about around here. Individual starting salary within this range may depend on geography, experience, expertise, and education/training.

AT&T is an Affirmative Action/Equal Opportunity Employer, and we are committed to hiring a diverse and talented workforce. EOE/AA/M/F/D/V

np*"
305,Principal Data Engineer-Snowflake,Paycor,Remote,"$98,370 - $167,946 a year","Principal Data Engineer
Remote
Paycor empowers leaders to develop winning teams. Our Human Capital Management (HCM) software modernizes every aspect of People Management from the way you recruit, onboard, develop, pay, and retain employees. Best of all? Our team is growing, and there’s never been a better time to join! If you love to Think Big, - Dream Big, Compete to Win, and thrive in a fast-paced environment, we want to hear from you!
Job Summary

As a Principal Data Engineer at Paycor, you will have a key role in Paycor’s success - responsible for building, managing, and monitoring the analytical data foundation for our leaders and our customers. You will ensure timely, complete, and accurate data in the Paycor Enterprise Data Warehouse with speedy and efficient data pipelines, optimized data models, and best practice approaches for logging, monitoring, and error-handling. The successful candidate will provide leadership, proactive expertise, mentoring and insight in all tasks and projects and help drive organizational improvement within a lean, fast paced and growing team. They will build and maintain database objects and procedures with cloud database tools, create and manage ELT/ETL and reverse ETL data pipelines with modern database integration tools and drive internal innovation for our cloud SAAS platform. We are looking for data engineering professionals that have successfully delivered scalable solutions in a modern DevOps managed agile software environment and been recognized as a go-to resource or industry expert. Are you ready to make a difference at Paycor?

Essential Duties and Responsibilities

Drive design and architecture discussions
- Be a recognized expert in one or more topics - internally and externally
Be able to contribute insightful papers/how to's, mini-trainings for our Data Community
Represent Paycor as a featured speaker at Informatica World or Snowflake Summit
Mentor junior team members to enable their growth
Builds and maintains database tables, views, and procedures*
Builds data ingestion objects and routines from multiple source systems to populate the Paycor data lake*
Ensures accuracy of data landing at source*
Monitors and logs the daily extraction, load, and transformation of data into the Paycor Data Warehouse*
Identifies errors and facilitates error correction while loading data in the data warehouse*
Collects requirements for new data sources, database fields, database views for the Paycor Data Warehouse
Adheres to standards, naming conventions, and templates for extracting, loading, and transforming data into the Paycor Data Warehouse
Ensures consistent alignment of data attributes and KPIs across all data sets in the Paycor Enterprise Data Marts
Works with DevOps team to document and migrate objects to production environments
Evaluates use of new technologies, such as integration tools, cloud database, and DevOps tools
Ensures proper migration of work items through the task backlog
Considers use of new connectors, APIS, or other data sharing/integration methods in ELT/ETL designs
Independently fills gaps, learn new technology and features and mentors other team personnel
Contributes knowledge to a center of expertise or leads internal support calls or develops process, procedure or implementation documentation and standards
Takes initiative to drive corporate direction and proactively support and re-engineer reporting structures, needs and upcoming business requirements
Other duties as assigned

Requirements

To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements are representative of the knowledge, skill, and/or ability required.

Bachelor’s degree or higher in computer science, computer engineering or related field and 7+ years of experience. Master’s Degree in business, analytics, or computer science preferred
At least 5-8 years of experience in SQL database development (SNOWFLAKE, MS SQL) or demonstrated breadth and depth of technical knowledge and
Solid command of SQL programming skills
Experience working in an agile development team
Experience with additional programming languages (python, java)
Experience with APIs and data formats such as JSON / AVRO
Experience with data integration technologies (Informatica, Talend, IBM Datastage, etc.)
Experience in Object Oriented Programming (Java, Scala, Python)
Applied understanding of data quality, integrity and heritage, governance and security
Demonstrated track record in delivering complex data marts and solutions
Superior organizational skills, with a disciplined approach to implementation, support, and documentation
Strong analytical, troubleshooting, and problem-solving skills

Join our team to make a difference in people’s lives, every day.

Paycor Total Rewards

We are proud to provide best-in-class benefits delivering a personal and professional experience that meets you where it matters most - the well-being of you and your loved ones.
Highlights include:

A flexible virtual-first work philosophy
An initial equity award in Restricted Stock Units (RSUs) for all new Associates and on-going award opportunities to participate in the value created through Paycor's growth
401(k) with $.65 match for every dollar contributed up to 6% of eligible pay
Generous paid time off in addition to 10 paid holidays (including YOUR Holiday to celebrate a day or holiday you hold special)
Three medical plan options – including a $0 cost option and travel & lodging support to ensure access to medical care
Paid leave for birth parents, non-birth parents, elder caregivers, and military support
Sabbatical opportunities for tenured Associates
Employee Stock Purchase Plan, which enables you to buy PYCR stock at a 15% discount
Inclusive and accessible Employee Assistance Program (EAP) to help with everyday challenges
We also offer competitive compensation determined by each individual's relevant experience, skills, and education. We anticipate the base pay for this position to be between $98,370-167,946. In addition to base pay, Paycor Associates are eligible for either a performance-based annual bonus or commission, depending on their position. If your desired salary falls outside of these rates, we hope you'll still apply as there may be other positions that better align.
For more information about our total rewards, please visit www.mypaycorbenefits.com."
306,Data Engineer,WCF Insurance,"Sandy, UT 84070•Hybrid remote",N,"Do you love researching data and applying your findings to create the best data structures for data analytics? Do you enjoy digging into all the details as you work through a data set and identifying patterns and relationships? If so, this job may be a fit for you! Keep reading and watch our video below to see why WCF was voted one of the best places to work again!
Position
The IT department has an immediate opening for someone who can demonstrate the WCF values to join their team as Data Engineer. This is a full-time, exempt position based out of WCF's Sandy, Utah headquarters. This position is open to internal and external candidates. This position is available or hybrid remote/in office work.

Responsibilities
Understand data requirements, concepts, data types and structures.
Understand and document business rules and cases to be applied to the ETL processes.
Develop and maintain code in the ETL processes using stored procedures, T-SQL, and SSIS, against a variety of data sources such as PostgreSQL, Microsoft SQL Server, flat files, etc.
Collaborate and participate in code review with peers.
Identify facts and dimensions required by data consumers to obtain reporting and analytical data requirements.
Identify, quantify, and propose automated solutions to address data integrity and data quality issues.
Ensure ETL processing uses standards, has good performance, and produces data with consistency and accuracy.
Implement quality insurance automated testing for data completion, accuracy, integrity, and consistency.

Qualifications
The most qualified candidate will have:
Proven work experience as an ETL or data developer.
Clear written and oral communication skills.
Strong data analytical skills to identify patterns, relationships, and flows, and troubleshoot data inconsistencies.
Working experience designing and developing data warehouse dimensional modeling.
Good understanding of SQL Server, including creating tables, stored procedures, views, setting up jobs, and troubleshooting code and data issues.
Experience with T-SQL, query performance tuning, indexing, and high-performance stored procedures.
Working experience developing SSIS packages.
Bachelor's degree in information technology or a related field and at least two years of work experience (or six years of directly related experience without a degree).
Ability to work well with others through communication and collaboration.

WCF INSURANCE DE&I MISSION
Promote and embrace a diverse, inclusive, equitable, and safe workplace.

WCF INSURANCE IS AN EQUAL OPPORTUNITY EMPLOYER
WCF Insurance provides equal employment opportunity to all qualified applicants and employees regardless of race, color, religion, sex, age, national origin, veteran status, disability that can be reasonably accommodated, or any other basis prohibited by federal, state, or local law."
307,Sr. Data Engineer,Warner Bros. Discovery,"New York, NY 10010","$98,007 - $182,013 a year","Every great story has a new beginning, and yours starts here.
Welcome to Warner Bros. Discovery… the stuff dreams are made of.
Who We Are…
When we say, “the stuff dreams are made of,” we’re not just referring to the world of wizards, dragons and superheroes, or even to the wonders of Planet Earth. Behind WBD’s vast portfolio of iconic content and beloved brands, are the storytellers bringing our characters to life, the creators bringing them to your living rooms and the dreamers creating what’s next…
From brilliant creatives, to technology trailblazers, across the globe, WBD offers career defining opportunities, thoughtfully curated benefits, and the tools to explore and grow into your best selves. Here you are supported, here you are celebrated, here you can thrive.
Every great story has a new beginning. We're excited to announce that Discovery and WarnerMedia have combined to become Warner Bros. Discovery. WBD is a premier global media and entertainment company offering audiences the world’s most differentiated and complete portfolio of content, brands and franchises across television, film, sports, news, streaming and gaming. We’re home to the world’s best storytellers, creating world-class products for consumers. From brilliant creatives to technology trailblazers and beyond, join us as we step into the next chapter.
The Job
As Sr. Data Engineerin the Customer team, you will lead the design, develop, deploy and manage customer AI data products supporting data science and analytic initiatives for D&AI at WBD streaming, regulate and standardize customer data integration, transformation /publication to drive the delivery of governed, tested, documented, and code-reviewed abstraction output through AI platform /microservices to data scientists, MLP, visualization and other self-service platforms. In this role you will interact with different functional areas to understand the analytical and production data requirements about customers, and collaborate with data engineers, data scientists and analysts, and researchers to ensure the proper end-to-end execution and on-going success for the team. This role will bring in both his/her expertise in a wide variety of big data processing frameworks, large scale database systems, stream data processing, API Development, data modeling skills and managing resources to drive the delivery of customer data products, but also the .
The Daily
Enhance and innovate streaming customer360 data products to the next level on pipeline design, process and publication to serve a variety of stakeholders.
Collaborate with data scientists and data engineers with D&AI to define technical requirements and data translation for analytic and ML feature engineering initiatives.
Work with solution analysts to understand the business problems and develop best-suited, robust and reusable data science output for customer analytics
Evaluate and facilitate the integration of additional sources of data (3rd party and 2nd party data) that can enhance analytic capability, continually expand the breadth and depth of user base and insightful data attributes.
Work closely with Data Product Management, Data Science, and Data Engineering to design and build fast and reliable services
Create libraries to enable production customerAI data products as effectively and quickly as possible
Ensure data quality by implementing re-usable data quality frameworks.
The Essentials
Bachelor’s degree in computer science or similar discipline.
5+ years of experience in data engineering / software engineering/ machine learning engineering and managing resources and project delivery.
Expertise in building and managing large volume data transformation(both streaming and batch) platforms and proficient with SQL.
Expertise in distributed data processing frameworks such as Apache Spark, Flink or Similar.
Expertise in at least few programming languages - Python, Java, Scala, or similar.
Proficiency with Git (or similar version control) and CI/CD best practices and managing workflows using Agile practices
Experience or certification on Databricks at data engineering and/or ML production
Working with AWS Cloud platform
Ability to write clear, concise documentation and to communicate generally with a high degree of precision
Care for the quality of the input data and how the processed data is ultimately interpreted and used
Experience with digital products, streaming services, or subscription products is preferred
Characteristics & Traits

Naturally inquisitive, critical thinker, proactive problem-solver, and detail-oriented.
Comfortable initiating projects from design to execution with minimal supervision
Ability to manage and balance multiple (and sometimes competing) priorities in a fast-paced, complex business environment and can manage time effectively to consistently meet deadlines
well-roundedness on analytic methodologies and execution

How We Get Things Done…
This last bit is probably the most important! Here at WBD, our guiding principles are the core values by which we operate and are central to how we get things done. You can find them at www.wbd.com/guiding-principles/ along with some insights from the team on what they mean and how they show up in their day to day. We hope they resonate with you and look forward to discussing them during your interview.
The Legal Bits…
In compliance with local law, we are disclosing the compensation, or a range thereof, for roles in locations where legally required. $98,007.00 - $182,013.00 salary per year. Other rewards may include annual bonuses, short- and long-term incentives, and program-specific awards. In addition, Warner Bros. Discovery provides a variety of benefits to employees, including health insurance coverage, an employee wellness program, life and disability insurance, a retirement savings plan, paid holidays and paid time off (PTO).
Warner Bros. Discovery embraces the opportunity to build a workforce that reflects the diversity of our society and the world around us. Being an equal opportunity employer means that we take seriously our responsibility to consider qualified candidates on the basis of merit, without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law.
If you’re a qualified candidate with a disability and you need a reasonable accommodation in order to apply for this position, please contact us at recruitadmin@warnermedia.com."
308,Data Analytics Engineer,Child Mind Institute,"445 Park Ave, New York, NY 10022","$120,000 - $177,000 a year","Summary:
The Child Mind Institute (CMI) seeks an individual with exceptional product acumen and intuition to join our team as a Data Analytics Engineer. This 3-year position will help CMI, through its partnership with California’s Department of Health Care Services (DHCS), with data and analytics for the no-code mental health delivery platform, MindLogger. MindLogger (by CMI) is empowering clinicians, researchers, and educators to deliver a fully customizable assessment and intervention platform for children’s mental health. Our mission is to transform children’s lives with technology that fits into where children eat, study and play.
As the first Data Analytics Engineer, this individual will play a critical leadership role, spanning data engineering and product analytics, with ownership over end-to-end product data and analytics strategy, from data creation and storage to self-service reporting and insight. As the data and analytics leader, you will partner with engineering, design, product, and research (and other cross-functional teams) and drive a culture of self-service analytics, measurement, and accountability, ensuring the product team has a clear line of sight into product engagement, growth and eventually monetization.
The individual must be equally adept at working with technical teams to define product measurement frameworks, data storage solutions, and various reporting platforms while working in partnership with business stakeholders to help run complex analyses and deliver results to inform product development decisions.
Reporting to the Program Director, this is a Full-Time, Exempt position at the Child Mind Institute headquarters in Midtown, New York (part-time remote work is possible).
The Child Mind Institute is proud to be named a Great Place to Work-Certified company! Our competitive compensation and benefits include medical insurance, 401(k) with match, flexible work schedules, paid parental leave, dependent care and discounted tickets and entertainment perks program. For more information about our benefits, please visit our employee benefits website.

Responsibilities will include, but are not limited to:
Leadership (25%):
Manage teams of engineers, data scientists, and business analysts, with the ability to hire, mentor, motivate, and hold team members accountable for delivering high-quality results.
Support with 3rd party vendor management, including vendor selection and term negotiations, in the data and analytics space.
Understand the value of data and proactively support senior management in making critical data-driven decisions.
Independently analyze, interpret, and derive insights into business decisions.
Technical (50%):
Implement and own data workflow management technologies (e.g., dbt, databricks).
Utilize data orchestration tools (e.g., Dagster, airflow).
Ability to operate in cloud data platforms (AWS) and analytics environments (e.g., Snowflake).
Guide the build-out of our back-end data infrastructure using knowledge of data architecture and various data storage and management frameworks (data lake, enterprise data warehouse, etc.).
Query data (SQL) and develop reports using BI reporting tools (i.e., Tableau, Amplitude, Google Analytics, Looker, QlikView, Periscope, PowerBI, etc.).
Set up and manage product experimentation and related tools (e.g., Otimizely, Google Analytics, etc.).
Communication and Collaboration with Strategic Focus (25%):
Derive insight from data and effectively socialize key takeaways to business partners and influence business decisions and priorities.
Develop collaborative relationships with program leadership and external stakeholders.

Qualifications:
Minimum of 7 years in Analytics / Data Science experience; at least 3 years in product and data analytics.
Minimum 3 years of experience building and developing analytics teams.
Expert-level understanding of SQL.
Expert-level knowledge of data and analytics infrastructure.
Expert-level development using BI tools such as Tableau, Google Analytics, Looker, QlikView, Periscope, and PowerBI.
Experience in a start-up environment defining and delivering the end-to-end product data and analytics roadmap, from back-end data infrastructure to reporting platforms.
Strong project management skills, with the ability to help business stakeholders and team members negotiate competing priorities.
Highly developed communication skills and the ability to story-tell with data to influence stakeholders and bridge the gap between technical and non-technical audiences.
Solid working knowledge of probability and statistics, with experience in programming languages used in data science (R, MATLAB, Python) a plus.
A passion for the Child Mind Institute mission.

Special Considerations:
Please upload your CV during the application process.
All new hires must be vaccinated and must stay up to date with vaccines against the COVID-19 virus unless they have been granted a reasonable accommodation for religion or disability. If you are offered employment with CMI, this requirement must be met by your date of hire, unless a reasonable accommodation for exemption is received and approved by CMI.
The Child Mind Institute is an equal opportunity employer and does not discriminate in employment based on race, religion (including religious dress and grooming practices), color, sex/gender (including pregnancy, childbirth, breastfeeding or related medical conditions), sex stereotype, gender identity/gender expression/transgender (including whether or not you are transitioning or have transitioned) and sexual orientation; national origin (including language use restrictions and possession of a driver's license issued to persons unable to prove their presence in the United States is authorized under federal law [Vehicle Code section 12801.9]); ancestry, physical or mental disability, medical condition, genetic information/characteristics, marital status/registered domestic partner status, age (40 and over), sexual orientation, military or veteran status, or any other basis protected by federal, state or local law or ordinance or regulation."
309,"Site Reliability Engineer, Data Platform- USDS",TikTok,"New York, NY","$136,800 - $259,200 a year","Responsibilities
TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Mumbai, Singapore, Jakarta, Seoul and Tokyo.

Why Join Us
At TikTok, our people are humble, intelligent, compassionate and creative. We create to inspire - for you, for us, and for more than 1 billion users on our platform. We lead with curiosity and aim for the highest, never shying away from taking calculated risks and embracing ambiguity as it comes. Here, the opportunities are limitless for those who dare to pursue bold ideas that exist just beyond the boundary of possibility. Join us and make impact happen with a career at TikTok.

About USDS
At TikTok, we're committed to a process of continuous innovation and improvement in our user experience and safety controls. We're proud to be able to serve a global community of more than a billion people who use TikTok to creatively express themselves and be entertained, and we're dedicated to giving them a platform that builds opportunity and fosters connection. We also take our responsibility to safeguard our community seriously, both in how we address potentially harmful content and how we protect against unauthorized access to user data.

U.S. Data Security (“USDS”) is a standalone department of TikTok in the U.S. This new security-first division was created to bring heightened focus and governance to our data protection policies and content assurance protocols to keep U.S. users safe. Our focus is on providing oversight and protection of the TikTok platform and user data in the U.S., so millions of Americans can continue turning to TikTok to learn something new, earn a living, express themselves creatively, or be entertained. The teams within USDS that deliver on this commitment daily span Trust & Safety, Security & Privacy, Engineering, User & Product Ops, Corporate Functions and more.

Team Introduction
TikTok's Data Platform Team focuses on challenges in the areas of data infrastructure and data products. The team is in charge of various aspects including Query Engine, Logging and Data Ingestion Infra, Experimentation Platform, as well as Workflow Management Platform. The goal is to support ad-hoc/interactive queries, batch pipelines, logging and ingesting large amounts of realtime data, and supporting A/B testing for all product features launches.

Site Reliability Engineering (SRE) combines software and systems engineering to build and run large-scale, massively distributed services and infrastructures. As a site reliability engineer in the data platform area, you will have the opportunity to manage the services and infrastructures in one of the largest data platforms in the world. You'll need to ensure the data, services and infrastructures are reliable, fault-tolerant, efficiently scalable and cost-effective. You'll also have the opportunity to design, build and deliver all kinds of systems as a software engineer.

Engage in and improve the whole lifecycle of service, from inception and design, through to deployment, operation and refinement
Ensure reliable, fault-tolerant, efficiently scalable and cost-effective data, services and infrastructures
Maintain services once they are live by measuring and monitoring availability, latency and overall system health. Practice sustainable incident response and blameless postmortems.
Establish best engineering practice for engineers as well as non-technical people
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business
Qualifications
BS or MS degree in Computer Science or related technical field or equivalent practical experience
Experience in the Big Data technologies(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.)
Experience with performing data analysis, data ingestion and data integration
Solid communication and collaboration skills
TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.

TikTok is committed to providing reasonable accommodations during our recruitment process. If you need assistance or an accommodation, please reach out to us at dennis.chau@tiktok.com
Job Information
The base salary range for this position in the selected city is $136800 - $259200 annually.



Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.



At ByteDance/TikTok our benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support ByteDancers to give their best in both work and life. We offer the following benefits to eligible employees:



We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care.



Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off(PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability.



We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice."
310,Azure Data Engineer,"DevCare Solutions Pvt., Ltd.",Remote,$80 - $85 an hour,"This position will require qualified T-SQL Developers to take thelead in thefollowing tasks:
Rationalizing and mapping data between transactional and dimensional database models/systems
Working closely with the database and system administrators to develop stored procedures and ETLprocesses related to thedata warehouse system.
Working in collaboration with vendors and other agencystaff to design,develop and managethe creation Synapse pipelines.
Ability to work independently and cooperatively as part of a team.
Ability to work under severetime constraints.
Must possess analytical and complex problem-solving skills.
REQUIRED SKILLS/EXPERIENCE YEARS
Extensive experience with development of stored procedures and ETL processes using T-SQL-7 years
Thorough understanding of data warehouse design hierarchies such as star and snowflake schemas-7 years
Use of ALM tools for work item management, version control, code analysis, and testing-7 years
Broad and extensive knowledge of the software development process and its technologies-7 years
Familiarity with continuous integration-7 years
Experience creating and scheduling elastic jobs-5 years
Experience with designing and modelling database structures based on business use cases-5 years
Job Type: Contract
Salary: $80.00 - $85.00 per hour
Experience level:
10 years
9 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote"
311,Data Engineer I,"ConstructConnect, Inc","Cincinnati, OH 45209",N,"ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.
The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.
The Opportunity
The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects to build and train predictive analytic models, design linear regression models using data
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Experience writing complex SQL Queries, Stored Procedures and Views
Experience building and optimizing data pipelines, architectures and data sets.
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
Understanding of version control and automated CI/CD with technologies such as Gitlab/ GitHub
1 – 3 years professional experience managing Data Lakes and Data Warehouses
Bachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)"
312,Data Engineer,"The Knowlton Group, LLC",Remote,"$80,000 - $120,000 a year","Help credit union get control of their data!
Description
Designs and implements SQL data models for our clients in the credit union industry. This role will involve developing new integrations with data sources our clients leverage, while also deploying existing code into new client environments.

Creative problem solving is essential in this position to quickly address changing client needs and adapt to an evolving dataset we have at our disposal.

Responsibilities:
Develop and manage the movement of data from source through to the data warehouse environment.
Design, test, and implement stored procedures to move data from a staging environment and/or operational data store into the dimensional data model.
Extend existing tables to add new columns and calculations based on client feedback.
Leverages existing data infrastructure to fulfill all data-related requests, perform necessary data housekeeping, data cleansing, normalization, hashing, and implementation of required data model changes.
Work with clients/analysts to define requirements for data desired to be provisioned updating defined requirement document templates.
Work with the technical manager to transform business requirements into appropriate schema and data model.
Troubleshoots problems, identifies possible solutions, and resolves them accordingly.

Daily tasks can include:
Consulting with credit union clients to understand their data requirements.
Use Azure tools to transform data within the data warehouse.
Tune databases and ETL for maximum performance.
Data Profiling, Data Cleansing, and Data Auditing.
Loading large volumes of data.
Decoding and writing complex SQL queries.
Performance tuning of queries and data loading process.
Modeling data into dimensional model structures.

Experience:
Extensive experience with Jack Henry Symitar Episys
Extensive experience with SQL
Experience with credit union and bank vendors (MeridianLink, Temenos, nCino, Fiserv, FICS etc.)
Experience developing with cloud services (Azure or AWS)
Bonus if you have experience in Power BI, Azure Databricks, Azure Data Factory, and Azure SQL.
Must be legally authorized to work in the United States.

Salary
$80,000 - $120,000 per year"
313,Cloud Data Engineer - Solution Specialist - Location Open,Deloitte,"200 Renaissance Center Suite 3900, Detroit, MI 48243",N,"Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with our US Delivery Center - we are breaking the mold of a typical Delivery Center.

Our US Delivery Centers have been growing since 2014 with significant, continued growth on the horizon. Interested? Read more about our opportunity below ...

Work you'll do/Responsibilities
Work with the team to evaluate business needs and priorities, liaise with key business partners and address team needs related to data systems and management.
Translate business requirements into technical specifications; establish and define details, definitions, and requirements of applications, components and enhancements.
Participate in project planning; identifying milestones, deliverables and resource requirements; tracks activities and task execution.
Generate design, development, test plans, detailed functional specifications documents, user interface design, and process flow charts for execution of programming.
Develop data pipelines / APIs using Python, SQL, potentially Spark and AWS, Azure or GCP Methods.
Use an analytical, data-driven approach to drive a deep understanding of fast changing business.
Build large-scale batch and real-time data pipelines with data processing frameworks in AWS, Azure or GCP cloud platform.

The Team

From our centers, we work with Deloitte consultants to design, develop and build solutions to help clients reimagine, reshape and rewire the competitive fabric of entire industries. Our centers house a multitude of specialists, ranging from systems designers, architects and integrators, to creative digital experts, to cyber risk and human capital professionals. All work together on diverse projects from advanced preconfigured solutions and methodologies, to brand-building and campaign management.

We are a unique blend of skills and experiences, yet we underline the value of each individual, providing customized career paths, fostering innovation and knowledge development with a focus on quality. The US Delivery Center supports a collaborative team culture where we work and live close to home with limited travel.

Qualifications Required
3+ years of experience in data engineering with an emphasis on data analytics and reporting.
3+ years of experience with at least one of the following cloud platforms: Microsoft Azure, Amazon Web Services (AWS), Google Cloud Platform (GCP), others.
3+ years of experience in SQL, data transformations, statistical analysis, and troubleshooting across more than one Database Platform (Cassandra, MySQL, Snowflake, PostgreSQL, Redshift, Azure SQL Warehouse, etc.).
3+ years of experience in the design and build of data extraction, transformation, and loading processes by writing custom data pipelines.
3+ years of experience with one or more of the follow scripting languages: Python, SQL, Kafka and/or other.
3+ years of experience designing and building solutions utilizing various Cloud services such as EC2, S3, EMR, Kinesis, RDS, Redshift/Spectrum, Lambda, Glue, Athena, API gateway, etc.
Bachelor's degree or equivalent work experience.
Travel up to 10% annually
Limited Sponsorship: Limited immigration sponsorship may be available.

Preferred
AWS, Azure and/or Google Cloud Platform Certification.
Master's degree or higher.
Expertise in one or more programming languages, preferably Scala, PySpark and/or Python.
Experience working with either a Map Reduce or an MPP system on any size/scale.
Experience working with agile development methodologies such as Sprint and Scrum."
314,azure Data Engineer - Direct Hire,Formac Inc,Remote,"$83,947 - $110,732 a year","he Data Engineer is a key position within the GetInsuredBusiness Intelligence and Reporting unit and will work closely with the Data Visualization Team Lead, clients, and various teams to develop, optimize, and oversee the conceptual, logical, and physical data systems, data warehouses, ETL, and analytics environments. This is a hands-on role which requires analyzing new data system requirements, design implementation, and optimization of the data architecture. The ideal candidate will have excellent analytical and problem-solving abilities, great communication skills, and be effective in working cross-functionally.
Responsibilities:
Work with partners across different practice areas to understand and evaluate business needs.
Consult with analytics team members to design and develop data engineering pipelines for analytic and dashboard projects.
Develop streamlined and governed processes for data ingestion into data lake
Understand and integrate data quality and data governance processes into data engineering pipelines
Develop data validation process for transforming and curating raw data into business ready datasets
Optimize query performance of large data sets for ad hoc exploration, analysis, and reporting
Communicate complex technical information to business customers and project teams in an effective and concise manner
Participate as a technical resource for a variety of information projects that will be dictated by current business needs and technological developments
Work with a variety of external IT consulting partners on developing optimal data solutions and provide effective direction to contracted resources
Requirements:
Bachelor's degree in Computer Science, Information Systems, Engineering or related fields.
5+ years of experience working with data, 3+ years’ experience working on data projects with a focus on data engineering
Experience with cloud platforms preferably Microsoft Azure and Databricks
Experience with large-scale data collection and analysis
Experience with batch, streaming, or API-based data integration patterns
Strong knowledge of programming languages, such as SQL and Python
Strong organizational skills and understanding of agile project management methodology
Strong written and oral communication skills with demonstrated ability to communicate with technical and non-technical partners
Documentation of data engineering processes
Experience optimizing query performance of large data sets
Developing and maintaining database schemas
Additional required qualifications:
C# programming, object-oriented programming, Python
PostgreSQL
Visual studio
Microsoft Azure
.Net core
Preferred qualifications:
Entity framework, multi-threading, Azure functions
Job Types: Full-time, Permanent
Pay: $83,947.48 - $110,731.54 per year
Benefits:
Dental insurance
Health insurance
Vision insurance
Experience level:
7 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Application Question(s):
your Expected Salary for this job
Education:
High school or equivalent (Required)
Experience:
data enginner: 6 years (Required)
Microsoft Azure: 1 year (Required)
Databricks: 1 year (Preferred)
API-based data integration: 1 year (Preferred)
C# programming, object-oriented programming, Python: 1 year (Preferred)
Work Location: Remote

Health insurance"
315,Big Data Engineer,Alt Shift USA,"Dallas, TX",N,"Multiple Bigdata Job Positions across Dallas, TX and Tampa, FL
Position 1: Big Data Engineer
Location: Dallas TX
Experience Required - 6+ years
Hadoop/HDFS.
Spark is a must. Scala preferred but Java is ok too.
Experience Spark core, Spark SQL, Spark Streaming ( these 3 are a must) and Spark ML is good to have.
Position 2: Sr. Big Data Engineer
Experience Required - 8-15 years
Location: Tampa, FL
Job Description
The client is looking for a senior engineer who can drive things rather than being managed by the client.
Hadoop/Hive and Kafka.
Spark streaming using Java is a must.
Position 3: Lead Big Data Engineer
Location: Dallas TX
Experience Required - 8-15 years
Job Description
Primary requirement - Spark, Scala developer with knowledge of Kafka.
Good to have exposure to other NoSQL databases like Casandra.
AWS experience will be a definite plus."
316,Data Engineer,American Unit,"Plano, TX•Remote",$55 - $60 an hour,"Job Title: Data Engineer ( W2 Position)
Location: Plano, Texas (100% Remote)
Duration: 12+ months Contract
Required Skills:
Most important: spark, R, SQL, machine learning
Must have: Hive, pig, python, SQL, machine learning, spark, big data, and PHD or Masters degree must have either one
Must Haves:
highly enthusiastic in uncovering actionable insights from data and conveying these insights to business as stories that stick.
MS or PhD in Mathematics, Computer Science, Applied Computers, Engineering, or Economics (required).
You have 2+ years of experience in Big Data environment specifically Hive/Spark where you have deployed reliable models that scale smoothly on high-volume (1TB+) & high-dimensionality (500+ variables per schema) data.
You are a solid coder who uses either SQL & (Python/Spark/R) to deploy machine learning products into Compliments.
Job Type: Contract
Salary: $55.00 - $60.00 per hour
Compensation package:
Hourly pay
Schedule:
8 hour shift
Experience:
spark, R, SQL, machine learning: 5 years (Required)
Hive, pig, python, SQL, machine learning, spark, big data,: 5 years (Required)
Data: 7 years (Required)
Work Location: Remote
Speak with the employer
+91 9949267763"
317,Construction Data Engineer,Zekelman Industries,"100 W Big Beaver Rd, Troy, MI 48084",N,"Job Description:

Z Modular, a division of Zekelman Industries, seeks a Construction Data Engineer to oversee the communications and documentation of our modular units in Chandler, AZ. Reporting to the Vice President, Field Services, the Construction Data Engineer is responsible for the coordination and organization of construction reporting, including developing various report formats, presentations and others as required by the VP, Field Services. At various intervals, create efficiency and optimization summaries, lessons learned, and general breakdown of previous reporting. The Construction Data Engineer will work with multiple other personnel within the Field Services and overall Z-Modular division to gather reporting as needed.

RESPONSIBILITIES:
Primarily function as data keeper of Project specific reporting. Including but not limited to Schedule, Budget, People Hours, Equipment Rental, Material Procurement, etc.
Format, Organize, Review and distribute draft reports as scheduled to Vice President, Field Services.
Finalize draft report into final report as scheduled.
Keeper of Lessons Learned formatting, organization and glossary. Distribute as regularly scheduled to all Field Services Personnel.
Provide efficiency and optimization summaries as needed. Develop statistical references to the same.
Assist in process implementation of efficiency and optimization tasks.
Attend weekly site specific and general team meetings as needed.
Other duties as assignment by management.
QUALIFICATIONS:
Bachelor’s degree in Building Science/Construction Management preferred
Microsoft Projects experience
Proficient computer skills in Microsoft Office
Microsoft D365 experience
Ability to multi-task in a fast-paced environment
Travel as required
Strong written and oral communication skills
Strong analytical skills
Ability to work in a fast-paced construction manufacturing/construction environment where upfront planning is pivotal to the success of our projects
Regular, full-time, predictable onsite attendance per the posted schedule is an essential function of this role
Lead and promote health and safety work practices as required by regulatory agencies and company policy
PHYSICAL ABILITIES:
Must be able to remain in a stationary position for several hours
Ability to lift, climb, bend, stoop, push and pull.
Ability to lift up to 10lbs
Must be able to stand for 10-12 hours
Ability to perform work in a plant/manufacturing/construction environment
Other physical requirements as needed for job
Let’s make it EZ:
Competitive salary
Exceptional benefits – medical, vision, dental
Work from home policy for eligible employees
Paid time off
401(k) with generous company match and immediate vesting
Annual bonus/ incentive programs
Tuition reimbursement
Zekelman Industries offers competitive compensation and excellent benefits, including low cost, high quality medical and dental benefits. In addition, we have an amazing tuition assistance program, a bonus plan, a 401(k) plan with a generous company match, immediate vesting and much more.
M/F/D/V
We are Zekelman Industries.
We manufacture superior quality tubular and related products and provide outstanding service for our valued customers.
We seek to continuously advance the skills and opportunities of our employees, utilizing the latest technology and management tools available to consistently increase profitability and the enterprise value of Zekelman Industries and of our customers."
318,Data Engineer,"Second Wave Delivery Systems, LLC",Remote,N,"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build and maintain scalable, high-performance data processing systems (batch and/or streaming)
Lead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable
Design our data models for optimal storage and retrieval and to meet critical product and business requirements
Contribute to tooling & standards to improve the productivity and quality of output across the company
Work and consult with various non-technical stakeholders
Participate in interviewing and onboarding of new team members

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
5+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks
Expert knowledge of relational databases and query authoring (SQL).
Strong experience developing in one of the following - Python, Java, Scala
Experience with building and managing data pipelines
Experience with big data technology, e.g: Airflow, Spark, Google Cloud Composer

Preferred Qualifications
Demonstrated experience developing machine learning algorithms in the healthcare field
Experience extracting and correlating medical findings from unstructured data such as PDF charts
Experience with big data analysis tools such as BigQuery and Looker"
319,Data Lineage (Informatica EDC) Engineer,City National Bank,"Los Angeles, CA","$101,231 - $172,355 a year","DATA SENIOR ENGINEER - TECHNOLOGY

WHAT IS THE OPPORTUNITY?

The Data Engineer Senior works with department and lines of business subject matter expert (SMEs) across the enterprise to meet departmental and organizational objectives. The engineer will follow end-to-end process standards and guidelines to ensure accurate and efficient build out of data pipeline architecture within project timeframes.

Technology and Innovation Division
As a member of City National's Technology & Innovation group, you will drive, develop, and maintain solutions for clients and colleagues. This is an exciting time of technology advancement and innovation across the bank, particularly within our technology teams.


WHAT WILL YOU DO?

Function in accordance with the Software Development Life Cycle (SDLC) framework and governance processes
Participate in development of test plans and perform quality assurance and testing of own work and that of peers
Function in accordance with the Software Development Life Cycle (SDLC) framework and governance processes
Ensure work includes necessary audit controls such as Audit, Balance and Control (ABC) Framework and security controls within all design and deliverables
Document data workflows including but not limited to: functional specs, technical specs, mapping / workflow diagram, testing plans, production “run books”, training materials, etc.
Accurately define and execute transformations, aggregations and other data manipulations to meet requirements
Participate in development of test plans and perform quality assurance testing of own work and that of peers
Identify development and data quality issues and work with senior team members and management to mitigate
Conduct peer and team review sessions
Assist in the creation of standards, templates and procedures for the department
Contribute to the design of the data structure/data model and data flow
Design and develop data workflows and mappings to extract, transform, and load data for purposes of analytics , reporting and integration


WHAT DO YOU NEED TO SUCCEED

Must-Have*
Bachelor's Degree
5+ years of relevant experience

Skills and Knowledge
Advanced senior professional with wide ranging experience uses professional concepts and to resolve complex issues in creative and effective ways
Perform various tasks to design and deliver data pipeline solutions with accuracy, reliability, and quality.
Requires specialized depth and or breadth of expertise
Compensation
Starting base salary: $101,231 - $172,355 per year. Exact compensation may vary based on skills, experience, and location. This job is eligible for bonus and/or commissions.

To be considered for this position you must meet at least these basic qualifications
The preceding job description has been designed to indicate the general nature and level of work performed by employees within this classification. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities, and qualifications required of employees assigned to this job.

INCLUSION AND EQUAL OPPORTUNITY EMPLOYMENT
City National Bank is an equal opportunity employer committed to diversity and inclusion. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status or any other basis protected by law.

ABOUT CITY NATIONAL

We start with a basic premise: Business is personal. Since day one we've always gone further than the competition to help our clients, colleagues and community flourish. City National Bank was founded in 1954 by entrepreneurs for entrepreneurs and that legacy of integrity, community and unparalleled client relationships continues to drive phenomenal growth today. City National is a subsidiary of Royal Bank of Canada, one of North America’s leading diversified financial services companies.

Equal Opportunity Employer Minorities/Women/Protected Veterans/Disabled"
320,Data Engineer,NucleusTeq,Remote,N,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation."
321,Data Engineer,Data Ideology,Remote,N,"Data Ideology
At DI, we provide Data & Analytics expertise to drive measurable business outcomes, often solving complex business problems for our clients. Our data analytics advisory services enable our customers to transform data into insights by driving a culture of empowerment and ownership of results. Our team consists of highly motivated individuals passionate about learning, understanding, collaborating, and intellectually curious. For more information about Data Ideology, visit www.dataideology.com
Data Engineer - Contract to Hire (CTH)
We are looking for a Data Engineer to join our growing team. Data Engineer will leverage their business and technical knowledge to develop production-ready data models by integrating multiple data sources while working with business and technical teams to understand business strategy and objectives, gather information, and ensure business requirements are being fulfilled throughout the entire data & analytics lifecycle.
Key Responsibilities
To perform in this position successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions. Other duties may be assigned to meet business needs.
Ability to collect and understand business requirements and translate those requirements into an actionable data warehouse plan.
Knowledge of multi-dimensional and tabular design patterns and ability to identify solutions that leverage these modeling techniques.
Ability to work within the SDLC framework in multiple environments and understand the complexities and dependencies of the data warehouse built within those constraints.
Ability to define and implement best practices across database design and ETL.
Ability to direct the work of others, including but not limited to directing ETL development, demonstrating an understanding of key concepts of ETL/ELT, including best practices for optimization and scheduling.
Supervisory Responsibilities: None
Qualifications
Education and Experience:
Proven understanding of Data Warehousing, Data Architecture, and BI.
Experience with data pipelines and architecture/engineering.
Knowledge of modern apps and data platforms.
Cloud-based project implementation.
Google BigQuery experience
Knowledge, Skills, and Abilities:
BI/Data Warehousing (3+ years) - Google BigQuery
Cloud platforms (1+ years) - Google Cloud Platform
Dimensional Data Modeling (3+ years)
ETL (3+ years)
SQL (3+ years)
Business Intelligence (1+ years) - Power BI
Work Environment:
Remote work from home.
Hours of work and days are generally Monday through Friday. Specific business hours will depend on client needs.
Physical Demands:
Must be able to remain in a stationary position 50% of the time.
The person in this position must occasionally move about inside the office to access file cabinets, library stacks, office machinery, etc.
Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and printer.
The person in this position frequently communicates with clients and coworkers. Must be able to exchange accurate information in these situations.
Benefits:
Unlimited Discretionary Time Off Policy
100% company paid - insurance (medical, dental, vision) for employees
100% company paid - short and long-term disability insurance for employees
100% company paid - life insurance and AD&D insurance for employees
100% company paid – employee assistance program
Retirement plans with company match
Training and Certification Reimbursement annually
Performance-based incentive program
Commission incentive program
Profit Sharing Plan
Referral Bonuses
Data Ideology is an EEO Employer
ut8TrtP2lw"
322,Data Engineer (W2 and onsite),Aalpha Tech Global,"Aalpha Tech Global in Dearborn, MI 48120",N,"Skills Required:
Critical thinking skills to propose solutions, test, and make them a reality. • Full Stack Data Engineering Competency in a public cloud – Google, MS Azure, AWS • Highly Proficient in SQL • Python, Java, Scala, or Go (or similar) • Cloud native technologist (ideally GCP) • Deep understanding of data service ecosystems including data warehousing, lakes, metadata, meshes, fabrics and AI/ML use cases. • User experience advocacy through empathetic stakeholder relationship. • Effective Communication both internally (with team members) and externally (with stakeholders)
Skills Preferred:
Extensive knowledge and understanding of GCP offerings, bundled services, especially those associated with data operations CloudConsole, BigQuery, DataFlow, DataFusion, PubSub / Kafka, Looker Studio, VertexAI • Experience with Teradata, Hadoop, Hive, Spark and other parts of Ford’s legacy data platform to support GCP migration. • Experience with recoding, re-developing and optimizing data operations, data science and analytical workflows and products. • Data Governance concepts including GDPR (General Data Protection Regulation), CCPA (California Consumer Protection Act), PoLP and how these can impact technical architecture
Job Type: Contract
Schedule:
8 hour shift
Application Question(s):
Do you hold valid work permit in US?
Experience:
data warehousing, lakes, metadata: 4 years (Preferred)
Data Engineer: 7 years (Preferred)
Google, MS Azure, AWS: 5 years (Preferred)
SQL: 4 years (Preferred)
Python, Java, Scala, or Go: 4 years (Preferred)
Cloud native technologist (ideally GCP): 4 years (Preferred)
Work Location: One location"
323,Big Data Engineer-Associate–Vice President,Deutsche Bank,"3000 Centre Green Way, Cary, NC 27513",N,"Job Title Big Data Engineer
Corporate Title Associate – Vice President
Location Cary, NC
Overview
Our Deutsche Bank Research and Index Technology team resides within the Origination and Advisory Technology department. We are an engineering focused organization striving for the highest quality architecture, design, and code. We are looking for a Senior Data Engineer to join our team. As a Senior Data Engineer, you will help to build our Research MIS and Data analytics systems while working in a fast-paced, agile environment.
What We Offer You
A diverse and inclusive environment that embraces change, innovation, and collaboration
A hybrid working model with up to 60% work from home, allowing for in-office / work from home flexibility, generous vacation, personal and volunteer days; A commitment to Corporate Social Responsibility
Employee Resource Groups support an inclusive workplace for everyone and promote community engagement
Access to a strong network of Communities of Practice connecting you to colleagues with shared interests and values
Competitive compensation packages including health and wellbeing benefits, retirement savings plans, parental leave, and family building benefits; Educational resources, matching gift, and volunteer programs
What You’ll Do
Consult on improving data reliability, efficiency, and quality
Ensure architecture supports business requirements
Collaborate with stakeholders to understand requirements, evaluate, and refine stories
Build reliable software that is easy to support in production and design, and develop high-quality and easily maintainable code
Design, implement, and test solutions, providing support through the production process
How You’ll Lead
Engage with your key stakeholders across all layers and all geographies
Take end to end accountability and connect people to drive a positive business outcome
Skills You’ll Need
Experience building enterprise applications using Java, Scala, and operating systems, such as Unix
Proficiency in Structured Query Language (SQL)
Experience with Apache Ecosystem, including Spark, Hive, and Hadoop
Knowledge of TeamCity (or similar) tool for Continuous Integration (CI) and Continuous Deployment (CD)
Experience with data structures, algorithms, and columnar Database Management System (DBMS), preferably ClickHouse
Skills That Will Help You Excel
Excellent communication skills
Strong ability to collaborate well with your peers
Expectations
It is the Bank’s expectation that employees hired into this role will work in the Cary office in accordance with the Bank’s hybrid working model.
Deutsche Bank provides reasonable accommodations to candidates and employees with a substantiated need based on disability and/or religion.
Deutsche Bank Values & Diversity
We believe talent is found in all cultures, countries, races, ethnicities, genders, sexual orientations, disabilities, beliefs, generations, backgrounds and experiences. We pursue a working environment where everyone can be authentic and feel a sense of belonging.
We are an Equal Opportunity Employer - Veterans/Disabled and other protected categories.
Click these links to view the following notices: EEO is the Law poster and supplement ; Employee Rights and Responsibilities under the Family and Medical Leave Act ; Employee Polygraph Protection Act and Pay Transparency Nondiscrimination Provision
Learn more about your life at Deutsche Bank through the eyes of our current employees: https://careers.db.com/life
Hear from our people and look inside our office: DB@The Muse
The California Consumer Privacy Act outlines how companies can use personal information. If you are interested in receiving a copy of Deutsche Bank’s California Privacy Notice please email HR.Direct@DB.com .
Our values define the working environment we strive to create - diverse, supportive and welcoming of different views. We embrace a culture reflecting a variety of perspectives, insights and backgrounds to drive innovation. We build talented and diverse teams to drive business results and encourage our people to develop to their full potential. Talk to us about flexible work arrangements and other initiatives we offer.
We promote good working relationships and encourage high standards of conduct and work performance. We welcome applications from talented people from all cultures, countries, races, genders, sexual orientations, disabilities, beliefs and generations and are committed to providing a working environment free from harassment, discrimination and retaliation.



We are an Equal Opportunity Employer - Veterans/Disabled and other protected categories. Click these links to view the following notices: ""EEO is the Law poster"" and supplement ; Employee Rights and Responsibilities under the Family and Medical Leave Act ; Employee Polygraph Protection Act and Pay Transparency Nondiscrimination Provision ."
324,Senior Data Warehouse Engineer,Plante Moran,United States•Remote,"$7,504 - $11,248 a month","Make your mark. Plante Moran’s technology services team has been awarded Insider Pro and Computerworld’s “100 Best Places to Work in IT” for five consecutive years. We are also previous recipients of the InformationWeek IT Excellence award and the CIO 100 award. If you’re seeking professional growth, like being innovative and challenged, and have a desire to work on impactful business technology solutions, we want to hear from you!

What does diversity, equity, and inclusion mean to Plante Moran? It means that all staff members have equitable and fair opportunities to succeed, in an inclusive environment, with their individual, unique identities.

We are looking for a Senior Data Warehouse Engineer to be a senior member of the Data Services team which includes all data warehouse, data engineering and visualizations, database operations, and data science capabilities. This role will be a team member on our data engineering team support all data integrations activities at Plante Moran.

Your role.

As a senior member of the Data Services team you will be the lead developer on complex data engineering projects. You will be expected to lead and mentor junior developers on best practices and project delivery. This role will also have responsibility for management of end to end delivery of assigned projects.

JOB RESPONSIBILITIES
Develop solutions to address current and future information system needs, and create process improvements and controls for defined solutions
Works closely with customers and colleagues to identify opportunities to utilize information systems to improve business processes, promote the strategic use of information while enabling seamless access to information.
Interacts with the staff to produce data mapping and requirements, deliver high quality solutions utilizing Microsoft Data Integration Stack
Provides prototyping solutions, prepares test scripts, and conducts tests and for data replication, extraction, loading, cleansing, and data modeling.
Possesses working knowledge of Relational Database Management Systems (DBMS) and data warehouse front-end tools.
Be proficient in creating and maintain SQL data structures and store procedures.
Works closely with the technical and business team lead to drive solution options analysis, development and implementation of BI solutions.
Contributing team member to the design and support of data architecture, database design and integration, transformations, and load processes.

QUALIFICATIONS:
SSIS Experience with Project Deployment Method and SQL Agent Jobs Set-up in an Azure data management environment.
Proven experience with data warehousing and data modeling, specifically building/understanding Microsoft Azure data management technologies
Comfortable with creating data flows, executing stored procedures within SSIS
Setting up and troubleshooting SQL Agent Jobs
SSMS and T-SQL Experience – Strong T-SQL Skills
Problem Solver – Interested in finding solutions to/supporting existing solutions, as well as, continuing to advance through new development
Nice to have: Informatica Experience (IICS) – Especially with Workday Integration Experience
Experience with Azure DevOps, ADF Experience (Running SSIS from ADF and native ADF pipelines)
Analytical approach to problem-solving; ability to use technology to solve business problems
Passionate about learning new technologies
Familiarity with database-centric applications
Strong verbal and written communication skills; strong time-management and organizational skills
Ability to work in a fast-paced environment
Proven ability to build out, enhance, and maintain the company data environment, including modifications and maintenance of various ETLs, transactional data sets, and company data assets.
Manage data access, data quality, and data delivery tasks within the framework of specific client BI and IT projects.
Ability to work assigned projects and trouble tickets, delivering results within promised timeframes.
Ability to validate & ensure production level functionality/style is thoroughly tested, and the code reflects the current standards.
3-5 years' experience using the Microsoft Azure Data integration tools including SSIS/IR, ADF and ADO
Our difference.

We are a nationally recognized public accounting, consulting, and wealth management firm, consistently ranked as one of FORTUNE magazine’s “100 Best Companies to Work For.” At Plante Moran, we live by the Golden Rule, fostering a relatively “jerk-free” culture with the lowest staff turnover rate in the industry. Our supportive network of well-rounded professionals is excited to catapult your growth and help pave your pathway to professional excellence.

Apply now. Experience our difference.

This is an exempt position, so you may have to work hours that exceed the standard 40-hour work week.

Plante Moran is committed to a diverse workplace. We strive to create a culture where each person feels accepted and valued. We believe that each person’s ultimate potential begins with first acknowledging their inherent dignity. When we can recognize — and celebrate — our many human differences, we’re able to create a workplace where all staff feel a sense of belonging and an opportunity to succeed. This allows us to attract and retain the best talent, serve clients through diverse thinking, and better represent and support the various communities in which we live and work. Plante Moran is an Equal Opportunity Employer.

Plante Moran maintains a drug-free workplace.

Interested applicants must submit their resume for consideration using our applicant tracking system. Due to the high volume of applications received, only candidates selected for interviews will be contacted. Candidates must be legally authorized to work in the United States without sponsorship, with the exception of candidates that are bilingual in Japanese and English. Unsolicited resumes from search firms or employment agencies, or similar, will not be paid a fee and become the property of Plante Moran.

The specific statements above are not intended to be all-inclusive.

#LI-LS2
#LI-Remote

Colorado Equal Pay for Equal Work Act (SB 19-085) Information
Compensation is commensurate with technical skills and experience and is provided in accordance with the CO Equal Pay for Equal Work Act.
Compensation information posted is based on a position being located in the state of CO. Please review position location for applicable geographic location.
Colorado monthly base range is as follows:
$7,504.00 to $11,248.00
We are pleased to offer eligible staff a robust benefits package. Eligibility and contribution requirements for some of these benefits vary based on the number of hours staff work per week. Highlights include health, dental, vision, disability and life insurance. These standard offerings are effective on the first of the month following your start date. In addition to this, eligible staff are able to take advantage of our Flexible Time Off and various pre-determined holidays as well as a balance fund, 401(k) plan, flexible benefits plans, business-related travel expense, lodging and meal reimbursement for business-related use. A Pension plan is also available for eligible administrative and paraprofessional staff. A discretionary bonus plan is available for eligible staff. Plante Moran also offers Interns and Contractors the option to elect health insurance under our contingent staff medical plan as of the 1st of the month following 60 days of employment in addition to limited paid time sick time. Seasonal staff are not eligible for benefits."
325,Data Engineer,Kaar Tech PH,Remote,"$2,000 - $5,000 a month","Responsibilities
· Strong experience with data engineering tools / frameworks on cloud for Object-oriented/object function scripting using languages such as Python, PySpark, Scala, or similar
· Strong ability to design, build and manage data pipelines in PySpark, Apache Airflow and related technologies for data structures encompassing data transformation, data models, schemas, metadata, and workload management.
· Strong experience with popular database programming in relational and nonrelational environments like Snowflake, AWS Redshift, Google Big Query, Azure SQL DB, and similar platforms
· Experience in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using traditional data integration technologies. These should include ETL/ELT, CDC, message-oriented data movement and upcoming data ingestion and integration technologies such as stream data integration.
· Strong experience in working with and optimizing existing data pipeline processes and data integration and data preparation flows and helping to move them in production
· Experience in working with both open-source and commercial message queuing technologies such as Kafka, Amazon Simple queuing Service, stream data integration technologies such as Apache Nifi, Apache Kafka Streams, Databricks and stream analytics technologies such as Apache Kafka KSQL
Job Types: Full-time, Contract
Salary: $2,000.00 - $5,000.00 per month
Schedule:
8 hour shift
Experience:
SQL (Preferred)
Python (Preferred)
Work Location: Remote"
326,Senior Data Engineer (USA Remote),N,Remote,"$130,000 - $160,000 a year","Company Overview:
Blue Orange Digital is a cloud-based data transformation and predictive analytics development firm with offices in NYC and Washington, DC. From startups to Fortune 500's, we help companies make sense of their business challenges by applying modern data analytics techniques, visualizations, and AI/ML. Founded by engineers, we love passionate technologists and data analysts. Our startup DNA means everyone on the team directly contributes to the company's growth.
Position Summary:
Blue Orange is seeking a Senior Data Engineer to join our talented multi-disciplinary team. The ideal candidate will have a passion for data analytics and quality. The ideal candidate will be well versed in SQL and modern data technologies, understand how to drive the extraction of business requirements for data visualizations & reporting, build and maintain robust data pipelines, assess data quality, and have excellent communications skills. This candidate will work on analyzing and extracting information from source data, help interpret and visualize data, define data quality needs, and help make sure they are met.
Responsibilities:
Work with the team and stakeholders to define requirements for the Data Warehouse and BI reports
Build and maintain robust end-to-end data pipelines
Work with source data systems to extract and analyze data to assist in defining the requirements for the data platform
Develop and maintain dashboards and reports, and model data based on client requirements
Work with the data team to implement the data quality standards and ensure ongoing quality assurance of that data process
Build robust and reusable codebases in Python
Support analytical and statistical reporting requirements and work with the data teams and subject matter experts to visualize the data in dashboards and reports
Work within an Agile environment to constantly deliver value for our clients.
Basic Qualifications:
5+ years of experience with data engineering, and data analysis, including data cleansing, and data visualization
Expert-level skills with SQL and statistical methods to analyze and model behavioral data
Expert-level skills in Python
Experience with dimensional modeling
BA or BS degree in a technical or quantitative field (e.g.: statistics, economics, computer science)
Excellent verbal and written English communication
Interacts with others using sound judgment, good humor, and consistent fairness in a fast-paced environment
Preferred Qualifications:
Hands-on experience with SAS
Experience in the Finance or Real estate industries
Our Benefits Include:
401k Matching
Unlimited PTO
100% remote role with an option for hybrid
Healthcare, Dental, Vision, and Life Insurance
Home office stipend
Salary: 130,000 - 160,000 annual salary (USD $)

Background checks may be required for certain positions/projects.
Blue Orange Digital is an equal-opportunity employer."
327,"Data Engineer III, IT",Pediatric Associates,"Plantation, FL 33324•Remote",N,"Fully Remote position

PRIMARY FUNCTION
The Data Engineer III is a senior level data engineer role and is responsible for designing & building a leading-edge Data & Analytics platform for enabling value-based healthcare, population health management, and enterprise analytics. Designs, develops, maintains, and supports the cloud-based (Microsoft Azure) big data platform and uses modern data engineering design patterns and tools.
ESSENTIAL DUTIES AND RESPONSIBILITIES
This list may not include all of the duties that may be assigned.
Owns solution design blueprints and architecture of the enterprise data platform features and functionality, including data ingestions, data integrations, data pipelines, data models, data quality, data governance.
Plays technical leadership role and leads other team members and guides them on solution design blueprints, data solutions development, and best practices for our enterprise data platform.
Designs, builds and maintains scalable, automated data pipelines to enable Reporting, Data Visualization, Advanced Analytics, Data Science, and Machine Learning solutions.
Supports critical data pipelines with a scalable distributed architecture, including data ingestion (streaming, events, and batch), data integration (ETL, ELT, Azure Data Factory), and distributed data processing using Databricks Data & Analytics and Azure Cloud Technology Stacks.
Builds cloud data solutions using multiple technologies, such as SQL, Python, Data Lake (Databricks Delta Lake), Cloud Data Warehouse (Azure Synapse), RDBMS, NoSQL databases.
Understands and implements best practices in managing data, including master data, reference data, metadata, data quality, and lineage.
Deploys, automates, maintains, and manages cloud-based production systems to ensure the availability, performance, scalability, and security of production systems.
Engages with cross-functional stakeholders to identify pain points, business and technical requirements, and to design data solutions using best-practice patterns and modern architecture.
Owns end-to-end design and development, testing, the release of critical components using Databricks technology stack and Microsoft Azure cloud platforms and services.
Performs other duties as assigned.
QUALIFICATIONS
EDUCATION: Minimum BA or BS degree in Computer Science, Information Systems, or related field required. MS in Business Analytics or related discipline preferred.
EXPERIENCE
Minimum 6 years of experience required in creating robust enterprise-grade data engineering pipelines using SQL, Python, Apache Spark, ETL, ELT, Databricks Technology Stack, Azure Cloud Services, Cloud-based Data and Analytics platforms required. 8-10 years preferred.
Minimum 3 years of experience required in solution design blueprinting and leading technical team members towards delivery of robust enterprise-grade data platform solutions.
Strong proficiency in SQL and data analysis required.
Experience in distributed data (structured, semi-structured, unstructured, streaming) processing techniques using Apache Spark, Hadoop, Hive, Kafka, and big data ecosystem technologies preferred.
Experience in data modeling and design for data warehouse, relational databases, and NoSQL data stores preferred.
KNOWLEDGE, SKILLS AND ABILITIES
Familiarity with Data Science and Machine Learning technologies, development process, and common Machine Learning libraries (e.g., Scikit-Learn, Tensorflow).
Strong problem-solving, critical thinking, verbal, and written communication skills.
Ability to influence decisions related to advanced analytics strategy & roadmaps, business use cases, and data platform capabilities.
Effective communication and collaboration with internal cross functional teams, leadership team, technology partners & vendors, and end users.
Excellent analytical, organizational skills and ability to work in a startup environment and to deliver on tight deadlines using Agile practices.
Healthcare industry experience highly desired.
TYPICAL WORKING CONDITIONS
Non-patient facing
May be either full time remote/telework or rotate working in the office and remote/telework
If remote, this job must be U.S. based
Indoor work; professional office environment
Operating computer
Reach outward
OTHER PHYSICAL REQUIREMENTS
Vision
Sense of sound
Sense of touch
Enter all here
PERFORMANCE REQUIREMENTS
Adhere to all organizational information security policies and protect all sensitive information including but not limited to ePHI and PHI (Protected Health Information) in accordance with organizational policy, Federal, State, and local regulations.


Location: Pediatric Associates · Data & Analytics
Schedule: Full Time, Days"
328,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
329,Data Engineer,Walt Disney Company,"500 S Buena Vista St, Burbank, CA 91521",N,"End and Direct Client: Walt Disney Company
Duration: 12+ Months
***********ONLY W2********NO C2C*********
JOB RESPONSIBILITIES:
Partner with technical and non-technical colleagues to understand data and reporting requirements.
Work with engineering teams to collect required data from internal and external systems.
Design table structures and define ETL strategy to build performant Data solutions that are reliable and scalable in a fast-growing data ecosystem.
Develop Data Quality checks and visualizations/dashboards
JOB REQUIREMENTS
Develop and maintain ETL routines using ETL.
Experience in Snowflake, (Snowflake Migration preferred)
Perform ad hoc analysis as necessary.
Perform SQL and ETL tuning as necessary.
Experience in Apache Spark
To follow-up with questions or to apply directly, please reach out Recruiter ""AJ"" @ 405-907-2956
Job Type: Contract
Pay: $85.00 - $95.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Burbank, CA 91521: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 10 years (Required)
Spark: 5 years (Required)
Python: 5 years (Required)
Snowflake: 2 years (Required)
Work Location: One location"
330,Data Engineer,Fidelity Investments,"Boston, MA 02210","$96,000 - $144,000 a year","Job Description:
Job Title: Data Engineer
Job Description:
We are seeking a highly skilled and motivated Data Engineer to join our team in Fidelity Labs. The Data Engineer will be responsible for crafting, building, and maintaining the data pipelines and infrastructure that support AI-enabled solutions in Fidelity Labs’ technology incubator. The ideal candidate will have a strong background in data engineering, with experience in designing and implementing data pipelines, warehousing, and modeling, while applying modern cloud tools.

Key Responsibilities:
Design and implement data pipelines to support the collection, storage, and processing of large datasets
Develop and maintain data warehousing solutions
Design and implement data models to support business requirements
Collaborate with data scientists and analysts to support the development and deployment of machine learning models
Ensure data quality and integrity
Stay up to date with emerging technologies and industry trends in data engineering
Qualifications:
Bachelor's degree in Computer Science, Information Systems, or a related field
3+ years of professional experience in data engineering
Strong experience with SQL and NoSQL databases
Experience with cloud-based data warehousing and data lake solutions such as AWS Redshift, Snowflake, and Google BigQuery
Experience with distributed computing frameworks such as Apache Spark and Dask
Experience with distributed training and fine-tuning of language models using libraries like TensorFlow, PyTorch and HuggingFace.
Experience with data storage and data management for large datasets like parquet, HDF5, etc.
Experience with data integration and ETL tools such as Apache NiFi, Apache Airflow, and Talend
Experience with programming languages such as Python, Java, and Scala
Experience with natural language processing (NLP) techniques and tools, such as tokenization, stemming, lemmatization, and sentiment analysis
Familiarity with large language models such as GPT-3, BERT, and RoBERTa
Experience working with unstructured data, such as text and audio
Experience with data pre-processing for language models like tokenization, vocabulary creation and crafting input features for language models.
Strong problem-solving and analytical skills
The Value You Deliver:
A qualified candidate will be bright, highly motivated and self-starting, able to work independently within a small dynamic team
Communicate issues and status to all levels of the organization, including senior management
Provide high quality work under tight deadlines
Ability to deal with ambiguity
Good communication skills, both written and verbal
Ability to focus on delivering customer value in a highly sophisticated and fast paced environment
Ability to work collaboratively and efficiently in a virtual team environment across multiple time zones
The Team
This opportunity is brought to you by Fidelity Labs, Fidelity Investments’ in-house software incubator and digital studio. Founded in 2005, Fidelity Labs has played a critical role in driving growth and innovation for the firm. The Fidelity Labs organization has a portfolio of new businesses and is constantly prototyping concepts for Fidelity’s next new ventures.
Fidelity Labs is a dynamic workplace that combines the best parts of startup life—building from scratch, adapting quickly, and moon-shot ambition—with the scale and stability of an industry leader. Learn more at labs.fidelity.com.
Please see below for the salary range for work locations in Colorado only:
N/A
Please see below for the salary range for work locations in New York City, Westchester County, NY and Jersey City, NJ only:
$96,000 - $144,000 per year
This position is eligible for incentive compensation or an annual bonus opportunity.
Please see below for the salary range for work locations in California only:
N/A
Please see below for the salary range for work locations in Washington only:
N/A
Certifications:
Company Overview
Fidelity Investments is a privately held company with a mission to strengthen the financial well-being of our clients. We help people invest and plan for their future. We assist companies and non-profit organizations in delivering benefits to their employees. And we provide institutions and independent advisors with investment and technology solutions to help invest their own clients' money.

Join Us
At Fidelity, you'll find endless opportunities to build a meaningful career that positively impacts peoples' lives, including yours. You can take advantage of flexible benefits that support you through every stage of your career, empowering you to thrive at work and at home. Honored with a Glassdoor Employees' Choice Award, we have been recognized by our employees as a Best Place to Work in 2023. And you don't need a finance background to succeed at Fidelity—we offer a range of opportunities for learning so you can build the career you've always imagined.
At Fidelity, our goal is for most people to work flexibly in a way that balances both personal and business needs with time onsite and offsite through what we’re calling “Dynamic Working”. Most associates will have a hybrid schedule with a requirement to work onsite at a Fidelity work location for at least one week, 5 consecutive days, every four weeks. These requirements are subject to change.
We invite you to Find Your Fidelity at fidelitycareers.com.

Fidelity Investments is an equal opportunity employer. We believe that the most effective way to attract, develop and retain a diverse workforce is to build an enduring culture of inclusion and belonging.
Fidelity will reasonably accommodate applicants with disabilities who need adjustments to participate in the application or interview process. To initiate a request for an accommodation, contact the HR Accommodation Team by sending an email to accommodations @fmr.com, or by calling 800-835-5099, prompt 2, option 3."
331,Data Engineer - Remote,SOSi,"Vienna, VA•Remote",N,"Overview
We are seeking a highly skilled and motivated Mid-Level Data Engineer to join our growing team. The successful candidate will have a strong background in data engineering, and a passion for working with large, complex data sets.
Responsibilities
Design and implement data pipelines, architectures and data sets to support business and data analytics needs.
Work with cross-functional teams to understand and define data requirements, and translate them into technical solutions.
Build, optimize, and maintain data storage and processing systems, including data warehousing, ETL, and data lake technologies.
Collaborate with data scientists and analysts to ensure data is accurate, complete, and accessible for analysis and reporting.
Monitor and troubleshoot data pipeline performance, and optimize as necessary.
Ensure data security and data governance compliance.
Continuously evaluate and implement new technologies and tools to improve data processing and storage capabilities.
Qualifications
3-5 years of experience in data engineering or related field.
Strong hands-on experience with data warehousing, ETL, and data lake technologies.
Experience with SQL and at least one programming language such as Python, Java or Scala.
Familiarity with big data technologies such as Hadoop, Spark, and Hive.
Experience with cloud-based data storage and processing solutions, such as AWS, Azure, or GCP.
Strong understanding of data modeling, data warehousing and data governance.
Experience with data security and compliance.
Strong problem solving, analytical, and communication skills.
Strong understanding of data governance and data privacy regulations.
Bachelor's degree in Computer Science, Data Science, or a related field.
Ability to obtain US government Security clearance.
Working Conditions
Full remote flexibility
Exovera offers a competitive salary and benefits package, as well as a dynamic and fast-paced work environment. If you are passionate about data engineering and want to be part of a team that is driving innovation and growth, we want to hear from you."
332,Data Engineer,Sadup Softech,United States,N,"USA ,
Contract to Hire
1 M ago
Experience
6+ Years
Positions opened
12
Job Description
.Experience extracting data from a variety of sources, and a desire to expand those skills (working knowledge in SQL and Spark is mandatory). Experience in Hadoop and Bigdata is required. Experience with Object Oriented Programming using Python and its design patterns. Experience handling Unix systems, for optimal usage to host enterprise web applications.
Desired Candidate Profile
Excellent Pyspark programming skills & familiarity with various data storage technologies and architectures
Excellent Data Analysis skills. Must be comfortable with querying and analyzing a large amount of data on Hadoop HDFS using Hive and Spark.
Excellent Communication Skills to Understand and Pass on Requirements."
333,Data Engineer,4A CONSULTING LLC,"Ellicott City, MD 21043•Remote",N,"Job Type
W2 – Full Time
Location
Remote within the continental United States
Responsibilities:
Join the 4A team as a Data Engineer and help us deliver innovative and mission-essential solutions for our clients. In this role, you will collaborate and be a proactive contributor with partners and clients to understand their business challenges and how technology can help them achieve their goals. You will apply data engineering best practices to automate and tune data pipelines in large scale environments in support of our clients, across different domains.
Your responsibilities will include:
Designing, developing, and testing infrastructure for ETL/ELT processes.
Developing, testing, and maintaining data architectures that align with business requirements.
Building analytical and visualization tools to support the data pipeline and operational monitoring.
Recommending changes to improve data reliability, quality, and efficiency in data pipelines.
Assembling and analyzing large datasets to answer business questions and research business issues
Engaging with stakeholders to gather project requirements and understand business needs.
Designing and delivering analytics for stakeholders.
Following best practices to document, review, and obtain approval of designs with clients, engineering teams, and other stakeholders.
Contributing to the documentation of data pipelines and infrastructure.
Leading the end-to-end testing of the data pipeline and ensuring data integrity.
Working closely with project stakeholders to ensure that the implemented solutions meet requirements and are completed on time and within budget.
Learning new domains, analyzing large, complex datasets, and comprehending how the data impacts the client’s business.
Working with the SDLC in use by the client (e.g., waterfall, agile, hybrid)
Required Qualifications:
3+ years of experience in systems development or integration projects (Agile experience preferred)
Strong experience in utilizing data pipeline tools for large, complex data architectures. 2+ years of experience in Snowflake is required and experience using Fivetran is preferred.
Expert knowledge of SQL and NO-SQL and experience in tuning SQL queries.
Expert knowledge of Python
Experience in data visualization tools (e.g., Power BI, Tableau).
Strong experience in data modeling, including relational models and star schemas.
Strong experience in the AWS environment and its services (e.g., Lambda, GLUE, Apache SPARK, EMR, Machine Learning, CI/CD, etc.)
Creative problem-solver with strong analytical skills and attention to detail.
Communicate effectively with both technical and non-technical clients, partners, and stakeholders.
Ability to work independently and as part of a team.
Proficient in O365 (Teams, Word, Excel, PowerPoint, SharePoint).
Working knowledge of Postman.
Experience in JIRA.
Preferred Qualifications:
Experience with and ability to describe complex systems integration projects
Certifications Preferred:
None
Education:
Bachelor's Degree from an accredited college or university with a major in Computer Science, Information Systems, Engineering, Business, or other related scientific or technical discipline.

Applicants must be authorized to work in the U.S.
Salary negotiated commensurate with experience
Benefits available for W2 employees
401K
Medical
Vision
Dental
AD&D (Basic Term Life)
Voluntary AD&D
Floating Holidays
Short Term Disability
Long Term Disability
FMLA
EAP
Tuition Reimbursement

4A Consulting, LLC is one of the fastest growing solutions delivery companies in Maryland, delivering on end-to-end Enterprise-wide information technology (IT) initiatives. 4A has extensive experience delivering superb IT consulting and support services to federal, state, and local agencies, including the Centers for Medicare and Medicaid Services, the Social Security Administration, Food & Drug Administration, and the State of Maryland. We cultivate a well-trained, technically savvy workforce through the acquisition of talent with specialized skills in program and technical management, cloud-based systems development & deployment, SAFe/Agile processes, and advanced integration technologies.
4A Consulting, LLC is proud to be an Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
4A is a certified Small Business Administration (SBA) Women-owned Small Business (WOSB)/Economically Disadvantaged Women-owned Small Business (EDWOSB), Maryland Department of Transportation Minority and Disadvantaged Small Business Enterprise (MBE/DBE), Minority Business Enterprise (National Minority Supplier Development Council), and Howard County (MD) Minority Business Enterprise IT firm.

Your Right to Work – In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire."
334,KPBS Data Engineer,San Diego State University Research Foundation,"San Diego, CA 92182•Hybrid remote","$85,000 - $130,000 a year","Overview:
Under the general direction of the Director of Digital, the Data Engineer is primarily responsible for integrating data from various CRM applications (iMIS (CRM), Eventbrite, Google Ads, Google Analytics, PBS, NPR) and other data sources into the software data warehouse (Snowflake) and customer data platform (Lytics).


This position will assist the Data Analyst in maintaining databases that house marketing and analysis data; working with subject matter experts in various departments at KPBS, working with staff on data creation policies and procedures and performing data quality checks, as needed. The position will be the lead on problem-solving technical issues within the Snowflake Data Warehouse and will work across departments to ensure the effectiveness of Key Performance Indicators within the organization.
The Data Engineer will provide program implementation and policy creation guidance and support. This includes responsibility for the monitoring of data quality, data governance, data access, data cleaning, data validity and timely consumption and delivery of the data to and from various endpoints.
Perks you'll enjoy as a member of #teamKPBS
Working on a college campus & in public media (access to campus facilities and staff discounts, farmer's market Tuesday's, tons of eateries, community events, entertainment)
Hybrid schedule, remote work flexibility & casual work attire
Paid time off on your birthday (can take on any day)
10 vacation days and 13 holidays off (enjoy 4+ days off over winter break)
Enjoy a 'beautiful day in the neighborhood' with local employee discounts
Opportunity drawings to attend SDSU basketball games, local concerts, and events
Free wellness classes & programs
Monthly pop-up events for staff & KPBS swag

Full benefits packages that are unmatched (medical, dental, vision, life)
Sick leave accruals and paid leave options
On-site childcare at a discounted rate
Opportunities for continued learning and professional development
Flexible spending account(s)
Employee assistance program
Matching and voluntary retirement savings plan

Salary range is $7,083.33/mo - $10,833.33/mo ($85,000 - $130,000/yr).
Responsibilities:
The following information is intended to be representative of the work performed by incumbents in this position and is not all-inclusive. The omission of a specific duty or responsibility will not preclude it from the position if the work is similar, related, or a logical extension of position responsibilities. Job descriptions may be changed at any time based on the needs of the department.

Data Management & Development (35%)
Build, manage and maintain data pipelines for data integration (ELT/ ETL)
Perform data quality checks and clean ups at various data collection and storage touchpoints
Ensure security of data during transit and at rest
Transform data for use in marketing and analytics applications
Data Administration (35%)
Perform as the lead administrator role for the Snowflake Data Warehouse
Act as the lead for problem-solving technical issues within or related to the Data Warehouse
Work across departments to ensure effective data usage in the organization
Create and maintain data policies that will ensure accuracy of reports and usage of data for marketing
Project Implementation (15%)
Work across departments to gather requirements, implement various projects as they arise
Support and Staff Training (10%)
Support Data Analyst by providing data for use in analysis, managing connections to BI applications, dashboard creation, etc.
Lead periodic trainings for Snowflake users
Train support staff on data creation policies and procedures, business process creation and other activities that impact the quality and usefulness of data collected
Convene and lead workgroups, create/implement policies and monitor and oversee the data quality across multiple systems

Other Duties as Assigned (5%)
Qualifications:
MINIMUM EDUCATION
Bachelor’s degree in computer science, information systems, business administration or similar field. Additional or equivalent years’ of work experience may be substituted for the required education on a year for year basis.
MINIMUM EXPERIENCE
Four-years of related work experience in data engineering; developing data pipelines
PREFERRED EXPERIENCE

2-3 years of which involve SQL database administration;
3-4 years of which include experience developing, maintaining and administering software applications on the linux operating system
KNOWLEDGE & ABILITIES
Demonstrated experience with data integration toolsets (i.e Airflow)
Demonstrated experience with writing and maintaining Data Pipelines
Demonstrated ability of good scripting, including Bash scripting and Python
Demonstrated experience with cloud technologies such as AWS, GCP, Azure
Demonstrated experience designing and working on complex data systems from design to delivery
Knowledge of and familiarity with Data Modeling techniques such as dimensional modeling and Data Warehousing standard methodologies and practices
Knowledge of SQL and ability to create queries to extract data and build performant datasets
Demonstrated expertise in using Snowflake cloud data warehouse including administration of roles, security, stages, etc.
Ability to integrate with 3rd party APIs
Ability to problem solve with strong attention to detail; excellent analytical and communication skills
Ability to work individually and as a part of a team
Ability to interact effectively with co-workers, vendors and consultants
Ability to understand and follow posted work rules and procedures
Ability to relate well to others within the project environment.
Ability to display motivation and strong interpersonal skills
Ability to communicate effectively, both orally and in writing
Ability to display organization, meet deadlines, display detail orientation, possess good judgment and common sense
Ability to demonstrate a high level of cross-cultural sensitivity
ADDITIONAL APPLICANT INFORMATION:
The COVID-19 vaccine is required by the CSU for all SDSU Research Foundation employees as a condition of employment. Should you be offered a position, you will be required to provide proof of vaccination status. Individuals who obtain an approved medical or religious exemption on file will be required to complete regular COVID-19 testing
A background check (including a criminal records check) and Livescan (fingerprint) must be completed satisfactorily before any candidate can be offered a position with SDSU Research Foundation/KPBS
San Diego State University Research Foundation is an EEO/AA/Disability/Vets Employer"
335,Data Engineer - Remote,ITEOM,Remote,"$130,000 - $160,000 a year","Our client is seeking a Data Engineer who would be responsible for design of data warehouse schemas as well as end-to-end design and implementation of fault tolerant and scalable data processing pipelines using a variety of technologies for orchestrating data movement, primarily Snowflake , DBT and Airflow . You'll work with a team of data engineers to design and implement solutions that provide business-critical insights.
Salary : Mid-Level - 130K to160K+ bonuses and equity options
Location: REMOTE WORK 100% is an option: (locations limited to CO, AZ, TX, VA, CA, GA, HI, MN, MI, NJ, NY, NC, OR, PA, WA)
How You’ll Do It
Working collaboratively with a team of other data engineers and developers of varying experience levels and areas of expertise, you'll be responsible for:
Responsibilities
Design and implement data warehouse schemas to store large data sets that can be easily and efficiently queried and joined together for reporting and analytical purposes.
Design and develop ETL/ELT pipelines to efficiently move and aggregate data, while optimizing for maximum code and data reuse.
Collaborate with other team members on improvements to existing systems
Investigate data anomalies and provide quick resolutions.
Provide technical support to business users and analysts.
What We’re Looking For
TOP SKILLS: ETL, Data Warehousing, SQL, Jinga Scripting, Python and Snowflake.
3+ years of working experience with large scale data warehouse systems.
Very strong knowledge of SQL and data manipulation best practices
Experience in building efficient and fault tolerant ELT/ETL data pipelines
Very strong knowledge of working with large scale datasets and data modeling and data warehouse design
Experience with Snowflake is preferable
Knowledge of DBT, Jinja scripting and Airflow is a big plus
Nice to Have
Experience working cross functionally with product and engineering teams
Desire to wear many hats and work your tail off for a bit - great earning potential and equity available.
Benefits: A diverse benefit package, sign on bonus, options, etc. is offered through our client.
About ITEOM: ITEOM Digital Technology Talent Partners collaborates with top technologists and top employers to align purpose and passion and re-incorporate belonging to make the job search and work experience more human. We are an equal opportunity employer; however, this role is not eligible for sponsorship at this time. Employees eligible for full time W2 employment are encouraged to apply."
336,Data Engineer,Starschema,"Arlington, VA•Remote",N,"Company Description

About Starschema

At Starschema we believe that data has the power to change the world and data-driven organizations are leading the way. We help organizations use data to make better business decisions, build smarter products, and deliver more value for their customers, employees and investors. We dig into our customers’ toughest business problems, design solutions and build the technology needed to address today’s unique challenges.

What you can expect as a Starschema team member
As a member of the Starschema team, you will be on the front lines of digital transformation, working with some of the most innovative Fortune 500 companies to drive innovation and realize the promise of data-driven cultures. You will learn and use the latest data-centric technologies along with the core industry technologies.
Our team is inclusive and fun. While we take our work seriously, we know how to have a good time while doing so. We encourage everyone to share their opinions and ideas, and our leadership wants to hear everyone’s input no matter what role they play in the organization.

Job Description

As a Data Engineer at Starschema, you will bring business value for our clients through end to end development, optimization and operation of automated reporting, data lakes and related software platforms. You will use the latest technologies like Apache Airflow, Apache Kafka, Apache Spark and AWS etc. We are seeking for experienced medior and senior professionals for our open position.
What will You do:
Build and maintain database/bigdata clusters;
Build dashboards for infrastructure management and reporting;
Design and deploy infrastructure management strategies to meet up time and monitoring SLA’s;
Deploy code release in QA and PROD;
Participate in building unit/performance/integration tests working with database developers;
Participate in database SQL optimization plan;
Deploy configuration and automation tools to remove manual steps in deploying, upgrading, and scaling systems and software across all environment.

Qualifications

We want to hear from you if You have:
At least 3 years of experience in data engineering field;
Solid background in Python and SQL;
Experience building data solutions using big data tools: Airflow, Spark, Kafka, AWS;
Experience with data pipeline and workflow management tools
Hands-on experience with requirements analysis, design, coding and testing patterns;
Has experience in engineering (commercial and open source) software platforms and large-scale data infrastructures;
Experience working with cloud computing environments;
Excellent communications skills in English (both written and oral);
Intelligent, communicative team-player personality, interested in and willing to learn new skills and technologies.

Additional Information

What's In It For You:
Remote work: You can work remotely from anywhere within the USA. Plus if you are based in Washington D.C area.
Eligibility: We are unable to support work visa for this specific position so we are open to receive application from candidates who are eligible to work in the USA.
Benefits & Community: A healthy lifestyle and the feeling of belonging are important to us, for both body and mind. We provide:
401K Insurance with matching
Employee Assistance Program (EAP)
Technical/Professional trainings
Remote work / Home Office opportunity
Start date: The sooner the better, but if you currently work somewhere and have a notice period, it is still fine, we will wait for the right person!"
337,Data Engineer,FutureFit AI,"Ontario, CA•Remote",N,"Join our Engineering Team!

The Opportunity
A growing company of less than 50, you will join a caring team of engineers, business, and product leaders; all focused on using technology to improve lives and outcomes for people going through career transitions. We care about the impact our technology can have on workers and bring deep empathy to our work.
We’re looking for an engineer ready to scale our data and ensure success in production environments: In this role, you will partner with our application engineering team to transform raw data into useful data for analytical purposes. With your strong analytical skills, ability to combine data from different sources, interest in analytics, and familiarity with several programming languages, we’ll ensure our FFAI profiles surface insights for better career recommendations for workers across the globe.
How You’ll Fulfill Your Mission

Partner - From exploring ways to enhance data quality and reliability, writing and automating tests, to further developing our recommender and inference systems. You’ll be collaborating with teams of data scientists and engineers to build and maintain data systems that generate easy-to-analyze datasets that drive our core insights.
Problem Solve - Serving as a technical leader for the business, we’ll need you to translate our data in ways that address our platform/business needs. This will require excellent communication skills since you’ll be working across various departments to understand how to best manage and scale our data
Implement - As an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up, we’ll be relying on your expertise to move our data models into production as we make use of 350+ million data points.
Why We Value You
Technical.
Strong expertise in Python and experience with Flask, FastAPI, OpenAPI / Swagger; some knowledge of numpy, pandas, and related technologies is an advantage.
Comfortable with architecture and design around various data paradigms (document stores, relational, graph) and databases (Mongo, Elastic, Postgres, etc.)
Experience in architecture and implementation of ETL processes, streaming data, data process orchestration (Airflow); exposure to MLOps and Machine Learning is an advantage
Collaborative. Experienced in using different technologies to collect and map an organization’s data landscape to help decision-makers find cost savings and engineering team optimization opportunities.
Internally Driven. Deeply motivated by building data infrastructure to provide data-driven insights, and empower teams to deliver business value.
Benefits: We provide a generous benefits package to all of our team members, including unlimited PTO, health care, technology reimbursement, and flexible schedules.
About FutureFit AI
FutureFit AI believes that the greatest challenge of the Future of Work is facilitating successful career transitions, and our mission is to address that challenge by using AI to transform education and economic mobility.
FutureFit AI is a growing, well-funded, company focused on using technology to improve the lives and outcomes for people going through career transitions. We bring innovation, creativity, and empathy to our work, and care deeply about the impact our technology can have on our customers and users.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment. Please contact us to request an accommodation.

FutureFit AI All rights reserved, we are proud to be an equal opportunity workplace. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate on the basis of race, religion, color, gender identity, sexual orientation, age, disability, veteran status, or other applicable legally protected characteristics. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply."
338,Junior Data Engineer,"Link Technologies, Inc.","Las Vegas, NV","$64,134 - $178,786 a year","Link Technologies (LinkTechConsulting.com), is currently seeking a Junior Data Engineer for a position in Las Vegas, NV. This will be an onsite/contract opportunity. This role will need to support on-call activities including over the weekend.
QUALIFICATIONS
Minimum three (3) years of data warehouse experience.
Strong understanding of SQL Server database security is required.
Ability to design and evaluate store procedure for business logic is required.
Ability to create and manage reports in SSRS 2019 and Power BI is required.
Bachelor’s degree or equivalent in related field.
MUST be able to acquire and retain a Nevada Gaming Control Board registration and other certification or license, as required by law or policy.
Understanding of Windows 2019 with SQL Server 2016 or higher.
Experience with SQL ETL construction with support of SSIS versions 2016 and 2019.
Experience with the management of transactional SQL replication.
Previous experience fulfilling ServiceNow and data warehouse incident tickets.
MUST have a strong grasp of relational and dimensional database modeling.
MUST have expert knowledge of the Microsoft BI Stack including SSIS, SSRS, SSAS, SharePoint, PowerPivot, etc.
Previously worked with automated file handling with ETL tools via FTP and other means.
Administrative experience with SQL Server 2008 – 2012.
Clear understanding of Windows, Enterprise Systems, Networking, and Enterprise Security.
MUST adhere to SOPs and methodologies and be open to updating them as needed.
Effective communication skills with the ability to work with all backgrounds and levels of experience.
MUST be available to work varied shifts, including nights, weekends, and holidays.
Willing to learn and adapt to new technologies and paradigms, i.e., Hadoop, Hive, etc.
Experience with Java or .NET development; C#, ASP.NET, scripting languages, and web site operations is a plus.
Data preparation for data science experience a plus.
Ability to understand big data, advanced analytic techniques, and real time data a plus.
Confidence with analytic tools, i.e., Spotfire, Tableau, SAS, Cognos a plus.
PREFERRED
Gaming and hospitality experience.
Experience with the following:Python, C#, and PowerShell
GCP; Big Query and dataflows
Azure; Synapse and data lake
IBMI DB2
Tableau and PowerBI
Azure DevOps, Jenkins, and JIRA
Informatica and Precisely; CDC Connect/DTShare
Kafka
TFS and Agile/SCRUM experience.
Previously worked with key concepts of Data Management, i.e., Data Quality and MDM.
DUTIES AND RESPONSIBILITIES
Create, develop, administer, and maintain Data Warehouse and ETL structures and solutions.
Be a point-of-contact and help with architecture, vision, problem anticipation, and problem solving for the assigned project, and become a subject matter expert for data and analytic users.
Offer L3 application and on-call support for data and integration solutions.
Contribute and be prepared to lead project teams within an agile environment.
Multitask and prioritize responsibilities including application and on call support, major and minor projects, functional conditions, and systems specifications.
Provide efficient documentation for delivered solutions and processes, incorporating documentation with the business knowledgebase.
Support subject matter expertise for establishing business requirements to address business opportunities or issues across business functions for data centric solutions in partnership with the Product Management team.
Keep up-to-date with technological trends and innovations in the data warehousing and data management domains.
Understand and fulfill business needs by designing and enhancing systems to transform, cleanse, and provision corporate data assets in a controlled, secure, and high-performance manner.
Take part in the ongoing data management maturation process.
Safety is an essential function of this job.
Consistent and regular attendance including on-call availability on a rotational basis is an essential function of this job.
Performs other related duties as assigned.
Link Technologies is an equal opportunity employer. All qualified applicants will receive consideration for employment without discrimination because of race, color, religion, sex, gender identity/expression, sexual orientation, national origin, protected veteran status, disability, or any other factors protected by law.
Job Type: Contract
Pay: $64,134.31 - $178,785.55 per year
Benefits:
Dental insurance
Health insurance
Vision insurance
Experience level:
3 years
Schedule:
Day shift
Evening shift
Night shift
On call
Overnight shift
Weekend availability
Ability to commute/relocate:
Las Vegas, NV: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data warehouse: 3 years (Required)
SQL Server Database Security: 3 years (Required)
SSRS 2019: 3 years (Required)
Power BI: 3 years (Required)
Windows 2019: 3 years (Required)
ServiceNow: 3 years (Required)
Work Location: One location

Health insurance"
339,Data Engineer,ZoomCare,Remote,"$92,000 - $132,000 a year","SUMMARY OF JOB PURPOSE AND FUNCTION
The Database Engineer ensures data pipelines are scalable, repeatable, secure, and can serve
multiple users. They help facilitate getting data from a variety of different sources, getting it in
the right formats, assuring that it adheres to data quality standards, and assuring that
downstream users can get that data quickly. The Database Engineer is responsible for processing diverse sources of data seamlessly. The Database Engineer has knowledge of the physical database design principles and the full SDLC (analysis, design, implementation, testing and deployment).
ESSENTIAL RESPONSIBILITIES AND TASKS:
Participate with Business and Technology Stakeholders to refine, create actionable objectives and estimates for completion of quality Software Requirements
Design and implement data storage (SQL, Postgres, AWS Glue, Aurora, Redshift, Pentaho, etc.)
Manage application code in a distributed source control system (git, including branching, merging, and rebasing)
Work with team members to design and implement data solutions in alignment with the project schedule.
Code, test, and document new or modified data systems to create robust and scalable data platforms, clearly documenting design decisions and system process
Perform peer reviews of team members application and test code and documentation, ensuring compliance with database design standards and principles and consistency across solutions
Resolves conflicts between models, ensuring that data models are consistent with the ecosystem model (e.g., entity names, relationships and definitions)
Help reproduce production system faults for troubleshooting and correction and provide recommendations for improvement of existing software systems
Expands and grows data platform capabilities to solve new data problems and challenges.
Creates data flow diagrams for all business systems.
Interprets data results to business customers.
Develops, tests, implements, and maintains database management applications.
Constructs and implements operational data stores.
Participates in creating, refining, managing and enforcing data management policies, procedures, conventions and standards, modeling best practices in data management.
Contributes to the establishment of business continuity & disaster recovery requirements, methods and procedures for data systems and databases.
Ensures compliance with all applicable data privacy and security requirements (HIPAA, SOC2, etc.)
EXPERIENCE, EDUCATION AND/OR TRAINING:
Between 2 and 7 years of data engineering, data science, or software engineering experience
Familiarity with system operations tools and environments (Linux/Unix, OS X, Windows, etc)
Competent ability to assist less experienced software engineers
Familiarity with and ability to operate in an agile development process
Bachelor in computer science, computer engineering, or equivalent work experience.
Understanding of how algorithms work and experience building high performance algorithms
Expert knowledge of data modeling and understanding of different data structures and their benefits and limitations under particular use cases
Experience with ETL, Data Warehousing, and Data Lake technologies
SPECIAL WORKING CONDITIONS:
Availability outside of scheduled working hours to address system critical failures (On Call, Tier 3 Support)
REPORTING STRUCTURE:
Data Engineers report to Engineering Leadership
COMPENSATION:
Pay: $92,000 - $132,000 per year
Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.

ZoomCare offers a competitive compensation and a robust benefits package including Health & Wellness Benefits, 401K with employer match, Paid Time Off, Paid Holidays, Paid Parental Leave, Sabbatical program and additional perks!"
340,Software Data Operations Engineer,MAQ Software,"Redmond, WA 98052","$80,000 - $120,000 a year","About MAQ Software
As 2021 Microsoft Power BI Partner of the Year, we enable leading companies to accelerate their business intelligence and analytics initiatives. Our solutions enable our clients to improve their operations, reduce costs, increase sales, and build stronger customer relationships.
Our clients consistently recognize us for providing architecture and governance frameworks, implementing best practices to optimize reports, and building team capability through training programs. Our innovative tools and 33 certified visuals expand Power BI capabilities to save time for decision makers.
As a premier supplier to Microsoft for two decades, our clients benefit from our extensive insights on the platform and engineering practices. As a Microsoft Partner with 10 Gold competencies, our clients improve their implementations with our breadth and depth of expertise.
With globally integrated teams in Redmond, Washington, and Mumbai, Hyderabad, and NOIDA India, we deliver solutions with increased velocity and tech intensity.
Inc. magazine has recognized us for sustained growth by listing us on the Inc. 5000 list ten times – a rare honor.
*
Engineering culture*
We foster a strong engineering culture with a can-do attitude. All our key managers come from excellent educational backgrounds and have significant experience growing a company and mentoring software engineers. Due to our smaller size, we adopt the latest technologies and computing trends ahead of the larger industry players. As a part of the company’s globally distributed engineering team, our engineers gain exposure to the latest software engineering practices and fast development cycles.
Our developers routinely work on challenging technical problems that utilize the latest technologies for fast-paced software delivery.
*
Examples of some of our projects:*
We built a supervised machine learning model that forecasts the impact of retail sales on our client’s overall revenue. We collected data from existing customer relationship management (CRM) and sales systems. We created a forecasting model in Azure Databricks using existing and custom linear regression to process the collected data. To reduce forecast runtime and achieve near real-time analysis, we modified the existing R libraries to SparkR. The improved insight helped our client proactively focus on retailers with the highest sales impact.
We built a check-in app for one of our client’s most attended event. A multinational technology company organizes an annual multi-event internal expo attended by thousands of their employees. The manual process of tracking attendance, sending acknowledgments, and receiving feedback was time consuming. To automate the process, we built a check-in app that uses mobile devices’ camera to capture the identification badge of each participant. The captured images are stored in an Azure Blob. An Azure Logic App reads the image content utilizing Optical Character Recognition (OCR) API to update attendance records. After the event, notifications are sent to attendees via Microsoft Teams to complete a feedback survey using a Microsoft Power Automate Bot. The Feedback App reports the survey responses to determine the Customer Satisfaction (CSAT) score of the event.
For another client with high volume data, we developed and implemented a hybrid data processing solution using Azure Stream Analytics and Azure Databricks to reduce data refresh time from 3 hours to less than 30 minutes. We sourced data from the Azure Event Hub, where refreshes originate. Refreshes are captured through stream analytics and the updated data is pushed to Azure Data Lake Storage (ADLS). The data is processed in ADLS, then pushed to Power BI for reporting.
To read about some of our recent projects, visit https://maqsoftware.com/case-studies
*
Responsibilities:*
Analyze existing systems (30%)
Collect requirement specifications to analyze business processes and determine the exact nature of user’s system requirements, map process flow, discuss with module leaders and core team members to decide on the architecture.
Analyze existing system structures to provide solutions to improve computer systems to use cloud-based systems and services.
Analyze user requirements to match data available to large computer database source systems to implement solutions at reasonable performance and cost.
Design the processing steps and propose new systems based the user’s requirements. Interact with systems analysts/programmers to develop data migration tools, create processes for the new computer system and attend to ad-hoc issues related to day-to-day activities. Work with software developers in the implementation and testing phase.
*
Develop specifications and workflow (25%)*
Prepare software specifications, flow charts, and process diagrams for software programmers to follow. Develop and maintain systems documentation such as design specifications, user manuals, technical manuals, descriptions of application operations, and methodology documentation.
Analyze feasibility using commercially available software systems (e.g., Microsoft Azure versus Amazon Web Services) and reporting systems (e.g., Power BI versus Tableau).
*
Analyze and verify implementation (25%)*
Interact with systems analysts/programmers to develop data migration tools, create processes for the new computer system and attend to ad-hoc issues related to day-to-day activities.
Work with other software developers in the implementation and testing phase.
Setup test environment and compare data from multiple sources to verify reports for end users.
*
Review implementation status and reporting (10%)*
Participate in technical collaboration meetings and periodical reviews of implementation status.
Report weekly task plan to the project management team for implementation of custom software.
*
Training and certifications (10%)*
Participate in technical trainings and complete relevant industry courses and certifications.
*
Qualifications:*
Undergraduate or graduate degree in Computer Science, Information Systems, Electrical Engineering, Applied Computational Math Sciences or related Engineering discipline.
*
Benefits & Salary:*
Annual pay range $80,000 - $120,000.
Paid time off.
Comprehensive medical, dental and vision insurance with employee premiums paid in full.
401(k) retirement plan with 3% company match and immediate vesting.
Job Type: Full-time
Pay: $80,000.00 - $120,000.00 per year"
341,Senior Data Engineer (Remote or Hybrid options available),WALGREENS,"Deerfield, IL 60015•Remote","$125,000 - $155,000 a year","Job Summary
We are looking for an experienced Senior Data Engineer looking to join our diverse technology team and help us take things to the next level. If this is you, keep reading!

The Sr. Data Engineer builds and maintains big data pipelines to support advanced analytics and data science solutions. The role also identifies valuable internal and external data, and collaborates closely with data scientists to define data for the design, development, and deployment of new solutions that support strategic business priorities.

Job Responsibilities
Develops software that processes, stores and serves data for use by others.
Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.
Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.
Develops and maintains optimal data pipelines into the advanced analytics platform, including design of data flows, procedures, and schedules. Ensures that optimal data pipelines are scalable, repeatable and secure.
Troubleshoots software and processes for data consistency and integrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.
Anticipates and prevents problems and roadblocks before they occur.
Interacts with internal and external peers and managers to exchange complex information related to areas of specialization.
Collaborate with data scientists to prepare data for model development

Why Walgreens?
No matter where you are, life at Walgreens is driven by a culture of helping others. From our store aisles and warehouses to our headquarters and technology hub, there’s a spirit of excitement and innovation everywhere you look. A career at Walgreens means joining a company that’s been dedicated to our customers and the communities we serve for over 120 years. And these days, you’ll find that our people are just as passionate, committed and supported as ever before.

There's no better feeling in a job than helping people live more joyful lives through better health in the communities you serve. And that’s why a career at Walgreens feels so good. With plenty of learning and growth opportunities, exciting challenges and talented teams, you’ll have everything you need to see your future in a whole new way.

We want to keep on driving change, and we’re committed to investing even more in DE&I, adding new initiatives, strengthening our nationwide veterans training program and expanding our Business Resource Groups. We’re listening, learning and taking big and bold action to ensure equity and inclusion for everyone.

About Walgreens Boots Alliance
Walgreens Boots Alliance (Nasdaq: WBA) is a global leader in retail pharmacy, impacting millions of lives every day through dispensing medicines, and providing accessible, high-quality care. With more than 170 years of trusted healthcare heritage and innovation in community pharmacy, the company is meeting customers’ and patients’ needs through its convenient retail locations, digital platforms and health and beauty products.

Including equity method investments, WBA has a presence in more than 25 countries, employs more than 450,000 people and has more than 21,000 stores.

WBA’s purpose is to help people across the world lead healthier and happier lives. The company is proud of its contributions to healthy communities, a healthy planet, an inclusive workplace and a sustainable marketplace. WBA is a participant of the United Nations Global Compact and adheres to its principles-based approach to responsible business.

WBA is included in FORTUNE’s 2021 list of the World’s Most Admired Companies*. This is the 28th consecutive year that WBA or its predecessor company, Walgreen Co., has been named to the list.

More company information is available at www.walgreensbootsalliance.com

Basic Qualifications
Bachelor's degree and at least 4 years of experience in data engineering OR a Graduate Degree in a technical discipline and at least 2 years of experience in data engineering
Deep knowledge of SQL
At least 2 years of experience with REST API development
Experience establishing and maintaining key relationships with internal (peers, business partners and leadership) and external (business community, clients and vendors) within a matrix organization to ensure quality standards for service.
Experience diagnosing, isolating, and resolving complex business issues and recommending and implementing strategies to resolve problems.
Experience presenting to all levels of an organization
Willing to travel up to 10% of the time for business purposes (within state and out of state).

Preferred Qualifications
Advanced level skill in RDBMS (MSSQL, PostGres, Oracle, etc.), Document Databases (MongoDb, Cosmos Document, Couchbase, etc.), Key-Value Databases (Azure Table Storage, Cosmos Table, Amazon Dynamo, etc.)
Advanced level skill in Wide-Column Databases (Cassandra, MariaDB, etc.) and Graph Databases (Neo4J, Cosmos Gremlin, etc.)
Deep knowledge of Advanced level skill in Data Warehousing and Data Lakes
Advanced level skill in Data Migration and Transformation
Experience with Azure application data processing tools like ADLS, Databricks, ADF, Azure Flow, Synapse, Power BI
Experience with messaging streaming systems such as Kafka or Azure Event Hubs
Experience with Azure application deployment
Experience working with Data Scientists and Machine Learning Engineers.
Experience developing APIs a serving/data exposition layer after the ETL process.
Ability to assess the effectiveness and accuracy of new data sources and data gathering technique and sound understanding of SQL and NO SQL databases.
Familiarity with all standard integration protocols and data security implications – API Open standards, SSL, oAuth2, Voltage encryption
An employee in this position can expect a salary between $125,000 and $155,000 plus bonus pursuant to the terms of any bonus plan, if applicable will depend on experience, seniority, geographic locations, and other factors permitted by law. To review benefits, please click here jobs.walgreens.com/benefits. Walgreens will provide applicants in other states with information related to the positions, to the extent required by state or local law, by calling 1-866-967-5492."
342,Staff Engineer (Data),Atlas Labs,Remote,"$200,000 - $240,000 a year","No Patient Left Behind
Atlas Health automates philanthropic aid to improve access, affordability, outcomes and health equity for vulnerable populations.
Through intelligent matching and enrollment to 20,000 philanthropic aid programs, healthcare organizations can improve patient outcomes and reputation, increase cash and reduce staff administrative burden.
At Atlas Health, we are dedicated to eliminating the unfortunate reality of patients not being able to afford to survive. We may not be providers, but we do save lives by providing patients with the ability to receive treatment, when they otherwise may have never started or stopped for fear of negative financial outcomes. We love that we are saving lives by improving access and affordability. We view every dollar we find for a patient as hope.
Ultimately, we want to ensure no patient is left behind.
Data Engineers on our data platform team develop systems that manage data flow throughout the Atlas infrastructure and support downstream data applications. These systems are responsible for handling sensitive patient data, and members of the team are technical champions of ensuring HIPAA standards for confidentiality and compliance. In addition to key data engineering activities within a given team, Staff Engineers are responsible for leading the broader enterprise implementation of initiatives and systems, working across teams to support the integration efforts of systems and eliciting feedback for technical and product leadership.

Responsibilities:
Engineer and evangelize scalable, reliable, and performant systems that manage data
Champion data and software engineering best practices throughout the Atlas organization
Lead data infrastructure development and implement scalable enterprise-capable data solutions.
Collaborate with platform engineers on creating reliable and secure systems
Collaborate closely with team members and product stakeholders, as well as other engineers across the organization
Create trustworthy, secure, governable, and standardized data systems for consumption
Develop readable, well-tested applications, APIs, and libraries
Develop systems and tools that improve the developer experience and enhance throughput
Provide guidance and mentorship to more members of the team on best practices and skills development
Partner with engineering and product leadership on architecture, feasibility, and org-readiness planning
Implement application observability in the form of metrics, logging, and monitoring
Provide support for platform concerns and coordinate response plans with appropriate stakeholders
Requirements:
Professional experience with cloud-based systems
Demonstrated ability to lead technology initiatives in prior roles and initiatives
Experience with data architectures and tools in support of streaming and batch driven data processing
Solid understanding of distributed task orchestration
Prior experience in developing and testing systems that manage data reliability, efficiency, and quality
Prior experience in API development and testing
Working knowledge of data management architectures and patterns
Comfort with developing IAC with a core understanding of security principles
Understanding of multiple software development and architectural paradigms
Familiarity with domain driven design concepts
Excellent collaboration and communication skills
Computer Science or related technical degree in a related field or equivalent technical experience
Bonus points:
Depth of knowledge in Google Cloud Platform
Experience working in an ePHI environment
Preferred Qualifications:
Familiarity with our current stack: Python, Apache Airflow, BigQuery & GCP Ecosystem
Salary: Salary for this position is $200,000-$240,000 per year.
Benefits:
We offer a comprehensive benefit plan for our U.S. based employees which includes:
Health, dental and vision insurance
401K
Flexible time off
Paid holidays
Why Join Our Team:
Because you’re motivated by a combination of success, working alongside incredible people, and have a passion for helping clients and patients. Atlas helps people access essential medical treatment, and avoid financial ruin from medical debt. You care about being a part of the journey and wish to play a key role in our organization’s success.
Atlas values diversity of all kinds, and we’re committed to building a diverse and inclusive workplace where we learn from each other. We are an equal opportunity employer and welcome people of all different backgrounds, experiences, abilities, and perspectives."
343,Data Engineer/SW Developer (TS/SCI FSP),IBM,"15036 Conference Center Dr, Chantilly, VA 20151","$103,000 - $225,000 a year","Introduction
As a Data Scientist at IBM, you will help transform our clients’ data into tangible business value by analyzing information, communicating outcomes and collaborating on product development. Work with Best in Class open source and visual tools, along with the most flexible and scalable deployment options. Whether it’s investigating patient trends or weather patterns, you will work to solve real world problems for the industries transforming how we live.

Your Role and Responsibilities
IBM is committed to providing National Security leaders with cutting edge data exploitation and analysis capabilities. Seeking a talented, versatile Data Engineer / Back End Developer with TS/SCI and Full Scope Polygraph.

The successful candidate will be part of a small agile team building cutting edge analytic capabilities including a data pipeline, data lake, and associated analytic and AI/ML capabilities for US Government customers. The team has is fast-paced, collaborative, and cohesive, and depends on team members to communicate openly and to design solutions and deliver quality code on a regular, aggressive clip.

The Data Engineer / Back End Developer will be a hands-on software developer who will design, develop, integrate, test, and deploy software using a combination of custom, open source, and off-the-shelf (GOTS/COTS) software packages and a variety of data types (structured, unstructured, non-text). The Data Engineer / Back End Developer will work as part of an Agile scrum team to analyze business requirements and translate them into technical tasks. They will enhance software functionality based on rapidly evolving mission needs and technology opportunities.

Level (entry, journeyman, senior, master) will be determined based on education and years of experience.

#CLEARED22

Required Technical and Professional Expertise
REQUIRED ON DAY 1: Active TS/SCI security clearance + Full Scope Polygraph
At least 2 years demonstrated hands-on programming experience with one or more of the following: Java, Node, Python, AWS
At least 2 years of experience architecting, designing and programming applications in hybrid, cloud or on-prem environments
Experience with Git/GitHub
Experience integrating with JSON-based REST APIs
Experience with Agile software development, Scrum, and software development lifecycle (SDLC)
Self-starter with an ability to work independently and in a team environment
Strong problem-solving, critical thinking, and analysis skills.Ability to articulate complex analytic problems, work effectively without detailed instructions, collaborate and ask questions
Ability to generate, follow, and update technical procedures and evaluate results

Preferred Technical and Professional Expertise
Bachelors or Maser's Degree in Computer Science, Computer Information Systems, Computer Engineering
At least 1 year of classified software engineering experience on IC systems and architectures
At least 1 year classified software engineering experience

About Business Unit
IBM Consulting is IBM’s consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients’ businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.
This job requires you to provide your COVID-19 vaccination status with supporting documentation, where legally permissible.

Your Life @ IBM
In a world where technology never stands still, we understand that, dedication to our clients success, innovation that matters, and trust and personal responsibility in all our relationships, lives in what we do as IBMers as we strive to be the catalyst that makes the world work better.
Being an IBMer means you’ll be able to learn and develop yourself and your career, you’ll be encouraged to be courageous and experiment everyday, all whilst having continuous trust and support in an environment where everyone can thrive whatever their personal or professional background.
Our IBMers are growth minded, always staying curious, open to feedback and learning new information and skills to constantly transform themselves and our company. They are trusted to provide on-going feedback to help other IBMers grow, as well as collaborate with colleagues keeping in mind a team focused approach to include different perspectives to drive exceptional outcomes for our customers. The courage our IBMers have to make critical decisions everyday is essential to IBM becoming the catalyst for progress, always embracing challenges with resources they have to hand, a can-do attitude and always striving for an outcome focused approach within everything that they do.
Are you ready to be an IBMer?

About IBM
IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement
IBM offers a competitive and comprehensive benefits program. Eligible employees may have access to:

Healthcare benefits including medical & prescription drug coverage, dental, vision, and mental health & well being
- Financial programs such as 401(k), the IBM Employee Stock Purchase Plan, financial counseling, life insurance, short & long- term disability coverage, and opportunities for performance based salary incentive programs
Generous paid time off including 12 holidays, minimum 56 hours sick time, 120 hours vacation, 12 weeks parental bonding leave in accordance with IBM Policy, and other Paid Care Leave programs. IBM also offers paid family leave benefits to eligible employees where required by applicable law
Training and educational resources on our personalized, AI-driven learning platform where IBMers can grow skills and obtain industry-recognized certifications to achieve their career goals
Diverse and inclusive employee resource groups, giving & volunteer opportunities, and discounts on retail products, services & experiences

The compensation range and benefits for this position are based on a full-time schedule for a full calendar year. The salary will vary depending on your job-related skills, experience and location. Pay increment and frequency of pay will be in accordance with employment classification and applicable laws. For part time roles, your compensation and benefits will be adjusted to reflect your hours. Benefits may be pro-rated for those who start working during the calendar year.

We consider qualified applicants with criminal histories, consistent with applicable law.

US Citizenship Required.

Being You @ IBM
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
344,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
345,Data Platform Engineer,Major League Baseball,"New York, NY","$130,000 - $150,000 a year","Launched in 2001 as the tech arm of Major League Baseball, MLBAM (Major League Baseball, Advanced Media) is now a leading authority in real-time sports data processing, distribution and analysis.
MLB's Technology team builds all of your favorite web and mobile applications for the game of baseball. Using cutting edge technology, our apps are consumed by fans, broadcasters, stadiums, and MLB teams.
We are seeking motivated engineers to build, maintain and use our platforms and services to manage data at MLB. We are looking for those with experience with Infrastructure as code, automated tests as well as modern observability principles. The platforms we create and manage securely move, transform, and enrich data for both analytic consumption as well as the delivery of actionable intelligence throughout the MLB organization. These platforms and services create a single source of truth of well-curated data domains that MLB depends on.
You will have the opportunity to contribute to many different projects and transfer your ideas into solutions for some of the most meaningful data problems in MLB!
SOME OF OUR SERVICE CAPABILITIES
Connect to a wide variety of source and target systems
Move data between systems
Transform / Enrich data
Create and manage MLB data products
Organize disparate data into business domains
Coordination services
EXAMPLES FROM OUR CURRENT AND FUTURE TECH STACK
We use python modules like gcp libraries, tox, pytest, and panda regularly. In addition, we want to use frameworks like DBT and Meltano. Kafka and containers will be a regular part of our infrastructure. Some of the GCP services we use are for Storage, Containers, Secrets, Big Query and monitoring. It is important to understand how to use SQL to enrich and organize data optimally in GCP.
DEVOPS TOOLS
We use tools like GitLab, Terraform, Ansible, Bash and Docker Compose.
METHODOLOGIES
We are an agile shop that believes in Infrastructure as Code (IaC) and crafting lite-weight asynchronous services with clean code that are tested in an automated way. We believe being very active on pull requests helps us win as a team!
YOUR BACKGROUND...
The ideal candidate should have 5+ years in software engineering delivering services with proficiency in Python and SQL. You have a solid understanding of IaC, SDLC, observability and how to build highly available services. We are looking for a self-sufficient engineer who can wear many hats and is confident as an individual contributor.
WHAT YOU'LL DO...
Analyze, design, code, test, configure and modify software for our platform, integrations and services using various programming languages, technologies and development methodologies.
Design, develop, test, debug and implement platforms, pipelines, solutions and/or software tools, and utilities for the purpose of assuring acceptable performance and service levels.
Participate in the automated delivery of software using source control, IaC throughout the entire delivery model
Ensures that implemented platforms, pipelines and solutions are optimally monitored, with relevant alerts, logging and tracing that guarantees the durability, availability and performance of our services.
Organize Data into well-curated data domains designed for consumption and performance in GCP to provide MLB a governed single source of truth.
Complete documentation that contributes value, including but not limited to testing, training and software delivery

Why MLB?
Major League Baseball (MLB) is the most historic of the major professional sports leagues in the United States and Canada. Employees love working at MLB because of the culture of growth, teamwork, and professionalism. Employees who are most successful at MLB take initiative, know how to identify problems and provide solutions, and always put the Team first. For those ready to step up to the plate and join the Major Leagues, MLB takes the same approach as teams do with their players: empowering our ""workforce athletes"" to be at their best by engineering experiences that put employees in the best position to succeed. Major League Baseball is looking for candidates who are passionate about growing America's pastime to best serve its fans for decades to come.
MLB's vision is to be the global sport of choice for youth to play, fans of all backgrounds to enjoy and a desired destination for employment. With a belief that the journey to growth and greatness is ongoing, MLB gives employees the opportunity to continue learning and honing their skills with programs such as: tuition reimbursement; mentorship programs; lunch and learns; online course subscriptions; paid industry certifications; business resource groups; and more.
MLB provides its employees with exceptional medical, dental, and vision coverage. Premiums are 100% employer covered to help employees focus on being their best!
Major League Baseball is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires accommodation, please let us know.
Are you ready to Step Up to the Plate? Apply below!
Per the NYC pay transparency law, the hiring range for this position is $130,000 to $150,000.
As a candidate for this position, your salary and related elements of compensation will be contingent upon your work experience, education, skills and any other factors Major League Baseball (MLB) considers relevant to the hiring decision. In addition to your salary, MLB believes in providing a competitive compensation and benefits package for its employees. MLB offers employees a full range of best in class benefits with no employee contributions towards medical, dental, and vision coverage premiums, as well as incentive and recognition programs, life insurance and automatic employer 401k contributions. All benefits are subject to eligibility requirements and the terms of official plan documents which may be modified or amended from time to time.
California Residents: Have you reviewed our CCPA policy?
California Consumer Privacy Act Policy"
346,Data Engineer,PitchBook Data,"1601 Union St, Seattle, WA 98101","$96,600 - $164,450 a year","At PitchBook, we are always looking forward. We continue to innovate, evolve and invest in ourselves to bring out the best in everyone. We're deeply collaborative and thrive on the excitement, energy and fun that reverberates throughout the company.
Our extensive mentorship, education and training programs help us create a culture of curiosity that pushes us to always find new solutions and better ways of doing things. The combination of a rapidly evolving industry and our high ambitions means there's going to be some ambiguity along the way, but we excel when we challenge ourselves. We're willing to take risks, fail fast and do it all over again in the pursuit of excellence.
If you have a good attitude and are willing to roll up your sleeves to get things done, PitchBook is the place for you.

About the Role:
The PitchBook Business Intelligence team embraces this data-driven ethos by delivering key analytics and insights internally to every facet of our business. Our work is manifested as tools and data to help the enterprise grow and run more efficiently at both strategic and tactical levels. Simply put, we believe that the data we build is our competitive advantage for making the most of our resources while we bring the most compelling product possible to market.
To that end, as our scope of data integration and analysis expands so do the needs of the Business Intelligence team. We're looking for a person with the ability to work with a range of data and reporting technologies (eg. Python, Docker, Tableau, Power BI) in order to build upon a strong foundation of rigor, quantitative techniques and efficient processing. The Data Engineer will join other Engineers and Analytics professionals as part of the team that develops data pipelines and insights for our internal stakeholders across Sales, Customer Success, Marketing, Research, Product, Finance and Administration.

Primary Job Responsibilities:
You'll be PitchBook's expert at building unified data tech to support advanced and automated business analytics
Design, develop, document and maintain database and reporting structures used to compile insights
Define, develop and review extract, transform and load processes and data modeling solutions
Consistently evolve data processes and techniques in accordance with industry best practices
Establish and help define reports and dashboards used to translate business data into insights, identify and prioritize operational improvement opportunities and measure business performance against objectives
Contribute to the ongoing improvement of quality assurance standards and procedures

Skills and Qualifications:
Bachelor's degree in Economics, Business, Finance, Engineering, Statistics, Computer Science or related fields
2 years of relevant work experience creating and maintaining data pipelines and architecture
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Experience with software programs, such as Tableau, Microsoft Power BI, Docker, Linux and Postgres
Ability to display complex quantitative data in a simple, intuitive format and to present findings in a clear and concise manner
Capable of investigating, familiarizing and mastering new data sets quickly
Excellent interpersonal skills, with the ability to communicate complex data issues correctly and clearly to both internal and external customers
Experience with presenting actionable insights to business stakeholders
Experience with: Airflow, Luigi, Amazon Web Services, Microsoft Azure, Git, Postgres, Debezium and Kafka is preferred
Experience with Snowflake development and cloud data warehousing is preferred

Benefits + Compensation at PitchBook:
Physical Health
Comprehensive health benefits
Additional medical wellness incentives
STD, LTD, AD&D and life insurance

Emotional Health
Paid sabbatical program after four years
Paid family and paternity leave
Annual educational stipend
Ability to apply for tuition reimbursement
CFA exam stipend
Robust training programs on industry and soft skills
Employee assistance program
Generous allotment of vacation days, sick days and volunteer days

Social Health
Matching gifts program
Employee resource groups
Subsidized emergency childcare
Dependent Care FSA
Company-wide events
Employee referral bonus program
Quarterly team building events

Financial Health
401k match
Shared ownership employee stock program
Monthly transportation stipend

Please be aware the above PitchBook benefit and perk offerings are subject to corresponding plan and policy documents and may change during the course of your employment.

Compensation
Annual base salary: $96,600-$164,450
Target annual bonus percentage: 10%

Starting pay will be based on several factors and commensurate with qualifications & experience. We also have a location-based compensation structure; there may be different ranges for candidates by location.

Life At PB:
We are consistently recognized as a Best Place to Work and our culture is at the heart of our success. It's our fundamental belief that people do and create great things and that people are the cornerstone of prosperity. We believe that proactively seeking out different points of view, listening to others, learning and reflecting on what we've heard creates a sense of belonging within PitchBook and strengthens the PitchBook community.

We are excited to get to know you and your background. Concerned that you might not meet every requirement? We encourage you to still apply as you might be the right candidate for the role or other roles at PitchBook.

#LI-BL1"
347,Data Engineer,Viridios,Remote,N,"About us:

Viridios AI is at the forefront of solutions for the climate and sustainability markets. We have developed AI-based fair valuation and risk management models that solves a major problem in value transparency and is quickly becoming the market reference for fair prices of carbon credits (offsets), from forestry and agriculture to renewable energy and energy efficiency activities worldwide. There is much to do when it comes to applying technology in going right by the planet and its inhabitants, and we are always looking for highly talented individuals to join us in this journey.
About the role:

For this role, we are looking for an enthusiastic and experienced Data Engineer to join a highly qualified team with domain expertise and years of experience in markets and technology. This is your opportunity to grow into and contribute to some of the most prominent and important themes of our age: climate, sustainability and AI. This is a full time permanent position. You will report to the CTO, Viridios AI. You will be working remotely.

Responsibilities:
Create and maintain optimal data pipeline architecture
Identify, design, and implement internal process improvements: automating manual processes, optimizing data quality, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and NoSQL database technologies
Create data tools for data scientist team members that assist them in building and optimizing our products into an innovative industry leader
Work with data and analytics experts to strive for greater functionality in our data systems

Requirements:
Bachelor's degree in a technical subject (e.g., technology, computer science, engineering, physics, mathematics, etc.)
Advanced experience with Relational (Oracle, MariaDB, PostgreSQL, etc.) and NoSQL databases (MongoDB, Redis, etc.)
Experience Building processes supporting data transformation, data structures, metadata, dependency and workload management
Experience with building and optimizing data pipelines, architectures and data sets
Experience with Unix shell scripting
Experience with Python 3.x
Experience with REST APIs
Fast learner, capable of quickly gaining domain expertise in the highly complex climate and sustainability markets
Clear analytical and problem-solving skills with the ability to envision and propose new and creative ways to solutions
Passion for technology and issues related to climate and sustainability markets
Ability to work well with teams in a collaborative way
Exceptional business and technical communication in English, verbal and written
Great to have:
Experience with Google Cloud Data Technologies (BigQuery, Cloud Storage, Cloud SQL, etc.)
Experience with writing production quality code (i.e. code that can be deployed to Cloud)
Data engineering certification (e.g. IBM Certified Data Engineer)
What We Offer:
Competitive base compensation
Annual cash bonus
Employee stock allocated upfront in a early stage company with tremendous upside
A vibrant and creative work environment
Exposure to one of the most exciting sectors today"
348,Data Engineer,On-Demand Group,"On-Demand Group in Minneapolis, MN 55402","Up to $100,000 a year","Data Engineer
Direct hire
This role develops enterprise data solutions that support the organization in achieving its strategic goals. Your work on cloud data pipelines will advance our enterprise data capabilities, while you get immersed in our collaborative, fun, and engaging culture. The position reports to our IT Data Engineering department and is a part of projects that align with key strategic initiatives to meet our business objectives. In your role, you will be part of a team that is responsible for the design, development, testing, deployment and support of cloud-based (GCP), data, analytical, and reporting applications.
Essential Duties & Responsibilities:
Develop and maintain custom ELT data pipelines with Python and SQL-based transformations running on the Google Cloud Platform.
Collaborate and implement event and batch based data science scoring pipelines.
Develop data access APIs to facilitate cross application data sharing.
Conduct and/or participate in requirements analysis sessions with internal customers, external vendors, and project teams.
Translate business requirements into technical designs.
Follow engineering best practice to ensure robust, tested, and reliable data pipelines.
Support data governance and security practices.
Follow agile development methodologies and actively participate in sprint planning sessions.
Support downstream users and resolve production issues with excellent customer service.
Job Skills:
Hands-on experience in creating API based data ingestion pipelines.
Good design skills in data pipeline, enrichment, and API patterns.
Good understanding of object oriented software engineering patterns.
Good relationship-building, customer service, and problem resolution skills.
Knowledge of software engineering, version control, and testing practices.
Knowledge of Agile software development methodologies.
Works effectively in a dynamic work environment with competing priorities.
Work Experience (6+ months experience each)
Python object oriented programming. Multi-language experience preferred.
API based custom data ingestion, particularly working with 3rd party vendor API's including reading API documentation, authentication, and bulk data staging strategies.
Cloud-based development. GCP preferred.
SQL
Experience with development in a version control, CI/CD environment.
Education:
From an accredited institution, Bachelor’s degree is required.
Job Type: Full-time
Salary: Up to $100,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Minneapolis, MN 55402: Reliably commute or planning to relocate before starting work (Required)
Experience:
Google Cloud Platform: 1 year (Preferred)
SQL: 1 year (Preferred)
Python: 1 year (Preferred)
Work Location: In person"
349,Data Engineer,N,Remote,N,"JOB SUMMARY
The Data Engineer is responsible for modeling complex problems, building pipelines, maintaining ETL processes and troubleshooting issues within a cloud environment. They utilize cloud databases, Databricks and databases to support a robust infrastructure which drives large, revenue generating data strategies.

ESSENTIAL DUTIES
Participate in use case feasibility discussions and translate business idea / business problems into use cases.
Provide support as needed to maintain and update models running in production environment.
Develop and maintain complex ETL processes and algorithms
Own and enhance ETL processes to move data from PMS to cloud databases
Monitor and troubleshoot processes on a daily basis
Document new and existing processes
Proactively and independently identify performance issues and recommend enhancements
Modernize legacy data models and pipelines using Databricks and cloud database capabilities.
QA and validate data moving between various parts of the company to ensure accuracy
Understand business problems/needs and provide proposed solutions.
KNOWLEDGE & REQUIREMENTS
REQUIRED QUALIFICATIONS
Experience working with healthcare professionals in a clinical setting
2+ years of data analysis experience
Understanding of algorithm design, machine learning, and applied statistics
Proven track record in use of SQL specifically in cloud databases and working with data including extracting information, validating data, creating and maintaining custom data structures.
Bachelor’s degree in Business, Computer Science, Information Systems or equivalent combination of education and experience.
Be able to work well with people of various backgrounds and education levels and establish cooperative working relationships with all coworkers.
Timely and effectively communicate information to and consult with others in order to complete work assignments.
Act in a responsible, trustworthy and ethical manner that considers the impact and consequences of one’s actions or decisions.
Communicate ideas, thoughts, and facts in writing through the use of proper grammar, spelling, document formatting and sentence structure.
Identify and respond to current and future clients’ needs; provide excellent client service.
Evaluate and analyze problems or tasks from multiple perspectives; adaptively employ problem solving methods to find creative or novel solutions; use logical, systematic and sequential processes to solve problems.
Complete assigned job tasks in an accurate and timely manner.
Carefully prepare for meetings and presentations; follow up with others to ensure that agreements, tasks or commitments have been fulfilled.
Demonstrate commitment to achieving Company’s core business objectives of increasing the role of pharmacy and improving patient health in America.

DESIRABLE QUALIFICATIONS
Experience working with healthcare professionals in a clinical setting.
Experience resolving issues that do not have clear answers.
Experience with Databricks (preferred), Talend, Informatica or other ETL tools.
Experience working in cloud databases (BigQuery preferred)
Highly motivated and possessed excellent interpersonal, problem solving, and technical skills.
High sense of urgency and accountability
Adaptable, friendly, and ability to work with a team.
Excellent attendance
Passion for data and digging into the minutia of datasets.
Take calculated risks based on data-driven analytics
Be a self-starter
Enjoy working in a fast-paced environment."
350,Data Platform Engineer,Holman,"Mount Laurel, NJ",From $110 an hour,"At Holman, we exist to provide rewarding careers and better lives for employees and their families. We hire, train, empower, and reward exceptional people. Our journey is guided by our desire to get it right every time and the acknowledgement that we have an opportunity to be better. To be better, we have to do better, and to do better we must know better. That’s why we are listening, open to learning new things – about ourselves and each other. We will never stop striving for improved diversity, equity, and inclusion because we are successful together when we feel trusted and supported. It’s The Holman Way.

At Holman, your total compensation goes beyond your paycheck. To position you for success and provide a rewarding career and better life for you and your family, Holman is proud to offer you the benefits you deserve; including protection against illness, disability, loss of work, or preparation for retirement. Below is a brief overview of these programs:

Health Insurance

Dental Insurance

Life and Disability Insurance

Flexible Spending and Health Savings Accounts

Employee Assistance Program

401(k) with Employer Match

Paid Time Off

Tuition Reimbursement

Exclusive pricing and concierge sales support on new and used vehicles

Holman is currently accepting applications for the role of Data Platform Engineer

Principal Purpose of Position:
Design, develop, document and execute data solutions, tools and practices
Analysis of requirements at sufficient level of detail to allow ETL solution to be developed
Development of ETL job flows according to company standards for naming, performance, restartability and performance.
Support testing and remediation of defects in newly-developed/modified ETL workflows
Promote ETL workflows to PROD and provide ongoing support in PRODUCTION, including monitoring and troubleshooting
Ability to create Power BI Datasets to support the Analytic Delivery team
Evaluate emerging data platform technologies
Lead technology implementations
Follow and contribute to best practices for data management and governance
Collaborate with the Data Architecture team to understand and implement load processes for reporting and analytic data structures (data warehouses, data marts and data lakes)
Performance tune and troubleshoot processes under development and in production as necessary.
Work with the Data Architects to augment ERD’s as changes are developed
Develop, maintain, and extend reusable data components
Provide timely project and task updates to all concerned parties
Monitor production data integration jobs and correct failures in a timely manner
Create and manage incident reports as they pertain to data integration processes
Perform all other duties and special projects as assigned.

Required Experience/Skills

2+ years Azure exposure (Any Resources: Databases, Data Factory, Synapse Studio, Storage Account, Power Platform)
2+ years ANSI SQL experience
1+ years data modeling exposure
Advanced problem solving/Critical thinking mindset

Preferred Experience/Skills

Azure connectivity/authentication (service principals, managed identities, certificates)
Power BI Dataset creation/maintenance
Azure Resources: DevOps, Logic Apps, Gen 2 Storage, Purview
SQL Server, Oracle, Python, Spark

Education and/or Training:
Bachelor’s degree in Computer Science or equivalent work experience

Compensation: Starting at $110,00 USD

Holman is a global automotive leader that serves both commercial and consumer clients The Holman Way by always doing the right thing for our people, our customers, and the community since 1924. The Holman story began nearly a century ago as a single Ford dealership in New Jersey. Today, Holman, headquartered in Mount Laurel, New Jersey, is one of the largest family-owned automotive service organizations in North America with more than 6,500 employees across North America, the UK, and Germany.

Holman delivers a unique range of automotive-centric services including industry-leading fleet management and leasing; vehicle fabrication and upfitting; component manufacturing and productivity solutions; powertrain distribution and logistics services; commercial and personal insurance and risk management; and retail automotive sales as one of the largest privately owned dealership groups in the United States. Guided by its deeply rooted core values and principles, Holman is continuously Driving What’s Right.

Holman provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training."
351,Senior Master Data Management (MDM) Engineer,Farmers Insurance Group,Remote,N,"We are Farmers!
We are… more than just your favorite commercials. We are a passionate, award winning, equal opportunity employer, committed to the strength of a diverse workforce. We are dedicated to supporting the well-being of our people through our extensive suite of benefits, as well as the well-being of the communities we serve through employee volunteer programs and nonprofit partnerships. Helping others in their time of need isn’t just our business – it’s our culture! We are Farmers!

Do you thrive in a high-volume, fast-paced environment? Do you enjoy the challenge of a position where no two days are alike? We are looking for positive, high-energy professionals who are not just looking for a job, but a meaningful career!
Job Summary
Join the Enterprise Data Organization during this exciting time as we create a paradigm shift by significantly elevating Data Engineering! We are a part of a larger enterprise filled with highly diverse and talented professionals who are extremely passionate about technology, data and the insights it can deliver.
About the Role
As a Senior Master Data Management (MDM) Engineer in Enterprise Data Organization, you'll lead development teams in the MDM area. The role requires you to think critically, design with cloud first principles. You partner with other engineers within data and application IT teams to deliver scalable, performant, highly available solutions to drive business impact. You'll have the opportunity to continuously learn and apply the latest innovations and best practices in this domain. You’ll also mentor other engineers and further develop your technical knowledge and skills to keep Farmers at the cutting edge of technology.
Essential Job Functions

Leads technical development efforts in alignment with MDM strategy
Serves as a key contact person in support of production problems/issues
Troubleshoots system performance issues and propose improvements
Continuously contributes to solutions and ideas that increase Pace, Efficiency, and Quality in the spirit of a Dev Ops model
Designs, develops and delivers high quality, scalable and efficient solutions and products on schedule
Produces elegant and efficient designs, high performance, and scalable code that allows for easy extension to future needs
Adheres to big picture view of how various connected system designs should be consolidated or affected
Effectively and actively plays the role of technical advisor and Subject Matter Expert for projects, providing advice on process and design
Initiates and actively facilitates meetings and issue resolution, involving the right individuals
Effectively performs code walk-throughs and reviews/approves test cases
Provides accurate and timely input to Release Manager and Integration Lead regarding status of technical tasks for self and team.
Tracks record of identifying largest risk areas and driving resolution of these issues.

Education Requirements
High school diploma or equivalent required. Bachelor’s degree preferred or equivalent, relevant experience.
Experience Requirements
Minimum of 5 years application development and solution architecture experience with proven track record of success
Solid understanding of master data, data quality and data stewardship concepts
Hands on experience and expertise in IBM MDM (Master Data Management) tool for Analysis, Design, Development, Integration and Testing Master Data Management Services. Strong preference to IBM Infosphere, Advanced Edition (AE).
Required minimum of 5 years professional experience designing and developing applications on DB2 DB.
Expertise on implementing MDM Batch processes, API/Services Framework, Messaging queues, Suspect Duplicate Processing using Probabilistic or Deterministic Matching Engines.
2+ years of experience with DevOps model: implement, maintain, and improve Continuous Integration and Continuous Delivery processes, tools, and environments where appropriate.
Effective problem solving and time management skills.
Knowledge of proper architectural disciplines
Proven independent contribution to Efficiency, Pace and Quality
Ability to explain complex concepts/issues in simple terms
Ability to accurately identify root cause of technical problems
Ability to identify and appropriately escalate issues/risks to management for direction
Special Skill Requirement
Experience in insurance industry.
Excellent communication skills; verbal and written.
Experience with Agile Scrum methodology.

Benefits
Farmers offers a competitive salary commensurate with experience, qualifications and location.
o CA Only: $114,080 - $165,400
o CO Only: $107,200 - $142,600
o New York City, NY/Westchester County: $132,320 - $165,400
o WA Only: $114,080 - $190,750
Bonus Opportunity (based on Company and Individual Performance)
401(k)
Medical
Dental
Vision
Health Savings and Flexible Spending Accounts
Life Insurance
Paid Time Off
Paid Parental Leave
Tuition Assistance

Job Location(s): R_US - RW - Remote Work
Candidate qualifications may be reviewed against the requirements for the associated levels of this positions core function"
352,Data Engineer,HealthVerity,Remote,N,"How you will help
You will support the engineering team’s data endeavors, diving in to fix issues, optimize processes, and automate what you do more than once. You’ll use the best tools for the job, whether modern and revolutionary or time tested and proven, to deliver elegant, scalable solutions that meet business and technical needs.

What you will do
Work with internal stakeholders to load data into HealthVerity's data warehouse
Troubleshoot and resolve issues relating to data integrity
Help establish procedures and best practices for transforming and storing data
Lead requirements gathering around data pipeline automation improvements
Work with some of the most exciting open-source tools like Spark, Hadoop, Docker, Airflow, Zeppelin
Leverage distributed computing and serverless architecture such as AWS EMR & AWS Lambda, to develop pipelines for transforming data
Perform root cause analysis on internal and external data and process to answer specific business questions and identify opportunities for improvement
Research and implement new technologies with a team of developers to execute strategies and implement solutions
Solve complex problems related to the real-time discovery of large data
Build POCs to evaluate and make right technology decisions

About you
Experienced in writing scalable applications on distributed architectures
Data driven, testing and measuring as much as you can
Eager to both review peer code and have your code reviewed
Comfortable on the command line and consider it an essential tool
Confident in SQL, you know it, write smart queries, it’s no big deal
Passionate designing and leading the implementation of resilient, distributed data platforms and large-scale infrastructures
Advanced knowledge of data, design and platform architecture disciplines

Required skills and experience
5+ years of work experience
3+ years of data engineering experience with Python or Scala
3+ years of experience with PySpark and Spark-SQL (writing, testing, debugging spark routines)
2+ years of experience in custom ETL design, implementation and maintenance with AWS EMR, AWS S3 service
Comfortable using AWS CLI and boto3
Comfortable using *nix command line (shell scripting, AWK, SED)
Experience with advanced SQL performance tuning and end-to-end process optimization
Experience of handling various data sets Structured, Semi Structured, Data at rest/ Motion

Desired skills and experience
Experience with data pipeline and workflow management tools: Apache Airflow, AWS Step Function, etc
Bonus: Experience with Spark applications in Databricks Notebooks and Delta tables
Bonus: 4+ years working with Medical claims, Pharmacy claims and Electronic Health (Medical) Records
Base salary for the role is commensurate with experience and can range between $63,000 - 250,000 + annual bonus opportunity.

About HealthVerity
At HealthVerity we are actively solving some of the greatest challenges in healthcare through innovative technology and data solutions. Our customers and partners including pharmaceutical manufacturers, payers and government organizations look to HealthVerity to partner on their most complicated use cases, leveraging our transformative technologies and real-world data infrastructure. The HealthVerity IPGE platform, based on the foundational elements of Identity, Privacy, Governance and Exchange, enables the discovery of RWD across the broadest healthcare data ecosystem, the building of more complete and accurate patient journeys and the ability to power best-in-class analytics and applications with flexibility and ease. To learn more about the HealthVerity IPGE platform, visit www.healthverity.com.

Why you'll love working here

We are making a difference – Our technology is at the forefront of some of the biggest healthcare challenges in the world.
We are one team – Our people define our culture and always will. We take time out to celebrate each other at the end of every week through company-wide shout outs, and acknowledge the value that each of us adds towards our greater mission. Come share all you have to offer.
We are learners – Every team member is continually learning, no matter if we've been in a role for one year or much longer. We are committed to learning and implementing what is best for our clients, partners, and each other.

Benefits & Perks
Compensation: competitive base salary & annual bonus opportunity (for non-commissioned roles)
Benefits: comprehensive benefits with coverage on Day 1, medical, dental, vision, 401k, stock options
Flexible location: our HQ is in Philadelphia with 50% of the team distributed across 25+ states
Generous PTO: Take time off as needed, targeted at 4 weeks per year, including vacation, personal and sick time, plus paid maternity and paternity leave.
Comprehensive and individualized onboarding: mentorship program, departmental talks, and a library of resources are available beginning day 1 for each new team member to minimize the stress of starting a new job
Professional development: biweekly 1:1s, hands-on leadership that is goal-and growth-oriented for each team member, and an annual budget to support professional development pursuits

HealthVerity is currently able to employ individuals residing in the following states: AZ, CA, CT, DC, DE, FL, GA, IL, IN, MA, MD, MI, MN, MO, NC, NH, NJ, NV, NY, OH, OK, PA, RI, SC, TN, TX, UT, VA, VT, WA, WI.

HealthVerity is an equal opportunity employer devoted to inclusion in the workplace. We believe incorporating different ideas, perspectives and backgrounds make us stronger and encourages an environment where ageism, racism, sexism, ableism, homophobia, transphobia or any other form of discrimination are not tolerated. All qualified job applicants will be given consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or on the basis of disability. At HealthVerity, we’re working towards an innovative and connected future for healthcare data and believe the future is better together. We can only do that if everyone has a seat at the table. Read our Equity Inclusion and Diversity Statement.

If you require a reasonable accommodation in completing this application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please direct your inquiries to careers@healthverity.com

Remote opportunities are not available in all areas and require team members to work from a fixed location due to tax and labor law implications - specific questions about remote positions can be discussed during the interview process with your recruiter."
353,Data Analyst Engineer,BLVD Management,"BLVD Management in Mesa, AZ 85206","Up to $120,000 a year","Data Analyst Engineer - SFR Real Estate Investment
BLVD Management, LLC
Who we are
BLVD is a boutique real estate investment fund with private equity backing and is acquiring single family homes across the country. The fund specializes in acquiring and renovating properties to hold as long-term rentals. We are looking for A+ candidates individuals to join our data analytics team.
Compensation and Benefits
Base Pay Range: $80,000 - $120,000
Annual Bonus Target: 10%
Competitive paid time off plan
401(k) plan with generous company matching
Medical, dental and vision insurance; along with company paid disability and life insurance
Flexible hours and hybrid work from home options
The compensation range for this role added below is specific to the location of this job posting. Actual starting pay is determined by various factors, including but not limited to: relevant experience, skill set, qualifications, and other business and organizational needs.
Job Responsibilities:
Design, build, and deploy data models from internal and external data sets to find opportunities and optimize business processes
Works closely with line of business owners, business analysts, data architects, and developers to create insights gained from analyzing company data and test the effectiveness of difference courses of action.
Develop and maintain scalable data pipelines and build out new API integrations to support continuing increases in data volume and complexity
Improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization
Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
Define company data assets and identify critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that support and extend quantitative analytic deployment solutions
Qualifications / Skills:
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Experience in programming languages such as Python, SAS, R
Experience with data warehouses and data lakes
Experience with Databricks including Delta Lake, Apache Spark, Pipelines and Notebooks
Experience in working with databases such as PostgreSQL, MongoDB, SQL, Snowflake
Experience in developing solutions on cloud computing platforms such as Google Cloud, AWS, Azure
Experience with CI/CD systems such as GitLab, DevOps, Cloud Build
Experience with analytics, reporting and BI platforms
Ability to learn and adopt new technologies and languages
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service
Experience in real estate acquisition and property management data analytics
Education, Experience, and Licensing Requirements:
Bachelor’s degree in computer/data science, or a related technical field or equivalent combination of training and work experience
4+ years of Python or .Net development experience
4+ years of SQL experience (No-SQL experience is a plus)
4+ years of experience with schema design and dimensional data modeling
Experience designing, building, and maintaining data processing systems
How to Apply
Please apply via this listing and include any other application requirements.
*Applicants are required to be eligible to lawfully work in the U.S. immediately; employer will not sponsor applicants for U.S. work authorization (e.g. H-1B visa) for this opportunity
Job Type: Full-time
Pay: Up to $120,000.00 per year
Benefits:
401(k)
Dental insurance
Flexible schedule
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Do you now, or will you in the future, require sponsorship for employment visa status (e.g., H-1B visa status, etc.) to work legally for our Company in the United States?
Experience:
Data science: 3 years (Required)
Database / ETL: 3 years (Required)
Databricks: 1 year (Required)
Real Estate SFR Analysis: 1 year (Preferred)
Work Location: One location

Health insurance"
354,Data Engineer,LTK,Remote,"Up to $250,000 a year","LTK is the largest influencer-driven platform used by millions of shoppers every month to find new styles and products curated by influencers all over the world. As a Senior Data Engineer II, you will be part of a growth-hungry, acceleration-minded team of problem-solvers and product-builders. This is an opportunity to grow with a fast-growing organization. We thrive in an extremely dynamic competitive environment. We focus relentlessly on the needs of our influencers, brands, and consumers. We seek thought-leaders who can collaborate effectively with product managers, designers, and engineers to identify actionable insights to help us grow.
Title: Senior Data Engineer II - Data Products | Ingestion and Curation
Responsibilities
Senior data engineering leader responsible for building robust, reliable, and usable data pipelines and data sets that are used to fuel LTK’s products, analytics, and data science solutions
Bring modern technical strategies that optimize data engineering technical approach, workflows, and processes
Lead data engineering optimization projects and initiatives
Lead all aspects of data pipeline design activities (definition, analysis, architecture design, estimation)
Participate in shaping, scoping, and setting technical direction for major projects and initiatives, working with product owners and engineering leads
Lead analytical dimensional model design activities and support on-going improvement and useability of high value data sets
Identify high value opportunities for using our data to drive the business forward
Set robust, comprehensive design and development standards with clarity
Develop reusable data engineering approaches and patterns
Serve as a mentor to more junior data engineers; guide and facilitate work of other team members
Lead out in an agile development environment, effectively planning, grooming, and delivering data engineering work
Produce high quality technical documentation and artifacts that are easily consumable by data teams and other engineering teams
Qualifications
Experience building production data pipelines, ELT/ETL solutions, data lakes, and data warehouses at a large scale
7-8+ years in data engineering role on cloud data platform (min 4 years AWS)
Programming proficiency in cloud data ecosystem using Python, R or Scala (min 5 years)
5+ years utilizing large scale Analytic/MPP data warehouse platform (Redshift, Snowflake, Vertica, etc.)
Highly proficient with complex SQL
Highly proficient with modern analytical/dimensional modeling approaches, key management techniques, and understanding of how data sets can power a variety of analytic use cases (clustering, segmentation, regression, etc.)
Highly proficient in engineering data pipelines using big data tooling (Spark, Hudi, Kafka)
Highly proficient working in AWS cloud environment (S3, Cloud Formation, RDS, AWS Glue, Athena, EMR, Kinesis, Redshift)
Strong understanding of cloud computing and optimization techniques
Demonstrated leadership and specialized expertise in at least three (3) of the following:
Batch data ingestion
Big data pipeline engineering and optimization
Analytic/MPP data warehouse optimization
Analytical data modeling
Event streaming data processing
Data pipeline metadata observability
Data Platform Cost Optimization
DataOps
Specialized expertise designing and working with at least three (3) of the following data domains:
Retail product data
Ecommerce data
Consumer profile data
Clickstream data
Consumer event data
Marketing channel/campaign data
Affiliate data
Retailer/Brand data
Preferred Qualifications
AWS Certified Data Analytics - Specialty Certification
Exposure and experience with multiple platform stacks and tooling (Databricks, Snowflake, DBT, Airflow Orchestration, etc.)
Job Type: Full-time
Pay: Up to $250,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Employee stock ownership plan
Stock options
Experience level:
7 years
Schedule:
Monday to Friday
Work Location: Remote

Health insurance"
355,Data Analytics Backend Engineer,thatgamecompany,"Santa Monica, CA 90404•Remote",N,"Data Analytics Backend Engineer
Remote – US and Canada only

thatgamecompany is best recognized for creating award-winning, enriching, and meaningful game titles such as Journey, Flower, and flOw. Our most recent game, Sky, is our most complex undertaking to date. It is a social network built around the values inherited from a powerful humanistic story. It is a live experience continuously evolving inside a global online theme park.
We are seeking passionate engineers to join us in building various core backend technologies, including but not limited to:
Micro services running on container orchestration for rapid iteration of in-game social features
A real-time, low-latency and multi-regional music jam service
Infrastructure to enable rich in-game user-generated content
These services and platforms will be the core technology powering our current and future game titles, and eventually made available to external customers. We believe these solutions will fundamentally transform the future of multiplayer social games. We are also live-operating Sky: Children of Light with millions of active users generating terabytes of data per day.
As a Data Analytics Backend Engineer, you will serve as a crucial nexus between Analytics and Backend Engineering. You will gain deep experience managing our backend stack and microservices, and work closely with Analytics teams to make sure we're efficiently tracking and storing all of the data we need to understand our millions of players.
On any given day at thatgamecompany, you might:
Design and implement large-scale, highly available backend systems that serve thousands to millions of concurrent players with a goal of zero downtime.
Configure new event streams to allow for real-time automated responses to player activity.
Embrace modern technologies, such as container and cluster management, to ensure a more elastic and robust backend stack.
Help to foster communication and collaboration between analytics stakeholders and product engineers.
Improve and maintain an agile and reliable development environment for the backend stack, so that people with varying skill sets in the company can make social experiments easily, and new hires can ramp up quickly.
Monitor the backend health and respond to any failures or issues in order to deliver a smooth online experience to players all over the world; keep improving DevOps tools to make the job more automatic and less error-prone.
We expect you to:
Have deep passion and thoughts for video games; be a gamer and think on behalf of players.
Be comfortable taking risks and innovating.
Enjoy working with fast-moving and rapidly-growing small teams.
Must - Haves:
Have thorough understanding of scalable and highly available backend systems; be familiar with open-source distributed system tech stacks, including but not limited to scalable databases, caching strategy, distributed transactions and DevOps tools.
Be able to extract useful information from different sources of logs, find correlations between multiple layers of systems and diagnose failures, suspicious behaviors and performance bottleneck from bottom to top.
Be comfortable to work with Linux ecosystem; be fluent in Linux or macOS bash CLI tools and Python scripting.
Have deep knowledge of at least one of Go, Erlang or C++.
Have deep knowledge of at least one SQL or NoSQL databases.
Have deep knowledge of at least one distributed message queue systems.
Eager to learn any new technology and always open to jump out of your comfort zone.
Nice to Haves:
Any of the following would be highly preferred, but most of all, we value engineers who are eager to learn new ways to deliver value to players:
Managed and maintained production environment on AWS or GCP.
Deployed services in Kubernetes with Helm and CI/CD tools.
Experience with Data Engineering or Analytics.
Experienced in schema design and performance tweaking of MongoDB and Redis.
We look forward to meeting you!

Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.

#LI-Remote"
356,Software Engineer - Data (Remote),Manufacturers Bank,"Scottsdale, AZ 85255•Remote",N,"NEW DIGITAL BANK MISSION STATEMENT:
Join us on our mission to create a completely new, 100% digital bank that truly serves customers' best interests. We are a close-knit and fun-loving team of seasoned financial services professionals who came together for the challenge of building a bank from scratch - and we are committed to doing it all the right way (from technology infrastructure to modern marketing to customer experience).
We work with the flexibility and speed of a start-up. But we also have significant stability and capital from being part of the SMBC Group (Sumitomo Mitsui Banking Corporation). SMBC is the 2nd largest bank in Japan and the 12th largest bank in the world with operations in over 40 countries. And SMBC is committed to disrupting the US marketplace with ground-breaking products.
It's the best of both worlds, and we are seeking proven marketing leaders to propel us towards a national launch. We have both the ambitious growth plans and the 'patient capital' necessary to execute a multi-year plan. Join us on the journey to deliver an exciting concept of evolved banking.

SUMMARY, PRINCIPAL DUTIES & RESPONSIBILITIES:
Develop reports, dashboards and KPI’s to help the organization make data driven decisions
Implement standard methodologies and best practices to ensure data model and dashboard design consistency across all reporting projects
Work with business and cross-functional teams to gather and document reporting requirements to meet business needs
Provide support as required to ensure the availability and performance of developed reports and dashboards
Provide technical assistance and cross training to business and internal team members
Collaborate with business partners for continuous improvement opportunities"
357,Senior Engineer - Data Products,Headspace,Remote,"$104,981 - $157,476 a year","at Headspace Health
Remote - United States
Headspace
and
Ginger
have recently merged to become
Headspace Health
! While roles are still being recruited separately on our respective websites, new hires from this point forward will be joining Headspace Health. For more information, please speak with your recruiter!
About the Senior Engineer – Data Products at Headspace Health:
Headspace Health is seeking an experienced Senior Engineer to join our Data Products team (part of our Data Engineering org). In this role, you will be responsible for architecting and implementing a set of core data sets in our data lake. And your customers are our data consumers, including analysts, machine learning engineers and data scientists.
How your skills and passion will come to life at Headspace Health:
Design and implement mission critical data pipelines for our company
Help create a set of high-quality, composable data products for our data consumers
Write well designed, testable, performant, and efficient code that runs on Apache Spark and Delta Lake
Lead the development of a world-class data lake that meets the strict security, privacy, and compliance requirements of the healthcare industry
Collaborate with the data analytics team to calculate and deliver key business metrics
Collaborate with the data science and machine learning team to build data sets used for model training and development
Mentor and coach other engineers to build a data-first culture at the company
What you’ve accomplished:
5+ years professional software development
You’ve built high quality data pipelines before with comprehensive unit tests suites, data quality checks etc.
Experience with Apache Spark and Delta Lake are a plus, but not required
Self-starter with the ability to thrive in a fast-paced startup environment
Exceptional oral and written communication skills
Experience coaching and mentoring team members
Experience collaborating with product partners
BA/BS degree in Computer Science, Engineering or equivalent
Pay & Benefits:
The base salary range for this role is determined by a number of factors, including but not limited to skills and scope required, relevant licensure and certifications, and unique relevant experience and job-related skills. The base salary range for this role is $104,981-$157,476.
At Headspace Health, cash salary is but one component of our Total Rewards package. We’re proud of our robust package inclusive of: base salary, stock awards, comprehensive healthcare coverage, monthly wellness stipend, retirement savings match, lifetime Headspace membership, unlimited, free mental health coaching, generous parental leave, and much more. Paid performance incentives are also included for those in eligible roles. Additional details about our Total Rewards package will be provided during the recruitment process.
How to get started:
If you’re excited by the idea of seeing yourself in this role at Headspace Health, please complete our application process below.
Our Commitment to Diversity & Inclusion:
Diversity, Equity, Inclusion, and Belonging (DEIB) is at the heart of our mission to improve the health and happiness of the world. At Headspace Health, we are committed to bringing together humans from different backgrounds and perspectives while providing employees with a safe and welcoming work environment free of discrimination and harassment. We continuously strive to create a diverse and inclusive environment where everyone can thrive, feel a sense of belonging, and do impactful work together. As an equal opportunity employer, we prohibit any unlawful discrimination against a job applicant on the basis of their race, color, religion, gender, gender identity, gender expression, sexual orientation, national origin, family or parental status, disability*, age, veteran status, or any other status protected by the laws or regulations in the locations where we operate. We respect the laws enforced by the EEOC and are dedicated to going above and beyond in fostering diversity across our workplace.
Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. A reasonable accommodation is a change in the way things are normally done which will ensure an equal employment opportunity without imposing undue hardship on Headspace Health. Please inform our Talent team by filling out
this form
if you need any assistance completing any forms or to otherwise participate in the application or interview process.
#LI-AT2

Headspace Health participates in the
E-Verify Program
.
Headspace Health is committed to protecting the privacy and security of your personal data."
358,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
359,Principal Data Engineer,Intone Networks,Remote,N,"The Principal Data Engineer / Lead will play a key role in in the Design and Development of Data models, datasets, and ETL/ELT pipelines from a variety of data sources to the Data Warehouse / Data Lake on Google Cloud Platform (GCP) for client’s flagship Priority Engine product. Priority Engine relies heavily on this foundational data layer. The Principal Data Engineer will help provide technical leadership and strategic direction for Priority Engine’s data platform. The position will be responsible for the delivery and technical quality of our programs, taking on complex projects. It will also partner with cross-functional stakeholders to drive initiatives forward, and mentor team members in technical design, project leadership, and running team processes. The position will be responsible to build and maintain scalable data infrastructure The candidate must have a keen sense for the business drivers, have a vision for the data strategy and help execute it. The candidate must be familiar with industry trends and best practices around data engineering on the cloud and apply them appropriately. Role / Responsibilities: Design, develop, and implement end-to-end data solutions (storage, integration, processing, access) in Google Cloud Platform (GCP); Create ETL/ELT pipelines that transform and process terabytes of structured and unstructured data in real-time; Design data models for optimal storage and retrieval to support sub-second latency; Deploy and monitor large database clusters that are performant and highly available; Deeply immerse in our Priority Engine Data Pipelines, to acquire full understanding and ability to converse with business representatives and individual contributors alike; and Incorporate solid understanding of cloud-based offerings in the space. Some upcoming technical challenges include scaling our data ingestion pipelines across a growing number of GCP, AWS, and data center-based data sources, reducing the latency of our product data ingestion pipelines through moving batch jobs into a streaming architecture, and extending our data lake architecture for the growing ecosystem of data ingestion and creation tools. Be a hands-on contributor on the team; Collaborate with product/application architects to develop holistic solutions; Implement operational procedures (logging, monitoring, alerting, etc.) for dependable running of pipelines/jobs; and Be autonomous. You own what you work on. You move fast, take ownership, and get things done. The Data lake/warehouse is a central and essential service for multiple initiatives, and you need to be able to multitask and juggle priorities on a regular basis with a deep understanding of business impacts of prioritization decisions. Skills/Requirements: BS Degree in Computer Science or a related field, 10+ years industry experience, 5+ years of experience building real-time and distributed system architecture, from whiteboard to production, Strong programming skills in Python and SQL, Versatility: 8+ years’ experience across the entire spectrum of data engineering, including: Cloud data stores (Example: GCP BigQuery, GCP BigTable, GCP FireStore, GCP CloudSQL, ELK) Data pipeline and workflow orchestration tools (e.g., Dataflow, Pentaho, Airflow, Azkaban) Data processing technologies (e.g., GCP BigQuery, Spark) Data messaging technologies (e.g., GCP PubSub, Kafka) Deployment and monitoring large database clusters in public cloud platforms (e.g., Docker, Terraform), Unix/Linux Shell scripting, Demonstrated knowledge of industry trends and standards, Ability to think through multiple alternatives and select the best possible solutions for strategic and tactical business needs, and Excellent communication. You will need communicate complex ideas effectively to both technical and non-technical audiences, and both verbally and in writing. Nice to Have: Experience with GCP Java Programming"
360,KPBS Data Engineer,San Diego State University Research Foundation,"San Diego, CA 92182•Hybrid remote","$85,000 - $130,000 a year","Overview:
Under the general direction of the Director of Digital, the Data Engineer is primarily responsible for integrating data from various CRM applications (iMIS (CRM), Eventbrite, Google Ads, Google Analytics, PBS, NPR) and other data sources into the software data warehouse (Snowflake) and customer data platform (Lytics).


This position will assist the Data Analyst in maintaining databases that house marketing and analysis data; working with subject matter experts in various departments at KPBS, working with staff on data creation policies and procedures and performing data quality checks, as needed. The position will be the lead on problem-solving technical issues within the Snowflake Data Warehouse and will work across departments to ensure the effectiveness of Key Performance Indicators within the organization.
The Data Engineer will provide program implementation and policy creation guidance and support. This includes responsibility for the monitoring of data quality, data governance, data access, data cleaning, data validity and timely consumption and delivery of the data to and from various endpoints.
Perks you'll enjoy as a member of #teamKPBS
Working on a college campus & in public media (access to campus facilities and staff discounts, farmer's market Tuesday's, tons of eateries, community events, entertainment)
Hybrid schedule, remote work flexibility & casual work attire
Paid time off on your birthday (can take on any day)
10 vacation days and 13 holidays off (enjoy 4+ days off over winter break)
Enjoy a 'beautiful day in the neighborhood' with local employee discounts
Opportunity drawings to attend SDSU basketball games, local concerts, and events
Free wellness classes & programs
Monthly pop-up events for staff & KPBS swag

Full benefits packages that are unmatched (medical, dental, vision, life)
Sick leave accruals and paid leave options
On-site childcare at a discounted rate
Opportunities for continued learning and professional development
Flexible spending account(s)
Employee assistance program
Matching and voluntary retirement savings plan

Salary range is $7,083.33/mo - $10,833.33/mo ($85,000 - $130,000/yr).
Responsibilities:
The following information is intended to be representative of the work performed by incumbents in this position and is not all-inclusive. The omission of a specific duty or responsibility will not preclude it from the position if the work is similar, related, or a logical extension of position responsibilities. Job descriptions may be changed at any time based on the needs of the department.

Data Management & Development (35%)
Build, manage and maintain data pipelines for data integration (ELT/ ETL)
Perform data quality checks and clean ups at various data collection and storage touchpoints
Ensure security of data during transit and at rest
Transform data for use in marketing and analytics applications
Data Administration (35%)
Perform as the lead administrator role for the Snowflake Data Warehouse
Act as the lead for problem-solving technical issues within or related to the Data Warehouse
Work across departments to ensure effective data usage in the organization
Create and maintain data policies that will ensure accuracy of reports and usage of data for marketing
Project Implementation (15%)
Work across departments to gather requirements, implement various projects as they arise
Support and Staff Training (10%)
Support Data Analyst by providing data for use in analysis, managing connections to BI applications, dashboard creation, etc.
Lead periodic trainings for Snowflake users
Train support staff on data creation policies and procedures, business process creation and other activities that impact the quality and usefulness of data collected
Convene and lead workgroups, create/implement policies and monitor and oversee the data quality across multiple systems

Other Duties as Assigned (5%)
Qualifications:
MINIMUM EDUCATION
Bachelor’s degree in computer science, information systems, business administration or similar field. Additional or equivalent years’ of work experience may be substituted for the required education on a year for year basis.
MINIMUM EXPERIENCE
Four-years of related work experience in data engineering; developing data pipelines
PREFERRED EXPERIENCE

2-3 years of which involve SQL database administration;
3-4 years of which include experience developing, maintaining and administering software applications on the linux operating system
KNOWLEDGE & ABILITIES
Demonstrated experience with data integration toolsets (i.e Airflow)
Demonstrated experience with writing and maintaining Data Pipelines
Demonstrated ability of good scripting, including Bash scripting and Python
Demonstrated experience with cloud technologies such as AWS, GCP, Azure
Demonstrated experience designing and working on complex data systems from design to delivery
Knowledge of and familiarity with Data Modeling techniques such as dimensional modeling and Data Warehousing standard methodologies and practices
Knowledge of SQL and ability to create queries to extract data and build performant datasets
Demonstrated expertise in using Snowflake cloud data warehouse including administration of roles, security, stages, etc.
Ability to integrate with 3rd party APIs
Ability to problem solve with strong attention to detail; excellent analytical and communication skills
Ability to work individually and as a part of a team
Ability to interact effectively with co-workers, vendors and consultants
Ability to understand and follow posted work rules and procedures
Ability to relate well to others within the project environment.
Ability to display motivation and strong interpersonal skills
Ability to communicate effectively, both orally and in writing
Ability to display organization, meet deadlines, display detail orientation, possess good judgment and common sense
Ability to demonstrate a high level of cross-cultural sensitivity
ADDITIONAL APPLICANT INFORMATION:
The COVID-19 vaccine is required by the CSU for all SDSU Research Foundation employees as a condition of employment. Should you be offered a position, you will be required to provide proof of vaccination status. Individuals who obtain an approved medical or religious exemption on file will be required to complete regular COVID-19 testing
A background check (including a criminal records check) and Livescan (fingerprint) must be completed satisfactorily before any candidate can be offered a position with SDSU Research Foundation/KPBS
San Diego State University Research Foundation is an EEO/AA/Disability/Vets Employer"
361,"Staff Engineer I, Data Engineering",Etsy,"117 Adams St, Brooklyn, NY 11201","$185,000 - $217,000 a year","Company Description

Etsy is the global marketplace for unique and creative goods. We build, power, and evolve the tools and technologies that connect millions of entrepreneurs with millions of buyers around the world. As an Etsy Inc. employee – whether a team member of Etsy, Reverb, Depop, or Elo7 – you’ll tackle unique, meaningful, and large-scale problems alongside passionate coworkers, all the while making a rewarding impact and Keeping Commerce Human.

Job Description

What’s the role?
Etsy’s IT Engineering team is seeking a Staff Software Engineer I, Data Engineering to join us in a new data engineering focussed function. You will be building a BigQuery-backed data warehouse for employee and organizational analytics, using Etsy’s existing data processing frameworks. You will work closely with other developers, data scientists, and analysts to write efficient pipelines aggregating data from various sources. This new position will play a ground-breaking role in driving more informed and objective decisions at Etsy by modernizing the way that we collect, manage and consume organizational information.
The IT Engineering team owns several aspects of business critical infrastructure and services that enable users to be productive in the Etsy offices as well as infrastructure that is critical for the operation of etsy.com. We are the connective tissue between various systems owned by other IT teams and develop and maintain many automations and integrations between business systems. We’re looking for people who excel at working with others, challenge the status-quo, and are outstanding problem-solvers. We value clear communication, honest feedback, and empathy for the users of our services. This is an exciting time to join Etsy, as we embark on next-generation work.
This is a full-time position reporting to Engineering Manager, IT Engineering, and the base salary range will be $185,000 - $217,000 USD per year. In addition to salary, you will also be eligible for an equity package, an annual performance bonus, and our competitive benefits that support you and your family as part of your total rewards package at Etsy. This role requires your presence in Etsy’s Brooklyn office in an in-person or flex capacity. Learn more about our Flex and Office-based work modes and workplace safety policies here.
What’s this team like at Etsy?
This is a new function within the existing IT Engineering team and you will have the opportunity to establish and shape a data engineering competence within the Internal IT organization.
You will initially work very closely with the HRIS and People Analytics team on an MVP for people data reporting.
What does the day-to-day look like?
Build ETL pipelines, data warehouses, data marts, and aggregate tables.
Experience building applications and managing infrastructure using one of the major cloud providers is preferred but not required. (We use Google Cloud here at Etsy).
You partner with Privacy, Data Platform, Data Application and HRIS teams.
Establish and document procedures, policies and technical instructions.
Of course, this is just a sample of the kinds of work this role will require! You should assume that your role will encompass other tasks, too, and that your job duties and responsibilities may change from time to time at Etsy's discretion, or otherwise applicable with local law.

Qualifications

Qualities that will help you thrive in this role are:
You understand that being an effective engineer is about communicating with people as much as it is about writing code.
You are a polyglot. We expect you to choose, propose and use the appropriate technology or programming language for the task at hand.
In addition to BigQuery SQL, our toolset includes Looker, Java, Python, and Spark, as well as Airflow, Terraform, and Kubernetes, and GCP services like Dataproc and Dataflow.
You are familiar with traditional data warehousing methodologies.
You have developed skills in SQL and writing efficient and optimized ETL pipelines.
Observability and metrics collection are standard considerations when crafting your implementation.
Experience working in an agile environment, participating in sprint planning, delivering work and contributing to a retrospective.
Ability to provide estimates or project ideas that will influence your team’s roadmap.

Additional Information

What's Next
If you're interested in joining the team at Etsy, please share your resume with us and feel free to include a cover letter if you'd like. As we hope you've seen already, Etsy is a place that values individuality and variety. We don't want you to be like everyone else - we want you to be like you! So tell us what you're all about.
Our Promise
At Etsy, we believe that a diverse, equitable and inclusive workplace furthers relevance, resilience, and longevity. We encourage people from all backgrounds, ages, abilities, and experiences to apply. Etsy is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. If, due to a disability, you need an accommodation during any part of the interview process, please let your recruiter know. While Etsy supports visa sponsorship, sponsorship opportunities may be limited to certain roles and skills.
For U.S. roles only:
Many Etsy roles are open to remote candidates, and you'll be able to identify which ones within the location header of each job description. We're open to remote hires from all U.S. states except Hawaii and Alaska."
362,Data Engineer,NuTechs LLC,"Troy, MI•Hybrid remote","$100,000 - $130,000 a year","Data Engineer
Troy (2-3 Days) / Remote (3-2 Days) - Local candidates preferred

Actively seeking a Data Engineer who possesses a strong passion for designing, optimizing, refactoring, and upgrading complex data solutions. The Insights team is a new team with an exponential growth opportunity – both in terms of technology and personnel, and they are looking for someone to share their mission that will transform their company’s future. With this position, you will have a rare opportunity to use your talents, passions, and expertise to help drive this massive change in how we build, organize, and optimize our backend systems and processes across all product lines. This position offers excellent career growth and promotional opportunities, and stellar compensation.

Required Experience (Must have)
Minimum 5+ years’ data engineering experience working with designing data models in both column store and relational databases, incorporating disparate data to solve complex business needs.
Minimum Bachelor’s degree or Graduate degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field.
Experience building and optimizing data pipelines, architectures, database modeling and data sets that meet business requirements.
Experience in building the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources in ‘big data’ technologies or similar. Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience in building processes supporting data transformation, analytics tools, data structures, metadata, dependency, and workload management. Assist Data Science team that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and optimizing our product into an innovative industry leader.
Experience in using Azure DevOps, or Jira project management/requirements management.
Experience with big data tools: Hadoop, Spark, Kafka, or a related technology.
Proficient in programming languages such as SQL, Python, R, Shell Scripting
Desired Experience (Nice to have)
Experience with NoSQL databases, including Postgres and Cassandra is a plus
Strong working experience with technologies like AWS/Databricks and (minimum 3+ years’ experience) working with big data, including expertise designing data models in both column store and relational databases, incorporating disparate data to solve complex business needs
AWS lambda functions (Python preferred)
AWS databases (Aurora, DynamoDB, or Redshift preferred)
AWS storage services (EC2, S3 preferred)
Data Lake experience or similar (AWS preferred)
Primary Responsibilities
Developing and implementing an overall organizational data strategy that is in line with business processes. The strategy includes data model designs, database development, implementation and management of data warehouses and data analytics systems.
Identifying data sources, both internal and external and work out a plan for data management that is aligned with organizational data strategy.
Coordinating and collaborating with cross-functional teams, stakeholders, and vendors for the smooth functioning of the enterprise data system.
Managing end-to-end data architecture, from selecting the platform, designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.
Identifying, designing, and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition."
363,ETL Spatial Data Engineer,"ESRI, Inc.","380 New York St, Redlands, CA 92373","$72,800 - $124,800 a year","Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where ®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1"
364,Data Migration Engineer,at Higher Logic,Remote,N,"Company Description

Higher Logic has been the industry leader in building community and engagement solutions since 2007.

We’re a thriving company made up of authentic people and unique perspectives. We have a shared passion for bringing people #AllTogether to create meaningful connections and rich discussions, unlocking the knowledge of this group.

Our team is made up of motivated individuals who share their expertise and live our values every day. We help our customers create human-focused experiences to build lasting bonds between their customers, members and employees.

Job Description

Our company is seeking a skilled Data Migration Engineer to join our team. The successful candidate will be responsible for managing and executing data migration projects across a variety of databases and platforms, including MySQL, PostgreSQL, and PHP. The Data Migration Engineer will collaborate with developers, project managers, and business stakeholders to ensure successful data migration of customers moving our platform. Although this position will be a part of a larger team, they will primarily operate independently on their own as the lead engineering resource.

Responsibilities:
Develop and maintain data migration plans and strategies for various databases and platforms.

Analyze and transform data from legacy systems into new target systems.

Develop scripts and database queries for data transformation, validation, and cleansing.

Conduct performance testing and analysis of data migration processes.

Collaborate with project managers and business stakeholders to understand requirements and priorities.

Develop documentation and provide training to end-users on data migration processes and procedures.

Troubleshoot issues related to data migration and implement appropriate solutions.

Required Skills:
Bachelor's degree in computer science, information technology, or related field.

3+ years of experience in data migration or related fields.

Expertise in database queries using MySQL, PostgreSQL, NoSQL databases.

Proficiency in ETL tools and processes.

Knowledge of data warehousing and business intelligence concepts.

Strong analytical and problem-solving skills.

Excellent communication and collaboration skills.

Ability to work independently and as part of a team.

Experience with Linux command-line and Structured Query Language (SQL).

Ability to read and interpret technical documentation.

Experience working with large data sets.

Experience processing JSON and CSV files.

Nice to have Skills:
Experience working PHP.

Experience writing bash scripts.

If you have experience in data migration and expertise in MySQL, PostgreSQL, and PHP, as well as proficiency in Linux command-line and SQL, and the ability to read and interpret technical documentation, as well as experience working with large data sets and processing JSON and CSV files, we encourage you to apply for this exciting opportunity!

EEO Disclosure

Higher Logic is committed to equal opportunity. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

Higher Logic is committed to ensuring that its application process provides an equal employment opportunity to all job seekers, including individuals with disabilities. If you believe you need a reasonable accommodation in order to search for a job opening or to submit an application, please contact us by emailing HRBenefits@higherlogic.com"
365,Data Engineer,Computer-Rx,Remote,N,"JOB SUMMARY
The Data Engineer is responsible for modeling complex problems, building pipelines, maintaining ETL processes and troubleshooting issues within a cloud environment. They utilize cloud databases, Databricks and databases to support a robust infrastructure which drives large, revenue generating data strategies.

ESSENTIAL DUTIES
Participate in use case feasibility discussions and translate business idea / business problems into use cases.
Provide support as needed to maintain and update models running in production environment.
Develop and maintain complex ETL processes and algorithms
Own and enhance ETL processes to move data from PMS to cloud databases
Monitor and troubleshoot processes on a daily basis
Document new and existing processes
Proactively and independently identify performance issues and recommend enhancements
Modernize legacy data models and pipelines using Databricks and cloud database capabilities.
QA and validate data moving between various parts of the company to ensure accuracy
Understand business problems/needs and provide proposed solutions.
Qualifications
KNOWLEDGE & REQUIREMENTS
REQUIRED QUALIFICATIONS
Experience working with healthcare professionals in a clinical setting
2+ years of data analysis experience
Understanding of algorithm design, machine learning, and applied statistics
Proven track record in use of SQL specifically in cloud databases and working with data including extracting information, validating data, creating and maintaining custom data structures.
Bachelor’s degree in Business, Computer Science, Information Systems or equivalent combination of education and experience.
Be able to work well with people of various backgrounds and education levels and establish cooperative working relationships with all coworkers.
Timely and effectively communicate information to and consult with others in order to complete work assignments.
Act in a responsible, trustworthy and ethical manner that considers the impact and consequences of one’s actions or decisions.
Communicate ideas, thoughts, and facts in writing through the use of proper grammar, spelling, document formatting and sentence structure.
Identify and respond to current and future clients’ needs; provide excellent client service.
Evaluate and analyze problems or tasks from multiple perspectives; adaptively employ problem solving methods to find creative or novel solutions; use logical, systematic and sequential processes to solve problems.
Complete assigned job tasks in an accurate and timely manner.
Carefully prepare for meetings and presentations; follow up with others to ensure that agreements, tasks or commitments have been fulfilled.
Demonstrate commitment to achieving Company’s core business objectives of increasing the role of pharmacy and improving patient health in America.

DESIRABLE QUALIFICATIONS
Experience working with healthcare professionals in a clinical setting.
Experience resolving issues that do not have clear answers.
Experience with Databricks (preferred), Talend, Informatica or other ETL tools.
Experience working in cloud databases (BigQuery preferred)
Highly motivated and possessed excellent interpersonal, problem solving, and technical skills.
High sense of urgency and accountability
Adaptable, friendly, and ability to work with a team.
Excellent attendance
Passion for data and digging into the minutia of datasets.
Take calculated risks based on data-driven analytics
Be a self-starter
Enjoy working in a fast-paced environment."
366,Snowflake Data Engineer with DBT (10+ Years),Hash Technologies,Remote,$70.79 - $80.52 an hour,"Title : Snowflake Data Engineer with DBT
Location : REMOTE
Job Description:
Must have about 10+ years of work experience with:
Programming/development
Data warehousing
Experience with the end-to-end implementation of Snowflake cloud data warehouse or end-to-end data warehouse implementations on-premises
Experience with Data Vault 2.0 data modeling
Experience with ETL/ELT, DBT Cloud, SnowSQL, SSIS, stored procedures, Python
Experience with version control tools like Azure DevOps
Experience Setting up resource monitors, Deploying RBAC controls, virtual warehouse sizing, query performance tuning, troubleshooting
Solid SSIS and SQL skills
Able to present complex technical concepts to audiences of varying sizes and levels.
Experience with Agile or similar scrum methodology
Ability to appropriately manage confidential property and corporate information.
Experience in working with all levels of management, including executives and IT operational teams.
Must possess in-depth analytical, problem solving and critical thinking skills and the ability to master new concepts quickly
Strong commitment to customer service
Ability to effectively communicate findings in English, in both written and oral form.
Ability to successfully acquire any licensing approvals that may be needed to provide services.
Must have the capability to learn other platforms as they may be adopted by the company.
Job Type: Contract
Salary: $70.79 - $80.52 per hour
Experience level:
9 years
Schedule:
8 hour shift
Experience:
Snowflake: 5 years (Required)
DBT: 3 years (Required)
Data warehouse: 9 years (Preferred)
Work Location: Remote"
367,Sr Data Engineer- REMOTE,FreeWheel,"Philadelphia, PA 19103•Remote","$90,654 - $212,469 a year","Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast.

Job Summary
FreeWheel helps the television industry generate revenue from premium video content through a robust technology platform. FreeWheel is hiring a virtual or New York based Senior Data Engineer. FreeWheel‘s AudienceXpress engineering team is responsible for building advanced solutions to process very high volume of raw from set top boxes on a state of the art platform and to making the data accessible in a secure and cost effective manner for other teams and our customers.
Job Description
Core Responsibilities
Develop software to run on cloud native big data infrastructure built on AWS using Spark, Lambda, S3, and other cloud native services.
Design and implement high-performance, scalable and optimized data solutions.
Write applications to derive insights from terabytes of highly valuable data sets.
Develop quick proof of concepts for marketing teams and clients with big data needs.
Manage and improve existing code base and add new features to support business needs.
Conduct design and code reviews.
Work in a collaborative manner with a smart, diverse engineering scaled agile (SAFe) team to rapidly deliver high quality solutions.
Interface with global engineering, product and operation teams in US/EU/China to incorporate their innovations and vice versa.
Mentor and provide direction in architecture and design to developers.
Consistent exercise of independent judgment and discretion in matters of significance.
Regular, consistent and punctual attendance. Must be able to work nights and weekends, variable schedule(s) and overtime as necessary.
Other duties and responsibilities as assigned.
Requirements:
Bachelor's or master’s degree in Computer Science or similar field of study
2+ years of experience building large scale applications as part of a team
Experience with bash, Linux utilities and environments
Proficiency with SQL and database general concepts
Hands-on experience with data processing and exposure to big data, especially spark (preferred)…Spark job debugging and optimization experience would be esp valuable
Expertise with at least two of Scala, Python, Java, Golang, C/C++, or similar
Flexibility with languages and tools and desire to learn whatever is necessary to get the job done
Experience with CI/CD tools like Jenkins
Experience in AWS technologies such as Lambda, Kinesis, EC2, IAM, CloudFormation, Route 53, ELB, VPC, Cloudwatch, Lambda, EMR, S3 and Big data related technologies like HIVE, Presto, Kafka (strongly preferred)
AWS certification (preferred): AWS Developer/Architect/DevOps/Big Data
Understanding of various columnar file formats like Parquet, ORC etc. (preferred)
Strong problem-solving and analysis skills, a team player
Employees at all levels are expected to:
Understand our Operating Principles; make them the guidelines for how you do your job.
Own the customer experience - think and act in ways that put our customers first, give them seamless digital options at every touchpoint, and make them promoters of our products and services.
Know your stuff - be enthusiastic learners, users and advocates of our game-changing technology, products and services, especially our digital tools and experiences.
Win as a team - make big things happen by working together and being open to new ideas.
Be an active part of the Net Promoter System - a way of working that brings more employee and customer feedback into the company - by joining huddles, making call backs and helping us elevate opportunities to do better for our customers.
Drive results and growth.
Respect and promote inclusion & diversity.
Do what's right for each other, our customers, investors and our communities.
Disclaimer:
This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications.
Comcast is an EOE/Veterans/Disabled/LGBT employer.
Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other basis protected by applicable law. Comcast will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of applicable law, including the Los Angeles Fair Chance Initiative for Hiring Ordinance and the San Francisco Fair Chance Ordinance.

Education
Bachelor's Degree
While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.
Relevant Work Experience
7-10 Years

Salary:
National Pay Range: $90,653.58 USD-$212,469.33 USD
Comcast intends to offer the selected candidate base pay within this range, dependent on job-related, non-discriminatory factors such as experience.

Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That’s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality – to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details."
368,Data Engineer - Remote,SOSi,"Vienna, VA•Remote",N,"Overview
We are seeking a highly skilled and motivated Mid-Level Data Engineer to join our growing team. The successful candidate will have a strong background in data engineering, and a passion for working with large, complex data sets.
Responsibilities
Design and implement data pipelines, architectures and data sets to support business and data analytics needs.
Work with cross-functional teams to understand and define data requirements, and translate them into technical solutions.
Build, optimize, and maintain data storage and processing systems, including data warehousing, ETL, and data lake technologies.
Collaborate with data scientists and analysts to ensure data is accurate, complete, and accessible for analysis and reporting.
Monitor and troubleshoot data pipeline performance, and optimize as necessary.
Ensure data security and data governance compliance.
Continuously evaluate and implement new technologies and tools to improve data processing and storage capabilities.
Qualifications
3-5 years of experience in data engineering or related field.
Strong hands-on experience with data warehousing, ETL, and data lake technologies.
Experience with SQL and at least one programming language such as Python, Java or Scala.
Familiarity with big data technologies such as Hadoop, Spark, and Hive.
Experience with cloud-based data storage and processing solutions, such as AWS, Azure, or GCP.
Strong understanding of data modeling, data warehousing and data governance.
Experience with data security and compliance.
Strong problem solving, analytical, and communication skills.
Strong understanding of data governance and data privacy regulations.
Bachelor's degree in Computer Science, Data Science, or a related field.
Ability to obtain US government Security clearance.
Working Conditions
Full remote flexibility
Exovera offers a competitive salary and benefits package, as well as a dynamic and fast-paced work environment. If you are passionate about data engineering and want to be part of a team that is driving innovation and growth, we want to hear from you."
369,Senior Data Engineer,DexCare,Remote,"$130,000 - $152,000 a year","At DexCare, we envision a world where all consumers enjoy exceptional access to the best expertise to prevent, treat and cure illness. DexCare’s mission is to empower those that provide care with the tools, systems and intelligence necessary to deliver on this vision.
DexCare is looking for a highly motivated and experienced Senior Data Engineer to join the team!
The Senior Data Engineer will report directly to VP of Data Science and be responsible for development of DexCare’s data services that support data dashboards and intelligent services.
DexCare is committed to building a workplace that fosters inclusivity alongside productivity. We bring together people with a wide variety of backgrounds, experiences and perspectives and encourages them to collaborate and think big in a safe blame-free environment. Our greatest asset is our team!
We firmly believe that our high-performing culture based on alignment, focus, prioritization, diversity, openness, transparency, and problem-solving are the keys to our success.
Responsibilities
Design and implement product cloud data warehouse schema for multiple stages of data.
Handle large sets of data from various of data sources.
Work closely with data engineers to build cloud data infrastructure to support our business needs.
Model the data and design ETL specifications to business requirements.
Deliver the data to the API components.
Perform data warehouse maintenance and upgrading.
Develop data security inside data services.
Code or test scripts that complete activities such as data cleanup, repairs, synchronization, and integration.
Consistently give technical proposals to optimize data warehouse capacity, scalability, and performance.
Establish the documentation of reports, develops, and maintains technical specification documentation for all reports and processes.
Qualifications
Passion for data warehousing and data analytics.
7+ years exp with software engineering and database management using relational databases.
6+ hands-on experience with MySQL, Postgres, Oracle, SQL Server or equivalent RDBS.
3+ years using Python
Familiar with Docker, Kubernetes, Helm, and other dev ops tools
Vast knowledge of database design and modeling in the context of data warehousing.
Ability to resolve data issues and business logic failures in the data warehouse.
Strong problem-solving skills with a can-do attitude.
Great software engineering best practices with git and version control systems.
Solid communication skills to explain complex data designs to both technical and non-technical people.
Self-starter and self-motivated.
Bachelor’s Degree in Computer Science or related areas.
Why DexCare
Mission-driven culture and flow
Being a part of our incredible talented and diverse team
Innovative product solution for proven, ongoing, and exponential current customer need
Interesting roles and experience opportunities
We are an early-stage company, not a start-up. Providence Health Care spun us out to enable other health care networks to access and benefit from our products, services, and future work.
We will recognize and reward your impact and results
Customer and revenue traction with large multi-year cloud software via signed licensing contacts
Venture-backed (Series B stage)
Who is DexCare?

DexCare, Inc. is a data-driven intelligence company focused on improving healthcare access for everyone. DexCare was incubated at Providence, one of the nation’s largest and most innovative health systems with over 50 hospitals across the Western United States.

DexCare is led by a mission-driven team experienced in building successful, innovative and transformative solutions to our customers biggest problems. We’ve built digital health companies from the ground up and worked in some of the world’s largest data, cloud platforms, healthcare companies and provider organizations in the world. We are backed by leading investors, including Kaiser Permanente, Providence Ventures, Mass General Brigham, Transformation Capital, Define Ventures, Frist Cressey Ventures and SpringRock Ventures.

What is DexCare?

DexCare’s core offering is a Platform-as-a-Service (PaaS) that intelligently orchestrates health system capacity and digital demand across all lines of care. In a complex and vast sea of virtual point solutions, DexCare is the operating system that makes it all work. DexCare’s data-driven intelligence engine allocates, flexes and optimizes resources to best meet both consumer demand and health system business goals—expanding the reach of health system service lines into a new, digital and on-demand consumer arena. The platform attracts and caters to consumers by providing a fully-digitized, unified experience that routes consumers to the best care options while leveraging existing EMR, caregiver and brand investments. DexCare enables leading health systems across the U.S. to attract 30% more new patients, capture 5x downstream revenue, generate over 20% per patient encounter in costs savings, and deliver a net promotor satisfaction score greater than 90. Discover more at www.dexcarehealth.com.

DexCare is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. DexCare does not exclude people or treat them differently because of race, color, national origin, age, disability, or sex. DexCare provides reasonable accommodation to all applicants who require such accommodation to apply for the position or to perform the essential functions of the job.

The salary range for this role is from $130,000 to $153,000. The hired applicant will also be eligible to receive an annual bonus. Your health and well-being ensure that we all perform at our best as a team. DexCare offers an outstanding benefits package, including healthcare benefits, short-term disability coverage, basic life insurance, 401k, and wellness benefits. Employees also receive the following: paid time off, nine paid holidays, unlimited PTO, in-person, hybrid, and remote working arrangements."
370,Data Center Engineer,TechMatrix Inc,"Jersey City, NJ",$60 - $61 an hour,"Datacenter Engineer
Location: Jersey City, NJ
Duration: 6 Months
Looking for strong Data Center Engineers, this individual will be onsite in Somerset, NJ 5 days a week supporting a total of 6 DC’s one physically and 5 remotely. The individuals who were not selected were more DC break fix and couldn’t do any basic configurations. This person will do rack stack work, break fix work, basic configs, as well as remote troubleshooting
Responsibilities
Provide leadership for delivery of 24/7 service operations and KPI compliance and audit.
Establish and maintain metrics, key performance indicators, and service level agreements to continually improve the DC performance.
Manage remote sites as well as schedule/monitor work performed by third party vendors.
Business Continuity Plan Management, assuring our BCP documentation is current and approved by management
Attend meetings to capture business requirements for DC Capacity planning and workflow management.
Maintaining corporate databases and ensuring compliance with internal and external audit regulations.
Support applications and systems by meeting or exceeding standards and service levels while minimizing operational risk.
Promptly schedule equipment installs and cabling request, engage partners. plan parallel work streams when possible.
Partner closely with stakeholders to improve production environment stability.
Leads the support of Data Center infrastructure
Reviews, participates, and implements procedures.
Research industry standards, best practices and new innovations in technology and makes recommendations.
Provide quality service and solutions consistent with objectives and client requirements.
Effectively handle incident management for outages.
Ensure timely notification and escalation of possible issues/problems, options, and recommendations for prompt resolution.
Dive into design, review, and integration of all application requirements like functional, security, integration, performance, quality, and operations.
Proficient with fluke testers and fiber testers, and scanner
Knowledge of interrelated power, space, and cooling as it relates to.
Job Type: Contract
Pay: $60.00 - $61.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Jersey City, NJ: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person"
371,Principal Software Engineer (SASE Data Solutions),Palo Alto Networks,"Santa Clara, CA","$166,600 - $269,500 a year","Company Description

Our Mission
At Palo Alto Networks® everything starts and ends with our mission:
Being the cybersecurity partner of choice, protecting our digital way of life.
We have the vision of a world where each day is safer and more secure than the one before. These aren’t easy goals to accomplish – but we’re not here for easy. We’re here for better. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.
We’re changing the nature of work. Palo Alto Networks is evolving to meet the needs of our employees now and in the future through FLEXWORK, our approach to how we work. From benefits to learning, location to leadership, we’ve rethought and recreated every aspect of the employee experience at Palo Alto Networks. And because it FLEXes around each individual employee based on their individual choices, employees are empowered to push boundaries and help us all evolve, together.

Job Description

Your Career
Palo Alto Networks's Prisma SD-WAN group is looking for a seasoned and accomplished engineer with experience in developing data solutions for Secure Access Service Edge(SASE). You will be part of a world-class software engineering team that works on various ground-breaking technologies. You will work with a multi-functional team of engineers to design and develop data platform with cloud based management, monitoring and analytics.
Your Impact
You will work in a small and fast paced team delivering leading edge SASE solution to
Deliver platform features for for anomaly detection, time series forecast, etc
Innovate algorithms that can operate at scale
Contribute to development efforts from rapid prototypes to large scale applications
Ensure performance and stability of software
Keep yourself up to date in data technologies and mentor others
Proactively drive product improvement and innovation

Qualifications

Your Experience
Experience in Redis, Influx DB, Elastic Search
Experience in developing high performance distributed software applications
Expertise in data structures and algorithms for large scale systems
Experience in data modeling and machine learning frameworks
Knowledge of BigData ecosystems
Strong proficiency in Golang and Python
7+ years of experience in developing software applications with Bachelors or Masters degree in computer science or equivalent military experience required

Additional Information

The Team
Our engineering team is at the core of Palo Alto Network Prisma SD-WAN development, which is part of the world class SASE solution. We are constantly innovating and challenging ourself. Our engineers don’t shy away from building products to solve problems no one has pursued before.
We define the industry, instead of waiting for directions. We need individuals who feel comfortable in ambiguity, excited by the prospect of a challenge, and empowered by the unknown risks facing our everyday lives that are only enabled by a secure digital environment.
Our Commitment
We’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together.
We are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.
Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.
All your information will be kept confidential according to EEO guidelines.
The compensation offered for this position will depend on qualifications, experience, and work location. For candidates who receive an offer at the posted level, the starting base salary (for non-sales roles) or base salary + commission target (for sales/commissioned roles) is expected to be between $166,600/yr to $269,500/yr. The offered compensation may also include restricted stock units and a bonus. A description of our employee benefits may be found here."
372,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
373,Data Engineer - Tekton Terraform,Altair Engineering,"Dearborn, MI",N,"Transforming the Future with Convergence of Simulation and Data
Data Engineer General
Job Summary:
Our client in Dearborn, MI is looking for an Data Engineer General. This is a contract position.
What You Will Do:
The GDIA Data Factory Platform covers all business processes and technical components involved in ingesting a wide range of enterprise data into the GDIA Data Factory (Data Lake) and the transformation of that data into consumable data sets in support of analytics. The Data Factory Enablement Team, as the name suggests enables teams build their solutions in the GCP Data Factory Platform by proving Tools, Guidelines, processes and support. We are looking for candidates who have a broad set of technology skills across areas and come from a background of DevOps, with exposure to infrastructure and solution monitoring. This person will be expected to provide consultative services to the Software Development and Database Engineering teams.
Key responsibilities include:
Work as part of an implementation team from concept to operations, providing deep technical subject matter expertise for successfully deployment of Ford's Data Platform.
Implement methods for standardization of all parts of the pipeline to maximize data usability and consistency.
Test and compare competing solutions and report out a point of view on the best solution.
Design and Build CICD Pipelines for Google Cloud Platform (GCP) services: BigQuery,DataFlow,Pub/Sub, Data Fusion and others.
Work with stakeholders including Analytics, Product, and Design teams to assist with data related technical issues and support their data infrastructure needs.
Develop IAC tekton pipelines to execute pattern playbooks and templates.
Designing cloud performance and monitoring strategies.
Designing and implementing workflows to automate the infrastructure release and upgrade process for applications in Dev, UAT and Production environments..
Mentor and grow technical skills of engineers across multiple sprint teams by giving high quality feedback in design and code reviews and providing training for new methods, tools, and patterns.
Basics:
Someone who understands Cloud as being a way to operate and not a place to host systems.
In-depth understanding of GCP product technology and underlying architectures.
Experience and very strong with development eco-system such as Git, Jenkins, Terraform and Tekton for CI/CD.
Experience in working with Agile and Lean methodologies.
At least 2 years of tekton experience.
At least 5 years of Terraform experience or 3 years with terraform certification.
At least 3 years of experience in Google cloud and Google cloud professional architect certification.
Certifications:
HashiCorp.
Terraform Associate.
Google Cloud Certified Associate Cloud Engineer.
Google Cloud Certified Professional Architect.
How You Will Be Successful:
Envision the Future
Communicate Honestly and Broadly
Seek Technology and Business “First”
Embrace Diversity and Take Risks
What We Offer:
Competitive Salary
Comprehensive Benefit Package
401(k) with matching contributions
Paid Time Off
Employee Discounts
Free training on all Altair products
Why Work with Us:
Altair is a global technology company providing software and cloud solutions in the areas of data analytics, product development, and high-performance computing (HPC). Altair enables organizations in nearly every industry to compete more effectively in a connected world, while creating a more sustainable future. With more than 3,000 engineers, scientists, and creative thinkers in 25 countries, we help solve our customer’s toughest challenges and deliver unparalleled service, helping the innovators innovate, drive better decisions, and turn today’s problems into tomorrow’s opportunities.
Our vision is to transform customer decision making with data analytics, simulation, and high-performance computing.
For more than 30 years, we have been helping our customers integrate electronics and controls with mechanical design to expand product value, develop AI, simulation and data-driven digital twins to drive better decisions, and deliver advanced HPC and cloud solutions to support unlimited idea exploration. To learn more, please visit altair.com
Ready to go? #ONLYFORWARD
At our core we are explorers; adventures; pioneers. We are the brains behind some of the world’s most revolutionary innovations and are not only comfortable in new and unchartered waters, we dive headfirst. We are the original trailblazers that make the impossible possible, discovering new solutions to our customer’s toughest challenges.
Altair is an equal opportunity employer. Our backgrounds are diverse, and every member of our global team is critical to our success. Altair’s history demonstrations a belief that empowering each individual authentic voice reinforces a culture that thrives because of the uniqueness among our team."
374,Data Engineer,Walt Disney Company,"500 S Buena Vista St, Burbank, CA 91521",N,"End and Direct Client: Walt Disney Company
Duration: 12+ Months
***********ONLY W2********NO C2C*********
JOB RESPONSIBILITIES:
Partner with technical and non-technical colleagues to understand data and reporting requirements.
Work with engineering teams to collect required data from internal and external systems.
Design table structures and define ETL strategy to build performant Data solutions that are reliable and scalable in a fast-growing data ecosystem.
Develop Data Quality checks and visualizations/dashboards
JOB REQUIREMENTS
Develop and maintain ETL routines using ETL.
Experience in Snowflake, (Snowflake Migration preferred)
Perform ad hoc analysis as necessary.
Perform SQL and ETL tuning as necessary.
Experience in Apache Spark
To follow-up with questions or to apply directly, please reach out Recruiter ""AJ"" @ 405-907-2956
Job Type: Contract
Pay: $85.00 - $95.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Burbank, CA 91521: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 10 years (Required)
Spark: 5 years (Required)
Python: 5 years (Required)
Snowflake: 2 years (Required)
Work Location: One location"
375,Data Platform Engineer,Holman,"Mount Laurel, NJ",From $110 an hour,"At Holman, we exist to provide rewarding careers and better lives for employees and their families. We hire, train, empower, and reward exceptional people. Our journey is guided by our desire to get it right every time and the acknowledgement that we have an opportunity to be better. To be better, we have to do better, and to do better we must know better. That’s why we are listening, open to learning new things – about ourselves and each other. We will never stop striving for improved diversity, equity, and inclusion because we are successful together when we feel trusted and supported. It’s The Holman Way.

At Holman, your total compensation goes beyond your paycheck. To position you for success and provide a rewarding career and better life for you and your family, Holman is proud to offer you the benefits you deserve; including protection against illness, disability, loss of work, or preparation for retirement. Below is a brief overview of these programs:

Health Insurance

Dental Insurance

Life and Disability Insurance

Flexible Spending and Health Savings Accounts

Employee Assistance Program

401(k) with Employer Match

Paid Time Off

Tuition Reimbursement

Exclusive pricing and concierge sales support on new and used vehicles

Holman is currently accepting applications for the role of Data Platform Engineer

Principal Purpose of Position:
Design, develop, document and execute data solutions, tools and practices
Analysis of requirements at sufficient level of detail to allow ETL solution to be developed
Development of ETL job flows according to company standards for naming, performance, restartability and performance.
Support testing and remediation of defects in newly-developed/modified ETL workflows
Promote ETL workflows to PROD and provide ongoing support in PRODUCTION, including monitoring and troubleshooting
Ability to create Power BI Datasets to support the Analytic Delivery team
Evaluate emerging data platform technologies
Lead technology implementations
Follow and contribute to best practices for data management and governance
Collaborate with the Data Architecture team to understand and implement load processes for reporting and analytic data structures (data warehouses, data marts and data lakes)
Performance tune and troubleshoot processes under development and in production as necessary.
Work with the Data Architects to augment ERD’s as changes are developed
Develop, maintain, and extend reusable data components
Provide timely project and task updates to all concerned parties
Monitor production data integration jobs and correct failures in a timely manner
Create and manage incident reports as they pertain to data integration processes
Perform all other duties and special projects as assigned.

Required Experience/Skills

2+ years Azure exposure (Any Resources: Databases, Data Factory, Synapse Studio, Storage Account, Power Platform)
2+ years ANSI SQL experience
1+ years data modeling exposure
Advanced problem solving/Critical thinking mindset

Preferred Experience/Skills

Azure connectivity/authentication (service principals, managed identities, certificates)
Power BI Dataset creation/maintenance
Azure Resources: DevOps, Logic Apps, Gen 2 Storage, Purview
SQL Server, Oracle, Python, Spark

Education and/or Training:
Bachelor’s degree in Computer Science or equivalent work experience

Compensation: Starting at $110,00 USD

Holman is a global automotive leader that serves both commercial and consumer clients The Holman Way by always doing the right thing for our people, our customers, and the community since 1924. The Holman story began nearly a century ago as a single Ford dealership in New Jersey. Today, Holman, headquartered in Mount Laurel, New Jersey, is one of the largest family-owned automotive service organizations in North America with more than 6,500 employees across North America, the UK, and Germany.

Holman delivers a unique range of automotive-centric services including industry-leading fleet management and leasing; vehicle fabrication and upfitting; component manufacturing and productivity solutions; powertrain distribution and logistics services; commercial and personal insurance and risk management; and retail automotive sales as one of the largest privately owned dealership groups in the United States. Guided by its deeply rooted core values and principles, Holman is continuously Driving What’s Right.

Holman provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training."
376,Data Pipeline Engineer,KC Current,"Riverside, MO",N,"The Kansas City Current is seeking a Data Pipeline Engineer that will oversee and maintain our data pipelines that feed into our customer and scouting database systems.
Responsibilities:
Includes but not limited to:
This role will be part of a cross-functional team helping to build and execute the Current’s data analytics strategy
Develop EL/ELT/ETL pipelines to make data available in our end-user data systems
Uses programming skills in Python, SQL or any of the major languages to build robust data pipelines and dynamic systems
Defining our various layers of data hygiene and processing; landing, conformity, standardization
Troubleshooting and correcting data errors and duplications
Managing multiple source data systems
Working with key stakeholders to ensure data meets their needs
Creating and maintaining accurate documentation
All new hires must be fully vaccinated for COVID-19 by date of hire, subject to legally mandated accommodations.
Required Skills/Abilities:
Experience with APIs
Experience working with Data Lakes, Pipelines, SQL Server, CRM and/or other data stores and sources
Data hygiene, manipulation, and standardization best practices
JSON, SQL, WDL Languages PYTHON a plus
Experience with Azure Data Services including; Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Service Bus, Azure SQL Database, Azure Data Lake
Experience with Microsoft Power Platform including; Power BI, Dataverse, Power Apps, Dynamics 365
Minimum Education and Qualifications:
Master’s Degree Preferred
Experience working with big data technologies, system design and algorithms
Experience thriving in cross-functional environments
Expertise in understanding of different types of database and data store technologies
Strong communication and documentation skills
Quick and self-motivated learner
Physical Requirements:
Prolonged periods sitting at desk and working on a computer.
Prolonged periods of standing and walking long distance.
Must be able to lift up-to 25 pounds at a time
Key Personality Traits
Authentic/Genuine/Transparent
Competitive
Lives for the challenge, especially to the status quo
Gets it done.
Doesn’t see obstacles, only opportunity.
Knows together is better, for team and community.
Optimistic
Always improving

We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, sexual orientation, age, disability, gender identity, marital or veteran status, or any other protected class."
377,Senior Data Engineer - DataBricks,Grainger,Illinois,N,"About Grainger:
Grainger is a leading broad line distributor with operations primarily in North America, Japan and the United Kingdom. We achieve our purpose, We Keep the World Working®, by serving more than 4.5 million customers with a wide range of products that keep their operations running and their people safe. Grainger also delivers services and solutions, such as technical support and inventory management, to save customers time and money.
We're looking for passionate people who can move our company forward. As one of the 100 Best Companies to Work For, we have a welcoming workplace where you can build a career for yourself while fulfilling our purpose to keep the world working. We embrace new ways of thinking and recognize everyone you. Find your way with Grainger today.

Position Details:
The Data Engineering team at Grainger is focused on transforming data from Grainger’s key domains into reliable and real-time analytics products that address key needs. You will be focused on building and operating data pipelines that power analytics ranging from key financial reports to production models that define Grainger.com’s user experience. You will play an important part in defining the strategy of the team, evaluating and integrating data patterns and technologies, and building data products alongside domain experts. You are a thoughtful observer who enjoys investigating business problems and building data solutions that address them. You are an technical teacher that can guide teams to adopt the capabilities and products you build.

You Will:
Design efficient and scalable data processing systems and pipelines on Databricks, Airflow, APIs, and AWS Services.
Create technical solutions that solve business problems and are well engineered, operable, maintainable, and delivered.
Design and implement tools to detect data anomalies. Ensure that data is accurate, complete, and across all platforms.
Develop data models and mappings and build new data assets required by users. Perform exploratory data analysis on existing products and datasets.
Provide technical guidance to help data users adopt new data pipelines and tools.
Develop scalable and re-usable frameworks for ingestion and transformation of large datasets.
Understand trends and latest technologies. Evaluate the performance and applicability of potential tools for our requirements.
Work within an Agile delivery / DevOps methodology to deliver product increments in iterative sprints.
Design, and maintain efficient and scalable data processing systems and pipelines on Databricks, Airflow, APIs, and AWS Services.
Create technical solutions that solve business problems and are well engineered, operable, maintainable, and delivered on schedule.
Design and implement tools to detect data anomalies. Ensure that data is accurate, complete, and across all platforms.
Develop data models and mappings and build new data assets required by users. Perform exploratory data analysis on existing products and datasets.
Provide technical guidance to help data users adopt new data pipelines and tools.
Develop scalable and re-usable frameworks for ingestion and transformation of large datasets.
Understand trends and emerging technologies. Evaluate the performance and applicability of potential tools for our requirements.
Work within an Agile delivery / DevOps methodology to deliver product increments in iterative sprints.
Work with our AI, Platform, and Business Analytics teams to build useful pipelines and data assets.
You Have:
Experience in batch and streaming ETL using Spark, Python, Scala on Databricks for Data Engineering or Machine Learning workloads.
Familiarity with AWS Services not limited to Glue, Athena, Lambda, S3, and DynamoDB
Experience prepping structured and unstructured data for data science models
Demonstrated experience implementing data management life cycle, using data quality functions like standardization, transformation, rationalization, linking and matching.
Familiarity with containerization and orchestration technologies (Docker, Kubernetes) and experience with shell scripting in Bash, Unix or windows shell is preferable.
Rewards and Benefits:
With benefits starting day one, Grainger is committed to your safety, health and wellbeing. Our programs provide choice and flexibility to meet our team members' individual needs. Check out some of the rewards available to you at Grainger
Medical, dental, vision, and life insurance plans
Paid time off (PTO) and 6 company holidays per year
Automatic 6% 401(k) company contribution each pay period
Employee discounts, parental leave, 3:1 match on donations and tuition reimbursement
A comprehensive set of emotional, financial, physical and social wellbeing programs
DEI Statement
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace."
378,Data Analyst Engineer,BLVD Management,"BLVD Management in Mesa, AZ 85206","Up to $120,000 a year","Data Analyst Engineer - SFR Real Estate Investment
BLVD Management, LLC
Who we are
BLVD is a boutique real estate investment fund with private equity backing and is acquiring single family homes across the country. The fund specializes in acquiring and renovating properties to hold as long-term rentals. We are looking for A+ candidates individuals to join our data analytics team.
Compensation and Benefits
Base Pay Range: $80,000 - $120,000
Annual Bonus Target: 10%
Competitive paid time off plan
401(k) plan with generous company matching
Medical, dental and vision insurance; along with company paid disability and life insurance
Flexible hours and hybrid work from home options
The compensation range for this role added below is specific to the location of this job posting. Actual starting pay is determined by various factors, including but not limited to: relevant experience, skill set, qualifications, and other business and organizational needs.
Job Responsibilities:
Design, build, and deploy data models from internal and external data sets to find opportunities and optimize business processes
Works closely with line of business owners, business analysts, data architects, and developers to create insights gained from analyzing company data and test the effectiveness of difference courses of action.
Develop and maintain scalable data pipelines and build out new API integrations to support continuing increases in data volume and complexity
Improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization
Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
Define company data assets and identify critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that support and extend quantitative analytic deployment solutions
Qualifications / Skills:
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Experience in programming languages such as Python, SAS, R
Experience with data warehouses and data lakes
Experience with Databricks including Delta Lake, Apache Spark, Pipelines and Notebooks
Experience in working with databases such as PostgreSQL, MongoDB, SQL, Snowflake
Experience in developing solutions on cloud computing platforms such as Google Cloud, AWS, Azure
Experience with CI/CD systems such as GitLab, DevOps, Cloud Build
Experience with analytics, reporting and BI platforms
Ability to learn and adopt new technologies and languages
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service
Experience in real estate acquisition and property management data analytics
Education, Experience, and Licensing Requirements:
Bachelor’s degree in computer/data science, or a related technical field or equivalent combination of training and work experience
4+ years of Python or .Net development experience
4+ years of SQL experience (No-SQL experience is a plus)
4+ years of experience with schema design and dimensional data modeling
Experience designing, building, and maintaining data processing systems
How to Apply
Please apply via this listing and include any other application requirements.
*Applicants are required to be eligible to lawfully work in the U.S. immediately; employer will not sponsor applicants for U.S. work authorization (e.g. H-1B visa) for this opportunity
Job Type: Full-time
Pay: Up to $120,000.00 per year
Benefits:
401(k)
Dental insurance
Flexible schedule
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Do you now, or will you in the future, require sponsorship for employment visa status (e.g., H-1B visa status, etc.) to work legally for our Company in the United States?
Experience:
Data science: 3 years (Required)
Database / ETL: 3 years (Required)
Databricks: 1 year (Required)
Real Estate SFR Analysis: 1 year (Preferred)
Work Location: One location

Health insurance"
379,Data Operations Engineer,Hudson River Trading,"1 Financial Sq, New York, NY 10005","$150,000 - $250,000 a year","Data is at the core of everything we do here at HRT. We excel at deriving deep insights from all types of data, allowing us to achieve consistent success in a competitive market. This is a challenging and exciting role that provides opportunities to contribute to the success of HRT.
The Data Operations Engineer will write software, explore data, work closely with our research and trading teams, and interact with a variety of external partners such as data providers, brokers, and exchanges.
The role requires strong data engineering skills, experience wrangling data, and the ability to provide exceptional support in a production trading environment.
Responsibilities
Data Engineering: Write tools to classify, onboard, and reconcile data. Onboard datasets using Airflow, explore data, and automate tasks using a modern Python data stack.
Production Support: Provide proactive oversight of our data pipeline, handle inquiries from internal customers, and manage issues through resolution.
Vendor Technical Specialist: Work closely with financial and alternative data vendors to acquire data and solve issues, including Bloomberg, Refinitiv, and many others. Be the internal expert on their various delivery platforms (FTP, APIs, etc) and databases.
Data Analysis: Parse, analyze, and understand data sets. Perform data reconciliations, validation, and quality checks. Identify and develop new processes within the data request process to enrich data.

Skills
Strong working knowledge of Python
Comfortable with the Linux command line
Experienced in at least one SQL dialect (PostgreSQL, MSSQL, MYSQL) and able to use others as needed
Able to provide technical support in a production trading environment
Track record of being detail-oriented (you love when everything lines up!)
Excellent communication skills, oral and written
Profile
A minimum of 2-4 years of experience wrangling data is preferred
Strong programming experience in Python
Experience managing ETL pipelines
Experience with financial datasets (e.g. Refinitiv, S&P, Bloomberg) is a plus
You enjoy problem solving and researching large datasets to resolve complex issues
You thrive when collaborating with people who motivate you and make you better
Annual base salary range of $150,000 to $250,000. Pay (base and bonus) may vary depending on job-related skills and experience. A sign-on and discretionary performance bonus may be provided as part of the total compensation package, in addition to company-paid medical and/or other benefits.
Culture
Hudson River Trading (HRT) brings a scientific approach to trading financial products. We have built one of the world's most sophisticated computing environments for research and development. Our researchers are at the forefront of innovation in the world of algorithmic trading.

At HRT we welcome a variety of expertise: mathematics and computer science, physics and engineering, media and tech. We're a community of self-starters who are motivated by the excitement of being at the cutting edge of automation in every part of our organization—from trading, to business operations, to recruiting and beyond. We value openness and transparency, and celebrate great ideas from HRT veterans and new hires alike. At HRT we're friends and colleagues – whether we are sharing a meal, playing the latest board game, or writing elegant code. We embrace a culture of togetherness that extends far beyond the walls of our office.

Feel like you belong at HRT? Our goal is to find the best people and bring them together to do great work in a place where everyone is valued. HRT is proud of our diverse staff; we have offices all over the globe and benefit from our varied and unique perspectives. HRT is an equal opportunity employer; so whoever you are we'd love to get to know you."
380,Data Engineer,Indiana Farmers Insurance,Indiana,N,"Description:
Indiana Farmers Insurance is currently looking for a Data Engineer to join our Information Technology Team. The ideal candidate will be located in Indiana, Illinois or Ohio.
The Data Engineer is primarily responsible for building data pipelines and creating analytical infrastructure to support business knowledge management. The right candidate will collaborate closely with other technical and business associates to define, design, and implement data analysis, modeling, reporting and dashboard creation. The Indiana Farmers Data Engineer will be responsible for collaborating effectively with subject matter experts to identify, validate, and turn into action relevant data points to help drive informed-decision support.
Benefits for the Data Engineer:
Free Health insurance
Free Dental insurance
Free Vision insurance
Free Life insurance
Free Short-Term & Long-Term Disability insurance
2% 401k Company Match
11% 401k Company Contribution
Excellent Paid Time Off
Day of Service
Charity Match Program
We promote from within our diverse workforce regularly and offer regular opportunities to learn and grow
Matching funds of up to $100 annually are available from the company for your favorite charitable organization
Associate recognition awards, fun gatherings, and opportunities to make friends are part of our culture
An on-site fitness center, as well as free and convenient parking right next to our building make life easier
Requirements:
Qualifications and Responsibilities for the Data Engineer:
Bachelor’s degree in computer science or engineering
Build infrastructure for optimal extraction, transformation and loading of data from various sources, using SQL and other ETL technologies
Ability to perform root cause analysis on external and internal processes and identify opportunities for improvement and actionable insights
A positive attitude, team mentality, and the interpersonal skills essential for effective communication with cross-disciplined teams
Promote understanding of data-related technical issues and plans for process improvement
Optimize data delivery and automate manual processes
Collaborate with various departments to capture and document data points using technology platforms
Develop intuitive analytical tools to provide actionable insight into key business performance metrics
Ability to build processes that support data transformation, workload management, data structures, dependency, and metadata
Why work for Indiana Farmers Insurance?
Imagine working for an employer like this:
95% retention of our associates over the last 5 years even with a very competitive employment market.
Financially stable as shown by our A (Excellent) rating by AM Best.
Truly customer focused, as all mutual insurance companies are and
A strong legacy of excellent performance throughout our 146 years in business!
For more information about Indiana Farmers Mutual Insurance Company, please go to
https://www.indianafarmers.com/about-us
Indiana Farmers Insurance is an equal opportunity employer!"
381,"VP, Market Data Engineer",Barclays,"745 7th Ave, New York, NY 10019","$210,000 a year","VP, Market Data Engineer
New York, New York
Barclays Services Corp.
What will you be doing?
On behalf of a global financial services organization, develop, create, and modify general computer applications software and specialized utility programs.
Analyze user needs and develop software solutions.
Design software with the aim of optimizing operational efficiency.
Develop monitoring and automation processes to manage real time market data feed handler estate.
Support a global work stream within specialized infrastructure to standardize automation across various technology disciplines including market data, networks, Unix, and middleware.
Assist in the integration of automation processes with monitoring frameworks including Elastic Logstash Kibana.
Review and monitor the health and quality of real-time market data environment.
Perform diagnostic work and joins major incident bridges as needed to investigate and remediate critical production issues affecting market data.
Develop and design Market Data products and support procedures where required.
Communicate with stakeholders to resolve any market data production issues and troubleshoots accordingly.
Undertake key business requests around value and applicability of market data cost saves and reducing of production.
Ensure that all activities and duties are carried out in full compliance with regulatory requirements, internal Barclays Policies, and Policy Standards.
What we’re looking for:
Minimum of Bachelor’s degree, or foreign equivalent, in Computer Science, Computer Engineering, Finance, Business Administration or related field and at least five (5) years of post-baccalaureate progressively responsible experience in a Market Data related occupation within the financial services industry.
Must have at least five (5) years of progressively responsible employment experience with each of the following required skills:
Programming and developing tools including BASH (Born Again Shell) and Python;
Subject Matter Expertise in Market Data concepts;
Knowledge of Configuration Management Tools, such as Ansible and Chef;
Experience operating various Linux command line utilities including Global Regular Expression Print (GREP), AWK, SORT, LS and PS;
Version Control Tools including Global Information Tracker (GIT) and STASH;
Computer Networks including Wireshark, Transmission Control Protocol Dump (TCPDump) and Transmission Protocol/ Internet Protocol Stack (TCP/IP Stack);
Open-source IT monitoring tools, including Elastic, Logstash, and Kibana (ELK) as well as SNMP (Simple Network Management Protocol) and syslog; and
Linux Operating Systems Fundamentals. Experience working with the Red Hat Enterprise Linux or CentOS distributions.
Where will you be working?
You will be working at our New York, New York location at 745 Seventh Avenue. This 37-story office tower is located in Times Square in the heart of Manhattan and features a cafeteria, fitness center and state-of-the-art LED signage on the façade of the building. The building is easily accessible to Restaurants, Shops and Public Transportation.
Interested and want to know more about Barclays? Visit home.barclays/who-we-are/ for more details.
Our Values
Everything we do is shaped by the five values of Respect, Integrity, Service, Excellence and Stewardship. Our values inform the foundations of our relationships with customers and clients, but they also shape how we measure and reward the performance of our colleagues. Simply put, success is not just about what you achieve, but about how you achieve it.
Our Diversity
We aim to foster a culture where individuals of all backgrounds feel confident in bringing their whole selves to work, feel included and their talents are nurtured, empowering them to contribute fully to our vision and goals.
It is the policy of Barclays to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, creed, religion, national origin, alienage or citizenship status, age, sex, sexual orientation, gender identity or expression, marital or domestic/civil partnership status, disability, veteran status, genetic information, or any other basis protected by law.
Dynamic working gives everyone at Barclays the opportunity to integrate professional and personal lives, if you have a need for flexibility then please discuss this with the hiring manager.
Our Benefits
Our customers are unique. The same goes for our colleagues. That's why at Barclays we offer a range of benefits, allowing every colleague to choose the best options for their personal circumstances. These include a competitive salary and pension, health care and all the tools, technology and support to help you become the very best you can be. We are proud of our dynamic working options for colleagues. If you have a need for flexibility then please discuss this with us.
Salary/Rate Minimum/yr: $210,000
Salary/Rate Maximum/yr: $210,000
The minimum and maximum salary/rate information above include only base salary or base hourly rate. It does not include any other type of compensation or benefits that may be available.
This position is eligible for incentives pursuant to Barclays Employee Referral Program."
382,Data Engineer I,NexTraq LLC,"1200 Lake Hearn Dr NE, Atlanta, GA 30319",N,"The Opportunity

NexTraq, a subsidiary of Michelin North America, is looking for a Data Engineer I. Are you a Database Administrator on the lookout for exciting and new challenges in the SQL landscape? Are you looking to apply your talent and development experience to provide solutions to complex challenges? Are you the type of person who can develop the grand plan and then get your hands dirty in its implementation? We’re looking for someone who will be integral to the design, development, implementation and maintenance of all databases, reporting solutions and data warehouses in production as well as our back-office infrastructure.
What You’ll Do
Install, configure, secure, optimize and maintain Microsoft SQL Server instances including SSAS and SSRS components
Monitor databases and related systems to identify capacity and performance issues
Design and/or provide analysis of Stored Procedures
Automate regular tasks using preferred scripting language
Participate in implementation of machine learning and big data analysis
Work with lead DBA to develop and implement performance and capacity remediation plans
Create ad-hoc reports
Maintain and enhance data recovery process, ETL processes and cube deployment processes
Understand partition architectures and handle issues in multiple environments (i.e. production, staging, qa)
Understand our DR setup and architecture and be able to handle DR scenarios.
Interact with multiple departments (Development, Finance, etc.)
Work in agile scrum environment
The Ideal Candidate
2+ years of experience
BS in Computer Science or related degree, or equivalent work experience
Excellent verbal and written communication skills are required

Benefits & Perks

As a Michelin Group Company, our “ICARE” corporate culture model defines the company values that guide how we work with each other and with our external customers & partners. Here are some of the other great reasons why our employees say they love to work here:

Competitive Health/Dental/Vision insurance with substantial company contribution
Company-Paid Life Insurance policy
401K Benefits with company matching and immediate full vesting/no waiting period
Healthcare benefits with costs 65% lower than the national U.S. average
HSA/FSA Healthcare account options with company contributions of $500/$1000
Short & Long Term Disability Insurance
Parental Leave: Additional 3 weeks covered at 100% in addition to standard STD
Competitive paid time off benefits throughout the year allowing employees to maintain work-life balance
Gym reimbursement program and half-price Weight Watchers’ discount program
Michelin Tire Rebate Program up to $750/year
Flex Hours and Telecommuting/Remote Work for many departments/positions
Magellan Employee Assistance Program (EAO) – provides free counseling/assistance
Who We Are
NexTraq (a wholly owned subsidiary of Michelin), is the GPS fleet management solution of choice for a growing number of customers with fleets ranging from 2 to more than 2,000 vehicles. Since its inception, NexTraq has been a pioneer and innovator in the telematics space with its award-winning platform and mobile app.
The NexTraq solution is a cloud-based application that enables service and distribution businesses to optimize fleet operations while reducing operational costs and maximizing revenue. To maintain our top position in the industry, we are looking to continually attract extraordinary individuals who mirror our corporate culture, objectives and possess an entrepreneurial spirit.
NexTraq does not discriminate on the basis of race, creed, color, ethnicity, national origin, religion, sex, sexual orientation, gender expression, age, height, weight, physical or mental ability, veteran status, military obligations, or marital status."
383,Data Engineer,Sarepta Therapeutics,United States•Remote,"$88,000 - $110,000 a year","[We generate a massive amount of biological data from high throughput, clinical and non-clinical experiments for our scientific programs. We are currently searching for a highly motivated data engineer to be a part of our Genomics and Data Science team. In this role, the individual will be responsible for contributing towards the development and maintenance of an efficient data platform and will work closely with other computational and bench scientists to support data-driven solutions for a range of research projects. It is an excellent opportunity to work at the forefront of precision genetic medicine, making an impact on the development of transformative therapies that may change the lives of patients.
Primary Responsibilities Include:
Develop and maintain an ecosystem of proprietary, partnered, public, and literature-curated datasets, and make them easily accessible to support R&D programs
Develop a framework to manage internally generated and/or externally acquired NGS data, and results of bench assays, and make them easily accessible to scientists
Clean, restructure, and enrich new datasets and perform exploratory analysis to determine data quality and usability
Collaborate cross-functionally to manage data warehousing and cloud infrastructure, and ensure compliance where necessary
Develop interactive applications and visual dashboards using R Shiny (preferred) or similar packages
Troubleshoot system environment, software, and workflow problems related to the deployment and operations of database and analytic pipelines
Create and maintain robust documentation of datasets and data workflow
In collaboration with computational scientists, evaluate proprietary and open-source tools to improve bioinformatics pipelines and implement them into research workflows
Support the analysis of biological data using statistical and computational methods to answer critical research questions
Desired Education and Skills:
BS/ MS / Ph.D. in a STEM discipline with prior data engineering experience. BS/MS with 4+ years of hands-on experience in the biotech/pharmaceutical industry or Ph.D. with 1+ years of professional experience.
Proficiency in designing relational and non-relational schemas and queries to capture and represent multi-type data
Experience developing and maintaining data warehouses
Experience working with cloud computing services, database systems, workflow management systems, and development tools in a production environment: AWS, Docker, Nextflow, GitLab, database technologies, and API standards
Proficient in test-driven development with one or more scripting languages: R and/or Python
Applied knowledge of data modeling, ETL, reporting tools, and data governance for structured and unstructured data
Experience developing visual dashboards or analytic applications (preferred)
Experience in working with bioinformatics pipelines and understanding of nuances of such pipelines
Ability to work on Linux environment and automate workflows with it (preferred)
Excellent interpersonal skills, a team player, and a continuous learner
#LI-Remote
The targeted salary range for this position is $88,000 - $110,000 per year. Sarepta is making a good faith effort to be transparent and accurate around our hiring ranges. The salary offer is commensurate with Sarepta’s compensation philosophy and considers factors including, but not limited to, education, training, experience, external market conditions, criticality of role, and internal equity.
Candidates must be authorized to work in the U.S.
Sarepta Therapeutics offers a competitive compensation and benefit package.
Sarepta Therapeutics is an Equal Opportunity/Affirmative Action employer and participates in e-Verify."
384,Data Engineer,WCF Insurance,"Sandy, UT 84070•Hybrid remote",N,"Do you love researching data and applying your findings to create the best data structures for data analytics? Do you enjoy digging into all the details as you work through a data set and identifying patterns and relationships? If so, this job may be a fit for you! Keep reading and watch our video below to see why WCF was voted one of the best places to work again!
Position
The IT department has an immediate opening for someone who can demonstrate the WCF values to join their team as Data Engineer. This is a full-time, exempt position based out of WCF's Sandy, Utah headquarters. This position is open to internal and external candidates. This position is available or hybrid remote/in office work.

Responsibilities
Understand data requirements, concepts, data types and structures.
Understand and document business rules and cases to be applied to the ETL processes.
Develop and maintain code in the ETL processes using stored procedures, T-SQL, and SSIS, against a variety of data sources such as PostgreSQL, Microsoft SQL Server, flat files, etc.
Collaborate and participate in code review with peers.
Identify facts and dimensions required by data consumers to obtain reporting and analytical data requirements.
Identify, quantify, and propose automated solutions to address data integrity and data quality issues.
Ensure ETL processing uses standards, has good performance, and produces data with consistency and accuracy.
Implement quality insurance automated testing for data completion, accuracy, integrity, and consistency.

Qualifications
The most qualified candidate will have:
Proven work experience as an ETL or data developer.
Clear written and oral communication skills.
Strong data analytical skills to identify patterns, relationships, and flows, and troubleshoot data inconsistencies.
Working experience designing and developing data warehouse dimensional modeling.
Good understanding of SQL Server, including creating tables, stored procedures, views, setting up jobs, and troubleshooting code and data issues.
Experience with T-SQL, query performance tuning, indexing, and high-performance stored procedures.
Working experience developing SSIS packages.
Bachelor's degree in information technology or a related field and at least two years of work experience (or six years of directly related experience without a degree).
Ability to work well with others through communication and collaboration.

WCF INSURANCE DE&I MISSION
Promote and embrace a diverse, inclusive, equitable, and safe workplace.

WCF INSURANCE IS AN EQUAL OPPORTUNITY EMPLOYER
WCF Insurance provides equal employment opportunity to all qualified applicants and employees regardless of race, color, religion, sex, age, national origin, veteran status, disability that can be reasonably accommodated, or any other basis prohibited by federal, state, or local law."
385,"Lead Engineer, Data Science - Remote",Anywhere Real Estate Inc,"Madison, NJ•Remote","$99,600 - $212,200 a year","Join us as we build a next-generation analytics platform and foundation to focus on better operating performance and enable greater growth. As a Lead Data Scientist, you’ll help build value-driven intelligent real estate analytics solutions with machine learning, deep learning, and reinforcement learning platforms to lay the foundation for optimizing agent and broker productivity and bring Anywhere renowned real estate brands into the next era of data-driven excellence.

What we’re looking for:
Data is the center of everything we do. We are looking for a candidate who loves data, is interested in data, and can use data to change the world for good.
You’re an experienced, talented, and motivated Lead Data Scientist who understands the business needs, has great problem-solving skills, and can independently take on tasks to drive to the solution. You are also able to mentor and lead teammates to use machine learning, deep learning model, and AI tools to solve the business problem and make an impact on the larger organization

What you’ll do:
As Lead Data Scientist, you’ll be responsible for mastering our data and working closely with the product team to design applied data science products that enable us to use it most effectively. You’ll be designing, developing, and implementing machine learning and predictive analytics models that meet the needs of millions of agents, brokers, home buyers, and sellers. Your role will involve working with a team of talented data scientists, engineers, product managers, and designers to investigate new data sources, perform data modeling, and deploy new features.

Skills, accomplishments, & interests you should have:
Master or Ph.D. in Economics, Statistics/Data Science, Mathematics, or related analytical discipline or equivalent combination of training and experience
At least 3 years of Data Science/Analytics experience
Strong communication skills
Strong problem-solving and integrative thinking skills
Proven experience with machine learning data modeling projects and model development for solving business problems with demonstrated expertise in data science and machine learning fundamentals
Solid knowledge of Python or R for statistical analysis, modeling, and visualization (pandas, numpy, scipy, scikit-learn, etc)
Expertise with a range of machine learning modeling techniques: Linear Regression, Random Forest, GBM, XGboost, GLM, SVM, etc.

#LI-ME1
#LI-Remote
Employment Type
Full-time
Company
Anywhere Real Estate Inc
Exciting News
We are excited to announce that Realogy is now Anywhere Real Estate Inc. It will take a few months for us to transition to our new brand. For more information about this change, please click here.
About Us
Anywhere Real Estate Inc. (NYSE: HOUS) is on a mission to empower everyone’s next move. Home to some of the most recognized brands in real estate Better Homes and Gardens® Real Estate, Century 21®, Coldwell Banker®, Coldwell Banker Commercial®, Corcoran®, ERA®, and Sotheby's International Realty® - the Anywhere portfolio includes franchise and brokerage operations as well as national title, settlement, and relocation companies and nationally scaled mortgage origination and underwriting joint ventures. Anywhere is focused on simplifying, digitizing and integrating the real estate transaction for all consumers, no matter where they may be in their home buying and selling journey. With innovative products and technology, Anywhere fuels the productivity of its approximately 196,200 independent sales agents in the US and approximately 136,400 independent sales agents in 118 other countries and territories.
At Anywhere, diversity fuels success – for our company and for our employees. We strive to be the preferred company for diverse talent, committed to creating an inclusive environment that encourages everyone to succeed. We pursue talent – strategic thinkers who are eager to innovate, focused on execution and accountable for results. We value diversity – respecting backgrounds, cultures, perspectives.
You’ll find our commitment to diversity reflected in our achievements:
Recognized as one of the World’s Most Ethical Companies since 2011
Anywhere has also been designated a Great Place to Work since 2019
Named one of LinkedIn’s Top Companies in the US.
Honored by Forbes as one of the World’s Best Employers for Diversity and Top Female Friendly Companies.
With diversity, we succeed together. We hope you’ll join us.

Compensation Range
$99,600 - $212,200; At Anywhere, actual compensation within that range will be dependent upon the individual’s skills, experience, and qualifications.
EEO Statement
EOE AA M/F/Vet/Disability"
386,Performance Driven Marketing - Data Engineer,General Motors,United States•Remote,"$124,504 - $190,696 a year","Job Description

We are seeking a talented and experienced Data Engineer and Steward to drive the development of our data products and services. The successful candidate will work closely with stakeholders to define product requirements, set priorities, and ensure the timely and successful delivery of data-driven solutions. The role requires a deep understanding of the data landscape, strong technical skills, and the ability to translate business needs into data solutions. This role will report to the Head of Marketing Data Ecosystem within GM's Performance Driven Marketing (PDM) team.

As a data engineer, you will be responsible for designing, building, and maintaining large-scale data processing systems and pipelines. You will work closely with stakeholders, product owners, business systems analysts, and analytics teams to understand data requirements, identify potential solutions, and develop high-quality data solutions. You will participate in defining architecture and modeling best practices within the team, and will use your expertise in data warehouse concepts, practices, and procedures to plan and accomplish goals.

The ideal candidate has a strong digital background and demonstrated ability to deliver scalable digital data solutions consisting of wide breadth of data sources. Strong knowledge of MarTec and digital marketing ecosystem across a variety of technology and subject matters is preferred. Global experience a plus. A proven track record of implementing cloud-oriented data solutions is a must. Knowledge and expertise of Marketing is preferable (email, CMS, AdTech, websites, etc.)

Key Responsibilities:
Design and build modern data solutions to enable marketing operation and analytics, including the development of data pipelines, ETL processes, data warehouses, and data lakes.
Collaborate with data scientists and analysts to design and implement scalable data solutions that meet business requirements.
Work with cross-functional teams to identify and prioritize data needs and develop strategies for data acquisition and integration.
Work on transformation of data into Facts and Dims to support analytics and reporting needs.
Implement and maintain data quality checks to ensure data accuracy and reliability.
Develop and implement best practices for data security, data management, and data governance.
Troubleshoot and resolve data-related issues, including performance, scalability, and availability.
Keep up-to-date with emerging trends and technologies in data engineering and identify opportunities to enhance our data infrastructure.

Additional Job Description

Job Related Experience:
Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field.
At least 5-7 years of professional experience in data-intensive programs, in roles that were responsible for data architect, data product design and implementation.
Proven experience as a Data Engineer or a similar role.
Strong knowledge of data modeling, data warehousing, and ETL techniques.
Experience with data integration tools and technologies.
Proficiency in SQL and one or more programming languages, such as Python, Java, or Scala.
Familiarity with cloud computing platforms, such as AWS, Azure, or Google Cloud.
Experience with Big Data technologies, such as Hadoop, Spark, and Hive.
Strong problem-solving and analytical skills.
Excellent communication and collaboration skills.

Compensation:
The expected base compensation for this role is: ($124,504 - $190,696). Actual base compensation within the identified range will vary based on factors relevant to the position.
Bonus Potential: An incentive pay program offers payouts based on company performance, job level, and individual performance.

Benefits:
Benefits: GM offers a variety of health and wellbeing benefit programs. Benefit options include medical, dental, vision, Health Savings Account, Flexible Spending Accounts, retirement savings plan, sickness and accident benefits, life insurance, paid vacation & holidays, tuition assistance programs, employee assistance program, GM vehicle discounts and more.

#LI-Remote, #gmremote

This is a remote position, the selected candidate can reside and perform the work from anywhere within the United States. We are unable to consider candidates who require sponsorship.

About GM

Our vision is a world with Zero Crashes, Zero Emissions and Zero Congestion and we embrace the responsibility to lead the change that will make our world better, safer and more equitable for all.

Why Join Us

We aspire to be the most inclusive company in the world. We believe we all must make a choice every day - individually and collectively - to drive meaningful change through our words, our deeds and our culture. Our Work Appropriately philosophy supports our foundation of inclusion and provides employees the flexibility to work where they can have the greatest impact on achieving our goals, dependent on role needs. Every day, we want every employee, no matter their background, ethnicity, preferences, or location, to feel they belong to one General Motors team.

Benefits Overview

The goal of the General Motors total rewards program is to support the health and well-being of you and your family. Our comprehensive compensation plan incudes, the following benefits, in addition to many others:
Paid time off including vacation days, holidays, and parental leave for mothers, fathers and adoptive parents;
Healthcare (including a triple tax advantaged health savings account and wellness incentive), dental, vision and life insurance plans to cover you and your family;
Company and matching contributions to 401K savings plan to help you save for retirement;
Global recognition program for peers and leaders to recognize and be recognized for results and behaviors that reflect our company values;
Tuition assistance and student loan refinancing;
Discount on GM vehicles for you, your family and friends.

Diversity Information

General Motors is committed to being a workplace that is not only free of discrimination, but one that genuinely fosters inclusion and belonging. We strongly believe that workforce diversity creates an environment in which our employees can thrive and develop better products for our customers. We understand and embrace the variety through which people gain experiences whether through professional, personal, educational, or volunteer opportunities.GM is proud to be an equal opportunity employer.

We encourage interested candidates to review the key responsibilities and qualifications and apply for any positions that match your skills and capabilities.

Equal Employment Opportunity Statements

GMis an equal opportunity employer and complies with all applicable federal, state, and local fair employment practices laws. GM is committed to providing a work environment free from unlawful discrimination and advancing equal employment opportunities for all qualified individuals. As part of this commitment, all practices and decisions relating to terms and conditions of employment, including, but not limited to, recruiting, hiring, training, promotion, discipline, compensation, benefits, and termination of employment are made without regard to an individual's protected characteristics. For purposes of this policy, ""protected characteristics"" include an individual's actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth, lactation and related medical conditions), gender identity or gender expression, sexual orientation, weight, height, marital status, military service and veteran status, physical or mental disability, protected medical condition as defined by applicable state or local law, genetic information, or any other characteristic protected by applicable federal, state or local laws and ordinances. If you need a reasonable accommodation to assist with your job search or application for employment, email us atCareers.Accommodations@GM.comor call us at 800-865-7580. In your email, please include a description of the specific accommodation you are requesting as well as the job title and requisition number of the position for which you are applying."
387,Google Cloud Data Engineer,Cirruslabs,"Atlanta, GA","$115,085 - $140,232 a year","Essential Functions:
Drives world-class design and development of analytical data pipelines.
Provides thought leadership and proposes industry standards and implementations.
Adhere to processes to ensure data pulled from various sources meets quality standards, is curated and enhanced for analytical use and there is a ""single source of truth ""
Work with counterparts from Tech to build frameworks that integrate data pipelines and machine learning models that facilitate use by data scientists for priority use cases; Enterprise Data and Analytics team focused on ""last mile "" transformations on select data required for use cases
Maintain database structure and standard definitions for business users across Macy's
Work with data architects to build the foundational extract / load / transform process and regularly review the architecture and recommend effectiveness improvements
Collaborate with Technology to future-proof data & analytics software, tools, and code to reduce risk and support pipeline owners
Work with Legal and Privacy teams to adhere to data privacy and security standards
Work with Data Architect to implement the data models, standards, and quality rules
Work with the Data Science team to understand data formatting and sourcing needs to enable them to build out use cases as efficiently as possible.
Keep data separated and secured using masking and encryption.
Explain the requirement to offshore team and create
Daily onsite-offshore coordination
Qualifications and Competencies
At least 8 years of overall experience in building ETL/ELT, data warehousing and big data solutions.
At least 5 years of experience in building data models and data pipelines to process different types of large datasets.
At least 3 years of experience with Python, Spark, Hive, Hadoop, Kinesis, Kafka.
Proven expertise in relational and dimensional data modeling.
Understand PII standards, processes, and security protocols.
Experience in building data warehouse using Cloud Technologies such as AWS or GCP Services, and Cloud Data Warehouse preferably Google BigQuery.
In depth knowledge in Big Data solutions and Hadoop ecosystem.
Strong SQL knowledge able to translate complex scenarios into queries.
Strong Programming experience in Java or Python
Experience in Google Cloud platform (especially BigQuery & Dataflow)
Experience with Google Cloud SDK & API Scripting
Experience in Hadoop ( Hive, MapReduce, Spark )
Experience in Onsite-Offshore coordination
Experience in Test Driven Development
Experience in Agile processes and DevOps methodologies.
Experience in NoSQL Databases
Experience with Data modeling and mapping.
Experience of Retail domain will be an added advantage
Google Cloud Professional Data Engineer Certification is an advantage.
Programming Language Java / Python
Google Cloud Big Query, Pub/ Sub, Dataflow, Composer DAGs, Cloud storage
CI/CD GitHub, Jenkins
Generic Managerial Skills
· Good organizational and problem-solving skills
· Good team player who is self-motivated and well organized
· Strong oral and written communication skills
· Ability to work with remote teams
· Ability to manage project scope
Job Types: Full-time, Contract
Salary: $115,085.46 - $140,231.95 per year
Benefits:
Health insurance
Experience level:
6 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road

Health insurance"
388,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
389,Data Engineer,Argano,Remote,N,"About Argano
Argano is a business modernization partner, purpose-built to give rise to the possibilities of the Digital Renaissance for companies with complex sales and operating environments. We innovate adaptive, efficient, cloud-based digital operating foundations on which the transformational businesses of the 21st century must be built. These modern, scalable, and sustainable foundations integrate operations from commerce to cash to close to consolidation and free our clients to innovate and respond in new and cost-effective ways. The Argano platform uniquely offers the advantage of integrated, world-class capability partners, working together to solve complex challenges across the full spectrum of our client's business. For more information, visit www.argano.com
Job Responsibilities:
Designing, constructing, testing, automating, and maintaining architectures and processing workflows
Building robust, efficient, and reliable data infrastructure to support eco-system
Integrating existing and new datasets into current inbound and outbound data pipelines
Supporting the effective development and deployment of new products such as database tables and views to support dashboards or reports
Driving the collection of new data and refinement of existing data sources
Managing, developing and documenting data and software quality controls for all data engineering activities, including data mapping across systems
Ongoing development and documentation of data engineering methods and tools to transform data into suitable structures for analysis.
Managing a structured testing programme for data workflows and data engineering tools developed and deployed.
Qualifications and Experience Levels:
Minimum 4+experience in related field
Bachelor of computer science filed and professional data engineering qualification.
Personal Attributes:
Experience with AWS architecture
Experience with Relational Databases (Redshift, PostgreSQL)
Experience with Non-Relational Data storage (Parquet, ORC)
Experience with Fivetran data movement platform (ETL)
Experience with SQL
Excellent communication skills (Written & Verbal) – should be able to communicate with various levels in the organization
Excellent people and stakeholder management skills
Analytical and problem-solving skills.
Analytical and Logical Thinking.
Open to travel between locations.
#Argano"
390,Data & Analytics Platform Engineer,Blue Cross and Blue Shield of North Carolina,"Durham, NC",N,"This role is responsible for administrating and implementing various platforms, including analytics, metadata, and governance. Responsible for architecting and engineering connections between various platforms, including middleware. Analyze current and future requirements to design analytics platform solutions while determining design feasibility effectively. Responsibilities will also include maintaining technical standards, procedures, and documentation in partnership with service partners as needed. Partner and consult with varied technical, business, project, and management staff to identify, plan, and assess platform designs and impacts while developing solutions to resolve design problems.

What You'll Do

Implement and administer platforms utilized within the organization.

Ensure data scientists, data engineers, and analysts have access to platform data.

Modify platforms to fit the needs of Data and Analytics and support advanced analytics.

Support the design of internal data automation projects by collaborating with IT stakeholders, service owners, and partners.

Provide guidance and consultation to stakeholders on platform capabilities.

Maintain appropriate standardized guidelines and processes to expedite workflow and ultimately improve efficiencies.

What You'll Bring!

Must have Adobe Analytics experience.

Experience with JavaScript is a plus.

Hiring Requirements

Bachelor's degree or advanced degree (where required)

3+ years of experience in related field.

In lieu of degree, 5+ years of experience in related field."
391,Data Engineer,Gateway Professional Network,Remote,"$95,000 - $145,000 a year","Apply Directly: https://gatewaypn.applicantstack.com/x/detail/a2bwypr26k74
Data Engineer
Job Description for Full-Time Position
Location: US Remote (must reside in US and we are unable to sponsor Visas at this time)
Pay Scale: $95,000 – $145,000 per Annum (based on experience)
Reports to: Ramon Navarro
Job Description Summary:
Individual to contribute to GPN Tech transformation assuming accountability for data engineering lifecycle including research, proof of concepts, design, development, test, deployment, and maintenance of enterprise-scale data integration solutions leveraging Microsoft Azure PaaS offerings. Extensive experience working on Agile Scrum/DevOps teams employing the latest CI/CD cloud-first best practices. Potential to become an SME on Platform capabilities (ingestion, storage, processing, and presentation patterns) and extend future-state strategic roadmap features.
What You’ll Do: Responsibilities
Provide technical direction to delivery scrum teams, extending patterns and establishing new ones as required
Create and review technical designs of data integrations across various systems within the enterprise and with external business partners over multiple transmission channels, data formats, and processing patterns
Extend/implement CICD pipelines and containerization strategy
Responsible for ETL and SQL development using Domain Architecture and Interface based programming
Build processes and diagnostics tools to troubleshoot, maintain and optimize solutions and respond to customer and production issues
Experience You’ll Need: Qualifications
Data Engineer with in-depth knowledge of developing data-driven solutions using Azure App Services, Function Apps, API Service Management, Active Directory, Azure Storage, Data Factory, Databricks, RedisCache, SQL DB, Key Vault, Service Bus, Event Hub and Application Insights
Bachelor’s degree in computer science or related discipline; r Equivalent Work Experience
5+ years of experience leveraging cloud data storage technologies specifically Microsoft Azure offerings
5+ years of experience developing on modern data ingestion tools/platforms/protocols (Kafka, APIs, EventHubs, ServiceBus, etc.)
5+ Experience with data integration best practices (ETL/ELT patterns, Data Factory, Streams Analytics, etc.)
5+ years of experience in ETL and SQL Programming
2+ years of experience in C# and T-SQL
3+ years of experience working with Microsoft Azure DevOps and CI/CD best practices
3+ years of experience working in an Agile Scrum environment
Experience with various data structure optimization techniques on a Data Streaming Platform
Experience with Power BI or other similar data visualization tools
Bonus Points: Additional qualifications if you have them!
Experience with Machine Learning and Artificial Intelligence based solutions
Knowledge of data regulations and compliance policies (e.g., HIPAA)
Data Visualization experience, or strong affinity for leveraging data to help transform/optimize business functions
Big Data experience
IT healthcare experience
IoT experience
Who We Are:
GPN Technologieswas founded in 2007 with the mission of providing “big business” infrastructure to independent practitioners in the ophthalmic industry. The company’s driving initiative has been empowering independents to be profitable and competitive in today’s market by making high-tech, business-critical tools accessible and meaningful in small business settings.
Our analytics platforms have revolutionized the way practitioners view, understand, and act on their business data. We have continued to expand and hone those platforms to help those practitioners succeed and thrive in an increasingly competitive, fast-paced marketplace.
Thousands of practitioners across the country are serving their patients, their team members, and their bottom lines more efficiently with EDGEPro.
Job Type: Full-time
Pay: $95,000.00 - $145,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Paid time off
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
Do you require Visa sponsorship?
Experience:
Relational databases: 2 years (Preferred)
Azure: 2 years (Preferred)
Big data: 2 years (Preferred)
SQL: 2 years (Required)
Microsoft SQL Server: 2 years (Required)
C#: 2 years (Required)
.NET: 3 years (Required)
Work Location: Remote"
392,Sr. Data Engineer,Hayward Industries,"1220 South Tryon St, Charlotte, NC 28203","$120,000 - $150,000 a year","Hayward Holdings Inc. (NYSE ""HAYW"") is the largest manufacturer of residential swimming pool equipment in the world, with a significant presence in the commercial pool market that is continuously growing. Hayward designs, manufactures and markets a full line of residential and commercial pool and spa equipment including pumps, filters, heating, cleaners, salt chlorinators, automation, lighting, safety, flow control and energy solutions at our company owned facilities. Headquartered in Charlotte, North Carolina, Hayward also has facilities in Tennessee, Arizona, and Rhode Island as well as Canada, Spain, France, Australia, and China. We are continuing to grow so jump in and join the team in Charlotte, NC!
The Global Data and Analytics organization uses cloud-base technology for data integration and data warehouse using Talend and Snowflake. This role will be supporting the senior executive team globally with their Data and Analytics data ingestion and transformation needs utilizing the existing technologies. This role is full-time midlevel role focusing on analyzing, building, and designing business intelligence models and solutions.
The role is key in synthesizing and distilling meaningful and accurate data for the decision makers at Hayward. Responsibilities include building and supporting BI solutions using Talend, Matillion, Snowflake, and other exciting technologies of data integration. This role also provides accurate level-of-effort estimates for data integration, creates data integration solutions, develops documentation to support production deployments, and supports data solutions maintenance.
Location:
Clemmons, NC (Piedmont Triad Area - Winston Salem, Greensboro, High Point) area with the ability to work remotely part of the time
Charlotte, NC area with the ability to work remotely part of the time
This position is not open to sponsorship at the time.
Responsibilities:
Design, develop, implement, and support new and existing data integration jobs using modern technologies.
Ingest data from multiple sources including transactional and operational databases, implement complex business logic, and produce 3rd normal form and dimensional model database objects in Snowflake.
Migrate datasets between existing and future database platforms.
Understand and implement best practice solutions per development standards.
Produce ad-hoc data extracts to answer business questions quickly and thoroughly.
Collaborate with Project Managers, IT Architecture, Business Analysts, and Report Developers to achieve business objectives.
Develop technology documentation to support production deployments as well as ongoing maintenance of reporting and data solutions.
Troubleshoot, diagnose, and resolve data quality and performance issues related to data integration and transformation.
Communicate complex topics and analyses to non-technical business personnel.
Present and participate in Architecture Review Board meetings. Review solutions presented by other IT architects and provide constructive feedback.
Provide accurate level of effort estimates for data integration development to management.
Qualifications:
Bachelor’s degree in computer science, information technology, or related field. Equivalent working experience in-lieu of a bachelor’s degree would be considered
5+ years advanced hands-on experience building data integration, ingestion and transformation solutions using tools such as Matillion, Nifi, Kafka (Confluent Cloud), Talend, dbt and Airflow
4+ years advanced SQL skills, preferably in Snowflake or similar cloud data warehouse
3+ years hands-on experience in data warehousing, and particularly in star-schema architecture
3+ years working in a DevOps environment. Assist with production support issues by acting as a subject matter expert in resolving incidents and problem tickets
2+ years object-oriented programming language development (Python, Shell scripting, Scala/Java)
2+ years API and Web Services integration (REST/SOAP/Bulk)
1+ year experience with AWS and related products (such as Lambda, ECS, EC2, App Gateway and similar)
Experience developing production solutions with distributed architectures and processing technologies
Experience in implementing CI/CD (e.g. Github Pipelines, AWS cloud build)
Hands on experience working in a public cloud environment and on-prem infrastructure
Extensive experience in multiple operating systems: Unix, Windows, Linux
Ability to analyze and apply critical thinking to resolve complex issues
Strong problem solving and collaboration skills
Commitment, technical curiosity, and relentless pursuit of practical knowledge
Preferred Qualifications:
Experience in global company
Experience in manufacturing environment
Exposure to leading BI reporting tools, preferably cloud based, such as Tableau, Power BI, and Cognos
Understanding of statistical concepts and intuition
Comprehension of data science concepts including Machine Learning and Artificial Intelligence
Experience building data software in microservices-oriented architecture and extensible REST APIs
Experience with containerized environments (Kubernetes and Docker) for database pipeline applications
Experience with observability tools and frameworks such as Grafana, Prometheus, and Cloudwatch
Technical writing skills
Certification in Snowflake or similar Cloud Data Warehouse platform
Familiarity with Agile development methodologies such as Scrum and Kanban
Job Type: Full-time
Pay: $120,000.00 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Yearly bonus
Experience level:
2 years
3 years
Schedule:
Day shift
Monday to Friday
No weekends
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Charlotte, NC 28203: Reliably commute or planning to relocate before starting work (Required)
Experience:
Snowflake: 1 year (Required)
DBT: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: Hybrid remote in Charlotte, NC 28203

Health insurance"
393,Senior Data Engineer - 100% Remote,WW International,"New York, NY•Remote","$170,000 - $190,000 a year","WW is looking for candidates to help change people's lives. We are a global wellness technology company inspiring millions of people to adopt healthy habits for real life. We do this through engaging digital experiences, face-to-face workshops and sustainable programs that encourage people to move more, shift their mindset and eat healthier while enjoying the foods they love. By drawing on over five decades of experience and expertise in behavioral science, we build communities in order to deliver wellness for all.
Who we are
The Data Warehouse Engineering team has one aim - end to end ownership of all analytics needs of WW from sourcing data from anywhere as needed, to transforming the data into analyzable structures at scale - by building generic configurable data applications, to empowering the analytics , product and business teams to leverage the data to better help members in their wellness journey
Thus our mission and guiding principle is â€œWe are going to treat data as a product, if end users don't flock to it, we haven't built it well enough. Adoption comes from building something so accurate, timely, reliable, accessible and innovative that everyone wants to use itâ€�
What you will do
Build Kafka consumer data applications
Push the envelope with Snowflake data platform
Create complex yet maintainable workflows in Prefect using python
Leverage new stack concepts such as ReverseETL to empower Marketing teams using platforms like HighTouch
Rethink how Python and SQL can work together to tame data challenges
Creatively solve for Data Observability leveraging things like DataDog , OpenTelemetry , MonteCarloData , Anomolo or equivalent
Create reusable , inheritable models in Looker that serves broad use cases from direct exploration and dashboarding to serving as a logical data layer for various integrations
Who you are
5+ years of experience in aspects of data engineering
2+ years in cloud native Data Warehouse technologies
Experience with building SQL based DW systems
Experience in writing modular , testable , though complex , ELT and ETL pipelines
Leverage a high level language - ideally Python, to build data applications
Wrestled with the trade offs involving design of data warehouses or data applications
Base salary may vary depending on, but not limited to: skills, experience, and location.
Base Salary
$170,000/yr to $190,000/yr
#LI-Remote
At WW, it is our priority to cultivate a diverse and inclusive workplace. We are committed as individuals, as an organization, and as fellow humans, to advocate for and support our employees, our members, and our communities. We are proud to be an equal opportunity employer and we do not discriminate on the basis of sex, race, color, creed, national origin, marital status, age, religion, sexual orientation, gender identity, gender expression, veteran status, or disability."
394,Data Engineer 3,PayPal,"San Jose, CA","$72,700 - $176,000 a year","At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.

Job Description Summary: This role is to support all the CRYPTO activities (CRYPTO Initiatives, product assessment and impact on existing reports for crypto, new crypto reporting obligations, enhancement to crypto reporting with new product changes(technical/functional)

Job Description:
Lead, develop, and grow a high performance, multi-function team of talented and passionate professionals, who are results driven to take the business forward and demonstrate superior leadership in line with the PayPal values.
Build scalable systems on Hadoop/Google Cloud Platform (GCP) Platform and integrate with multiple applications.
Lead technical discussions, participate in code reviews, guide the team in engineering best practices. Must be able to write quality code and build secure, highly available systems. 75% of the job requires production quality coding.
Provide technical leadership and contribute to the definition, development, integration, test, documentation, and support across multiple platforms (GCP, SAP HANA, FI/CO, Bank Analyzer(sAFI/FPSL), S/4HANA, Hadoop, and Python).
Evaluate requirements, map them to business processes, create functional and solution architectures, and ensure a proper implementation.
Architect the end-to-end functional software landscape and solution design to support the to-be business processes.
Provide direction to the implementation teams and work as intermediary across the local, near-shore and offshore teams.
Must have demonstrably strong interpersonal and communication skills (both written and verbal), to include speaking clearly and persuasively in positive or negative situations
Experienced in balancing production platform stability, feature delivery, and retirement of technical debt across a broad landscape of technologies

PayPal is committed to fair and equitable compensation practices.

Actual Compensation is based on various factors including but not limited to work location, and relevant skills and experience.

The total compensation for this practice may include an annual performance bonus (or other incentive compensation, as applicable), equity, and medical, dental, vision, and other benefits. For more information, visit https://www.paypalbenefits.com.

The U.S. national annual pay range for this role is $72700 to $176000

Our Benefits:
At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:
To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.

PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.

As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated."
395,Data Engineer,PitchBook Data,"1601 Union St, Seattle, WA 98101","$96,600 - $164,450 a year","At PitchBook, we are always looking forward. We continue to innovate, evolve and invest in ourselves to bring out the best in everyone. We're deeply collaborative and thrive on the excitement, energy and fun that reverberates throughout the company.
Our extensive mentorship, education and training programs help us create a culture of curiosity that pushes us to always find new solutions and better ways of doing things. The combination of a rapidly evolving industry and our high ambitions means there's going to be some ambiguity along the way, but we excel when we challenge ourselves. We're willing to take risks, fail fast and do it all over again in the pursuit of excellence.
If you have a good attitude and are willing to roll up your sleeves to get things done, PitchBook is the place for you.

About the Role:
The PitchBook Business Intelligence team embraces this data-driven ethos by delivering key analytics and insights internally to every facet of our business. Our work is manifested as tools and data to help the enterprise grow and run more efficiently at both strategic and tactical levels. Simply put, we believe that the data we build is our competitive advantage for making the most of our resources while we bring the most compelling product possible to market.
To that end, as our scope of data integration and analysis expands so do the needs of the Business Intelligence team. We're looking for a person with the ability to work with a range of data and reporting technologies (eg. Python, Docker, Tableau, Power BI) in order to build upon a strong foundation of rigor, quantitative techniques and efficient processing. The Data Engineer will join other Engineers and Analytics professionals as part of the team that develops data pipelines and insights for our internal stakeholders across Sales, Customer Success, Marketing, Research, Product, Finance and Administration.

Primary Job Responsibilities:
You'll be PitchBook's expert at building unified data tech to support advanced and automated business analytics
Design, develop, document and maintain database and reporting structures used to compile insights
Define, develop and review extract, transform and load processes and data modeling solutions
Consistently evolve data processes and techniques in accordance with industry best practices
Establish and help define reports and dashboards used to translate business data into insights, identify and prioritize operational improvement opportunities and measure business performance against objectives
Contribute to the ongoing improvement of quality assurance standards and procedures

Skills and Qualifications:
Bachelor's degree in Economics, Business, Finance, Engineering, Statistics, Computer Science or related fields
2 years of relevant work experience creating and maintaining data pipelines and architecture
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Experience with software programs, such as Tableau, Microsoft Power BI, Docker, Linux and Postgres
Ability to display complex quantitative data in a simple, intuitive format and to present findings in a clear and concise manner
Capable of investigating, familiarizing and mastering new data sets quickly
Excellent interpersonal skills, with the ability to communicate complex data issues correctly and clearly to both internal and external customers
Experience with presenting actionable insights to business stakeholders
Experience with: Airflow, Luigi, Amazon Web Services, Microsoft Azure, Git, Postgres, Debezium and Kafka is preferred
Experience with Snowflake development and cloud data warehousing is preferred

Benefits + Compensation at PitchBook:
Physical Health
Comprehensive health benefits
Additional medical wellness incentives
STD, LTD, AD&D and life insurance

Emotional Health
Paid sabbatical program after four years
Paid family and paternity leave
Annual educational stipend
Ability to apply for tuition reimbursement
CFA exam stipend
Robust training programs on industry and soft skills
Employee assistance program
Generous allotment of vacation days, sick days and volunteer days

Social Health
Matching gifts program
Employee resource groups
Subsidized emergency childcare
Dependent Care FSA
Company-wide events
Employee referral bonus program
Quarterly team building events

Financial Health
401k match
Shared ownership employee stock program
Monthly transportation stipend

Please be aware the above PitchBook benefit and perk offerings are subject to corresponding plan and policy documents and may change during the course of your employment.

Compensation
Annual base salary: $96,600-$164,450
Target annual bonus percentage: 10%

Starting pay will be based on several factors and commensurate with qualifications & experience. We also have a location-based compensation structure; there may be different ranges for candidates by location.

Life At PB:
We are consistently recognized as a Best Place to Work and our culture is at the heart of our success. It's our fundamental belief that people do and create great things and that people are the cornerstone of prosperity. We believe that proactively seeking out different points of view, listening to others, learning and reflecting on what we've heard creates a sense of belonging within PitchBook and strengthens the PitchBook community.

We are excited to get to know you and your background. Concerned that you might not meet every requirement? We encourage you to still apply as you might be the right candidate for the role or other roles at PitchBook.

#LI-BL1"
396,Midlevel Software Data Engineer,Peraton,"Laurel, MD 20723","$146,000 - $234,000 a year","Responsibilities:
Peraton is hiring a Software Data Engineer to join our Cyber Mission business unit in Fort Meade, Maryland. As a Software Data Engineer on our team, you will support one of the largest enterprise-wide engineering contracts within the Intelligence Community. This is a full-time position requiring 1880 hours of support per year; and work is performed at a customer location. As an Enterprise Infrastructure Software Data Engineer on our team, you will develop, maintain, and enhance complex and diverse software systems based upon documented requirements. You will review and test software components for adherence to the design requirements and document test results; utilize software development and software design methodologies appropriate to the environment; and provide specific input to the software components of system design to include the use of Commercial Off-The-Shelf (COTS) / Government Off-The-Shelf (GOTS in place of new development. You will also develop software solutions by analyzing system performance standards, confer with users or system engineers; analyze usage and work processes; and investigate problem areas.
Qualifications:
Individual Capabilities/Experience Required:
Five (5) years of demonstrated experience in Software Engineering with a Bachelor's degree in Engineering, Computer Science, Mathematics, or appropriate field from an accredited college or university; or three (3) years of experience with a qualifying Master's degree; or zero (0) years of experience with a qualifying PhD. Six (6) years of Software Engineering experience with a high school diploma or GED would also be acceptable.
Position requires TS/SCI clearance with polygraph
#LI-AH1 #PeratonIntelCareers
Applicants selected will be subject to a government security investigation and must meet eligibility requirements for access to classified information.

Peraton is an Equal Opportunity/Affirmative Action Employer. We consider applicants without regard to race, color, religion, age, national origin, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, marital status, veteran status, disability, genetic information, citizenship status, or membership in any other group protected by federal, state, or local law.
Peraton Overview:
Peraton drives missions of consequence spanning the globe and extending to the farthest reaches of the galaxy. As the world’s leading mission capability integrator and transformative enterprise IT provider, we deliver trusted and highly differentiated national security solutions and technologies that keep people safe and secure. Peraton serves as a valued partner to essential government agencies across the intelligence, space, cyber, defense, civilian, health, and state and local markets. Every day, our employees do the can’t be done, solving the most daunting challenges facing our customers.
Target Salary Range: $146,000 - $234,000. This represents the typical salary range for this position based on experience and other factors. EEO Tagline (Text Only): An Equal Opportunity Employer including Disability/Veteran."
397,Data Engineer/Analyst,Labcorp,"Durham, NC 27709",N,"Labcorp is recruiting a Data Engineer/Analyst for a dynamic team in either Burlington or RTP, NC.
Get ready to redefine what’s possible and discover your extraordinary potential at Labcorp. Here, you’ll have the opportunity to personally advance healthcare and make a difference in peoples’ lives with your bold ideas and unique point of view. With the support of exceptional people from across the globe and an energized purpose, you’ll be empowered to own your career journey with mentoring, training and personalized development planning.

At Labcorp we believe in the power of science to change lives. We are a leading global life sciences company that delivers answers for crucial health questions because we know that knowledge has the potential to make life better for all. Through our unparalleled diagnostics and drug development capabilities, we provide insights and accelerate discoveries that not only empower patients and providers but help medical, biotech and pharmaceutical companies transform ideas into innovations.
Overview:
Reviews, evaluates, and maintains data for computer processing. Analyzes data for system performance and functionality. Works directly with users to resolve data conflicts. Recommends methods, tools, or software to maximize performance.
Skill Requirements
Position Responsibilities And Expected Outcomes:
Work on complex data initiatives with broad impact and act as key participant in large scale software planning for the Technology area
Perform data analysis and modelling tasks within a data warehouse environment.
Perform SQL queries to analyze and troubleshoot issues.
Discover problems in data and applications and design solutions with engineering and product leadership.
Strategically collaborate and consult with internal partners to resolve highly risky data engineering challenges
Demonstrated ability to solve complex data engineering problems; end to end problem resolution and continuous improvement mindset.
In-depth knowledge and experience with Data Engineering essentials such as scripting languages, source control, workflow scheduling, relational and non-relational SQL, and development of complex data solutions
Required Qualifications:
2+ years of Data Engineering experience, or equivalent demonstrated
2+ years of relevant Python development experience
Experience with Databricks , Spark, Hive, AWS EMR/S3, Data Stage or similar systems for performance.
Experience with AWS technologies like Lambda, S3
Familiarity with modern build pipelines, tools, CI/CD concepts
Experience in data modelling within a data warehouse environment
Desired qualifications:
ETL (Software agnostic) Experience
Databricks, Hive, Datastage and Oracle SQL Experience
Experience in query and/or SQL tuning
CI/CD Tool Experience
Experience working in an Agile Team
Understanding of SDLC Requirements
Knowledge on mainframe
License/Certification/Education:
Normally requires a B.S. Degree in Computer Science w/1-3 years of relevant experience.
Labcorp is proud to be an Equal Opportunity Employer:
As an EOE/AA employer, Labcorp strives for diversity and inclusion in the workforce and does not tolerate harassment or discrimination of any kind. We make employment decisions based on the needs of our business and the qualifications of the individual and do not discriminate based upon race, religion, color, national origin, gender (including pregnancy or other medical conditions/needs), family or parental status, marital, civil union or domestic partnership status, sexual orientation, gender identity, gender expression, personal appearance, age, veteran status, disability, genetic information, or any other legally protected characteristic. We encourage all to apply.
For more information about how we collect and store your personal data, please see our Privacy Statement."
398,Senior Data Engineer (Remote or Hybrid options available),WALGREENS,"Deerfield, IL 60015•Remote","$125,000 - $155,000 a year","Job Summary
We are looking for an experienced Senior Data Engineer looking to join our diverse technology team and help us take things to the next level. If this is you, keep reading!

The Sr. Data Engineer builds and maintains big data pipelines to support advanced analytics and data science solutions. The role also identifies valuable internal and external data, and collaborates closely with data scientists to define data for the design, development, and deployment of new solutions that support strategic business priorities.

Job Responsibilities
Develops software that processes, stores and serves data for use by others.
Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.
Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.
Develops and maintains optimal data pipelines into the advanced analytics platform, including design of data flows, procedures, and schedules. Ensures that optimal data pipelines are scalable, repeatable and secure.
Troubleshoots software and processes for data consistency and integrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.
Anticipates and prevents problems and roadblocks before they occur.
Interacts with internal and external peers and managers to exchange complex information related to areas of specialization.
Collaborate with data scientists to prepare data for model development

Why Walgreens?
No matter where you are, life at Walgreens is driven by a culture of helping others. From our store aisles and warehouses to our headquarters and technology hub, there’s a spirit of excitement and innovation everywhere you look. A career at Walgreens means joining a company that’s been dedicated to our customers and the communities we serve for over 120 years. And these days, you’ll find that our people are just as passionate, committed and supported as ever before.

There's no better feeling in a job than helping people live more joyful lives through better health in the communities you serve. And that’s why a career at Walgreens feels so good. With plenty of learning and growth opportunities, exciting challenges and talented teams, you’ll have everything you need to see your future in a whole new way.

We want to keep on driving change, and we’re committed to investing even more in DE&I, adding new initiatives, strengthening our nationwide veterans training program and expanding our Business Resource Groups. We’re listening, learning and taking big and bold action to ensure equity and inclusion for everyone.

About Walgreens Boots Alliance
Walgreens Boots Alliance (Nasdaq: WBA) is a global leader in retail pharmacy, impacting millions of lives every day through dispensing medicines, and providing accessible, high-quality care. With more than 170 years of trusted healthcare heritage and innovation in community pharmacy, the company is meeting customers’ and patients’ needs through its convenient retail locations, digital platforms and health and beauty products.

Including equity method investments, WBA has a presence in more than 25 countries, employs more than 450,000 people and has more than 21,000 stores.

WBA’s purpose is to help people across the world lead healthier and happier lives. The company is proud of its contributions to healthy communities, a healthy planet, an inclusive workplace and a sustainable marketplace. WBA is a participant of the United Nations Global Compact and adheres to its principles-based approach to responsible business.

WBA is included in FORTUNE’s 2021 list of the World’s Most Admired Companies*. This is the 28th consecutive year that WBA or its predecessor company, Walgreen Co., has been named to the list.

More company information is available at www.walgreensbootsalliance.com

Basic Qualifications
Bachelor's degree and at least 4 years of experience in data engineering OR a Graduate Degree in a technical discipline and at least 2 years of experience in data engineering
Deep knowledge of SQL
At least 2 years of experience with REST API development
Experience establishing and maintaining key relationships with internal (peers, business partners and leadership) and external (business community, clients and vendors) within a matrix organization to ensure quality standards for service.
Experience diagnosing, isolating, and resolving complex business issues and recommending and implementing strategies to resolve problems.
Experience presenting to all levels of an organization
Willing to travel up to 10% of the time for business purposes (within state and out of state).

Preferred Qualifications
Advanced level skill in RDBMS (MSSQL, PostGres, Oracle, etc.), Document Databases (MongoDb, Cosmos Document, Couchbase, etc.), Key-Value Databases (Azure Table Storage, Cosmos Table, Amazon Dynamo, etc.)
Advanced level skill in Wide-Column Databases (Cassandra, MariaDB, etc.) and Graph Databases (Neo4J, Cosmos Gremlin, etc.)
Deep knowledge of Advanced level skill in Data Warehousing and Data Lakes
Advanced level skill in Data Migration and Transformation
Experience with Azure application data processing tools like ADLS, Databricks, ADF, Azure Flow, Synapse, Power BI
Experience with messaging streaming systems such as Kafka or Azure Event Hubs
Experience with Azure application deployment
Experience working with Data Scientists and Machine Learning Engineers.
Experience developing APIs a serving/data exposition layer after the ETL process.
Ability to assess the effectiveness and accuracy of new data sources and data gathering technique and sound understanding of SQL and NO SQL databases.
Familiarity with all standard integration protocols and data security implications – API Open standards, SSL, oAuth2, Voltage encryption
An employee in this position can expect a salary between $125,000 and $155,000 plus bonus pursuant to the terms of any bonus plan, if applicable will depend on experience, seniority, geographic locations, and other factors permitted by law. To review benefits, please click here jobs.walgreens.com/benefits. Walgreens will provide applicants in other states with information related to the positions, to the extent required by state or local law, by calling 1-866-967-5492."
399,Staff Data Engineer,Imagine Pediatrics,"Nashville, TN•Remote","From $170,000 a year","About Imagine Pediatrics:
We are Imagine Pediatrics, a devoted and compassionate pediatrician-led medical group that is creating the world our sickest children deserve. Founded on the belief that medically complex children deserve better care than the current system allows for, we deliver personalized and comprehensive care by partnering with families to provide unparalleled support, virtually and in-home, ensuring they spend more time in their homes and less in the hospital.
We're guided by five core values:
Children first. We put the best interests of children above all. We are uncompromising in our commitment to improving the lives of children and families by bringing the best care to them.
Earn trust. We cherish the trusting relationships we build with the children and families we serve, our partners, and each other. We seek first to understand and speak honestly.
Innovate today. We refuse to accept the way things have always been done. Children and families are waiting for our help and their bravery demands that we relentlessly challenge assumptions to develop new approaches.
Embrace humanity. We lead with empathy and authenticity, presuming competence and good intentions. When we stumble, we use the opportunity to learn and grow stronger.
One team, diverse perspectives. We work alongside families as one team towards a shared purpose. We champion diversity and acknowledge the contributions of others.

About the Role:
As Staff Data Engineer with Imagine Pediatrics, you will be responsible for designing, building and supporting our data pipelines and data integrations, with a focus on scale, stability and quality. You will find and build solutions to achieve goals and solve problems.
You will:
Be an effective communicator while interacting with technical and non-technical audiences
Communicate with business stakeholders to understand goals and translate them to technical architecture and requirements
Have an iterative, collaborative and transparent approach to building technical solutions and products
Estimate product scope and timelines with milestones
Lead and mentor other data engineers to follow best engineering practices
Produce technical solutions that satisfy business requirements with a focus on scalability, stability, efficiency, maintainability and extensibility
Build software using modern & stable technologies
About You:
First and foremost, you're passionate and committed to creating the world our sickest children deserve. You want an active role in building a diverse and values-driven culture. Things change quickly in a startup environment; you accept that and are willing to pivot quickly on priorities. In this role, you will need:
Deep understanding and experience with building end-to-end data pipelines in production environments using Javascript and/or Python
Experience hosting application services on cloud providers (AWS, GCP, Azure)
Experience with Snowflake, SQL & data analysis
Experience building data pipelines for healthcare data
Experience working with Medicaid/Medicare government plans strongly preferred
Experience with auxiliary data ingestion and transformation tools (dbt, Hevo, or similar)
Experience working in a fast-paced, hypergrowth, collaborative environment

(California, New York, Colorado only) Base salary starts at $170,000/yr. + bonus + equity participation program + full benefits including 401k with company match

We Value Diversity, Equity, Inclusion and Belonging
We believe that creating the world our sickest children deserve requires a diverse team with diverse perspectives. We're proud to be an equal opportunity employer. People seeking employment at Imagine Pediatrics are considered without regard to race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, marital or veteran status, age, national origin, ancestry, citizenship, physical or mental disability, medical condition, genetic information or characteristics (or those of a family member), pregnancy or other status protected by applicable law."
400,Data Analytics Engineer,Intelligent Medical Objects,Remote,N,"Research shows that women and underrepresented groups only apply to jobs only if they think they meet 100% of the qualifications on a job description. IMO is committed to considering all candidates even if you don’t think you meet 100% of the qualifications listed. We look forward to receiving your application!

Work that is meaningful. A job that has impact. Colleagues that inspire. That’s what you’ll find at Intelligent Medical Objects (IMO), a growing health IT company creating clinical terminology and insights solutions that are used by more than 740,000 US physicians and 4,500 US hospitals to power better patient care and support meaningful analytics.

The Data Analytics Engineer will support our software developers, database architects, data analysts, and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Join our growing Software Engineering department as a Data Analytics Engineer to help design, create, and support high quality solutions that support 80% of US clinicians and build the application of Data Engineering within IMO!
Responsibilities
Build complex visualizations that tell a story about data.
Embed analytical solutions into web and mobile applications using embedded analytics.
Design, develop and maintain business intelligence solutions using PowerBI/Tableau, SQL, and other tools
Maintain analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Develop proficiency in our tech stack for optimal ETL from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Deliver quality products leveraging the values of transparency, inspection, and adaptation in an agile way. Takes ownership to proactively anticipate the implications and consequences of situations and acts appropriately to make decisions.
Requirements
A relevant technical BA/ BS Degree and one year of experience, or three years of relevant professional experience implementing well-architected data pipelines that are dynamically scalable, highly available, fault-tolerant, and reliable for analytics and platform solutions.
Strong experience with SQL and data warehousing techniques. Work with relational and NoSQL databases such as PostgreSQL, Dynambo DB, MongoDB, and Elasticsearch.
Expertise in data visualization using tools like Power BI, Tableau, or angular/Kendo.
Experience with embedded analytics, including integrating BI dashboards into web and mobile applications.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
Experience with manipulating, processing, and extracting value from disconnected datasets.
Experience with AWS cloud services, such as EC2, EMR, RDS, Redshift.
Experience with object-oriented/functional scripting languages, such as Python, Java, C++, Scala, etc.
#LI-JI1
#LI-Remote

At IMO, we celebrate diversity and are committed to creating an inclusive environment for all employees. IMO is proud to be an equal opportunity workplace and is an affirmative action employer.

IMO also provides visa sponsorship opportunities. Please don't hesitate to apply if you meet all the qualifications for this position and require visa sponsorship."
401,Data Engineer General,Global Information Technology,"Dearborn, MI",N,"Job Title: Data Engineer General
Job Location: Dearborn, MI
Job Type: Contract

Job Description:
Experience with Alteryx, Qlikview, SQL, R, or Python a plus.
Experience creating Data Models and Data products.
Experience working with data in Informatica.
Experience working with data in SAP - S4HANA Strong collaboration and influencing skills, and the ability to energize a multi-functional team.
Proven problem formulation with the ability to take complex problems and break them down to build and implement an action plan.
Ability to communicate findings to make data analysis meaningful and understandable by Data Operations team members, business partners, including IT and analytic teams.
Ability to optimally communicate information and ideas in written and verbal formats, including process documentation.
Experience developing data standards.
Experience working in Hadoop, particularly with HDFS and Hive.
Experience Required:
3+ years of progressive responsibilities in managing data and data processes
Education Required:
Bachelor's degree in Business, Finance, Computer Science, Engineering, Statistics, Economics or equivalent experience.

Interested candidates can send their updated resumes at jobs@global-itech.com"
402,Data Engineer,Leidos,"8301 Greensboro Dr, McLean, VA 22102","$97,500 - $202,500 a year","Description
Job Description:
Leidos seeks a data engineer to provide programming support for loading large volume data collections into a variety of data holding areas both on premises and in the cloud. The successful candidate will join a highly collaborative, cross disciplinary team of data scientists, data engineers, and applications developers to deliver mission critical analytics support to our customer.
Primary Responsibilities:
Analyze new data collections and determine optimal data ingest processes
Create and maintain NiFi Schemas for new and existing data feeds
Develop tools or modify existing ones to preprocess, modify, aggregate, load, index, and archive data collections into clusters and AWS in near real time
Implement all proper access controls
Generate metrics to track data ingest optimization and ensure data integrity and provenance are maintained
Document data flows
Basic Qualifications:
Must have a Bachelor's degree in a relevant field and minimum of 8 years experience OR a Master's degree and at least 6 years of experience
Must have a TS/SCI with Poly to be considered
Experience in Computer Science, Computer Engineering, Systems engineering, or closely related discipline
Extensive experience in Extract-Transform-Load processes
Demonstrated extensive experience with cloud services (AWS strongly preferred)
Demonstrated experience with cloud based database services such as Databricks and EMR
Understanding of SQL database structures and mapping data structures between different SQL databases
Knowledge of API development techniques
Preferred Qualifications:
Demonstrated experience with AWS long-term storage options
Experience with NiFi strongly preferred, but would train if candidate has experience in Java or using Pentaho
Experience working with a Hadoop cluster
Familiarity with open source data engineering tools and processes
Experience designing for intelligence applications

Pay Range:
Pay Range $97,500.00 - $150,000.00 - $202,500.00
The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law."
403,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
404,Senior Engineer - Data Products,Headspace Health,"Santa Monica, CA•Remote","$104,981 - $157,476 a year","Headspace and Ginger have recently merged to become Headspace Health! While roles are still being recruited separately on our respective websites, new hires from this point forward will be joining Headspace Health. For more information, please speak with your recruiter!
About the Senior Engineer – Data Products at Headspace Health:
Headspace Health is seeking an experienced Senior Engineer to join our Data Products team (part of our Data Engineering org). In this role, you will be responsible for architecting and implementing a set of core data sets in our data lake. And your customers are our data consumers, including analysts, machine learning engineers and data scientists.
How your skills and passion will come to life at Headspace Health:
Design and implement mission critical data pipelines for our company
Help create a set of high-quality, composable data products for our data consumers
Write well designed, testable, performant, and efficient code that runs on Apache Spark and Delta Lake
Lead the development of a world-class data lake that meets the strict security, privacy, and compliance requirements of the healthcare industry
Collaborate with the data analytics team to calculate and deliver key business metrics
Collaborate with the data science and machine learning team to build data sets used for model training and development
Mentor and coach other engineers to build a data-first culture at the company
What you've accomplished:
5+ years professional software development
You've built high quality data pipelines before with comprehensive unit tests suites, data quality checks etc.
Experience with Apache Spark and Delta Lake are a plus, but not required
Self-starter with the ability to thrive in a fast-paced startup environment
Exceptional oral and written communication skills
Experience coaching and mentoring team members
Experience collaborating with product partners
BA/BS degree in Computer Science, Engineering or equivalent
Pay & Benefits:
The base salary range for this role is determined by a number of factors, including but not limited to skills and scope required, relevant licensure and certifications, and unique relevant experience and job-related skills. The base salary range for this role is $104,981-$157,476.
At Headspace Health, cash salary is but one component of our Total Rewards package. We're proud of our robust package inclusive of: base salary, stock awards, comprehensive healthcare coverage, monthly wellness stipend, retirement savings match, lifetime Headspace membership, unlimited, free mental health coaching, generous parental leave, and much more. Paid performance incentives are also included for those in eligible roles. Additional details about our Total Rewards package will be provided during the recruitment process.
How to get started:
If you're excited by the idea of seeing yourself in this role at Headspace Health, please complete our application process below.
Our Commitment to Diversity & Inclusion:
Diversity, Equity, Inclusion, and Belonging (DEIB) is at the heart of our mission to improve the health and happiness of the world. At Headspace Health, we are committed to bringing together humans from different backgrounds and perspectives while providing employees with a safe and welcoming work environment free of discrimination and harassment. We continuously strive to create a diverse and inclusive environment where everyone can thrive, feel a sense of belonging, and do impactful work together. As an equal opportunity employer, we prohibit any unlawful discrimination against a job applicant on the basis of their race, color, religion, gender, gender identity, gender expression, sexual orientation, national origin, family or parental status, disability*, age, veteran status, or any other status protected by the laws or regulations in the locations where we operate. We respect the laws enforced by the EEOC and are dedicated to going above and beyond in fostering diversity across our workplace.
Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. A reasonable accommodation is a change in the way things are normally done which will ensure an equal employment opportunity without imposing undue hardship on Headspace Health. Please inform our Talent team by filling out this form if you need any assistance completing any forms or to otherwise participate in the application or interview process.
#LI-AT2

Headspace Health participates in the E-Verify Program.
Headspace Health is committed to protecting the privacy and security of your personal data. Please view our privacy notice here."
405,Fullstack Data Visualization Software Engineer,Apple,"Cupertino, CA",N,"Summary
Posted: Sep 20, 2022
Role Number: 200389461
The Software Engineer will work in a small and dynamic team to implement exciting technologies for future Apple projects. In this position, you will work together with other engineers to bring up and support important systems to enable computer vision related tasks across the organization, in this instance, helping organizations visualize and understand calibration data. The role is a fullstack SWE role coding Python/JavaScript/C++ (40/40/20) and is 20/80 front-end/backend. Ideally, the candidate has some experience with Javascript frontends, Kubernetes, Docker, PostgreSQL, Python and perhaps even with Flask and Vue.js
Key Qualifications
Solid software engineering practices
Desired technologies: Javascript frontends, Kubernetes, Docker, PostgreSQL, Python
Good understanding of Flask and Vue.js
Good understanding of databases design and implementation
Excellent problem solving skills
Excellent communication skills
Description
Video Engineering group is looking for a software engineer. In this position, you will work together with other engineers to bring up and support important systems to enable computer vision related tasks across the organization.
Education & Experience
Bachelors degree or higher in Computer Science or equivalent field.
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $130,000 and $242,000, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program."
406,Software Data Operations Engineer,MAQ Software,"Redmond, WA 98052","$80,000 - $120,000 a year","About MAQ Software
As 2021 Microsoft Power BI Partner of the Year, we enable leading companies to accelerate their business intelligence and analytics initiatives. Our solutions enable our clients to improve their operations, reduce costs, increase sales, and build stronger customer relationships.
Our clients consistently recognize us for providing architecture and governance frameworks, implementing best practices to optimize reports, and building team capability through training programs. Our innovative tools and 33 certified visuals expand Power BI capabilities to save time for decision makers.
As a premier supplier to Microsoft for two decades, our clients benefit from our extensive insights on the platform and engineering practices. As a Microsoft Partner with 10 Gold competencies, our clients improve their implementations with our breadth and depth of expertise.
With globally integrated teams in Redmond, Washington, and Mumbai, Hyderabad, and NOIDA India, we deliver solutions with increased velocity and tech intensity.
Inc. magazine has recognized us for sustained growth by listing us on the Inc. 5000 list ten times – a rare honor.
*
Engineering culture*
We foster a strong engineering culture with a can-do attitude. All our key managers come from excellent educational backgrounds and have significant experience growing a company and mentoring software engineers. Due to our smaller size, we adopt the latest technologies and computing trends ahead of the larger industry players. As a part of the company’s globally distributed engineering team, our engineers gain exposure to the latest software engineering practices and fast development cycles.
Our developers routinely work on challenging technical problems that utilize the latest technologies for fast-paced software delivery.
*
Examples of some of our projects:*
We built a supervised machine learning model that forecasts the impact of retail sales on our client’s overall revenue. We collected data from existing customer relationship management (CRM) and sales systems. We created a forecasting model in Azure Databricks using existing and custom linear regression to process the collected data. To reduce forecast runtime and achieve near real-time analysis, we modified the existing R libraries to SparkR. The improved insight helped our client proactively focus on retailers with the highest sales impact.
We built a check-in app for one of our client’s most attended event. A multinational technology company organizes an annual multi-event internal expo attended by thousands of their employees. The manual process of tracking attendance, sending acknowledgments, and receiving feedback was time consuming. To automate the process, we built a check-in app that uses mobile devices’ camera to capture the identification badge of each participant. The captured images are stored in an Azure Blob. An Azure Logic App reads the image content utilizing Optical Character Recognition (OCR) API to update attendance records. After the event, notifications are sent to attendees via Microsoft Teams to complete a feedback survey using a Microsoft Power Automate Bot. The Feedback App reports the survey responses to determine the Customer Satisfaction (CSAT) score of the event.
For another client with high volume data, we developed and implemented a hybrid data processing solution using Azure Stream Analytics and Azure Databricks to reduce data refresh time from 3 hours to less than 30 minutes. We sourced data from the Azure Event Hub, where refreshes originate. Refreshes are captured through stream analytics and the updated data is pushed to Azure Data Lake Storage (ADLS). The data is processed in ADLS, then pushed to Power BI for reporting.
To read about some of our recent projects, visit https://maqsoftware.com/case-studies
*
Responsibilities:*
Analyze existing systems (30%)
Collect requirement specifications to analyze business processes and determine the exact nature of user’s system requirements, map process flow, discuss with module leaders and core team members to decide on the architecture.
Analyze existing system structures to provide solutions to improve computer systems to use cloud-based systems and services.
Analyze user requirements to match data available to large computer database source systems to implement solutions at reasonable performance and cost.
Design the processing steps and propose new systems based the user’s requirements. Interact with systems analysts/programmers to develop data migration tools, create processes for the new computer system and attend to ad-hoc issues related to day-to-day activities. Work with software developers in the implementation and testing phase.
*
Develop specifications and workflow (25%)*
Prepare software specifications, flow charts, and process diagrams for software programmers to follow. Develop and maintain systems documentation such as design specifications, user manuals, technical manuals, descriptions of application operations, and methodology documentation.
Analyze feasibility using commercially available software systems (e.g., Microsoft Azure versus Amazon Web Services) and reporting systems (e.g., Power BI versus Tableau).
*
Analyze and verify implementation (25%)*
Interact with systems analysts/programmers to develop data migration tools, create processes for the new computer system and attend to ad-hoc issues related to day-to-day activities.
Work with other software developers in the implementation and testing phase.
Setup test environment and compare data from multiple sources to verify reports for end users.
*
Review implementation status and reporting (10%)*
Participate in technical collaboration meetings and periodical reviews of implementation status.
Report weekly task plan to the project management team for implementation of custom software.
*
Training and certifications (10%)*
Participate in technical trainings and complete relevant industry courses and certifications.
*
Qualifications:*
Undergraduate or graduate degree in Computer Science, Information Systems, Electrical Engineering, Applied Computational Math Sciences or related Engineering discipline.
*
Benefits & Salary:*
Annual pay range $80,000 - $120,000.
Paid time off.
Comprehensive medical, dental and vision insurance with employee premiums paid in full.
401(k) retirement plan with 3% company match and immediate vesting.
Job Type: Full-time
Pay: $80,000.00 - $120,000.00 per year"
407,Data Engineer,On-Demand Group,"On-Demand Group in Minneapolis, MN 55402","Up to $100,000 a year","Data Engineer
Direct hire
This role develops enterprise data solutions that support the organization in achieving its strategic goals. Your work on cloud data pipelines will advance our enterprise data capabilities, while you get immersed in our collaborative, fun, and engaging culture. The position reports to our IT Data Engineering department and is a part of projects that align with key strategic initiatives to meet our business objectives. In your role, you will be part of a team that is responsible for the design, development, testing, deployment and support of cloud-based (GCP), data, analytical, and reporting applications.
Essential Duties & Responsibilities:
Develop and maintain custom ELT data pipelines with Python and SQL-based transformations running on the Google Cloud Platform.
Collaborate and implement event and batch based data science scoring pipelines.
Develop data access APIs to facilitate cross application data sharing.
Conduct and/or participate in requirements analysis sessions with internal customers, external vendors, and project teams.
Translate business requirements into technical designs.
Follow engineering best practice to ensure robust, tested, and reliable data pipelines.
Support data governance and security practices.
Follow agile development methodologies and actively participate in sprint planning sessions.
Support downstream users and resolve production issues with excellent customer service.
Job Skills:
Hands-on experience in creating API based data ingestion pipelines.
Good design skills in data pipeline, enrichment, and API patterns.
Good understanding of object oriented software engineering patterns.
Good relationship-building, customer service, and problem resolution skills.
Knowledge of software engineering, version control, and testing practices.
Knowledge of Agile software development methodologies.
Works effectively in a dynamic work environment with competing priorities.
Work Experience (6+ months experience each)
Python object oriented programming. Multi-language experience preferred.
API based custom data ingestion, particularly working with 3rd party vendor API's including reading API documentation, authentication, and bulk data staging strategies.
Cloud-based development. GCP preferred.
SQL
Experience with development in a version control, CI/CD environment.
Education:
From an accredited institution, Bachelor’s degree is required.
Job Type: Full-time
Salary: Up to $100,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Minneapolis, MN 55402: Reliably commute or planning to relocate before starting work (Required)
Experience:
Google Cloud Platform: 1 year (Preferred)
SQL: 1 year (Preferred)
Python: 1 year (Preferred)
Work Location: In person"
408,Data Engineer/Business Analyst,Broadleaf Inc,"Hyattsville, MD 20783•Remote",N,"Team Broadleaf is searching for a candidate to join our team who will work with customers and within a small development group and will provide analysis of existing customer requirements with attention to how improvements can be made will be critical to the success of this contract.
This is a Business Analytics and Data Visualization position and candidate will be responsible for the position includes, but not limited to; managing and updating and working with administrators on various databases; updating existing and creating new extractions, transformations, and loading (ETL) processes as needed. Assisting Federal POCs with data gathering, verifying, building pipelines, and analyzing data; developing data dashboard using Business Intelligence (BI) tools such as but not limited to Tableau. Presenting the information to stakeholders as well as conducting demonstrations as needed. Candidate will need to communicate and collaborate with client internal and external stakeholders and support data science by collaborating across business and Information technology.
Clearance Level: US Citizen and currently hold a minimum NACI-T1 clearance
Certifications: Hold current professional certifications to meet the DoD 8570.01 directive or able to obtain the missing certification before the hire date
BASIC QUALIFICATIONS:
At least four (4) years experience in data analytics using Tableau and or other BI tools.
At least two (2) to three (3) years experience in building and configuring Databases.
Experience developing and administrating dashboards using Tableau and or BI tools by merging data from different sources and format.
Expert understanding of databases, data hierarchy, data architecture and analytical skills to comprehend data for creating meaningful data dashboard for senior government management.
Experience with focusing on building, managing, and optimizing data pipelines that facilitate data movement.
Responsible for reducing manual data work and improving productivity. Experienced with employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparations in integration tasks.
Excellent inter-personal skills.
Excellent written and verbal communication skills.
Self-motivated.
Attention to details.
Pro-active with can-do attitude.
Sense of ownership working independently and/or with team members.
DISIRED EXPERIENCE:
Experience supporting Federal contracts, advance MS Excel knowledge.
EEO Employer F/M/Vet/Disabled"
409,Data Engineer,"DataHaven Solutions, Inc","Columbia, MD 21046•Hybrid remote","$145,000 - $190,000 a year","Job description
If you are looking for a new opportunity with a small and growing company that offers extremely competitive compensation and a spectacular benefits package, then DataHaven Solutions might be the place for you. If you want to work for a company where you can actively participate in shaping the company's direction and whose owners are building a company that truly cares about their employees' happiness, then DataHaven Solutions is DEFINITELY the place for you.
We believe that your job is something that should challenge you but also bring you enjoyment. We feel that your employer should support your career goals and nurture growth. If you are looking for a new role or just seeing what is out there, we would love to hear from you.
We are looking for a TS with CI poly cleared, Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst or engineer. Work is performed mostly on customer site in Ft. Meade, MD. That being said this is hybrid position, so there will be some remote work involved (33% estimated).
Security Clearance:
*Candidate must have an active TS with CI poly U.S. government security clearance and therefore all candidates must be a U.S. citizen.
Hiring Bonus:
** $10,000 signing bonus ** for anyone hired by 3/31/2023
Place of Performance:
Hybrid (this position is mostly on-site in Ft. Meade but 33% of your work will be remote.)
Essential Job Responsibilities:
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.
Minimum Qualifications:
**Candidate MUST HAVE a TS with CI poly clearance**
Amazon EMR experience
**NiFi experience
5 years experience as a developer, analyst, or engineer with a Bachelors in related field; OR 3 years relevant experience with Masters in related field; OR High School Diploma or equivalent and 9 years relevant experience.
Experience with programming languages such as Python and Java.
Proficiency with acquisition and understanding of network data and the associated metadata.
Fluency with data extraction, translation, and loading including data prep and labeling to enable data analytics.
Experience with Kibana and Elasticsearch.
Familiarity with various log formats such as JSON, XML, and others.
Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).
Ability to decompose technical problems and troubleshoot system and dataflow issues.
Must be able to work on customer site in Ft. Meade, MD most of the time.
Preferred Experience:
Experience with NOSQL databases such as Accumulo desired
Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer
Job Type: Full-time
Salary: $145,000.00 - $190,000.00 per year (depending upon seniority level)
Job Type: Full-time
Pay: $145,000.00 - $190,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Columbia, MD 21046: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Are you a U.S. citizen?
Do you have NiFi experience?
Do you have experience with AWS EMR?
Education:
Bachelor's (Required)
Experience:
relevant: 5 years (Required)
Security clearance:
Top Secret (Required)
Work Location: Hybrid remote in Columbia, MD 21046

Health insurance"
410,Cleared Data/ETL Engineer,ReindeerTek,"ReindeerTek in Reston, VA 20190","Up to $225,000 a year","ReindeerTek is seeking a Data/ETL Engineer for a technical development program supporting a classified customer. This role designs, implements, and maintains standard data interfaces for data ingest including Extract/Transform/Load (ETL) methodology and implementation, APIs, RESTful Web Services, data quality, and data cleansing. This role collaborates across a team to meet complex data needs between legacy and next-generation systems.
Required Skills:
Develops methods, processes, and systems to consolidate and analyze structured and unstructured and diverse sources
Experience with ETL data tools such as Pentaho and Apache Nifi
Knowledge with cross-discipline teams to ensure connectivity between various data sources and business problems
Experience integrating data models with workflow management tools to ensure ""gold copy"" data integrity
Desired Skills:
Identifies meaningful insights and improvements and communicates findings and recommendations
Develops information tools, algorithms, dashboards, and queries to monitor and improve business performance
Maintains awareness of emerging analytics and big-data technologies
Job Type: Full-time
Pay: Up to $225,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Referral program
Retirement plan
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
COVID-19 considerations:
All employees will be required to be vaccinated to be considered.
All employees onsite are vaccinated, and mask use is optional.
Application Question(s):
What kind of Polygraph have you taken?
Will you be able to reliably commute to Reston, VA 20190 for this job 5 days a week?
Do you have an active TS/SCI with polygraph clearance?
Security clearance:
Top Secret (Required)
Work Location: One location

Health insurance"
411,Data Engineer,Parker,"New York, NY",N,"About the role
Parker's mission is to increase the number of financially independent people. We believe we can achieve this goal by building tools that enable independent business owners to scale their businesses profitably. Our first product combines a virtual credit card system with dynamic spending limits and software tooling to help merchants grow and optimize their profitability.
We are growing very fast - in less than five months, we grew to millions in card volume. We have a significant waitlist of customers waiting to use our product. We are looking to expand our headcount quickly to support the demand. Our investors include Solomon Hykes (founder of Docker), Paul Buchheit (founder of Gmail), Paul Graham (founder of Y Combinator), Robert Leshner (founder of compound.finance), and many more. We have raised over $30M from top-tier fintech investors.
Basic Qualifications
Minimum of three years of experience with modern data engineering technologies such as: S3, Redshift, SQL, DBT, Python, etc.
Interest in building robust pipelines that will be mission critical for our business
Proven abilities in developing your own scalable, products/applications from scratch
Who you are
A passionate, determined, and curious thinker who thinks about problems from an engineering perspective
Desire to work in a fast-paced environment, continuously grow and master your craft
Strong communicator who can articulate your ideas clearly and participate in making difficult decisions
A hacker with general interest in learning about new technologies and coming up with solutions to unsolved problems
Nice to haves
Experience working within payments or at a fintech company/on a fintech project
Experience and subject matter expertise in the e-commerce industry
Interest/contributions to open source software
Compensation
The annual salary for this role in NYC is $150,000 - $204,000. We also offer start-up equity along with health, dental, and vision benefits."
412,Data Engineer,High5,United States,N,"Required Skills:
Bachelor’s degree or four or more years of work experience.
3 or more years of relevant work experience.
Experience programming/scripting/coding (SQL, Python, etc.) and knowledge of relational databases.
Experience with data platforms & tools: Teradata, Oracle, BigQuery, Snowflake, Stibo, etc.
Experience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, MySQL, etc."
413,DATA ENGINEER III,STAND 8,Remote,$66 - $76 an hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

Data Engineering and operations team is looking for a expert Data Lineage Engineer. The Data Engineer Senior works with department and lines of business subject matter expert (SMEs) across the enterprise to meet departmental and organizational objectives. The engineer will follow end-to-end process standards and guidelines to ensure accurate and efficient build out of data pipeline architecture within project timeframes.

General job duties
Create end to end lineage for various data pipelines and application ecosystem using EDC, custom lineage solutions.
Assist in creating standards, templates and procedures for data lineage process.
Do ETL development using Informatica IICS as required.
Function in accordance with the Software Development Life Cycle (SDLC) framework and governance processes
Participate in development of test plans and perform quality assurance and testing of own work and that of peers
Function in accordance with the Software Development Life Cycle (SDLC) framework and governance processes
Ensure work includes necessary audit controls such as Audit, Balance and Control (ABC) Framework and security controls within all design and deliverables
Document data workflows including but not limited to: functional specs, technical specs, mapping / workflow diagram, testing plans, production “run books”, training materials, etc.
Accurately define and execute transformations, aggregations and other data manipulations to meet requirements
Participate in development of test plans and perform quality assurance testing of own work and that of peers
Identify development and data quality issues and work with senior team members and management to mitigate
Conduct peer and team review sessions
Assist in the creation of standards, templates and procedures for the department
Contribute to the design of the data structure/data model and data flow
Design and develop data workflows and mappings to extract, transform, and load data for purposes of analytics , reporting and integration

Experience
B.S. in Computer Science, MIS or related degree
Must Have
Expert level EDC engineer with experience creating end to end data lineage using Informatica EDC in PROD environment.
Solid experience in building custom lineage using various technologies such as Python, scripting languages, other programming languages.
Hands-on development experience with ETL tools such as Informatica IICS/Powercenter and database development.
Experience working with business stakeholders and data governance teams to integrate data lineage and business metadata.
Experience creating standards, templates and procedures for data lineage process.
Tools/technology used with EDC: Informatica IICS, Oracle, Snowflake, Salesforce, SAP BODS, Vendor Apps such AFS, FIS etc.
Nice to have
Experience in building lineage using Collibra.
Experience in building API solutions.
Experience with other ETL tools such as SAP BODS
Experience with Oracle, Snowflake
The US base range for this contract position is $66-$76/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training."
414,Senior Backend Engineer - Data,Customer.io,"Portland, OR•Remote","$118,000 - $167,400 a year","Hi, I'm Julia, Engineering Manager at Customer.io. I'm looking for a Senior Back-End Engineer to join our remote team.
We're fun-loving, passionate, and highly effective people. We value diversity (our core values include awkwardness for a reason) and believe embracing unique perspectives is crucial to our success. We ship fast, with just enough processes to keep us coordinated.
Our mission is to provide a best-in-class message management system to help our customers quickly craft meaningful content that reaches their customers. They send us data about their customers, then automate email, push, slack, and other messages based on that data. We ingest billions of data points and send billions of messages per year. Our back-end is written in Go.
We are looking for collaborative senior backend developers who love solving large-scale puzzles and are excited to help us build a performant platform that our customers love. Our small, remote-first team is fun-loving, passionate, and highly effective. We offer many opportunities to grow and refine your skills while having a significant impact.
We seek product-minded, empowered individuals who work collaboratively with their peers on challenging problems and get those solutions into customers' hands quickly. We value diversity, attracting the best people in the world to serve as colleagues. Our flexibility and freedom to work from anywhere in the world enables you to craft a work environment in which you can do your best work.
What you'll do:
Build complex new solutions and solve highly complex scalability challenges at the core of our systems
Be a model and mentor for writing well-architected, well-tested, and easy-to-understand services in Go that help our customers easily send timely & relevant messages at scale
Apply your unique perspective to help us define and build the next generation of features and infrastructure needed to help our customers and our company to flourish
Work with our tech support teams to diagnose operational issues and help our customers to achieve their goals with our product
Take an active role in a friendly, supportive squad that encourages you and the entire company to grow as individuals, professionals, and teams
Mentor your coworkers through code review, pair programming, team collaboration, and training to help improve our collective knowledge and best practices together
What we're looking for:
An independent mindset backed by excellent communication skills and a desire to help us make significant decisions in an empathetic and respectful way
A deep understanding of problems of scalability and working with extensive datasets. Significant experience building out performant distributed applications on cloud infrastructure, preferably in Go
Experience working with customers and applying a deep understanding of their needs to produce timely solutions to their real-world problems.
A collaborative mindset backed by excellent communication skills and a desire to help us make decisions in an empathetic and respectful way
Ability to quickly and independently dig into complex problems, identify critical factors and propose solutions with limited mentorship.
Able to work core hours of 9 a.m. ET to noon ET to interact with the team.
Comfort working in UNIX environments and applying modern collaborative development practices.
Experience in working with RDB systems, preferably MySQL
Experience working within a distributed company is preferred

About Customer.io
Our mission at Customer.io is to power automated communication that people like to receive. Today over 4,700 internet businesses use Customer.io to manage, send, and track performance of email, SMS, push notifications, and in-app messages. Unlike typical marketing platforms, Customer.io helps businesses increase relevance by using behavioral data: what people do or don't do when logged in to a web or mobile app.
We offer a starting salary of USD $118,000 - $167,400 USD (or equivalent in local currency) depending on experience and subject to market rate adjustment.
Benefits at Customer.io include:
Unlimited PTO - we encourage 20 vacation days (in addition to holidays and sick days) so that you can unwind, unplug, and recharge
100% medical, dental, vision, and supplemental insurance for you and your dependents
12 weeks paid parental leave - for birth, adoption, or foster care
401k retirement matching - up to 5% dollar for dollar match to retirement contributions
Health and wellness allowance - up to $200 USD per month that can be used for your healthy living needs, including gym membership, acupuncture, massage, or bike repairs
Home office stipend - up to $2,000 USD to help you get your home office set up so you can do your best work
Internet + cell phone reimbursement - up to $200 USD per month for your internet and cell phone plans
Co-working space reimbursement - up to $300 USD per month for those times you'd prefer to work in a co-working environment
Learning + development - up to $2,000 USD reimbursement per year to use on conferences, books, classes, workshops, and passion projects - anything that will help you develop your skills
1 month paid sabbatical after four years at Customer.io - to treat yourself to a vacation, or spend however you choose
1 annual company summit per year and opportunities to meet in smaller groups throughout the year
Flexible schedule, work anywhere you want! - as long as you have a reliable internet connection and some overlapping work time with your manager, you can work where and when you want
All final candidates will be asked to complete an employment and education verification authorization form (which allows us to verify your job history and education listed on your resume) as part of our pre-employment process.
Customer.io recognizes the stifling impact of systemic injustice on diverse communities. We commit to using our influence to increase inclusion and equity within the tech industry. We strive to build an inclusive team culture, implement bias-free hiring practices, and develop community partnerships to expand our global impact.
Join us!
Check out our careers page for more information about why you should come work with us! We are passionate about our core values of Empathy, Transparency, Responsibility, and Awkwardness and are looking for new coworkers to share and build that passion!
How to apply
Apply at the link below and tell us why you're interested in the position! We plan to respond to all applicants with a status update about your application.
Here's what you can expect from our hiring process (order is subject to change):
30 minute video call with the Hiring Manager
50 minute video call with two potential teammates
Homework assignment + Review call with two potential teammates
Final call with our VP of Engineering"
415,Data Engineer,FutureFit AI,"Ontario, CA•Remote",N,"Join our Engineering Team!

The Opportunity
A growing company of less than 50, you will join a caring team of engineers, business, and product leaders; all focused on using technology to improve lives and outcomes for people going through career transitions. We care about the impact our technology can have on workers and bring deep empathy to our work.
We’re looking for an engineer ready to scale our data and ensure success in production environments: In this role, you will partner with our application engineering team to transform raw data into useful data for analytical purposes. With your strong analytical skills, ability to combine data from different sources, interest in analytics, and familiarity with several programming languages, we’ll ensure our FFAI profiles surface insights for better career recommendations for workers across the globe.
How You’ll Fulfill Your Mission

Partner - From exploring ways to enhance data quality and reliability, writing and automating tests, to further developing our recommender and inference systems. You’ll be collaborating with teams of data scientists and engineers to build and maintain data systems that generate easy-to-analyze datasets that drive our core insights.
Problem Solve - Serving as a technical leader for the business, we’ll need you to translate our data in ways that address our platform/business needs. This will require excellent communication skills since you’ll be working across various departments to understand how to best manage and scale our data
Implement - As an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up, we’ll be relying on your expertise to move our data models into production as we make use of 350+ million data points.
Why We Value You
Technical.
Strong expertise in Python and experience with Flask, FastAPI, OpenAPI / Swagger; some knowledge of numpy, pandas, and related technologies is an advantage.
Comfortable with architecture and design around various data paradigms (document stores, relational, graph) and databases (Mongo, Elastic, Postgres, etc.)
Experience in architecture and implementation of ETL processes, streaming data, data process orchestration (Airflow); exposure to MLOps and Machine Learning is an advantage
Collaborative. Experienced in using different technologies to collect and map an organization’s data landscape to help decision-makers find cost savings and engineering team optimization opportunities.
Internally Driven. Deeply motivated by building data infrastructure to provide data-driven insights, and empower teams to deliver business value.
Benefits: We provide a generous benefits package to all of our team members, including unlimited PTO, health care, technology reimbursement, and flexible schedules.
About FutureFit AI
FutureFit AI believes that the greatest challenge of the Future of Work is facilitating successful career transitions, and our mission is to address that challenge by using AI to transform education and economic mobility.
FutureFit AI is a growing, well-funded, company focused on using technology to improve the lives and outcomes for people going through career transitions. We bring innovation, creativity, and empathy to our work, and care deeply about the impact our technology can have on our customers and users.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment. Please contact us to request an accommodation.

FutureFit AI All rights reserved, we are proud to be an equal opportunity workplace. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate on the basis of race, religion, color, gender identity, sexual orientation, age, disability, veteran status, or other applicable legally protected characteristics. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply."
416,Senior Data Engineer,Catalyte,Remote,"$150,000 - $170,000 a year","Sr. Data Engineer- Catalyte
Catalyte is looking for an innovative Senior Data Engineer to join our team and help build out our Data and Analytics platform. If you want to put your skills to use to help find and create future developers, come talk to us!
Responsibilities:
· Working with stakeholders to ensure that business data needs are met
· Building data ingestion pipelines to ensure timely and quality data into our Data Lake
· Contributing to the data platform design and architecture decisions
· Ensuring that data is stored, queried and displayed securely
· Working with and evaluating existing data-centric applications and tools
· Working on an Agile team to build high quality and modular data structures
· Mentoring junior developers to allow them to grow their data career
Qualifications:
· At least 5 years working in a Data Developer, Engineer or Architect role with a broad focus on data movement, querying (SQL), analytics, storage and processing
· At least 3 years’ experience with Python
· At least 3 years’ experience with Spark (PySpark)
· Experience with Databricks a plus
· Excellent skills in both RDBMS (SQL Server, PostgreSQL) and NoSQL technologies
· Exposure to data security regulation and enforcement - GDPR, HIPAA, etc.
· Cloud experience is desired - AWS preferred
· Experience building pipelines in a modular, scalable way
· Strong analytical skills
· Curiosity about data and the ideal way to structure it to extract meaning from it
· A “can do” attitude and a passion for innovation
· Familiarity with Salesforce and Salesforce API
· Experience with Machine Learning Algorithms a plus
· Experience with Tableau Desktop and Server a plus
Catalyte provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Job Type: Full-time
Pay: $150,000.00 - $170,000.00 per year
Benefits:
401(k)
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Relocation assistance
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Work Location: Remote

Health insurance"
417,Senior Data Engineer (Remote),CareFirst BlueCross BlueShield,"Owings Mills, MD 21117•Remote",N,"Resp & Qualifications
PURPOSE:
The Senior Data Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (Relational and NoSQL, distributed and converged) with emphasis on reliability, automation and performance. This role will focus on developing solutions and helping transform the company's platforms deliver data-driven, meaningful insights and value to company.

ESSENTIAL FUNCTIONS:
Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using multiple technologies.
Interprets data, analyzes results using statistical techniques, and provides ongoing reports. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.
Develops data models by studying existing data warehouse architecture; evaluating alternative logical data models including planning and execution tables; applying metadata and modeling standards, guidelines, conventions, and procedures; planning data classes and sub-classes, indexes, directories, repositories, messages, sharing, replication, back-up, retention, and recovery.
Creates data collection frameworks for structured and unstructured data.
Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies.
Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
Applies and implements best practices for data auditing, scalability, reliability and application performance.

SUPERVISORY RESPONSIBILITY:
Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.

QUALIFICATIONS:

Education Level: Bachelor's Degree in Computer Science, Information Technology or Engineering or related field OR in lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.

Experience: 5 years Experience with database design and developing modeling tools. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.

Knowledge, Skills and Abilities (KSAs)
Knowledge and understanding of at least one programming language (i.e., SQL, NoSQL, Python).
Knowledge and understanding of database design and implementation concepts.
Knowledge and understanding of data exchange formats.
Knowledge and understanding of data movement concepts.
Strong technical and analytical and problem solving skills to troubleshoot to solve a variety of problems.
Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities.
Department
Department:Actuarial Systems
Equal Employment Opportunity
CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
Hire Range Disclaimer
Actual salary will be based on relevant job experience and work history.
Where To Apply
PeopleSoft/Self Service/Recruiting
Closing Date
Please apply before: 03/21/2023
Federal Disc/Physical Demand
Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
PHYSICAL DEMANDS:
The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
Sponsorship in US
Must be eligible to work in the U.S. without Sponsorship.
#LI-CB1"
418,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
419,Data Engineer,Crowdz,"Nashville, TN",N,"About Crowdz (www.crowdz.io): Crowdz, a Series A+ funded and rapidly growing Silicon Valley-based fintech startup and creator of the world’s first global receivables-financing marketplace, is looking for a passionate and dedicated Senior Credit and Data Analyst to help expand economic opportunities for small and midsize businesses (SMEs) and mid-market enterprises (MMEs) by enabling them to accelerate and optimize their cash flow
What you’ll do
As a Data Engineer,
You will be charged with enhancing the data architecture to provide business insights and accessibility. In addition, this role will give you the opportunity to automate data generation and analytics processes, validate data accuracy, and implement predictive methods. The goal is to enable data-driven decision-making and insights.
Key responsibilities include:
Collaborate with stakeholders to ensure reliable data for critical business decisions
Extracting, ingesting, and transforming large datasets
Up to date with advances in digital analytics tools and data manipulation products
Understand data concepts such as big data, and data landscaping and are able to conduct data modeling for specific use cases and user stories
Knowledge of cloud computing and how to apply it to database technologies such as relational databases and NoSQL as well as data streaming
Understanding of industry-recognized data-modeling patterns and standards
Qualifications:
Bachelor's Degree in Computer Science or related field
3+ years of experience with data analytics
Experience with Apache Spark, SQL, NoSQL (MongoDB), GraphQL, and PostGres
Able to work with business and technology stakeholders in an agile distributed team to translate business problems into data designs
Understand how to reduce data latency in the query design, are familiar with database ACID, transactional systems, event-based systems, or even blockchain technologies
What we offer:
This is a great role in a vibrant and growing team in an organization with an inclusive culture
You will have the freedom to define the vision and the opportunity to make things happen
You can be flexible in how and where you work (remote working)
You will work with a truly international team and international customers
Your work will be diverse and challenging
We offer a competitive salary, benefits package, and a stock option plan"
420,DATA ENGINEER III,STAND 8,Remote,$66 - $76 an hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

Data Engineering and operations team is looking for a expert Data Lineage Engineer. The Data Engineer Senior works with department and lines of business subject matter expert (SMEs) across the enterprise to meet departmental and organizational objectives. The engineer will follow end-to-end process standards and guidelines to ensure accurate and efficient build out of data pipeline architecture within project timeframes.

General job duties
Create end to end lineage for various data pipelines and application ecosystem using EDC, custom lineage solutions.
Assist in creating standards, templates and procedures for data lineage process.
Do ETL development using Informatica IICS as required.
Function in accordance with the Software Development Life Cycle (SDLC) framework and governance processes
Participate in development of test plans and perform quality assurance and testing of own work and that of peers
Function in accordance with the Software Development Life Cycle (SDLC) framework and governance processes
Ensure work includes necessary audit controls such as Audit, Balance and Control (ABC) Framework and security controls within all design and deliverables
Document data workflows including but not limited to: functional specs, technical specs, mapping / workflow diagram, testing plans, production “run books”, training materials, etc.
Accurately define and execute transformations, aggregations and other data manipulations to meet requirements
Participate in development of test plans and perform quality assurance testing of own work and that of peers
Identify development and data quality issues and work with senior team members and management to mitigate
Conduct peer and team review sessions
Assist in the creation of standards, templates and procedures for the department
Contribute to the design of the data structure/data model and data flow
Design and develop data workflows and mappings to extract, transform, and load data for purposes of analytics , reporting and integration

Experience
B.S. in Computer Science, MIS or related degree
Must Have
Expert level EDC engineer with experience creating end to end data lineage using Informatica EDC in PROD environment.
Solid experience in building custom lineage using various technologies such as Python, scripting languages, other programming languages.
Hands-on development experience with ETL tools such as Informatica IICS/Powercenter and database development.
Experience working with business stakeholders and data governance teams to integrate data lineage and business metadata.
Experience creating standards, templates and procedures for data lineage process.
Tools/technology used with EDC: Informatica IICS, Oracle, Snowflake, Salesforce, SAP BODS, Vendor Apps such AFS, FIS etc.
Nice to have
Experience in building lineage using Collibra.
Experience in building API solutions.
Experience with other ETL tools such as SAP BODS
Experience with Oracle, Snowflake
The US base range for this contract position is $66-$76/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training."
421,Data Engineer,Fortress Information Security,Remote,N,"Data Engineer

What you can expect as a Data Engineer at Fortress:
The Data Engineer will be responsible for defining data input rules and processes to ensure data quality and integrity. The data input comes from many stakeholders and the candidate will need to identify anomalies and develop programmatic solutions to solve problems. A successful candidate will be well versed in the implementation of data management strategies and data technologies. Day to day activities include data cleanup, documentation and definition of solution architectures, and hands on development activities.

Responsibilities include:
Develop an in-depth knowledge of Fortress’s data, data flows, and processes
Create and document data management standards, policies, and best practices
Drive alignment on enterprise data models and definitions
Provide technical guidance
Develop with a focus on automation and consistency
Can work independently or as a member of a team environment
Anticipate, recognize, report, and help resolve issues
Ability to understand how technical requirements’ impact current processes
Ability to understand and facilitate coordination of data requirements across teams
Minimum Qualifications:
2+ years of experience in a data engineer or equivalent role
Expertise in structured and unstructured databases such as PostgreSQL, MongoDB, and ElasticSearch
Expertise in building data models and complex SQL queries
Expertise in data quality validation with the ability to conduct data analysis, investigation, and document resolution
Experience in data process design, implementation, and improvement
Ability to lead your own projects and operate with a high degree of autonomy in a remote working environment
Must have ability to explain technical concepts and make decisions with non-technical team members
Develops clean and intuitive code
Excellent written and verbal communication skills
Preferred Experience:
Experience with Jira
Experience with Python and data analysis libraries such as Pandas
Use of agile and DevOps practices for project and software management including continuous integration and continuous delivery
Excellent time management skills and proven ability to multi-task competing priorities
Education:
Bachelor's Degree in Information Technology, Computer Science, Data Engineering, Data Analytics or equivalent degree from an accredited University
Employee Benefits:
Remote and Hybrid working environment
Competitive pay structure
Medical, dental, vision plans with employees covered up to 90% with highly progressive options for dependents and families
Company paid life, short- and long-term disability insurance
Employee Assistance Program
401(k) match
Paid time off and holiday pay
Access to thousands of Learning & Development courses that range from mental health and wellbeing, stress, and time management to an array of technical and business-related courses
Employment Perks:
We provide each employee with professional growth opportunities through succession planning, up-skilling, and certifications
Tuition and certification reimbursement
Employee Referral Programs
Company Sponsored Events
Fortress is proud to be an Equal Opportunity Employer. All employees and applicants will receive consideration for employment without regard to age, color, disability, gender, national origin, race, religion, sexual orientation, gender identity, protected veteran status, or any other classification protected by federal, state, or local law. Fortress Information Security takes part in the E-Verify process for all new hires.

For positions located in the US, the following conditions apply. If you are made a conditional offer of employment, you will have to undergo a drug test. ADA Disclaimer: In developing this job description care was taken to include all competencies needed to successfully perform in this position. However, for Americans with Disabilities Act (ADA) purposes, the essential functions of the job may or may not have been described for purposes of ADA reasonable accommodation. All reasonable accommodation requests will be reviewed and evaluated on a case-by-case basis."
422,Data Engineer,Zoox,"Foster City, CA","$143,000 - $300,000 a year","The Data team leverages data from our autonomous vehicles and operations to determine autonomy and service readiness. We provide the foundation for strategic decision-making at Zoox. You will develop and implement the next generation of our data pipeline to ensure visibility into our business as we scale toward the launch of an autonomous mobility service. You will define the system and build the pipeline to enable Zoox to develop and scale with a data-first culture.

You will join a diverse, experienced team with rapidly growing scope and responsibility while also having access to one of the most unique data sets in the autonomous vehicle industry. Hence, we are seeking all skill levels to grow with the team.
Responsibilities
Designing, building, and maintaining the infrastructure that transforms autonomous vehicle data at scale to support analytics throughout the company
Defining and executing on how data from perception, prediction, planning and other parts of the autonomous stack is consumed to generate valuable insights by data scientists, engineers, and business users
Establishing robust data integrity monitoring so that company-wide metrics are based on accurate data
Partnering with engineering and product teams to define data consumption patterns and establish best practices
Qualifications
Experience designing and building complex data infrastructure at scale
Exceptional Python or Scala skills
Advanced SQL and data warehousing experience
Experience operating a workflow manager such as Airflow
Experience with large scale streaming platforms (e.g. Kafka, Kinesis), processing frameworks (e.g. Spark, Hadoop) and storage engines (e.g. HDFS, HBase)
A strong DataOps mindset and opinions on next-generation warehousing tools
Bonus Qualifications
Basic fluency in C++
Familiarity with or exposure to experimentation platforms
Compensation
There are three major components to compensation for this position: salary, Amazon Restricted Stock Units (RSUs), and Zoox Stock Appreciation Rights. The salary will range from $143,000 to $300,000. A sign-on bonus may be part of a compensation package. Compensation will vary based on geographic location, job-related knowledge, skills, and experience.

Zoox also offers a comprehensive package of benefits including paid time off (e.g. sick leave, vacation, bereavement), unpaid time off, Zoox Stock Appreciation Rights, Amazon RSUs, health insurance, long-term care insurance, long-term and short-term disability insurance, and life insurance.

ABOUT ZOOX

Zoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of artificial intelligence, robotics, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team.

Follow us on LinkedIn

Vaccine Mandate
Employees working in this position will be required to have received a single dose of the J&J/Janssen COVID-19 vaccine OR have completed the two-dose Pfizer or Moderna vaccine series. In addition, employees will be required to receive a COVID-19 booster vaccine within two months of becoming eligible for the booster vaccine.

Employees will be required to show proof of vaccination status upon receipt of a conditional offer of employment. That offer of employment will be conditioned upon, among other things, an Applicant’s ability to show proof of vaccination status. Please note the Company provides reasonable accommodations in accordance with applicable state, federal, and local laws.

About Zoox
Zoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of robotics, machine learning, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team.

Follow us on LinkedIn

A Final Note:
You do not need to match every listed expectation to apply for this position. Here at Zoox, we know that diverse perspectives foster the innovation we need to be successful, and we are committed to building a team that encompasses a variety of backgrounds, experiences, and skills."
423,Sr Data Engineer,OscarMike,"Irving, TX 75063",N,"Sr Data Engineer (Python/Databricks)

Direct Hire 1099 or W2 only.
The client is not offering VISA sponsorship or subcontracting for this position.
This job is located onsite in Irving, TX. Local applicants only.

Our client needs highly enthusiastic Python/Databricks engineers to create, tune, maintain and build reporting for the Big Data platforms that drive their retail business. This role is key to the company's continued market dominance, and it requires people that are passionate about data and seeking perfection in their development.

Requirements:

KEY DUTIES AND RESPONSIBILITIES:

Performing systems integration design and development in cloud architecture design (Azure)
Building sizing and cost models for Cloud Services (Azure)
Review enhancement requests and recommend best solution to stakeholders as SME.
Containerizing ingestion processes using docker and kubernetes services.
Understand business requirements to design , build, develop and test data integration pipelines.
Plan, direct DevOps implementation and system hardware/software updates for better performance, stability.
Work on the BI dashboards to support retail stores and the Azure AI platform that runs the personalized consumer experiences used by millions each day.

EDUCATION AND EXPERIENCE:

EDUCATION: Bachelors/4 Yr Degree
YEARS OF RELEVANT WORK EXPERIENCE: 5+ years
YEARS OF MANAGEMENT EXPERIENCE: NA

SPECIFIC KNOWLEDGE AND SKILLS:

3+ years or demonstrated mastery of development with programming languages - Python, SQL, Unix.
2+ years experience with Apache Spark and Databricks cloud tool sets (AWS or Azure).
1+ years experience with GCP/Azure Cloud Big Data tools (PowerBI/DataLake/ Azure Databricks).
Self-motivated with excellent analytical, problem solving, verbal and communication skills.
Proficiency in API security frameworks, token management and UAC including OAuth, JWT, etc.
Basic knowledge of using Devops, Github, JIRA tools and ability to work in agile environment.
Bachelor's degree - Computer Science or equivalent."
424,Data Platform Engineer,Holman,"Mount Laurel, NJ",From $110 an hour,"At Holman, we exist to provide rewarding careers and better lives for employees and their families. We hire, train, empower, and reward exceptional people. Our journey is guided by our desire to get it right every time and the acknowledgement that we have an opportunity to be better. To be better, we have to do better, and to do better we must know better. That’s why we are listening, open to learning new things – about ourselves and each other. We will never stop striving for improved diversity, equity, and inclusion because we are successful together when we feel trusted and supported. It’s The Holman Way.

At Holman, your total compensation goes beyond your paycheck. To position you for success and provide a rewarding career and better life for you and your family, Holman is proud to offer you the benefits you deserve; including protection against illness, disability, loss of work, or preparation for retirement. Below is a brief overview of these programs:

Health Insurance

Dental Insurance

Life and Disability Insurance

Flexible Spending and Health Savings Accounts

Employee Assistance Program

401(k) with Employer Match

Paid Time Off

Tuition Reimbursement

Exclusive pricing and concierge sales support on new and used vehicles

Holman is currently accepting applications for the role of Data Platform Engineer

Principal Purpose of Position:
Design, develop, document and execute data solutions, tools and practices
Analysis of requirements at sufficient level of detail to allow ETL solution to be developed
Development of ETL job flows according to company standards for naming, performance, restartability and performance.
Support testing and remediation of defects in newly-developed/modified ETL workflows
Promote ETL workflows to PROD and provide ongoing support in PRODUCTION, including monitoring and troubleshooting
Ability to create Power BI Datasets to support the Analytic Delivery team
Evaluate emerging data platform technologies
Lead technology implementations
Follow and contribute to best practices for data management and governance
Collaborate with the Data Architecture team to understand and implement load processes for reporting and analytic data structures (data warehouses, data marts and data lakes)
Performance tune and troubleshoot processes under development and in production as necessary.
Work with the Data Architects to augment ERD’s as changes are developed
Develop, maintain, and extend reusable data components
Provide timely project and task updates to all concerned parties
Monitor production data integration jobs and correct failures in a timely manner
Create and manage incident reports as they pertain to data integration processes
Perform all other duties and special projects as assigned.

Required Experience/Skills

2+ years Azure exposure (Any Resources: Databases, Data Factory, Synapse Studio, Storage Account, Power Platform)
2+ years ANSI SQL experience
1+ years data modeling exposure
Advanced problem solving/Critical thinking mindset

Preferred Experience/Skills

Azure connectivity/authentication (service principals, managed identities, certificates)
Power BI Dataset creation/maintenance
Azure Resources: DevOps, Logic Apps, Gen 2 Storage, Purview
SQL Server, Oracle, Python, Spark

Education and/or Training:
Bachelor’s degree in Computer Science or equivalent work experience

Compensation: Starting at $110,00 USD

Holman is a global automotive leader that serves both commercial and consumer clients The Holman Way by always doing the right thing for our people, our customers, and the community since 1924. The Holman story began nearly a century ago as a single Ford dealership in New Jersey. Today, Holman, headquartered in Mount Laurel, New Jersey, is one of the largest family-owned automotive service organizations in North America with more than 6,500 employees across North America, the UK, and Germany.

Holman delivers a unique range of automotive-centric services including industry-leading fleet management and leasing; vehicle fabrication and upfitting; component manufacturing and productivity solutions; powertrain distribution and logistics services; commercial and personal insurance and risk management; and retail automotive sales as one of the largest privately owned dealership groups in the United States. Guided by its deeply rooted core values and principles, Holman is continuously Driving What’s Right.

Holman provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training."
425,Data Engineer,Leidos,"8301 Greensboro Dr, McLean, VA 22102","$97,500 - $202,500 a year","Description
Job Description:
Leidos seeks a data engineer to provide programming support for loading large volume data collections into a variety of data holding areas both on premises and in the cloud. The successful candidate will join a highly collaborative, cross disciplinary team of data scientists, data engineers, and applications developers to deliver mission critical analytics support to our customer.
Primary Responsibilities:
Analyze new data collections and determine optimal data ingest processes
Create and maintain NiFi Schemas for new and existing data feeds
Develop tools or modify existing ones to preprocess, modify, aggregate, load, index, and archive data collections into clusters and AWS in near real time
Implement all proper access controls
Generate metrics to track data ingest optimization and ensure data integrity and provenance are maintained
Document data flows
Basic Qualifications:
Must have a Bachelor's degree in a relevant field and minimum of 8 years experience OR a Master's degree and at least 6 years of experience
Must have a TS/SCI with Poly to be considered
Experience in Computer Science, Computer Engineering, Systems engineering, or closely related discipline
Extensive experience in Extract-Transform-Load processes
Demonstrated extensive experience with cloud services (AWS strongly preferred)
Demonstrated experience with cloud based database services such as Databricks and EMR
Understanding of SQL database structures and mapping data structures between different SQL databases
Knowledge of API development techniques
Preferred Qualifications:
Demonstrated experience with AWS long-term storage options
Experience with NiFi strongly preferred, but would train if candidate has experience in Java or using Pentaho
Experience working with a Hadoop cluster
Familiarity with open source data engineering tools and processes
Experience designing for intelligence applications

Pay Range:
Pay Range $97,500.00 - $150,000.00 - $202,500.00
The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law."
426,Software Engineer - QA - Data (Remote),Manufacturers Bank,"Scottsdale, AZ 85255•Remote",N,"NEW DIGITAL BANK MISSION STATEMENT:
Join us on our mission to create a completely new, 100% digital bank that truly serves customers' best interests. We are a close-knit and fun-loving team of seasoned financial services professionals who came together for the challenge of building a bank from scratch - and we are committed to doing it all the right way (from technology infrastructure to modern marketing to customer experience).
We work with the flexibility and speed of a start-up. But we also have significant stability and capital from being part of the SMBC Group (Sumitomo Mitsui Banking Corporation). SMBC is the 2nd largest bank in Japan and the 12th largest bank in the world with operations in over 40 countries. And SMBC is committed to disrupting the US marketplace with ground-breaking products.
It's the best of both worlds, and we are seeking proven marketing leaders to propel us towards a national launch. We have both the ambitious growth plans and the 'patient capital' necessary to execute a multi-year plan. Join us on the journey to deliver an exciting concept of evolved banking.

JOB SUMMARY, DUTIES & RESPONSIBILITIES:
Partner with Data Engineering development teams to enable code delivery, automated testing, and assurance of product reliability.
Create, communicate, and enforce data quality management policies, processes, and procedures.
Create effective test plans and data sets related to functional testing and end-to-end testing for ETL, database and reports.
Review requirements, specifications, and technical documentation to provide meaningful feedback.
Articulate test results, progress, and milestones to leadership and development teams.
Create jobs and scripts to automatically test the quality of data throughout our data warehouse environments.
Create, document, and execute test cases to support product releases."
427,Data Engineer,High5,United States,N,"Required Skills:
Bachelor’s degree or four or more years of work experience.
3 or more years of relevant work experience.
Experience programming/scripting/coding (SQL, Python, etc.) and knowledge of relational databases.
Experience with data platforms & tools: Teradata, Oracle, BigQuery, Snowflake, Stibo, etc.
Experience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, MySQL, etc."
428,Data Engineer,Kaar Tech PH,Remote,"$2,000 - $5,000 a month","Responsibilities
· Strong experience with data engineering tools / frameworks on cloud for Object-oriented/object function scripting using languages such as Python, PySpark, Scala, or similar
· Strong ability to design, build and manage data pipelines in PySpark, Apache Airflow and related technologies for data structures encompassing data transformation, data models, schemas, metadata, and workload management.
· Strong experience with popular database programming in relational and nonrelational environments like Snowflake, AWS Redshift, Google Big Query, Azure SQL DB, and similar platforms
· Experience in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using traditional data integration technologies. These should include ETL/ELT, CDC, message-oriented data movement and upcoming data ingestion and integration technologies such as stream data integration.
· Strong experience in working with and optimizing existing data pipeline processes and data integration and data preparation flows and helping to move them in production
· Experience in working with both open-source and commercial message queuing technologies such as Kafka, Amazon Simple queuing Service, stream data integration technologies such as Apache Nifi, Apache Kafka Streams, Databricks and stream analytics technologies such as Apache Kafka KSQL
Job Types: Full-time, Contract
Salary: $2,000.00 - $5,000.00 per month
Schedule:
8 hour shift
Experience:
SQL (Preferred)
Python (Preferred)
Work Location: Remote"
429,Data Engineer,Synchrony,"Chicago, IL 60601","$60,000 - $112,000 a year","Job Description:
Role Summary/Purpose:
The candidate Big Data Engineer will join an Agile scrum team and perform functional & system development for Synchrony’s Enterprise Data Lake.

As a Big Data Engineer the ability to integrate data across internal and external sources, provide analytical insights, and integrate with our critical systems are key skills. The engineer will participate in data analysis efforts to ensure the delivery of high-quality data ingestion, standardization and curation and maintain compliance with the applicable Data Sourcing, Data Quality, and Data Governance standards. The engineer will drive quality through the entire software development lifecycle with focus on functional requirements, efficiency, and methodology. The engineer will work cross-functionally with operations, other data engineers and product owner to assure capabilities are delivered that meet business needs.

We’re proud to offer you choice and flexibility. You have the option to be remote, and work from home, or come into one of our offices. You may be occasionally requested to commute to our nearest office for in person engagement activities such as team meetings, training and culture events.
Essential Responsibilities:
Develop big data applications for Synchrony in Hadoop ecosystem
Participate in the agile development process including backlog grooming, coding, code reviews, testing and deployment
Work with team members to achieve business results in a fast paced and quickly changing environment
Work independently to develop analytic applications leveraging technologies such as: Hadoop, NoSQL, In-memory Data Grids, Kafka, Spark, Ab Initio
Provide data analysis for Synchrony’s data ingestion, standardization and curation efforts ensuring all data is understood from a business context
Identify enablers and level of effort required to properly ingest and transform data for the data lake.
Profile data to assist with defining the data elements, propose business term mappings, and define data quality rules
Work with the Data Office to ensure that data dictionaries for all ingested and created data sets are properly documented in data dictionary repository
Ensure the lineage of all data assets are properly documented in the appropriate enterprise metadata repositories
Assist with the creation and implementation of data quality rules
Ensure the proper identification of sensitive data elements and critical data elements
Create source-to-target data mapping documents
Test current processes and identify deficiencies
Investigate program quality to make improvements to achieve better data accuracy
Understand functional and non-functional requirement and prepare test data accordingly
Plan, create and manage the test case and test script
Identify process bottlenecks and suggest actions for improvement
Execute test script and collect test results
Present test cases, test results, reports and metrics as required by the Office of Agile
Perform other duties as needed to ensure the success of the team and application and ensure the team’s compliance with the applicable Data Sourcing, Data Quality, and Data Governance standards


Qualifications/Requirements:
Bachelor's degree in a quantitative field (such as Engineering, Computer Science, Statistics, Econometrics); in lieu of degree, High School Diploma/GED and minimum 2 years of Information Technology experience
Hands-on experience writing shell scripts, complex SQL queries, Hive scripts, Hadoop commands and Git
Ability to write abstracted, reusable code components
Programming experience in at least one of the following languages: Scala, Java or Python
Analytical mindset
Willingness and aptitude to learn new technologies quickly
Superior oral and written communication skills;
Ability to collaborate across teams of internal and external technical staff, business analysts, software support and operations staff.

Desired Characteristics:
Performance tuning experience
Exposure to the following Ab Initio tools: GDE – Graphical Development Environment; Co>Operating System ; Control Center; Metadata Hub; Enterprise Meta>Environment; Enterprise Meta>Environment Portal; Acquire>It; Express>It; Conduct>It; Data Quality Environment; Query>It.
Familiar with Ab Initio, Hortonworks/Cloudera, Zookeeper, Oozie and Kafka
Familiar with Public Cloud (i.e. AWS, GCP, Azure) data engineering services
Familiar with data management tools (i.e. Collibra)
Background in ETL, data warehousing or data lake
Strong business acumen including a broad understanding of Synchrony business processes and practices
Demonstrated ability to work effectively in an agile team environment
Financial Industry or Credit processing experience
Experience with working on a geographically distributed team managing onshore/offshore resources with shifting priorities
Previous experience working in client facing environment
Proficient in the maintenance of data dictionaries and other information in Collibra
Excellent analytical, organizational and influencing skills with a proven track record of successfully executing on assignments and initiatives

Grade/Level: 09

The salary range for this position is 60,000.00 - 112,000.00 USD Annual

Salaries are adjusted according to market in CA, NY Metro and Seattle.
Some positions are bonus eligible.
Eligibility Requirements:
You must be 18 years or older
You must have a high school diploma or equivalent
You must be willing to take a drug test, submit to a background investigation and submit fingerprints as part of the onboarding process
You must be able to satisfy the requirements of Section 19 of the Federal Deposit Insurance Act.
New hires (Level 4-7) must have 9 months of continuous service with the company before they are eligible to post on other roles. Once this new hire time in position requirement is met, the associate will have a minimum 6 months’ time in position before they can post for future non-exempt roles. Employees, level 8 or greater, must have at least 24 months’ time in position before they can post. All internal employees must consistently meet performance expectations and have approval from your manager to post (or the approval of your manager and HR if you don’t meet the time in position or performance expectations).
Legal authorization to work in the U.S. is required. We will not sponsor individuals for employment visas, now or in the future, for this job opening. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.
Our Commitment:
When you join us, you’ll be part of a diverse, inclusive culture where your skills, experience, and voice are not only heard—but valued. We celebrate the differences in all of us and believe that our individual, unique perspectives is what makes Synchrony truly a great place to work. Together, we’re building a future where we can all belong, connect and turn ideals into action. Through the power of our 8 Diversity Networks+, with more than 60% of our workforce engaged, you’ll find community to connect with an opportunity to go beyond your passions.
This starts when you choose to apply for a role at Synchrony. We ensure all qualified applicants will receive consideration for employment without regard to age, race, color, religion, gender, sexual orientation, gender identity, national origin, disability, or veteran status.
Reasonable Accommodation Notice:
Federal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment.
If you need special accommodations, please call our Career Support Line so that we can discuss your specific situation. We can be reached at 1-866-301-5627. Representatives are available from 8am – 5pm Monday to Friday, Central Standard Time
Job Family Group:
Information Technology"
430,Snowflake Data Engineer with DBT (10+ Years),Hash Technologies,Remote,$70.79 - $80.52 an hour,"Title : Snowflake Data Engineer with DBT
Location : REMOTE
Job Description:
Must have about 10+ years of work experience with:
Programming/development
Data warehousing
Experience with the end-to-end implementation of Snowflake cloud data warehouse or end-to-end data warehouse implementations on-premises
Experience with Data Vault 2.0 data modeling
Experience with ETL/ELT, DBT Cloud, SnowSQL, SSIS, stored procedures, Python
Experience with version control tools like Azure DevOps
Experience Setting up resource monitors, Deploying RBAC controls, virtual warehouse sizing, query performance tuning, troubleshooting
Solid SSIS and SQL skills
Able to present complex technical concepts to audiences of varying sizes and levels.
Experience with Agile or similar scrum methodology
Ability to appropriately manage confidential property and corporate information.
Experience in working with all levels of management, including executives and IT operational teams.
Must possess in-depth analytical, problem solving and critical thinking skills and the ability to master new concepts quickly
Strong commitment to customer service
Ability to effectively communicate findings in English, in both written and oral form.
Ability to successfully acquire any licensing approvals that may be needed to provide services.
Must have the capability to learn other platforms as they may be adopted by the company.
Job Type: Contract
Salary: $70.79 - $80.52 per hour
Experience level:
9 years
Schedule:
8 hour shift
Experience:
Snowflake: 5 years (Required)
DBT: 3 years (Required)
Data warehouse: 9 years (Preferred)
Work Location: Remote"
431,Data Engineer/Analyst,Labcorp,"Durham, NC 27709",N,"Labcorp is recruiting a Data Engineer/Analyst for a dynamic team in either Burlington or RTP, NC.
Get ready to redefine what’s possible and discover your extraordinary potential at Labcorp. Here, you’ll have the opportunity to personally advance healthcare and make a difference in peoples’ lives with your bold ideas and unique point of view. With the support of exceptional people from across the globe and an energized purpose, you’ll be empowered to own your career journey with mentoring, training and personalized development planning.

At Labcorp we believe in the power of science to change lives. We are a leading global life sciences company that delivers answers for crucial health questions because we know that knowledge has the potential to make life better for all. Through our unparalleled diagnostics and drug development capabilities, we provide insights and accelerate discoveries that not only empower patients and providers but help medical, biotech and pharmaceutical companies transform ideas into innovations.
Overview:
Reviews, evaluates, and maintains data for computer processing. Analyzes data for system performance and functionality. Works directly with users to resolve data conflicts. Recommends methods, tools, or software to maximize performance.
Skill Requirements
Position Responsibilities And Expected Outcomes:
Work on complex data initiatives with broad impact and act as key participant in large scale software planning for the Technology area
Perform data analysis and modelling tasks within a data warehouse environment.
Perform SQL queries to analyze and troubleshoot issues.
Discover problems in data and applications and design solutions with engineering and product leadership.
Strategically collaborate and consult with internal partners to resolve highly risky data engineering challenges
Demonstrated ability to solve complex data engineering problems; end to end problem resolution and continuous improvement mindset.
In-depth knowledge and experience with Data Engineering essentials such as scripting languages, source control, workflow scheduling, relational and non-relational SQL, and development of complex data solutions
Required Qualifications:
2+ years of Data Engineering experience, or equivalent demonstrated
2+ years of relevant Python development experience
Experience with Databricks , Spark, Hive, AWS EMR/S3, Data Stage or similar systems for performance.
Experience with AWS technologies like Lambda, S3
Familiarity with modern build pipelines, tools, CI/CD concepts
Experience in data modelling within a data warehouse environment
Desired qualifications:
ETL (Software agnostic) Experience
Databricks, Hive, Datastage and Oracle SQL Experience
Experience in query and/or SQL tuning
CI/CD Tool Experience
Experience working in an Agile Team
Understanding of SDLC Requirements
Knowledge on mainframe
License/Certification/Education:
Normally requires a B.S. Degree in Computer Science w/1-3 years of relevant experience.
Labcorp is proud to be an Equal Opportunity Employer:
As an EOE/AA employer, Labcorp strives for diversity and inclusion in the workforce and does not tolerate harassment or discrimination of any kind. We make employment decisions based on the needs of our business and the qualifications of the individual and do not discriminate based upon race, religion, color, national origin, gender (including pregnancy or other medical conditions/needs), family or parental status, marital, civil union or domestic partnership status, sexual orientation, gender identity, gender expression, personal appearance, age, veteran status, disability, genetic information, or any other legally protected characteristic. We encourage all to apply.
For more information about how we collect and store your personal data, please see our Privacy Statement."
432,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
433,"Data Engineer, US",Chartboost,"Boise, ID•Remote","$50,000 - $166,500 a year","Who we are:
Chartboost is the leading in-app monetization and programmatic advertising platform. We reach a global audience of over 700 million monthly active users and process over 2.7 trillion monthly advertising auctions. The Chartboost SDK is one the most widely integrated mobile ad SDKs and through the Chartboost Exchange, Ad Network, DSP and other services, we empower mobile app developers to build businesses, while connecting advertisers to highly engaged audiences.
Chartboost is looking to hire an accomplished Data Engineer to join our US team, helping us build and maintain a data platform that supports diverse uses. This includes data analysis, exploration, aggregation, user modeling, and scalable training systems. As part of the Data Team you will be a key player in our small, nimble, internationally-distributed team and drive significant impact all across the company's data technology and business.
We currently use Python, Java, and Scala to develop tools with Spark, Kafka, Airflow, MySQL, Druid, Spinnaker, and Kubernetes. We run in both GCP and AWS, but primarily work with Dataproc, Dataflow, BigQuery and Big Table. You will have the opportunity to join us in exploring new technologies and use them to design, deploy and operate highly performant systems.
In this role you are expected to be comfortable working to high standards as a professional data engineer, dealing with huge amounts of business-critical data (PBs), and to contribute across a full spectrum of responsibilities from architecture to ops.
Impact you will make:
Develop high-quality reliable data pipelines that convert data streams into valuable information
Design, implement and deploy both real time and batch data processing pipelines for internal and external customers
Develop tools to monitor, debug, analyze and operate our data infrastructure
Design and implement data technologies that can scale for hundreds of millions of users
Collaborate with our product and business teams to deliver valuable new features and functions
Who you are:
BS in Computer Science or related technical discipline or equivalent experience
2+ years of professional experience in data engineering environments
2+ years of experience with SQL and programming in any of Python/Java/Scala or similar HLL
Experience with data pipelines processing larger than 10TB of data is a plus
Experience working in cloud environments, ideally with GCP or AWS
Strong experience in improving performance of queries and data jobs and scaling the system for exponential growth in data volumes and traffic
Expert debugging skills and enthusiasm for automation to deliver high-quality reliable systems
Comfortable with modern development tools such as Git and Confluence and working in a distributed agile team environment with both high autonomy and regular collaboration
Perks:
Competitive salary with bonuses based on performance
Restricted Stock Units (RSUs) - you will have the potential of RSUs depending on the level/role
Comprehensive medical, dental and vision insurance
401(k) plan with match through Fidelity
Catered lunches and fully stocked kitchens
Commuter Program
Flex Vacation – personal time to refresh your mind/body/soul, spend time with loved ones and celebrate life events. There is no accrual or specific limit to the amount of time an employee may use
More about us:
We are proud of the product we've built and appreciate the impact it has on other people's businesses and lives. We want to be surrounded by people who are always finding opportunities to try something new and grow. We love data and anything that helps drive intelligent decisions and always design with the user in mind. Sounds like a fit? Join us, and be part of the team that will change the future of mobile gaming!
We are an equal opportunity employer — we celebrate diversity and are committed to creating an inclusive environment for all employees and make our hiring decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
California Residents, please review the Chartboost California Employment Candidate Privacy Notice before submitting any personal information.
This is a fully remote role that may be based anywhere in the United States.

The pay range for this position in California at the start of employment is expected to be between $80,000 and $185,000 per year. For applicants based in Colorado at the start of employment is expected to be between $50,000 and $166,500 per year. For applicants based in New York City and New Jersey at the start of employment is expected to be between $75,000 and $185,000 per year.
However, base pay offered is based on market location, and may vary further depending on individualized factors for job candidates, such as job-related knowledge, skills, experience, and other objective business considerations. Subject to those same considerations, the total compensation package for this position may also include other elements, including a bonus and/or equity awards, in addition to a full range of medical, financial, and/or other benefits. Details of participation in these benefit plans will be provided if an employee receives an offer of employment. If hired, employee will be in an ""at-will position"" and the Company reserves the right to modify base salary (as well as any other discretionary payment or compensation or benefit program) at any time, including for reasons related to individual performance, Company or individual department/team performance, and market factors.
The pay range for this position for applicants based in Washington at the start of employment is expected to be between $70,000 and $166,500 per year.
However, base pay offered is based on market location, and may vary further depending on individualized factors for job candidates, such as job-related knowledge, skills, experience, and other objective business considerations. Subject to those same considerations, the total compensation package for this position may also include other elements, including a bonus and/or equity awards and eligibility to participate in our 401(K) plan, in addition to a full range of medical, dental, vision, and basic life insurance. Employees will also receive 13 paid holidays per calendar year, unlimited discretionary time off, and will receive 10 sick days per calendar year. Details of participation in these benefit plans will be provided if an employee receives an offer of employment. If hired, employee will be in an ""at-will position"" and the Company reserves the right to modify base salary (as well as any other discretionary payment or compensation or benefit program) at any time, including for reasons related to individual performance, Company or individual department/team performance, and market factors."
434,"Engineer, Data",Ayar Labs,"San Jose, CA","$142,500 - $177,500 a year","Job Title: Data Engineer:

We are seeking a Data Engineer to design, build, and maintain the infrastructure necessary for the automated storage, processing, and analysis of measurement data and design information. As a Data Engineer, you will be responsible for the entire lifecycle of data pipeline development. This includes working with small, manually generated input datasets with a wide variety of formats, and later developing highly automated and rigorously defined pipelines. In addition, you will drive the adoption of data format specifications from manufacturing suppliers, as well as adhere to data format specifications from customers.

The Test Team is responsible for systems that collect measurements for the Design Teams (R&D, product development), Manufacturing and Operations (quality control screens), and Reliability Teams (product qualification, ongoing reliability testing). You will collaborate with Test Engineers to automate raw-to-parametric data processing in hands-off pipelines and provide Python tools and frameworks for data producers and consumers. This opening is our second Data Engineer on the team. Code for extracting and storing parameters from raw data has been written by many different stakeholders in many different styles, but within Ayar Labs Engineering you will find a collaborative atmosphere with people eager to help our new data engineering development efforts succeed in owning and developing this code into uniform, scalable, professionally maintained systems.

Responsibilities:
Developing, maintaining, and scaling data pipelines: Designing, developing, and maintaining ETL pipelines to extract data from various sources, transform it, and load it into a data warehouse or other data storage systems.
Building and maintaining data infrastructure: Designing and maintaining data storage and processing systems such as databases, data lakes, and data warehouses.
Ensuring data quality and integrity: Ensuring the accuracy, consistency, and reliability of the data by developing and implementing data validation and testing strategies.
Collaborating with data scientists and analysts: Working closely with data scientists and analysts to understand their data needs and help them to access and analyze data effectively.
Automating data processing tasks: Developing automation scripts to perform routine data processing tasks, such as data cleaning, data transformation, and data loading.
Implementing data security and access controls: Ensuring the security and privacy of data by implementing data access controls, data encryption, and data masking.
Staying up-to-date with the latest technologies: Keeping up-to-date with the latest technologies, trends, and best practices in data engineering and applying them to improve the efficiency and effectiveness of data processing and analysis.
Requirements:
Bachelor’s Degree in Computer Science, Software Engineering, or similar with emphasis on Data Engineering; OR Bachelor’s degree in Applied Physics, Engineering, or similar with 3 years of experience in a Data Engineering role
Experience processing test and measurement data from electronic engineering, photonics, or similar; and meeting requirements of internal data customers in R&D, product engineering, or manufacturing
Proficiency in Python, NumPy, Pandas, git.
Experience with Extract, Transform, Load and/or Extract, Load, Transform paradigms.
Baseline proficiency with statistical analysis.
Experience with data modeling, data warehousing, and ETL tools.
Excellent communication and collaboration skills - must be able to present plans, gather feedback, and proactively seek information relevant to assigned projects.
Preferences:
Familiarity with Dagster or other data orchestration tools
Proficiency with SQL and relational database systems
Domain-specific knowledge in electronics or photonics test and measurement, or background in electronic or photonic experiments.
Experience with MongoDB.
Familiarity with cloud computing platforms such as AWS, Azure, or Google Cloud Platform.
If you are a Data Engineer with a passion for building and maintaining data infrastructures, we encourage you to apply for this exciting opportunity.
Salary Range: $142,500 to $177,500
NOTE TO RECRUITERS:
Principals only. We are not accepting resumes from recruiters for this position. Remuneration for recruiting activities is only applicable subject to a signed and executed agreement between the parties. Please don’t send candidates to Ayar Labs, and do not contact our managers.
About Ayar Labs:
At Ayar Labs we’re about to revolutionize computing by moving data with light. We’re unleashing processing power for artificial intelligence, high performance computing, cloud and telecommunications by removing the bottlenecks created by today’s electrical I/O - making it possible to continue scaling computing system performance. Ayar Labs is the first to deliver an optical I/O solution that combines in-package optical I/O chiplets and multi-wavelength remote light sources to replace traditional electrical I/O. This silicon photonics-based I/O solution enables chips to communicate with each other from millimeters to kilometers, to deliver orders of magnitude improvements in latency, bandwidth density, and power consumption.

With our strong collaborations with industry leaders and government, our deep ties to MIT and UC Berkeley, and our commitment to hiring the best engineers in photonics and electronics, joining our team gives you the opportunity to collaborate with renowned experts on challenging, paradigm-shifting work.

We are passionate about delivering in-package optical I/O at scale, leveraging the strength of our patent portfolio and our team of leading interdisciplinary experts. We believe that deep cross-collaboration between teams facilitated by honest, open debate is the best way to drive innovation and achieve big wins. Join our team and experience the possibilities.

Resources:
Executives from Intel and GLOBALFOUNDRIES share their thoughts on Ayar Labs and the promise of in-package optical I/O (video)
Ayar Labs in the News and Recent announcements
LinkedIn and Twitter

Ayar Labs is an Affirmative Action/Equal Opportunity Employer and is strongly committed to all policies which will afford equal opportunity employment to all qualified persons without regard to age, national origin, race, ethnicity, creed, gender, disability, veteran status, or any other characteristic protected by law."
435,Senior Data Engineer - Remote,Mass General Brigham(PHS),"Somerville, MA 02145•Remote",N,"Senior Data Engineer - Remote
- (3235191)

Summary
As a member of the Enterprise Operations Insight team, the individual will be part of a fast-paced environment committed to delivering critical strategic and operating insights to key business executives and decision makers across the Mass General Brigham enterprise. This new analytics department within the MGB Office of the COO will be responsible for the delivery of data, analysis, and insights necessary to meet the patient clinical care delivery mission, vision, strategy, goals, and objectives of the MGB system, and sites and affiliates. The person will leverage Data Integration tools such as SQL, SSIS, Informatica, Snowflake and others to integrate data into MGB’s data resources for use with internal dashboards, reports and scorecards, as well as for adhoc analyses. The person will have responsibility for building and managing ETL jobs, job scheduling, and monitoring system performance. The incumbent will have a good understanding of data warehouse implementations as well as possess good data analysis, and SQL skills. This person will take on project management responsibilities to manage the development and implementation of new features and functions for projects that are small to medium in size. This person will need to work independently as well as part of a collaborative team.

Principle Duties and Responsibilities
Work with end users, EOI analysts, technical staff, and project team members to plan, design, develop, implement, and enhance business analytics capabilities through data integration of various Mass General Brigham and external data sources.
Perform data analysis, including data profiling into MGB data warehouses, to obtain a good understanding of data availability, relationships and nuances for consideration of ETL solution design.
Develop and maintain data integration/ETL jobs in order to deliver information requests and deepen the analytics capabilities of MGB operations, fiscal, and administrative staff.
Triage, troubleshoot and resolve data issues from end users and internal team members.
Develop and maintain standards pursuant to MGB’s architecture, report documents, dashboards, cubes, and other data warehousing development practices.
Partner with MGB Digital (IS) teams to leverage existing tools, methods, and processes in order to deliver timely information to operations stakeholders across MGB, including partnering on an overall platform redesign, leveraging Microsoft Azure, Snowflake, and Informatica cloud tools.
Create and/or perform peer review of Business ETL code from other analysts and programmers to ensure that all code and processes conform to MGB and EOI standards and guidelines.
Create and execute test plans and test scripts for unit and system testing of ETL code and reports, based upon requirements specifications.
Develop and enforce change management and versioning processes surrounding new releases of ETL jobs and evaluate and recommend new tools and methodologies to assist with change and version management.
Migrate ETL job changes between development, test, training, and production environments and maintain a proper development cycle.
Assist with performing upgrades of MGB’s data environment when necessary.
Assist with performing change impact analysis on code changes in order to assist with estimating level of effort required for code changes and testing.
Review current operational data structures and data flows and recommend optimizations and opportunities for automation.
Evaluate and recommend new tools and methodologies to assist in meeting business requirements through data integration and reporting.
Provide training to EOI team and end-users on key analytic and reporting systems and databases when necessary.
May represent EOI on internal and external committees or task forces as needed. The incumbent may play a role in presenting data and analyses to key stakeholder groups independently, as requested.
Performs all other related duties as required.

Skills and Abilities

Two (2) or more years of experience as an ETL developer or similar role working with complex SQL queries and extracting data.
Proven analytical and problem-solving skills, along with strong written and verbal communication skills.
Experience with relational database management systems (RDBMS), and an understanding of relational database design.
Experience with cloud environments such as Microsoft Azure and Snowflake preferred
Experience with data integration tools such as Informatica preferred
Excellent written and oral communication skills
Proven ability to pay attention to details in a fast-paced environment with multiple initiatives. Keeps track of various deadlines and is consistent in meeting them.
Ability to be flexible, versatile and adaptable in day to day activities conducted in a fast-paced environment.

Qualifications
Must have a bachelor’s degree preferably in the area of management information systems or similar; masters degree is beneficial.
Must possess strong oral and written communication skills.
Must be capable of working independently with limited to no supervision.
Must possess a strong background in data warehousing projects relative to data integration, showing progressive experience in this area.
Must be able to identify, triage, and resolve or dispatch issues.
Must be willing to contribute to and foster a team player culture where all are encouraged and willing to share information accurately.
Experience with cloud data warehousing environments such as Microsoft Azure and Snowflake are a plus
Experience with ETL tools such as Informatica, Data Stage, and Ab Initio are a plus
Must possess strong data analysis skills and be able to perform data analysis using SQL, SAS or similar query languages
Experience in Health Care a plus, but not required

EEO Statement

Mass General Brigham (formerly Partners HealthCare) is an Equal Opportunity Employer & by embracing diverse skills, perspectives and ideas, we choose to lead. All qualified applicants will receive consideration for employment without regard to race, color, religious creed, national origin, sex, age, gender identity, disability, sexual orientation, military service, genetic information, and/or other status protected under law.

Primary Location MA-Somerville-MGB Assembly Row
Work Locations MGB Assembly Row 399 Revolution Drive Somerville 02145
Job Business and Systems Analyst
Organization Mass General Brigham(PHS)
Schedule Full-time
Standard Hours 40
Shift Day Job
Employee Status Regular
Recruiting Department MGB Digital
Job Posting Mar 6, 2023"
436,Senior Data Engineer (Remote),CareFirst BlueCross BlueShield,"Owings Mills, MD 21117•Remote",N,"Resp & Qualifications
PURPOSE:
The Senior Data Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (Relational and NoSQL, distributed and converged) with emphasis on reliability, automation and performance. This role will focus on developing solutions and helping transform the company's platforms deliver data-driven, meaningful insights and value to company.

ESSENTIAL FUNCTIONS:
Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using multiple technologies.
Interprets data, analyzes results using statistical techniques, and provides ongoing reports. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.
Develops data models by studying existing data warehouse architecture; evaluating alternative logical data models including planning and execution tables; applying metadata and modeling standards, guidelines, conventions, and procedures; planning data classes and sub-classes, indexes, directories, repositories, messages, sharing, replication, back-up, retention, and recovery.
Creates data collection frameworks for structured and unstructured data.
Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies.
Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
Applies and implements best practices for data auditing, scalability, reliability and application performance.

SUPERVISORY RESPONSIBILITY:
Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.

QUALIFICATIONS:

Education Level: Bachelor's Degree in Computer Science, Information Technology or Engineering or related field OR in lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.

Experience: 5 years Experience with database design and developing modeling tools. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.

Knowledge, Skills and Abilities (KSAs)
Knowledge and understanding of at least one programming language (i.e., SQL, NoSQL, Python).
Knowledge and understanding of database design and implementation concepts.
Knowledge and understanding of data exchange formats.
Knowledge and understanding of data movement concepts.
Strong technical and analytical and problem solving skills to troubleshoot to solve a variety of problems.
Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities.
Department
Department:Actuarial Systems
Equal Employment Opportunity
CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
Hire Range Disclaimer
Actual salary will be based on relevant job experience and work history.
Where To Apply
PeopleSoft/Self Service/Recruiting
Closing Date
Please apply before: 03/21/2023
Federal Disc/Physical Demand
Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
PHYSICAL DEMANDS:
The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
Sponsorship in US
Must be eligible to work in the U.S. without Sponsorship.
#LI-CB1"
437,Data Engineer,Legal & General America,"3275 Bennett Creek Ave, Frederick, MD 21704",N,"Overview:
Purpose and Objective

Legal and General America is looking for a Data Engineer to help build and expand a leading data platform in the financial services industry.

The Data Engineer will contribute to the growth of our Enterprise Data Warehouse and our wider cloud-based data platform. They will deliver excellent data products and adhere to key design principles while following established best practices. The Data Engineer will work closely with our architects and infrastructure teams to build secure, scalable solutions for operational, reporting, and analytical data needs. The Data Engineer may also collaborate directly with customers on their mission-critical data workloads and analytical needs. As a Data Engineer, you will get the opportunity to develop your skills and collaborate across Legal & General’s US based and international teams in an engaging and dynamic environment.
Responsibilities:
Essential Responsibilities
Drive data projects through to delivery removing obstacles as needed to ensure success.
Gain a working understanding of Legal and General America as a business and collaborate with leaders to contribute to cohesive, end-to-end experiences for customers.
Consult on data projects by analyzing complex end to end data product requirements and existing business processes. Contribute to the development and implementation of data solutions.
Translate business data stories into a technical story breakdown with work estimates.
Implement production processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
Develop and maintain scalable data pipelines for both streaming and batch requirements and build out new API integrations to support continuing increases in data volume and complexity.
Responsible for producing data building blocks, data models, and data flows for varying client demands such as dimensional data, standard and ad hoc reporting, data feeds, dashboard reporting, and data science research & exploration.
Identify changes needed to improve customer/partner experience through a data driven approach.
Maintain knowledge and understanding of corporate security procedures and policies for implementation, auditing, and documentation.
Keep up to date with industry trends and apply to projects as appropriate.
Prepare technical documentation and participate in code reviews to ensure that application code meets internal standards and follows industry best practices.

Qualifications:
Education
BS or MS degree in Computer Science or Engineering OR equivalent years of work experience.

Experience/Knowledge
3+ years of hands-on experience architecting and implementing complex data solutions
1+ years of experience architecting and implementing large operational data stores
1+ years of experience with Big Data technologies like Apache Spark
Experience with DevOps for Data preferred.
Experience with Machine Learning, Natural Language Processing, Deep Learning, and related areas would be beneficial.
Strong verbal and written communication skills with excellent interpersonal communication and collaboration skills.
Ability to communicate technical details clearly across organizational boundaries.
Cloud and services experience, with Azure cloud experience highly desirable.
Excellent analytical skills with systematic and structured approach to solution design
Experience with or exposure to Agile and interactive development processes.

Interpersonal Skills
Desire to learn, embrace challenges, see failures as essential to mastery, learn from criticism and find lessons and inspiration in the success of others.
Help to create a diverse and inclusive culture where everyone can bring their full and authentic self, all voices are heard, and where we do our best work as a result.
Confident and articulate in expressing ideas to a mixed audience with varying views and challenges.
A sense of personal ownership and accountability is required, together with the ability to develop and deliver to deadlines.
Self-motivated with a strong drive for reaching across teams and organizations to make progress collaboratively.

Technical Skills
Knowledge of Azure SQL database, Databricks, and Apache Spark
Experience working with Azure Synapse, Azure Data Lake, Azure Analysis Services, Azure Data Factory desirable.
Experience with Lakehouse architecture desirable.
Experience with Azure ML and Azure Purview desirable.
Experience with Power BI or Tableau desirable.
Preferred Certifications
Data Engineering on Microsoft Azure DP-203

Reports To
Diretor of Data Engineering"
438,Sr. Data Engineer,Cars.com,"Chicago, IL 60606•Remote",N,"ABOUT US:
At Cars.com, we help shoppers meet their perfect car match, and people find their perfect career match. As one of the top places to work in Chicago, according to The Chicago Tribune, Built-In Chicago and others, we pride ourselves on a culture of growth and innovation.
Cars.com has revolutionized the automotive industry for both shoppers and sellers through technology and solutions for buyers and sellers alike. We never shy away from a challenge, move fast, collaborate across functions to approach problems from every angle. We've built a culture that's second-to-none and share core values that keep everyone working full-speed at the same goals with the same open, outcome-driven and bold attitudes.
Cars.com is a CARS brand. CARS includes the following brands: Cars.com, Dealer Inspire, DealerRater, FUEL, CreditIQ & Accu-Trade. Learn more here!
Data is the driver for our future at Cars. We're searching for a collaborative, analytical, and innovative Sr Data Engineer to build scalable and highly performant platforms, systems and tools to enable innovations with data. If you are passionate about building large scale systems and data driven products, we want to hear from you.

Responsibilities Include:
Build data pipelines and deriving insights out of the data using advanced analytic techniques, streaming and machine learning at scale
Work within a dynamic, forward thinking team environment where you will design, develop, and maintain mission-critical, highly visible Big Data and Machine Learning applications
Build, deploy and support data pipelines and ML models into production.
Act as technology lead on projects to help drive best practice design decisions
Work in close partnership with other Engineering teams, including Data Science, & cross-functional teams, such as Product Management & Product Design

Required Skills
Ability to develop Spark jobs to cleanse/enrich/process large amounts of data.
Experience with tuning Spark jobs for efficient performance including execution time of a job, execution memory, etc.
Experience mentoring and leading developers and projects of medium to high complexity.
Experience designing dimensional data models.
Sound understanding of various file formats and compression techniques.
Experience with source code management systems such as Github and developing CI/CD pipelines with tools such as Jenkins for data.
Ability to understand deeply the entire architecture for a major part of the business and be able to articulate the scaling and reliability limits of that area; design, develop and debug at an enterprise level and design and estimate at a cross-project level.
Excellent communication and collaboration skills.

Required Experience
Software Engineering | 7+ years of designing & developing complex, batch processes at enterprise scale; specifically utilizing Python and/or Scala.
Big Data Ecosystem | 5+ years of hands-on, professional experience with tools and platforms like PySpark, Airflow, and Redshift.
AWS Cloud | 3+ years of professional experience in developing Big Data applications in the cloud, specifically AWS.

Preferred:
Experience working with Clickstream Data
Experience working with digital marketing data
Experience with developing REST APIs.
Experience with Shell Scripting
Experience in deploying ML models into production and integrating them into production applications for use.
Experience with machine learning / deep learning using R, Python, Jupyter, Zeppelin, TensorFlow, etc.
#LI-JL #LI-REMOTE
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
439,"Data Engineer, Platform",NexHealth,"San Francisco, CA•Remote",N,"About NexHealth
Our healthcare system is frustratingly analog. When you live in a world of one-tap car rides, meal delivery, and unlimited streaming, why do you have to call to schedule an appointment with a doctor and are still handed a clipboard to fill in a form? NexHealth's mission is to accelerate innovation in healthcare. We're doing this by connecting patients, doctors, and developers. We're the first to fully automate the integration with health record systems, paving the way for a new generation of disruptive healthtech companies.
Here's some of what we've accomplished:
$125M Series C at $1B valuation
Manage more than 38 million patient records
100%+ annual revenue growth
Top 10% of Inc. 5000 (2022)
Engineering at NexHealth
Our engineering team needs to:
Work with a highly-aligned engineering team committed to balancing productivity, quality, and stress in a way that lets NexHealth be incredibly successful while fitting into the lives of our team members
NexHealth is looking to change the way doctors and patients think about their health data and enable a modern customer experience for patients while meeting doctors and office staff at their current technology level; we are putting upward pressure on the acceptable patient experience in the marketplace
NexHealth's engineering team is still making critical design and architecture decisions that will affect our business for years to come—getting ahead of scaling and growth issues while we address our daily challenges will be a constant tension that will require sound, calm judgment, and an adaptable mindset
Our problem requires us to build a unique and diverse set of software that presents novel challenges you probably haven't dealt with before.
What You'll Do:
As a Data Engineer, you'll help expand our business by building the systems that connect, collect, and transform data from across the company. You'll support analytics and business decisions across the company and help to shepherd our data models.
We're looking for engineers who can demonstrate expertise in several of the following areas:
Building and maintaining large PostgreSQL clusters, ETL pipelines, and data warehouses
Building fault-tolerant systems that are easy for future engineers to understand, monitor, and reason about
Making decisions based on evidence and data, collecting that evidence when needed, and helping others to collect and analyze that data rather than relying on assumptions
Communicating about your work, the consequences of your work for our customers, and what people should expect next, including likely decisions or adverse events
Minimizing threat surface area and anticipating vulnerabilities when making design decisions, even under stress
Delivering small units of value to production continuously, locking in gains so a change in priorities doesn't result in piles of unfinished work
Considering operational concerns when building software to ensure we can manage our technology ecosystem effectively
Handling incidents with discipline and a calm sense of urgency, using a risk-based response and concise, thoughtful communication
Holding the team accountable to high standards while boosting morale and driving toward shared goals
Promoting and enabling teammates and their careers while building redundancy and resiliency in capabilities and knowledge among team members
We have aggressive growth plans, so leadership skills are essential for everyone as we help onboard new team members and face big and sometimes difficult decisions.
Technologies We Use Every Day:
Python
Postgres and Snowflake
Datadog
Amazon Web Services
Kubernetes
GitHub
Ruby
Rails
Sidekiq
NexHealth Values
Solve the customer's problems, not yours
When making decisions, think from the perspective of the customer. It's easy to make decisions that make our lives simpler, but not the customers.
Do the things others are not willing to do
As a Nexer, always go after the hardest problems. Pursue things at the highest quality. Move at the fastest pace.
Take ownership
Act like a founder. Own your roles, destinies, mistakes, behavior, and our mission. The buck stops with each of us - no blaming or excuses.
Say what's on your mind, with positive intent
Be direct, proactive, transparent, and frequent in your communication.
Default trust
As a Nexer, you do not have to earn trust, trust is given to you by default. If we by default trust each other, our speed of communication, feedback, information sharing, and overall improvements will be a lot faster.
Think in first principles
We first identify the problem and then break it down to its fundamentals before diving into solutions. We constantly ask ""why"" to validate our assumptions.
Benefits
Competitive salary plus equity
Full Medical, Dental and Vision
Unlimited PTO
#LI-Remote"
440,Data Engineer,Ideal Business Advisors,"McKinney, TX","$130,000 - $180,000 a year","The Data Engineer will construct and leverage a unified data and analytic platform to deliver consistently valuable business insights. The Data Engineer implements domain driven designs of Data Models, Data APIs and Data Products. Responsible for collecting, translating, and validating data for analysis.
RESPONSIBILITIES
Utilizes a broad set of skills ranging from programming to database design and system architecture
Develops solutions in conformity with the enterprise architecture priorities and direction to enable business imperatives
Executes modern data engineering principals and data modernization practices, following target state architecture and roadmaps
Develops enterprise solutions related to operational data stores, data warehouses, analytics, and business intelligence
Implements and maintains the database systems, ensuring that the systems provide the upmost efficiency, accuracy, and availability in support of the company’s goals and strategies
Provides expertise across the Data & Analytics domain including data storage (Data Lake & Data Warehouse), data integration, business intelligence, artificial intelligence & machine learning
Prepares technical design specifications and other documentation
Designs and develops Data Models, Data Pipelines, Data Warehouses, Data Lakes, ETL/ELT processes
Ensures data is secure and compliant under regulations
Continuously improves efficiency, accuracy, and effectiveness of the data
REQUIREMENTS
Education, Experience & Minimum Requirements
Bachelor’s degree in computer science, data science, or relevant field: Master’s degree preferred.
A minimum of 10 years of designing, building and managing modern enterprise data.
Banking, insurance, or other similar financial services experience strongly preferred.
Technical Competencies
Excellent knowledge of best practices, industry trends and compliance requirements in Data Modeling, Governance and Data Management
Strong knowledge of data profiling, Data standardization, transformation, Data cleansing
Big Data architecture and knowledge of tools and technology (Big Data / Hadoop HDFS, MapReduce, Hive, HBase, Sqoop, Flume, Spark) /AWS /Bigdata platforms products)
Strong experience building big data solutions using Data Lake, Data Warehouse, Data Products
Expert level understanding of different data storage technologies, and appropriate use cases for each – Relational, NoSQL, Time-Series, Graph, etc.
Expert level understanding of different data-integration technologies – ETL/ELT, Streaming, Replication, etc.
Experience in data streaming technologies
Azure (SQL, Data Factory, Data Hub, etc.)
GCP (AI/ML, Big Query, etc.)
Containerization (i.e., Kubernetes)
Graphite, Splunk, Kafka, Snowflake
API management/gateway, JSON, Python, Query tuning
Functional Competencies
Excellent interpersonal, written, and verbal skills
Strong strategic mindset in connecting business drivers and a foundational understanding of company data in support of data driven approaches to analytics and insights
An agile, adaptable mindset that enables effective complex problem solving
Ability to balance competing priorities while maintaining professional discretion
Strong attention to detail
Job Type: Full-time
Pay: $130,000.00 - $180,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
McKinney, TX: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person

Health insurance"
441,Data Engineer,Acutus Medical,"2210 Faraday Ave Ste 100, Carlsbad, CA 92008",N,"Acutus Medical is totally focused on the development and commercialization of solutions that improve the way complex cardiac arrhythmias are diagnosed and treated. Our passionate, driven team of innovative professionals are dedicated to providing better tools for clinicians and making life better for the millions of people who suffer from these challenging conditions around the world. Are you ready to be a part of a dynamic, innovative team with a shared purpose that truly matters? If so, we are currently looking for a Data Engineer to join us in our important mission.
Position Overview
The Data Engineer is a key contributor on the Medical Affairs team overseeing the collection, preparation, quality, accessibility, and organization of Acutus-supported research data. The Data Engineer, working closely with all internal stakeholders, will be responsible for the curation and quality control of high volumes of data. This role will drive the future of our data infrastructure, ensuring data is reliable, and accessible to stakeholders. The data curation platform will integrate data from a variety of internal and external sources to support quantitative data analytics. The Data Engineer will also optimize data flow and collection, ensure data delivery is efficient, robust, standardized, organized and well labeled. The ideal candidate is proficient in Scientific Computing languages (MATLAB, Python or C++), and has experience in populating and maintaining large databases (SQL). Effective performance in this role is demonstrated by delivering high-quality data and making an impact on our decision-making process while proactively enhancing current operational efforts.

Duties and Responsibilities
Collect and assemble large complex datasets that meet technical specifications for use in scientific and developmental purposes
Process and curate high volumes of data through internal/external software sources
Crafting, maintaining efficient data pipeline architecture and ensuring quality control of the data
Optimizing data flow, ensuring data robustness, organization and consistent self-explanatory labeling
Identifying, designing, and implementing internal process improvements, such as automating manual processes, optimizing standardized data delivery, re-designing infrastructure for greater scalability, etc.
Working with Field, Technical and Medical Affairs team members to assist with data retrieval, data-related technical issues and support their data infrastructure needs
Works under general guidance. Must understand overall project goals. Is a collaborative team player and demonstrates open-mindedness and flexibility. Creates innovative and creative solutions to keep projects on-track. Maintains a high degree of quality in all work performed.
Must demonstrate mutual respect, ongoing communication and a positive outlook with both internal and external team members.

Qualifications
Requires a Bachelor's degree in Information Technology or in another Scientific/Engineering discipline preferably with Research experience, or a Master's degree.
Experience in Scientific Computing languages (MATLAB, Python, C++)
Experience scraping data from various sources, centralizing in existing or to be developed databases
Working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Ability to learn usage of new software with limited guidance (e.g., quick capacity to learn how to use our software “the Frame”)
Demonstrate an aptitude and enthusiasm for learning. Must be able to understand job duties and responsibilities, have the necessary skills/knowledge and be willing and able to continue learning and growing within the field.
Must be accurate, have strong attention to detail, and demonstrated critical thinking
Responds positively to direction from leadership and guidance received on work assignments
Demonstrates strong technical ability at a fundamental level. Will regularly collaborate with senior Medical Affairs and R&D team members. Works on moderately complex problems where analysis of situations or data requires in-depth evaluation. Exercises judgment within generally defined practices in selecting methods and techniques for obtaining solutions.
Committed to high quality standards and proactive in finding solutions to achieve successful outcomes.
Strong verbal and written communication skills with the ability to produce accurate, punctual reports/information. Must be able to read, write and speak effectively. Must be able to effectively communicate to different audiences (no technical background) and skill levels within the organization
Strong listening skills with the ability to seek constructive feedback and remain flexible and open-minded. Able to quickly adapt to change.
Capable of working under pressure and in a timely manner.

Founded in 2011, Acutus Medical is headquartered in Carlsbad, CA. We pride ourselves on being an innovative company comprised of dedicated and talented industry leaders working together to make a distinctive mark within the Electrophysiology market. Our team works diligently to fulfill the mission of bringing advanced tools for physicians and hospitals to access, identify, diagnose and treat complex arrhythmias to in order to optimize and expand the success of cardiac ablation.

The anticipated salary range for candidates is $95,000 to $115,000. The final salary offered to a successful candidate will be dependent on several factors that may include but are not limited to the type and length of experience within the job, type and length of experience within the industry, education, etc.

Our employees enjoy working in a company that truly cares about them, their career and overall wellbeing. We offer competitive salaries, comprehensive benefits, paid time off, holidays and a variety of health and wellness programs. We are steadily growing and look forward to adding more talent to our team.

Acutus Medical provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Acutus uses E-Verify for all employment verifications.
We are not accepting resumes from 3 rd party headhunters or agencies."
442,Azure Data engineer (Remote),Cognizant,"Deerfield, IL 60015•Remote",N,"We are Cognizant Artificial Intelligence
Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. However, clients need new business models built from analyzing customers and business operations at every angle to really understand them.
With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks
*You must be legally authorized to work in United States without the need of employer sponsorship, now or at any time in the future *
This is a remote position open to any qualified applicant in the United States.

Job Title: Azure Data engineer (Remote)
Roles & Responsibilities
Develop database solutions to store and retrieve information
Install and configure information systems to ensure functionality
Analyze structural requirements for new software and applications
Migrate data from legacy systems to new solutions
Design conceptual and logical data models and flowcharts
Improve system performance by conducting tests troubleshooting and integrating new elements
Optimize new and current database systems
Define security and backup procedures
Coordinate with the Data Science department to identify future needs and requirements
Provide operational support for Management Information Systems MIS
Design and Develop applications using Azure ADF Databricks
In depth understanding of Spark Architecture including Spark Core Spark SQL Data Frames Spark Streaming RDD caching Spark MLib
Use Spark SQL with various data sources like JSON Parquet and Key Value Pair
Create tables partitioning bucketing loading and aggregating data using Spark SQLScala
Understand and deploy Azure cloud and Data lake and Analytics solutions on Azure
Build ingestion to ADLS and enable BI layer for Analytics

Job summary
8-10 years of experience is required
Hands on development experience using Azure ADF Databricks
In depth understanding of Spark Architecture including Spark Core Spark SQL Data Frames Spark Streaming RDD caching Spark MLib
Expertise in using Spark SQL with various data sources like JSON Parquet and Key Value Pair
Experience in creating tables partitioning bucketing loading and aggregating data using Spark SQLScala
In depth understanding of Azure cloud and Data lake and Analytics solutions on Azure
In-depth understanding of database structure principles
Experience gathering and analyzing system requirements
Primary Skills Hands on Proficiency in Spark development PySpark
Experience in designing and developing data pipelines using ETL solutions Talend would be ideal
Experience in Big Data ecosystem components Hive HDFS etc.
Experience with RDBMS systems MySQL SQL Server etc.
Experience with Agile Development methodologies
Strong Team player with leading development team with multivendor agile team Secondary Skills Exposure to NoSQL database platforms MongoDB CosmosDB etc.

Salary and Other Compensation:
The annual salary for this position is depending on experience and other qualifications of the successful candidate.
This position is also eligible for Cognizant’s discretionary annual incentive program, based on performance and subject to the terms of Cognizant’s applicable plans.
Benefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements:
Medical/Dental/Vision/Life Insurance
Paid holidays plus Paid Time Off
401(k) plan and contributions
Long-term/Short-term Disability
Paid Parental Leave
Employee Stock Purchase Plan
Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.
#LI-KM1 #CB #Ind123
Employee Status : Full Time Employee
Shift : Day Job
Travel : No
Job Posting : Apr 12 2023
About Cognizant
Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.
Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.
Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.
If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information."
443,"Data Scientist / Data Engineer, Quant Modeling - USDS",TikTok,"Mountain View, CA","$123,911 - $269,689 a year","Responsibilities
TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity, celebrate individuality and bring people together. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Mumbai, Singapore, Jakarta, Seoul and Tokyo.

Why Join Us
At TikTok, our people are humble, intelligent, compassionate and creative. We create to inspire - for you, for us, and for more than 1 billion users on our platform. We lead with curiosity and aim for the highest, never shying away from taking calculated risks and embracing ambiguity as it comes. Here, the opportunities are limitless for those who dare to pursue bold ideas that exist just beyond the boundary of possibility. Join us and make impact happen with a career at TikTok.

About USDS
At TikTok, we're committed to a process of continuous innovation and improvement in our user experience and safety controls. We're proud to be able to serve a global community of more than a billion people who use TikTok to creatively express themselves and be entertained, and we're dedicated to giving them a platform that builds opportunity and fosters connection. We also take our responsibility to safeguard our community seriously, both in how we address potentially harmful content and how we protect against unauthorized access to user data.

U.S. Data Security (“USDS”) is a standalone department of TikTok in the U.S. This new security-first division was created to bring heightened focus and governance to our data protection policies and content assurance protocols to keep U.S. users safe. Our focus is on providing oversight and protection of the TikTok platform and user data in the U.S., so millions of Americans can continue turning to TikTok to learn something new, earn a living, express themselves creatively, or be entertained. The teams within USDS that deliver on this commitment daily span Trust & Safety, Security & Privacy, Engineering, User & Product Ops, Corporate Functions and more.

About Team & Role
The Data Science team of the US Tech Service department at TikTok USDS is responsible for providing high-quality and timely data solutions for the business and for building easy-to-use and scalable data products to support business growth.

For the quant modeling role, we are seeking a data scientist with strong data engineering and modeling background. You will be working on designing and monitoring core metrics, identifying the root causes for metric movements, and building models to inform optimal business decisions. You will also have the opportunity to build, optimize, and grow one of the largest data platforms in the world.
Qualifications
Qualifications
BS/MS in Computer Science, Math, Economics, Statistics, or other science and engineering fields
2+ years of experience in statistical modeling (Regression, Classification, Clustering, etc.)
2+ years of experience in data engineering (data ingestion, data integration, data analysis, data modeling, ETL)
Proficiency in SQL, Python / R
Hand-on experience in Big Data technologies(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.)
Self-motivated, detail-oriented, learn autonomously, and highly organized
Ability to conduct rigorous analysis and communicate conclusions to both technical and non-technical audiences
TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.

TikTok is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to us at debjani.sarkar@tiktok.com
Job Information
The base salary range for this position in the selected city is $123911 - $269689 annually.



Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.



At ByteDance/TikTok our benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support ByteDancers to give their best in both work and life. We offer the following benefits to eligible employees:



We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care.



Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off(PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability.



We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice."
444,Reservoir Data Engineer I/ II/ III (remote),EQT Corporation,"Pittsburgh, PA 15222•Remote",N,"Location: Pittsburgh, PA
Job Category: Asset Performance
Sub-Department: RSVR ENGR
Job ID: 2058
EQT is the United States largest producer of natural gas with a goal to reach Net Zero by or before 2025 (among the fastest in the industry).
From the office to the field, the #EQTeam is fueling the future. Power your potential with us.
At EQT, we are making strides toward becoming the best producer by creating long-term value for all stakeholders, including employees, landowners, communities, industry partners and investors. Our vision is to evolve EQT into a modern, connected, digitally enabled organization.
With an incredibly collaborative culture and a determined, progressive workplace, EQT was both named a National Top Workplace, as well as one of Pittsburgh’s Best Places to Work!
Join our Qrew!
Here is how the Reservoir Data Engineer role will impact our business:
The Reservoir Engineer I/II/III main goals is to realize the full value potential of our upstream assets by having a great understanding of the reservoir. This role will help the Reservoir Engineering team build, track, and maintain engineering tools, insights, and dashboards to deliver data driven type curves and perform well analysis lookback. Furthermore, this role with help ensure our well design is optimized and is evolving through a methodically planned science and technology program.
The Reservoir Data Engineer's responsibilities include but are not limited to:
· Provide petroleum reservoir data engineering support by leveraging analytics and programming skills to our digital oilfield environment
· Improve reservoir characterization, streamline asset performance insights, and optimize the way we work leveraging modern methods for data analytics
· Lead evolution of the petroleum reservoir data process management, insight electrification & department optimization
· Develop database programming and coding solutions to electrify our processes that follow IT guidelines for data modeling and data engineering tools and platforms
· Develop petroleum statistical computing methods to electrify the petroleum reservoir engineering analysis methods
· Support evolution efforts for evolving the way we use and consume data from enterprise apps
· Lead data-driven approaches and apply data security strategy to solve business problems
· Build and maintain type curves, ensure they are aligned with geology, are data-driven, reviewed regularly to ensure they are holding up to expectations
· Use reservoir engineering principles (pressure normalize rate, numerical modeling, RTA and similar technical expertise) to perform well performance reviews
· Evaluate our peers and non-op partners to feed the evolution of our science program, technology adoption and benchmark our well design performance
· Reservoir engineering support on A&D opportunities, New Ventures, and various corporate requests
Required Experience and Skills:
· Bachelor's degree in petroleum engineering or other engineering curriculum
· 1-5 years of related experience, in petroleum engineering or other data engineering or scientific roles
· Certificate or graduate degree in data analytics, data science, or data engineering with an emphasis on petroleum systems (preferred)
· Proficient in R and/or Python and SQL programming languages
· Strong coding experience in R/ RStudio Workbench, Azure Data Studio, Spotfire, Power BI, Alteryx, IronPython
· Databases Systems: Microsoft Azure and API integration methods
· Must love technology, science, and innovation. Always on the hunt for new tech to evolve the way we work
· Knowledge of basic engineering concepts, fundamentals, and theory
· Ability to think analytically and solve basic problems

The selected incumbent will be placed into the position that best suits their abilities and experience level.
Remote work is being considered for this role.
EQT Corporation and its subsidiaries is an Equal Opportunity Employer – Disabilities/Veterans"
445,Data Analytics Engineer,Sales Boomerang,Remote,N,"About Sales Boomerang and Mortgage Coach:

Hundreds of leading lenders rely on award-winning technologies from Sales Boomerang and Mortgage Coach to build lasting borrower relationships that maximize lifetime customer value. We are reimagining the lending and financial space by connecting borrowers with the right loan at the right time. Our team is always on the lookout for passionate and hardworking members who can help take us to the next level in providing lending for all.

Why join the Sales Boomerang and Mortgage Coach family?
As a remote organization since pre-COVID, our culture is built around creative and efficient ways to stay connected with each other while providing employees the autonomy to get their work done in the best way that fits their lifestyle. Whether you need to take time off to be with your family or go to a doctor’s appointment, we don’t micromanage your time as long as you’re getting your work done and meeting your goals.
Our fun and inclusive culture includes regular online and in-person activities, such as holiday parties, monthly team-building exercises and quarterly in-person events. Our team and company meetings are not all work and no play. We love getting to know each other on a personal level, so we can build a solid team that works well together.
We are an open organization and welcome honest feedback from our employees. Managers meet with their teams on a one-on-one basis every week to review projects and discuss concerns or roadblocks. Both sides have the freedom to share feedback.
The sky is the limit when it comes to your career growth here. Each employee can take ownership of their role and move into more senior-level positions. We give yearly reviews to every employee, and raises are based on goals and performance.

Job Summary:
As an Analytics Engineer, you will be responsible for developing, maintaining, and enhancing our data infrastructure and analytics capabilities. You will work with cross-functional teams to implement analytics models and create business critical dashboards that provide visibility into our product value and performance.

Supervisory Responsibilities:
This position has no direct supervisory responsibilities
Duties/Responsibilities:
Analytics development, building models to consume, transform, and expose data to stakeholders and production systems
Developing and maintaining Tableau dashboards and reports that enable stakeholders to gain insights and make data-driven decisions
Become an expert with our data warehouse and key data sources, understanding the definition, context, and proper use of attributes and metrics
Collaborate with our data and engineering teams to define and develop data pipelines, and manage our analytics stack
Create structured and well-documented data sources empowering self-service data insights within the organization
Influence processes, tools, and systems that will allow us to make better decisions in a scalable way
Required Skills/Abilities:
Strong knowledge of SQL (preferably Redshift, Snowflake, BigQuery) and how to write efficient SQL queries
DBT - Data modeling, version control, documentation and testing
Familiarity with BI tools (preferably Tableau, Looker, Mode or equivalent) and experience distributing data insights via reports and dashboards
Ability to manage ambiguity, complex problem spaces, and competing priorities
Experience working with stakeholders to intake analytics requests and translate them into tested and reproducible solutions.
Education and Experience:
BS degree in Computer Science or related technical field, or equivalent practical experience
Experience with the modern data stack (Fivetran / Snowflake / dbt / Tableau / Hightouch or equivalents)
Experience with Tableau and building visualizations and dashboards
Experience with CRMs and Marketing automation tools such as Salesforce is a plus
Experience working with financial reporting metrics (P&L, ARR, Churn, etc.) is a plus
Additional Job Requirements:
Must successfully pass a background check
Provide references
Applicants may be subject to Employment, Education, and Certification Verification.
For salary requirements, please reach out to hr@salesboomerang.com.
Our benefits include but are not limited to the following: 100% company paid medical, dental and vision; company matching 401(k), paid maternity and paternity leave, unlimited PTO package, ongoing professional development and certification opportunities, competitive salary, special employee discounts, wellness perks and flexible time off.
Sales Boomerang and Mortgage Coach are equal opportunity employers committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy or any protected characteristic as outlined by federal, state or local laws.
This policy applies to all employment practices within our organization, including hiring, recruiting, promotion, termination, layoff, recall, leave of absence, compensation, benefits, training, and apprenticeship. Sales Boomerang and Mortgage Coach makes hiring decisions based solely on qualifications, merit, and business needs at the time."
446,Data Engineer,N,"Charlotte, NC 28208",N,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language."
447,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
448,SQL Data Engineer,BPM LLP,United States,N,"BPM Overview :
What does BPM stand for? Innovation, opportunity, community, diversity, inclusivity, flexibility and so much more.

B-P-M stands for “Because People Matter,” because at our core, our people drive everything we do and how we do it.

We are a forward-thinking, full-service accounting firm providing modern solutions to businesses across the globe. We focus on comprehensive assurance, tax, and consulting services for our clients, and we provide our people and our community with the resources to lead meaningful and purposeful lives.

While we are one of the largest California-based accounting firms, our flexible work locations and schedules mean we have professionals across the continent. Our teams and our clients drive us to provide quality services and ignite unique insights and ideas that contribute to our continued success. Our clients come from different backgrounds and industries, which keep our people intellectually challenged every day.

Our initiatives and ideals lead to our continued recognition as one of the “Best Places to Work” in the Bay Area and beyond. We are dedicated to providing meaningful careers for all of our employees along with fostering an environment that allows an integrated lifestyle. Our flexible culture allows our professionals to live a balanced lifestyle between their work responsibilities and personal commitments.

Job Responsibilities:
To gain intimate knowledge of STAR data structures and code (stored procedures, functions, and triggers).
To understand reports that STAR has developed for BPM and be able to modify existing or create new ones.
Understand current pain points with STAR, research daily issues and work with STAR engineers to resolve them.
Learn BPA system within STAR. Be able to modify and create alerts and other aspects of it using STAR SQL Databases including modifying Stored Procedures.
Work closely with the Integration framework and be able to assist other developers in implementing integration modules.
Oversee STAR new releases and analyze compatibility with existing code and systems.
Take over regular update procedures currently performed by STAR such as Budget and Rates updates.
Creating and running ad-hoc SQL Queries of various complexity.
Assist with technical training and documentation on newly released STAR modules.

Qualifications / Skills:
Profound knowledge of RDBMS data design principles.
5-7 years of working with MS T-SQL writing queries and stored procedures.
Experience performing root cause analysis on data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with structured and unstructured datasets.
Expert knowledge of SQL queries performance, scaling, and optimization.
3-5 years of working experience with SQL Server Reporting Services and SSRS.
Solid experience working with SSMS or similar Integrated environments.
Excellent communication skills.

Highly Desired:
Experience with Azure SQL DB.
Familiarity working with .Net Framework, .Net Core, C#.
Development experience in working with Accounting Systems and Industries

Note: STAR will assist with all aspects of initial training related to their structures and systems
Wondering if you should apply?

BPM is powered by knowledgeable, enthusiastic, and forward-thinking people committed to developing a culture of inclusion. We recognize, develop, and empower talent and encourage diversity of thought. Your point of view, skillset and experience will only make us stronger, so if you're eager to share new ideas and try new things, we want to hear from you.

***************

BPM provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance.

Please note - this posting is for prospective candidates only. Unsolicited third party resume submissions will be considered property of BPM and will not be acknowledged or returned."
449,Software Engineer - Data Engineer,RevaComm,United States•Remote,N,"About Us
RVCM (Revacomm) is a leader in Agile Software Development, User-Centered Design, and DevSecOps. As an enterprise digital transformation company, we transform organizational challenges into powerful digital capabilities through fresh experiences and innovative technology. Here, you'll bring amazing user experiences to life through your knowledge and perspective. The problems you will solve are as dynamic as our customers, who range from the Department of Defense to financial institutions, to healthcare and even state and local agencies.

We are embracing the future of work. Make an impact, wherever you are. Home is the place where you connect with 'ohana (family), friends, and what makes you feel good. Now, work from home allows you the opportunity to make a real impact and help bring out the best in one another.

Seeking a Software Engineer/Data Engineer with data pipeline experience to join a team rapidly developing a data lake within a federal agency. The federal agency is looking at building out a scalable and dynamic data lake to make data available from various parts of the enterprise for data driven decisions. The Data Engineer will work with various departments to identify business use cases and implement ETL mechanisms of getting data into the data lake. The Data Engineer will support with data hygiene and data management. Contribute to the overall strategy of maintaining and growing the data lake to support different constituents and stakeholders. Provide analysis and feedback on governance and value of data within the data lake platform.
What You'll Do Here

Design, implement, and support data solutions that facilitate the flow and transformation of data from sources into the data lake
Architecture design and review of Cloud data pipeline, including data ingestion, extraction, and transformation services.
Implement and enhance support tools for monitoring and acting on data pipeline issues and interpret trends and patterns.
Technical design in meeting requirements of service stability, reliability, scalability, and security
Design review and code review with peer engineers
Guiding testing architecture for large scale data ingestion and transformations.
Work closely with different application and system teams to identify valuable data and business use cases
Troubleshoot any issues with the pipeline, storage, and analytic tools a part of the platform
Research, test, and implement new tools that will support the platform
Build CI/CD pipelines to be implemented within the platform

What You'll Bring to the Team
Proficiency in database queries (SQL)
Proficiency coding in Python
Experience and understanding of AWS technologies such as Lambda, DBS, S3, AWS SDK
Development experience performing ETL and/or data pipeline implementations
Experience in cloud-based big data projects (preferably in either AWS or Azure) with related tools
Working knowledge of Data Lake technologies, data storage formats and query engines, and associated concepts for building optimized solutions at scale.
Organized and goal-focused, ability to deliver in a fast-paced environment.
Strong understanding of distributed systems and Restful APIs.
Experience in designing data streaming and event-based data solutions (Kafka, Kinesis, or like)
Experience building data pipelines (Flink, Spark or like)
Skills and experience in:
Coding
Data modeling
API
Python
SQL
Data lake technologies
Snowflake
Kafka
BI tools
DevOps
Join Our 'Ohana
The 'ohana-oriented mindset is one of the pillars from which our company has been built upon. We wouldn't be here without each remarkable individual that has passed through our doors. No matter where we go in the world, it's essential that everyone under the RVCM roof, first and foremost, feel like they are ohana.

We believe in providing a safe space for all members of the RVCM 'ohana to grow and thrive. Diversity, Equity, and Inclusion is at the heart of who we are and everyone should feel valued and free to bring their most authentic self to work - without fear, without judgment, and in consideration of all backgrounds. Creating this environment is important, not only for our organization, but also for our customers and our communities.
In addition to an incredible culture you will find several benefits to being part of the RVCM ohana, including:

Fully remote work within the U.S.
Company Provided Laptop & Charger Plus A Home Office Bonus: $1,000 upon hire; $500 per year thereafter to purchase any additional equipment to improve productivity at home
Comprehensive medical, dental, and vision insurance
401(k) with company match
Health Care and Dependent Care Savings Accounts
15 days of PTO
11 Paid Holidays
Full Paid Holiday Break (Last Week of the Year)
Continuous Education & Training
Mentor Programs
Relocation expenses not covered."
450,Data Engineers,Enable Data,"Minneapolis, MN",N,"Data engineers develop, maintain, test and evaluate big data solutions within client organizations using technologies such as Spark, MapReduce, or NoSQL. A big data engineer builds large-scale data processing systems, is an expert in data pipeline and warehousing solutions.
Candidates interested in this position should have sufficient experience in software engineering such as object-oriented design, coding and testing patterns as well as experience in engineering software platforms and large-scale data infrastructures using commercial and open source technologies. Examples would include integration tools like Informatica and Talend, or orchestration solutions like Airflow or Oozie.
Data engineers should have extensive knowledge in different programming or scripting languages like Java, Linux, C++, PHP, Ruby, and Python. Also expert knowledge should be present regarding different (NoSQL or RDBMS) databases such as MongoDB or HBase. Building data processing systems with Hadoop and Hive using Java or Python should be common knowledge to a data engineer.

Benefits
Enable Data offers employees the following benefits:
Health Insurance
401k Contribution
Paid Time Off (PTO)
Training Reimbursement
Certification Reimbursement"
451,"Data Engineer (ETL Developer) – ETL, SQL, SSIS – Las Vegas, NV (HYBRID) 42211","PRIMUS Global Services, Inc","Las Vegas, NV",N,"We have an immediate long-term opportunity with one of our key clients for a position of Data Engineer, to work in Las Vegas, NV on hybrid basis.

The ideal applicant must have strong experience in SSIS, ETL and SQL.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Sneha
PRIMUS Global Services
Direct: 972-471-9498
Phone No: 972-753-6500 Ext: 405
Email: jobs@primusglobal.com"
452,Cloud Data Engineer,Intone Networks,Remote,N,"Cloud Data Engineer Perfect English/Communication skills Required 5+ years Data Engineering Java or Scala Development experience use cases with data structures and algorithms extensive experience with Apache Spark, Data Plan Storage, Delta Lake, Delta Pipelines, Performance Engineering, in addition to standard database/ETL knowledge building cloud data pipelines, architectures, data sets with advanced knowledge of stream-based, API data extraction processes (bulk API is a must) Building data models, DB schemas for read and write performance optimization Supporting data transformation in serverless, overseeing metadata automation, management in a multitenant cloud environment Role The Data & Artificial Intelligence (DAI) practice is an exciting solutions consulting group within the well-established Technology Partners' organization. This growing team is obsessed with solving the hardest problems through enabling data teams, analysts, and business users. Our work leveraging delta technologies and AI allow our customers to focus on high-ceiling work critical their overall mission (clinical cancer outcomes, education, biomedical research, precision shopper analytics, etc). Our team is well aligned with Microsoft and Databricks as channel partners - as a result, most technical work takes place within Azure and leverages Databricks' Apache Spark engine; along with our own proprietary enhancements. The responsibilities below require extensive knowledge in Apache Spark, Data Plan Storage, Delta Lake, Delta Pipelines, and Performance Engineering, in addition to standard database/ETL knowledge. Requirements - BS in Computer Science (preferable MS or PhD in distributed data systems or quantitative fields) - Understands the working nature of a multi-year effort of iterative deliverables within both client and product environments - Driven by delivering value and impact daily - 3+ years of full stack engineering experience (java or scala) - 3+ years of experience in real-world use cases with data structures and algorithms - 2+ years of experience in distributed systems, databases, and Spark - 5+ years of experience in a data engineering role - 5+ yeas of SQL experience working with relation relational databases, writing complex queries for a variety of databases - 3+ years of experience building cloud data pipelines, architectures, and data sets with advanced knowledge of stream-based, API data extraction processes (bulk API is a must) - 3+ years of experience building data models, DB schemas for read and write performance optimization - 2+ years of experience supporting data transformation in serverless and overseeing metadata automation and management in a multitenant cloud environment - Experience of BI tools is a plus"
453,Data Engineer level 3,Info Origin Inc.,Remote,$40 - $60 an hour,"Perfect Opportunity || Data Engineer Level 3 || Contract || 100% Remote-Interested?
We are hiring a Data Engineer Level 3 for our US client.
Following is the job description:
Position Title: Data Engineer Level 3
Location: New York, NY
Position Type: 100% Remote
Interview Process: Webcam/Skype
Position Duration: Contract
Who can Apply: US citizen, Green card holder or any visa holder in USA.
Required Skills:
Design, develop/ map, test, and support integration maps with mapping translation software.
Complete Integration implementation tasks with precise attention to detail
Support integration-based initiatives and tasks as required.
Experience with different health care formats such as X12, NCPD, FHIR, XML, CDA, JSON and HL7.
Experience in mapping in the healthcare industry is required.
Experience with SQL for ad hoc queries
Communication protocol experience such as sFTP
Strong Communication Skills both written and verbal.
Experience with Rhapsody Mapping software
Rhapsody Integration software experience or certification
To Know more, Kindly Apply...!!!!
Job Type: Contract
Salary: $40.00 - $60.00 per hour
Experience level:
7 years
Schedule:
8 hour shift
Experience:
Rhapsody: 7 years (Required)
SFTP: 7 years (Required)
HL7: 5 years (Required)
Work Location: Remote"
454,Data Engineer,Maddisoft,"Houston, TX 77042•Hybrid remote","$90,000 - $105,000 a year","Maddisoft has the following immediate opportunity, let us know if you or someone you know would be interested. Send in your resume ASAP. - U.S. Citizens and those authorized to work in the U.S. are encouraged to apply. Looking for W2 CONTRACT, Send in resume along with LinkedIn profile without which applications will not be considered. Call us NOW! ***Visa sponsorship is available for this position.***
Job Title: Data Engineer
Location: Houston Texas- Hybrid(2 day/week onsite)
Our Client is seeking a Data Engineer, Business Intelligence to join its Retail Business Intelligence team to strengthen the BI capabilities of client's Retail business. The position requires 4+ years of data management, modelling, and BI experience. The successful candidate will possess a strong data modeling, data visualization background and will provide expertise and best data engineering practices. The candidate should have a demonstrable record of being able to interact effectively with business stakeholders in all functional areas to gather and understand business requirements and reporting needs. Ability to learn new technologies, welcome challenges, accept new responsibilities, work well as individual and as a team and adapt quickly is essential.
Essential Duties/Responsibilities:
Gather large and complex data sets to meet business requirements.
Re-designing data marts for scalability and optimization.
Ensure data architecture supports existing and future requirements of evolving business model
Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Create and maintain documentation for each step in the data lifecycle
Develop and maintain operational dashboard and executive-level reporting using tools like Tableau, or Power BI
Interact with functional business units to analyze, gather, and prioritize business and reporting requirements
Partner with the Enterprise Data and Analytics leaders and teams to manage the key data processes, tools, and architectures needed to deliver confidence in regular reports.
Collaborate with Enterprise Data and Analytics team to implement data management, data quality best practices and standards
Partner with business leaders to develop, monitor, and report on marketing tests for new or enhanced products and campaigns
Provide ad hoc report for Retail organization for experiments in market.
Hands on development of data models in Oracle, AWS and data visualizations.
Formulate and provide status updates on project plans and schedules to meet goals/objectives
Minimum Requirements:
Bachelor's degree in Computer Science, Computer Engineering, Information Systems, or related discipline/experience with 4+ years of experience or master’s degree in Computer Science, Computer Engineering, Information Systems, or related discipline/experience with 3+ years
Hands on experience in one of programming languages like SAS/R/Python is a must.
4+ years of professional experience in business intelligence and data analysis
3+ years of SQL experience.
3+ years of programming experience.
Experience with dimensional modeling and improving data reliability and data quality.
Good understanding of cloud technologies with 1+ years hands on experience in AWS and big data distributed ecosystems.
2+ year’s experience with any one of visualization tools like Tableau/Power BI.
Good understanding of relational and non-relational databases
Experience working with large datasets.
Preferred Qualifications:
Gas or power industry experience with knowledge of the retail electricity market.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Strong organizational skills and comfort interacting with different lines of business, IT, and senior management
Comfortable with ambiguity; ability to work with incomplete data or open-ended questions, and translate into actionable items
Additional Knowledge, Skills, and Abilities:
Ability to manage multiple priorities while committed to deadlines and work overtime when required
Proficient in effectively communicating to a wide range of audiences, both written and oral
Self-starter with strong work ethic
Ability to evaluate and learn modern technologies quickly
Strong data due diligence skills ensuring data accuracy and reliability
Job Types: Full-time, Contract
Pay: $90,000.00 - $105,000.00 per year
Benefits:
401(k)
Health insurance
Life insurance
Experience level:
4 years
Schedule:
8 hour shift
Ability to commute/relocate:
Houston, TX 77042: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 4 years (Preferred)
BI: 4 years (Preferred)
Work Location: Hybrid remote in Houston, TX 77042

Health insurance"
455,Azure Data Engineer,Devcare Solutions,"Devcare Solutions in Durham, NC 27701",$75 - $85 an hour,"Build data analytics solutions using Azure Synapse serverless SQL pools
Performing data engineering with Azure Synapse Apache Spark Pools
Working with Data Warehouses using Azure Synapse Analytics
Transferring and transforming data with Azure Synapse Analytics pipelines
Experience configuring, designing, developing and testing dash boards using Power BI
Experience in using Oracle, Oracle Utilities, ETL tools or any other relational database and tools
Experience with PL/SQL or other database scripting and Relational Database modeling.
Experience with structured system development methodologies
Ability to develop relationships/partnerships with customer by responding to needs and exhibiting an sense of urgency; independently identify options.
Ability to organize, prioritize, and follow complex and/or detailed technical procedures.
Experience with Agile (Scrum, Kanban, or SAFe) development
Data Warehouse experience
Experience with Business Intelligence tools such as Business Objects
Experience with Cloud SaaS and PaaS
Knowledgeable in DevOps practices and Shift Left testing
Experience developing applications using low code no code technologies like Salesforce, Microsoft dynamics and others
Job Type: Contract
Pay: $75.00 - $85.00 per hour
Benefits:
Dental insurance
Experience level:
10 years
11+ years
5 years
6 years
7 years
8 years
9 years
Schedule:
8 hour shift
Ability to commute/relocate:
Durham, NC 27701: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
PL/SQL: 5 years (Required)
Oracle: 5 years (Required)
ETL: 5 years (Required)
Business intelligence: 5 years (Required)
DevOps practices: 2 years (Required)
Agile: 2 years (Required)
Work Location: One location
Speak with the employer
+91 9106376210"
456,Data Engineer,N,Remote,N,"Welcome to Zartico. We know applying for and taking on a new role at any company requires a leap of faith. Here are a few things about Zartico we think you should know:

Zartico’s Purpose: We believe tourism is a force for good because it builds connection, understanding, and appreciation of our world’s cultures, history and natural resources. We believe data and the right metrics allow us to make better decisions because transparent data helps focus on the right issues, problems and therefore, solutions, to be better stewards of our world's most precious destinations.
Zartico’s Mission: Zartico’s mission is to empower DMOs to be better stewards of the world’s tourist destinations through improved data intelligence and decision-making. Makers of the first Destination Intelligence Platform, Zartico harnesses and streamlines complex data to provide a full-spectrum of data science, benchmarking and analytical services for use in marketing, community development and sustainability efforts. Based in Salt Lake City, Utah, Zartico has over thirty years of experience in technology, tourism, and destination and travel marketing.
If this resonates with you, Zartico is hiring Data Engineers.

As an engineer on the core product team you'll provide intelligence to the rest of the company that will enable making better product decisions. You'll make use of the latest advances in large scale data processing to uncover insights in data. You’ll work on building critical data warehouse tables with world-class engineers towards the mission of enabling data-driven products and insights at Zartico.
You will develop data infrastructure that is able to ingest and transform data at scale coming from many different sources, different customers, and in many different varieties.
You will build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources and build robust data pipelines that collect, process, and compute business metrics from activity data.
You will create critical datasets for machine learning, growth funnels, business forecasting, and many other strategic initiatives.

Primary Technologies Required
Python
Google Cloud Platform (BigQuery, GoogleCloud Storage)
SQL
Infrastructure as Code (e.g. Terraform)

Extra Credit for

Experience with data visualizations

Experience and Mindset Needed
2+ Years of experience as a Data Engineer or Software Engineer
BA/BS in a quantitative or computer science field
Fluency in SQL and a programming language
A successful history of manipulating, processing and extracting value from large disconnected datasets
Quick learner - we face new challenges every day
Expert in coding -we develop real products

Compensation
Competitive Salary and Benefits (unlimited PTO)
Stock incentive program
Career plan and lifelong learning - We want you to grow in your role based on their curiosity, passion, and ability to learn and evolve

Zartico’s Commitment to Diversity and Inclusion
Diversity, inclusion, and belonging. We’re building a global community—one that’s safe for people of all backgrounds. We are an equal opportunity employer where our diversity and inclusion are central pillars to our company strategy. We look for applicants who understand, embrace and thrive in a multicultural and increasingly globalized world. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. When you join our team, you agree to a code of conduct.

Hiring Update: Due to the COVID-19 pandemic, our hiring process will now be completely virtual. All interviews and onboarding activities will be held online or over the phone."
457,Data Engineer II,dv01,"Atlanta, GA•Remote",N,"dv01 is lifting the curtain on the largest financial market in the world: structured finance. The $16+ trillion market is the backbone of everyday activities that empower financial freedom, from consolidating credit card debt and refinancing student loans, to buying a home and starting a small business.
dv01's data analytics platform brings unparalleled transparency into investment performance and risk for lenders and Wall Street investors in structured products. As a data-first company, we wrangle critical loan data and build modern analytical tools that enable strategic decision-making for responsible lending. In a nutshell, we're helping prevent a repeat of the 2008 global financial crisis by offering the data and tools required to make smarter data-driven decisions resulting in a safer world for all of us.
More than 400 of the largest financial institutions use dv01 for our coverage of over 75 million loans spanning mortgages, personal loans, auto, buy-now-pay-later programs, small business, and student loans. dv01 continues to expand coverage of new markets, adding loans monthly, and developing new technologies for the structured products universe.
To get a better idea of what a year at dv01 looks like, check out our 2022 Year in Review page here: https://dv01.co/year-in-review/2022/ If that looks like fun to you, get in touch because we'd love to hear from you.
YOU WILL:
Be at the heart of dv01. You will operate as the bridge between the engineering and finance teams, contributing to a variety of integral processes that drive dv01 on a daily basis. Every new dataset that gets integrated within dv01 will have your fingerprints all over it.
Be an owner of dv01's most valuable asset. You'll own the business logic in our data pipeline, encapsulating all the knowledge we've accumulated across hundreds of datasets. The output from the pipeline powers all of dv01's customer offerings and is critical to the success of our business.
Be customer-facing. You will have direct exposure to high-level contacts at hedge funds, banks, and asset originators, providing valuable insights to help them answer complex questions.
Work with state-of-the-art technology. You'll work with popular, modern, and exciting open source technologies like Apache Spark and Airflow. The skills you develop here will serve you well beyond dv01.

YOU ARE:
A well-rounded engineer. You have 3+ years of professional programming experience with Apache Spark, Scala, Java, R, or Python. You are able to write thought-out code while accounting for resource and performance constraints and are also capable of performing ad-hoc data investigations with SQL.
Interested and experienced in both engineering and finance. You're looking to grow your skills in both disciplines and are excited about the synergies between finance and technology. You're capable of understanding how investors evaluate loan portfolios and the complexities of amortization, prepay, and default.
A first-rate collaborator and communicator. You're comfortable working alongside analysts and subject matter experts and translating their requirements into code. You thrive on interacting with clients to best understand and satisfy their needs.
Excited about big data. You should have 2+ years of professional engineering experience working with large datasets, with exposure to large datasets related to loan products an added plus. You enjoy working with data, from expressing complex business logic as scalable data processing logic to configuring and debugging intricate big data pipelines. You love the intricate details of a thorough investigation, but also stay aware of the bigger picture while operating across multiple threads of work.
In good faith our salary range for this role is $120,000 - $145,000 but are not tied to it. Final offer amount will be at the company's sole discretion and determined by multiple factors, including years and depth of experience, expertise, and other business considerations.
Our community is fueled by diverse people who welcome differing points of view and the opportunity to learn from each other. Our team is passionate about building a product people love and a culture where everyone can innovate and thrive.

BENEFITS & PERKS:
Unlimited PTO. Unplug and rejuvenate, however you want—whether that's vacationing on the beach or at home on a mental-health day.
In-House Personal & Performance Development Coach. Recharge with our in-house personal & performance development coach, who is here to listen and help guide in your self-development and overall wellness.
$1,000 Learning & Development Fund. No matter where you are in your career, always invest in your future. We encourage you to attend conferences, take classes, and lead workshops. We also host hackathons, brunch & learns, and other employee-led learning opportunities.
Remote-First Environment. People thrive in a flexible and supportive environment that best invigorates them. You can work from your home, cafe, or hotel. You decide.
Health Care and Financial Planning. We offer a comprehensive medical, dental, and vision insurance package for you and your family. We also offer a 401(k) for you to contribute.
Free Equinox Membership or $1,650 Annual Fitness Fund. Regular exercise offers a plethora of mental and physical health benefits. You can either enroll in an all-access Equinox membership or at your preferred gym. Or take advantage of our fitness fund, which can be used toward at-home workout equipment (yes, including a Peloton).
New Family Bonding. Primary caregivers can take 12 weeks off 100% paid leave, while secondary caregivers can take 3 weeks. Returning to work after bringing home a new child isn't easy, which is why we're flexible and empathetic to the needs of new parents.
dv01 is an equal opportunity employer and all qualified applicants and employees will receive consideration for employment opportunities without regard to race, color, religion, creed, sex, sexual orientation, gender identity or expression, age, national origin or ancestry, citizenship, veteran status, membership in the uniformed services, disability, genetic information or any other basis protected by applicable law."
458,Data Engineer,MerchantE,"Alpharetta, GA 30009",N,"Who Are We?
MerchantE is an innovative, technology-focused company providing a full-service platform to support the payment processing needs for merchants of all sizes, including small business retail shops, B2B wholesalers, and global eCommerce enterprises. We partner with financial institutions, software developers, independent sales organizations, and agents to bring our solutions to market.
Why Join Us?
We're growing and we're looking for collaborative, innovative, and hard-working individuals to grow with us! We offer a modern and inspiring work environment where your ideas and contributions are valued. Come experience, first-hand, the impact of your contributions.
The Opportunity:
As we are embarking on our journey to a modern architecture on the cloud, we are looking for talented engineers to join us to build our future. You will get an opportunity to work on challenging problems in a high-volume and mission critical environment. You will be building data pipelines that support serverless data warehouses solving data issues that support a variety of merchants.
Your Responsibilities will require you to:
Build data pipelines to build state-of-the art data platform to support millions of daily transaction volume.
Build serverless data lakes utilizing programming languages like Java, Scala, Python and Open-Source RDBMS and NoSQL databases, and Cloud based data warehousing services such as Redshift and Snowflake.
Use tools like Kafka to stream real time data.
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies.
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, mentoring other members of the engineering community.
Collaborate with digital product managers and deliver robust cloud-based solutions that drive powerful experiences to help drive small business in the world.
Conduct reviews with other team members to make sure code is rigorously designed, elegantly coded and effectively tuned for performance.
Drive automated CI/CD release pipelines.
Qualifications
Bachelor's Degree in Computer Science or related technical field.
2+ years of experience in application development including SQL and Java
(Internship experience does not apply)
Experience working on real-time data and streaming applications.
Preferred Qualifications:
Master's Degree in Computer Science or related technical field.
1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
1+ years of experience with Distributed data/computing tools (Kinesis, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
1+ year experience working on real-time data and streaming applications.
1+ years of experience with NoSQL implementation (DynamoDB, Mongo, Cassandra)
1+ years of data warehousing experience (Redshift or Snowflake)
1+ years of experience with UNIX/Linux including basic commands and shell scripting.
1+ years of experience with Agile engineering practices
#LI-KB1
MerchantE is an Equal Opportunity Employer committed to a diverse workforce."
459,Senior BI Data Engineer,Teletrac Navman,United States,"$125,000 - $145,000 a year","The Company
Teletrac Navman is a software-as-a-service (SaaS) provider leveraging location-based technology that empowers people managing mobile assets to move their business forward with certainty.
The Position
We’re looking to add a Senior BI Data Engineer to join Teletrac Navman! This role will design, implement, and maintain systems used to collect and analyze business intelligence data.
In this role, you’ll get to:
Create and maintain optimal data pipeline architecture.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Keep TN data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues.
Other duties and/or projects, as needed.
Qualifications
At Teletrac Navman, we believe in your potential to make an impact. And we believe in giving you the opportunity, accountability and visibility to do just that.
We are looking for people who have:
Bachelor degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
At least 10+ years of software development experience
At least 5+ years of experience in a Data Engineer role
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience using the following software/tools:
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Redshift.
Experience with data pipeline and workflow management tools: Amazon SWF, AWS Step Functions, Apache Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift, Glue, Lambda etc., with development in Node JS preferred.
Experience with stream-processing systems preferred: Kinesis, Storm, Spark-Streaming, etc.
Hands on coding experience with object-oriented/object function scripting languages: C#, Java, Python, Scala, etc.
Analytical, framework thinker.
Comfortable working in a fast-paced environment.
Perks!
Variety of medical, dental, and vision plans, including a wellness program
15 vacation days + 5 sick days + 2 floating holidays + 11 major holidays
Employee Assistance Program (EAP): Access to a range of support and resources such as counseling services and support for major life events, pet insurance, weight and health coaching, financial and legal concerns, and more
Family Planning: Paid parental/family leave of absence (LOA), adoption assistance up to $20,000, wide range of fertility care and support with the Maven Clinic, adult and elder care support, and more
Flexible work: Teletrac Navman is committed to providing a work environment that maximizes functionality, collaboration and work/life satisfaction with flexibility where available
401k: Pre- and Post-Tax (Roth) opportunities to save for retirement starting day one
Learning & Growth: Full access to LinkedIn Learning, product training, tuition reimbursement program, student debt repayment program with Fidelity
Community Impact: Charitable fundraising activities and a paid Day of Caring for volunteering
The base compensation range for this position is $125,000 to $145,000 per annum. Your actual base salary will be determined based upon a number of factors which may include relevant experience, skills, location (labor market data), credentials (education, certifications), and internal equity.
Vontier partners with you and your family on your health and wellness journey. Visit VontierBenefits.com to view our benefits. We offer a premium suite of health and wellness programs for you and your family. With programs for family planning from Maven Clinic to managing diabetes like Livongo, coverage for women's health, support for adult and elder care, paid parental leave, a generous 401(k) plan with matching company contributions and more. Vontier is here for all stages of life.
Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. The Company reserves the right to modify this information at any time, subject to applicable law.
Teletrac Navman is a leading software-as-a-service (SaaS) provider leveraging location-based technology that empowers people managing mobile assets to move their business forward with certainty. It tracks and manages more than 500,000 vehicles and assets for more than 40,000 companies around the world. With headquarters in Orange County, CA, we have an international presence with additional offices in the United States, United Kingdom, ANZ and Mexico. Check our website at www.teletracnavman.com.
Vontier is a $3B global industrial technology company focused on smarter transportation and mobility. Our six operating companies—Gilbarco Veeder-Root, Global Traffic Technologies, Teletrac Navman, Matco Tools, Hennessy Industries, and DRB Systems—are united by a powerful purpose: mobilizing the future to create a better world. Our portfolio of trusted brands includes market-leading expertise in mobility technologies, retail and commercial fueling, fleet management, telematics, vehicle diagnostics and repair, and smart cities. Vontier’s pioneering solutions advance safety, security, efficiency, and sustainability worldwide.
""Vontier Corporation and all Vontier Companies are equal opportunity employers that evaluate qualified applicants without regard to race, color, national origin, religion, ancestry, sex (including pregnancy, childbirth and related medical conditions), age, marital status, disability, veteran status, citizenship status, sexual orientation, gender identity or expression, and other characteristics protected by law."""
460,Principal Software Engineer- Data & Analytics,DICK'S Sporting Goods,"12936 Cortez Blvd, Brooksville, FL 34613","$95,100 - $158,800 a year","At DICK’S Sporting Goods, we believe in how positively sports can change lives. On our team, everyone plays a critical role in creating confidence and excitement by personally equipping all athletes to achieve their dreams. We are committed to creating an inclusive and diverse workforce, reflecting the communities we serve.
If you are ready to make a difference as part of the world’s greatest sports team, apply to join our team today!
WHAT YOU'LL DO IN THIS ROLE
Everything we do at DICK’S Sporting Goods centers around our customers, or as we call them, our athletes. From online to in-store, we’re disrupting retail through technology.
Our Software Engineering team are key disruptors. As a Principal Software Engineer, you are an expert of your craft. You are a natural leader. The idea of disrupting an industry with technologies you help create is invigorating.
We empower our engineers to tear down the silos and work collaboratively with their product team to deliver solutions. We work in nimble product teams composed of engineers, designers, and product managers that aren’t bogged down by project plans and politics, but are empowered to continuously create, test and execute. One of the best parts is after the work is done, you get to see the results of your work in our stores, online and throughout our business. You get to make a difference.
First, let’s talk Tech.
Here is a list
Python, SQL, Databricks, Air Flow, Azure Data Factory, Snowflake, Qlik, PowerBI
As a Principal Software Engineer at DICK’S you will:
Design, develop, and launch extremely efficient and reliable data pipelines to move data and to provide intuitive analytics to our partner teams.
Make data more discoverable and easier to use for Data Scientists, Analysts and Other Product teams across the company.
Mentor and Lead engineering teams and team members in software delivery within Data in an Agile Environment
Own complete solution across entire life cycle while utilizing strong problem-solving ability
Lead design sessions and code reviews to elevate the quality of data engineering across the organization
Participate in an on-call rotation for support during and after business hours
Work collaboratively with designers, product managers, and engineers
Consistently look for opportunities to develop our Software Developers, sharing your expertise and lessons
Spend about 30% of your time coding so you can keep up to date on your technical skills
Leverage cutting-edge technologies and modern practices
Build and ship high-quality code at a rapid pace
Work with great people in an engaging, remote environment while still having access to the many amenities the Pittsburgh campus has to offer, including a world-class health club, hoteling space, collaboration rooms, and enabled technology for hybrid meetings.
Bring at least 10-15 years of experience to the table
WHAT WE'RE LOOKING FOR:
We believe that there is always a better way.
So, we will expect you to:
Review and discuss code with engineering peers and leaders to understand best practices and optimal design patterns
Continuously learn modern software design and development core practices, utilizing articles, MeetUps, conferences, tech talks, etc.
Engineers at this level are able to deliver complex tasks to production, working independently when required.
They use best practices in high quality code and continue to push and share their knowledge.
Take the initiative to drive new ideas and projects
Understand the business context of the larger domain and potentially other domain areas
Able to advocate and evangelize specific technologies within and outside the wider organization
Able to support and mentor seasoned technologists in a structured manner and ad-hoc
A willingness to continuously learn, experiment, and innovate. Innovation is a critical part of our culture and an expectation for all teammates. If you like brainstorming with your peers on how to disrupt your product domain – or retail as a whole – you’ve come to the right place!
If you’re excited about joining the DSG team, we’d love to meet you.
Apply today!
Targeted Pay Range: $95,100 – $158,800

We believe that there is always a better way.
So, we will expect you to:
Review and discuss code with engineering peers and leaders to understand best practices and optimal design patterns
Continuously learn modern software design and development core practices, utilizing articles, MeetUps, conferences, tech talks, etc.
Engineers at this level are able to deliver complex tasks to production, working independently when required.
They use best practices in high quality code and continue to push and share their knowledge.
Take the initiative to drive new ideas and projects
Understand the business context of the larger domain and potentially other domain areas
Able to advocate and evangelize specific technologies within and outside the wider organization
Able to support and mentor seasoned technologists in a structured manner and ad-hoc
A willingness to continuously learn, experiment, and innovate. Innovation is a critical part of our culture and an expectation for all teammates. If you like brainstorming with your peers on how to disrupt your product domain – or retail as a whole – you’ve come to the right place!
If you’re excited about joining the DSG team, we’d love to meet you.
Apply today!"
461,Forward-Deployed Data Engineer,Boston Consulting Group,"Washington, DC",N,"Who We Are

Boston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we work closely with clients to embrace a transformational approach aimed at benefiting all stakeholders—empowering organizations to grow, build sustainable competitive advantage, and drive positive societal impact.

Our diverse, global teams bring deep industry and functional expertise and a range of perspectives that question the status quo and spark change. BCG delivers solutions through leading-edge management consulting, technology and design, and corporate and digital ventures. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, fueled by the goal of helping our clients thrive and enabling them to make the world a better place.

Practice Area Profile

Boston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we work closely with clients to embrace a transformational approach aimed at benefiting all stakeholders—empowering organizations to grow, build sustainable competitive advantage, and drive positive societal impact.

Our diverse, global teams bring deep industry and functional expertise and a range of perspectives that question the status quo and spark change. BCG delivers solutions through leading-edge management consulting, technology and design, and corporate and digital ventures. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, fueled by the goal of helping our clients thrive and enabling them to make the world a better place.

BCG GAMMA combines innovative skills in computer science, artificial intelligence, statistics, and machine learning with deep industry expertise. The BCG GAMMA team is comprised of world-class data scientists and business consultants who specialize in the use of advanced analytics to get breakthrough business results. Our teams own the full analytics value-chain end to end: framing new business challenges, building fact-bases, designing innovative algorithms, creating scale through designing tools and apps, and training colleagues and clients in new solutions. Here at BCG GAMMA, you’ll have the chance to work with clients in every BCG region and every industry area. We are also a core member of a rapidly growing analytics enterprise at BCG - a constellation of teams focused on driving practical results for BCG clients by applying leading edge analytics approaches, data, and technology.

What You'll Do

As a forward-deployed Data Engineer, you’ll be part of our rapidly growing engineering team and help build the next generation of AI solutions. You’ll have the chance to partner with clients in a variety of BCG regions and industries, and on key topics like climate change, enabling them to design, build, and deploy new and innovative solutions. Additional responsibilities will include developing and delivering thought leadership in scientific communities and papers as well as leading conferences on behalf of BCG GAMMA.

WHO YOU ARE

We are looking for talented individuals with a passion for data engineering, software development and transforming organizations into AI led innovative companies
Apply data engineering practices and standards to develop robust and maintainable solutions
Actively involved in every part of the software development life cycle
Experienced at guiding non-technical teams and consultants in standard methodologies for large-scale data engineering
Motivated by a fast-paced, service-oriented environment and interacting directly with clients on new features for future product releases
Enjoy collaborating in teams to share software design and solution ideas
A natural problem-solver and intellectually curious across a breadth of industries and topics

What You'll Bring (Experience & Qualifications)

YOU BRING
Master’s Degree in Computer Science or relevant field
Experience in data engineering and working with global and remote agile squads
Proficiency with analytic software programming ideally in python, C++ or SCALA
Fluency with the storage, manipulation, and management of relational, non-relational and streaming data structures, specifically SQL, Spark, and Hadoop
Proficiency with infrastructure as code principles
Experience working on AWS, Azure, or Google cloud infrastructure
NICE TO HAVE
DevOps: Docker, Kubernetes, CI/CD, Terraform
Understanding of parallel computing
Full stack development: GraphQL, React, JavaScript, TypeScript
Data Science and machine learning (Pandas, Scikit learn)

The BCG GAMMA team is a community of world-class data scientists and business consultants who specialize in the use of advanced analytics to get breakthrough business results. Your colleagues will combine innovative skillsets in computer science, artificial intelligence, statistics and machine learning with deep industry expertise. You will work with teams to own the full analytics value-chain end to end: framing new business challenges, building fact-bases, designing innovative algorithms, creating scale through designing tools and apps, and training colleagues and clients in new solutions.

WORK ENVIRONMENT
Fluency in English is required as well as fluency in the local language for some locations
Ability to travel based on client and business needs. Expect 30-50%

FOR U.S. APPLICANTS: Boston Consulting Group (“BCG”) is an Equal Opportunity/Affirmative Action employer. All qualified applicants will be considered for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by applicable law.

The first year base compensation for this role is:

Data Engineer I: $110,000 USD
Data Engineer II: $145,000 USD
Data Engineer III: $160,000 USD

In addition to your base salary, you will also be eligible for an annual discretionary performance bonus and BCG’s Profit Sharing and Retirement Fund (PSRF) contribution. BCG also provides a market leading benefits package described below.

At BCG, we are committed to offering a comprehensive benefit program that includes everything our employees and their families need to be well and live life to the fullest. We pay the full cost of medical, dental, and vision coverage for employees - and their eligible family members.* That’s zero dollars in premiums taken from employee paychecks. All our plans provide best in class coverage:

Zero dollar ($0) health insurance premiums for BCG employees, spouses, and children
Low $5 (USD) copays for trips to the doctor, urgent care visits and prescriptions for generic drugs
Dental coverage, including up to $5,000 (USD) in orthodontia benefits
Vision insurance with coverage for both glasses and contact lenses annually
Reimbursement for gym memberships and other fitness activities
Fully vested retirement contributions made annually, whether you contribute or not
Generous paid time off including vacation, holidays, and annual office closure between Christmas and New Years
Paid Parental Leave and other family benefits such as elective egg freezing, surrogacy, and adoption reimbursement
Employees, spouses, and children are covered at no cost. Employees share in the cost of domestic partner coverage."
462,Principal Data Engineer,Movable Ink,"New York, NY 10018•Remote","$220,000 - $270,000 a year","Movable Ink scales content personalization for marketers through data-activated content generation and AI decisioning. The world's most innovative brands rely on Movable Ink to maximize revenue, simplify workflow and boost marketing agility. Headquartered in New York City with close to 600 employees, Movable Ink serves its global client base with operations throughout North America, Central America, Europe, Australia, and Japan.
As a Principal Data Engineer at Movable Ink, you will be a key contributor to our mission of unifying and enhancing our data-driven offerings. You will work closely with product managers, data scientists, and software engineers to evolve our data architecture and products . Your work will be critical in driving business growth by unlocking differentiated data capabilities at scale.

Responsibilities
Design and implement data processing pipelines for batch and real-time data ingestion, transformation, and storage
Develop and maintain scalable and reliable data storage solutions, including relational and NoSQL databases, data lakes, and data warehouses
Develop APIs and data services that enable efficient and secure access to data for internal and external customers
Participate in release planning, sprint planning, standups, and retrospectives to ensure successful project delivery and alignment

Requirements
Experience designing and implementing large-scale data processing pipelines using distributed systems and frameworks
Experience with data modeling and database design for relational and NoSQL databases
Familiarity with cloud computing platforms such as AWS or GCP and their data services
Excellent problem-solving and communication skills, and ability to work collaboratively with cross-functional teams including product managers, data scientists, and software engineers
Experience building and supporting data products within martech or B2B SaaS


-

The base pay range for this position is $220,000-$270,000/year, which can include additional on-target commission pay/bonus. The base pay offered may vary depending on job-related knowledge, skills, and experience. Stock options and other incentive pay may be provided as part of the compensation package, in addition to a full range of medical, financial, and/or other benefits, depending on the position ultimately offered.
Studies have shown that women, communities of color, and historically underrepresented people are less likely to apply to jobs unless they meet every single qualification. We are committed to building a diverse and inclusive culture where all Inkers can thrive. If you're excited about the role but don't meet all of the abovementioned qualifications, we encourage you to apply. Our differences bring a breadth of knowledge and perspectives that makes us collectively stronger.
We welcome and employ people regardless of race, color, gender identity or expression, religion, genetic information, parental or pregnancy status, national origin, sexual orientation, age, citizenship, marital status, ethnicity, family or marital status, physical and mental ability, political affiliation, disability, Veteran status, or other protected characteristics. We are proud to be an equal opportunity employer."
463,Data Security Administrator/Engineer,Intone Networks,Remote,N,". Job Description: Our client is currently seeking a consultant to come in and serve as a Security Engineering tools and data security risk policies/procedures and risk mitigation focused Data Security Administrator who will be responsible for planning, coordinating and implementing Securitymeasures for information systems to regulate access to computer data files and prevent unauthorized modification, destruction or disclosure of information. A typical data security administrator is responsible for planning, coordinating and implementing security measures to safeguard the computer/information databases across the enterprise. Job Responsibilities: • Identify security issues and risks, and develop mitigation plans • Design, implement, support, and evaluate security-focused tools and services including project leadership roles • Develop and interpret security policies and procedures Participate in security compliance efforts Develop and deliver training materials and perform general security awareness and specific security technology training Evaluate and recommend new and emerging security products and technologies Qualifications: • Bachelor's degree in a technical field such as computer science, computer engineering or related field required • 8+ years experience required • Strong experience and detailed technical knowledge in security engineering, system and network security, authentication and security protocols, cryptography, and application security • Consistent implementation of security solutions • Experience in infrastructure or application-level vulnerability testing and auditing"
464,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
465,"VP, Market Data Engineer",Barclays,"745 7th Ave, New York, NY 10019","$210,000 a year","VP, Market Data Engineer
New York, New York
Barclays Services Corp.
What will you be doing?
On behalf of a global financial services organization, develop, create, and modify general computer applications software and specialized utility programs.
Analyze user needs and develop software solutions.
Design software with the aim of optimizing operational efficiency.
Develop monitoring and automation processes to manage real time market data feed handler estate.
Support a global work stream within specialized infrastructure to standardize automation across various technology disciplines including market data, networks, Unix, and middleware.
Assist in the integration of automation processes with monitoring frameworks including Elastic Logstash Kibana.
Review and monitor the health and quality of real-time market data environment.
Perform diagnostic work and joins major incident bridges as needed to investigate and remediate critical production issues affecting market data.
Develop and design Market Data products and support procedures where required.
Communicate with stakeholders to resolve any market data production issues and troubleshoots accordingly.
Undertake key business requests around value and applicability of market data cost saves and reducing of production.
Ensure that all activities and duties are carried out in full compliance with regulatory requirements, internal Barclays Policies, and Policy Standards.
What we’re looking for:
Minimum of Bachelor’s degree, or foreign equivalent, in Computer Science, Computer Engineering, Finance, Business Administration or related field and at least five (5) years of post-baccalaureate progressively responsible experience in a Market Data related occupation within the financial services industry.
Must have at least five (5) years of progressively responsible employment experience with each of the following required skills:
Programming and developing tools including BASH (Born Again Shell) and Python;
Subject Matter Expertise in Market Data concepts;
Knowledge of Configuration Management Tools, such as Ansible and Chef;
Experience operating various Linux command line utilities including Global Regular Expression Print (GREP), AWK, SORT, LS and PS;
Version Control Tools including Global Information Tracker (GIT) and STASH;
Computer Networks including Wireshark, Transmission Control Protocol Dump (TCPDump) and Transmission Protocol/ Internet Protocol Stack (TCP/IP Stack);
Open-source IT monitoring tools, including Elastic, Logstash, and Kibana (ELK) as well as SNMP (Simple Network Management Protocol) and syslog; and
Linux Operating Systems Fundamentals. Experience working with the Red Hat Enterprise Linux or CentOS distributions.
Where will you be working?
You will be working at our New York, New York location at 745 Seventh Avenue. This 37-story office tower is located in Times Square in the heart of Manhattan and features a cafeteria, fitness center and state-of-the-art LED signage on the façade of the building. The building is easily accessible to Restaurants, Shops and Public Transportation.
Interested and want to know more about Barclays? Visit home.barclays/who-we-are/ for more details.
Our Values
Everything we do is shaped by the five values of Respect, Integrity, Service, Excellence and Stewardship. Our values inform the foundations of our relationships with customers and clients, but they also shape how we measure and reward the performance of our colleagues. Simply put, success is not just about what you achieve, but about how you achieve it.
Our Diversity
We aim to foster a culture where individuals of all backgrounds feel confident in bringing their whole selves to work, feel included and their talents are nurtured, empowering them to contribute fully to our vision and goals.
It is the policy of Barclays to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, creed, religion, national origin, alienage or citizenship status, age, sex, sexual orientation, gender identity or expression, marital or domestic/civil partnership status, disability, veteran status, genetic information, or any other basis protected by law.
Dynamic working gives everyone at Barclays the opportunity to integrate professional and personal lives, if you have a need for flexibility then please discuss this with the hiring manager.
Our Benefits
Our customers are unique. The same goes for our colleagues. That's why at Barclays we offer a range of benefits, allowing every colleague to choose the best options for their personal circumstances. These include a competitive salary and pension, health care and all the tools, technology and support to help you become the very best you can be. We are proud of our dynamic working options for colleagues. If you have a need for flexibility then please discuss this with us.
Salary/Rate Minimum/yr: $210,000
Salary/Rate Maximum/yr: $210,000
The minimum and maximum salary/rate information above include only base salary or base hourly rate. It does not include any other type of compensation or benefits that may be available.
This position is eligible for incentives pursuant to Barclays Employee Referral Program."
466,Data Analytics Engineer,"Modernizing Medicine, Inc.","Boca Raton, FL 33431•Remote",N,"At Modernizing Medicine, we look for passionate, innovative, creative Rock Stars!
South Florida Business Journal, Business of the Year 2022
BIG Awards for Business, Company of the Year 2021
Best in Biz Award (Silver), Fastest-Growing Company of the Year 2021
South Florida Business Journal, Best Places to Work 2021
Inc. Magazine Best Workplaces of 2020
Modernizing Medicine is delivering truly disruptive and transformative products and services that will impact the healthcare industry. The work we do makes a difference.
Our web and mobile applications are transforming healthcare information technology to increase practice efficiency and improve patient outcomes. We offer end-to-end specialty-specific solutions from practice management, through EMR to Revenue Cycle Management (RCM) that maximize office interactions, patient visits, collections and reimbursements.
Mod Med is hiring an experienced Data Analytics Engineer with a passion for technology. In this role, you will help validate, maintain, extract, and report on our data. You will be working in an energetic, dynamic, and highly creative environment along with a group of highly talented, passionate, and successful individuals. The work will range from simple to complex reporting on the information, trends, and patterns contained in the data.
The Role:
Gather, analyze, refine, validate, report, and publish data.
Deliver complete, concise, and accurate business intelligence reports/visuals.
Work with multifunctional teams to determine and articulate report requirements.
Analyze complex data to provide operational intelligence.
Monitor and manage data life cycle process.
Development of data quality metrics and controls.
Descriptive and summary statistics.
Data visualization supporting insights.
Compile, review, and analyze data through database and statistical programming.
Follow best practice guidelines for business intelligence at Mod Med.
Share insights across the enterprise.
Skills & Requirements:
Knowledge of data modeling, query languages (SQL, etc.), and key trends in reporting, business intelligence, and analytical processes.
Experience working in a data analyst role actively engaged with data and business intelligence.
Knowledge and understanding of relational databases.
Ability to learn new database concepts, processes, tools, and best practices.
Strong verbal and written communication skills.
Strong analytical and problem solving skills.
Strong commitment to quality, architecture, and documentation.
Experience with python, R or other programming languages.
Experience with big data tools (Hadoop, Hive, Impala, Spark, Data bricks).
3-5 years of experience.
Bachelor's Degree in Engineering, Computer Science, or other relevant degree, preferred.
Modernizing Medicine Benefit Highlights:
Health Insurance, 401(k), Vacation, Employee Assistance Program, Flexible Spending Accounts
Employee Resource Groups
Professional development opportunities including tuition reimbursement programs and unlimited access to LinkedIn Learning
Weekly catered breakfast and lunch, treadmill workstations, quarterly onsite massages, onsite dry cleaning, onsite car wash and many more!
#LI-K M1
#LI-REMOTE"
467,Data Engineer/Analyst,Labcorp,"Durham, NC 27709",N,"Labcorp is recruiting a Data Engineer/Analyst for a dynamic team in either Burlington or RTP, NC.
Get ready to redefine what’s possible and discover your extraordinary potential at Labcorp. Here, you’ll have the opportunity to personally advance healthcare and make a difference in peoples’ lives with your bold ideas and unique point of view. With the support of exceptional people from across the globe and an energized purpose, you’ll be empowered to own your career journey with mentoring, training and personalized development planning.

At Labcorp we believe in the power of science to change lives. We are a leading global life sciences company that delivers answers for crucial health questions because we know that knowledge has the potential to make life better for all. Through our unparalleled diagnostics and drug development capabilities, we provide insights and accelerate discoveries that not only empower patients and providers but help medical, biotech and pharmaceutical companies transform ideas into innovations.
Overview:
Reviews, evaluates, and maintains data for computer processing. Analyzes data for system performance and functionality. Works directly with users to resolve data conflicts. Recommends methods, tools, or software to maximize performance.
Skill Requirements
Position Responsibilities And Expected Outcomes:
Work on complex data initiatives with broad impact and act as key participant in large scale software planning for the Technology area
Perform data analysis and modelling tasks within a data warehouse environment.
Perform SQL queries to analyze and troubleshoot issues.
Discover problems in data and applications and design solutions with engineering and product leadership.
Strategically collaborate and consult with internal partners to resolve highly risky data engineering challenges
Demonstrated ability to solve complex data engineering problems; end to end problem resolution and continuous improvement mindset.
In-depth knowledge and experience with Data Engineering essentials such as scripting languages, source control, workflow scheduling, relational and non-relational SQL, and development of complex data solutions
Required Qualifications:
2+ years of Data Engineering experience, or equivalent demonstrated
2+ years of relevant Python development experience
Experience with Databricks , Spark, Hive, AWS EMR/S3, Data Stage or similar systems for performance.
Experience with AWS technologies like Lambda, S3
Familiarity with modern build pipelines, tools, CI/CD concepts
Experience in data modelling within a data warehouse environment
Desired qualifications:
ETL (Software agnostic) Experience
Databricks, Hive, Datastage and Oracle SQL Experience
Experience in query and/or SQL tuning
CI/CD Tool Experience
Experience working in an Agile Team
Understanding of SDLC Requirements
Knowledge on mainframe
License/Certification/Education:
Normally requires a B.S. Degree in Computer Science w/1-3 years of relevant experience.
Labcorp is proud to be an Equal Opportunity Employer:
As an EOE/AA employer, Labcorp strives for diversity and inclusion in the workforce and does not tolerate harassment or discrimination of any kind. We make employment decisions based on the needs of our business and the qualifications of the individual and do not discriminate based upon race, religion, color, national origin, gender (including pregnancy or other medical conditions/needs), family or parental status, marital, civil union or domestic partnership status, sexual orientation, gender identity, gender expression, personal appearance, age, veteran status, disability, genetic information, or any other legally protected characteristic. We encourage all to apply.
For more information about how we collect and store your personal data, please see our Privacy Statement."
468,Data Engineer,GEMINI INDUSTRIES,"Fairfax, VA",N,"POSITION DESCRIPTION
Gemini Industries Inc. provides technical, management and operations services to support National Security projects. We provide rapid response to the critical needs of our customers and those they serve. We perform analyses and develop operations plans to anticipate and prepare for the future. And we deliver advanced technology to improve our customer’s success in executing its mission.
Gemini seeks innovative, results-oriented individuals with the creativity, initiative, and intelligence to overcome any challenge and succeed. Members of the Gemini team thrive in a culture that is anticipatory, agile, and schedule-driven; with a sense of urgency and a drive to succeed. Our culture involves:
· The best and brightest personnel
· Work at a high operations tempo
· Integrated teams delivering rapid solutions
· An attitude that balances “I can make it better” with “As long as we succeed”
Position: Data Engineer
Location: Fairfax, VA
Clearance: TS/SCI
Education: Bachelor’s Degree in Applied Data Engineering or Science, and Computer Science fields of study, such as statistics, mathematics, data analytics, data science, computer science or other technology related fields.
Outcomes:
Develop and implement data engineering eco-systems and architectures that enable data Extraction, Transformation and Loading (ETL) operations for predictive and prescriptive data modeling.
Will have the ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions.
Build networking connections and opportunities for data acquisition.
Responsibilities:
The Combatant Command Intelligence Enterprise Management Support Office (CCI EMSO) requires a Data Engineer to implement methods to improve data reliability and quality. The Data Engineer will combine raw data from different data sources to create consistent and machine-readable formats that will be passed to the Data Science Team for Data Science Analysis and Studies, with a specific focus on creating and engineering data pipelines that can transfer the necessary Data Sets via Application Programming Interfaces (APIs), Resilient Distributed Datasets (RDD) and direct interface with the data source to the CCI EMSO data repository. The Data Engineer shall develop and implement data engineering eco-systems and architectures that enable data Extraction, Transformation and Loading (ETL) operations for predictive and prescriptive data modeling. Data Engineering builds will include, but not be limited to, establishment of a Data Lake and Eco-System which can move data from the DevSecOps to Data Science Production and Analysis Environment. The primary responsibilities of the CCI EMSO Data Engineer will be analyzing raw data, developing, and maintaining a repository of CCI EMSO specific data sets, and improving data quality and efficiency. CCI EMSO provides OSD senior leaders with data driven analyses using the scientific method to enable senior leadership decision making to deliver modernized capabilities that ensure the Combatant Commands are equipped with modern systems and the Joint Force is aligned with future Joint Warfighting Concepts.
Specific responsibilities include but are not limited to:
· Develop, construct, and deploy data lake and ecosystems infrastructure that will include a DevSecOps and production environment.
· Analyze and organize raw data for Data Science analysis.
· Build Data Systems and data transfer pipelines to ensure the CCI EMSO Data Science Team has the requisite data to answer Combatant Command, ISREC and ISPR Data Science Requests for Information (RFI).
· Evaluate CCI EMSO mission and vision needs and objectives and codify a data governance plan that supports them.
· Prepare data for prescriptive and predictive modeling.
· Build algorithms and prototypes to meet the objectives and requirements set by the Combatant Commands, ISREC and ISPR.
· Combine raw data from different sources and normalize into import ready data sets to pass to the CCI EMSO Data Science Team for data science analysis.
· Draft, develop and formalize a data governance plan, in conjunction with the CCI EMSO data scientists, that enhances data quality and reliability.
· Identify networking connections and opportunities for data acquisition.
· Work with stakeholders including data, design, product, and executive teams and assisting them with data-related technical issues.
· Identifying, designing, and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.

The candidate must have the following qualifications:
· A minimum of 10 years of experience (including demonstrated expert ability to use statistical software programs (e.g., R, Python, Weka, Apache Spark, and SQL).
· Ability to build and optimize data sets, ‘big data’ data pipelines and architectures as required by the CCI EMSO mission and vision and ISREC and ISPR Strategic Guidance.
· Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions.
· Excellent analytic skills associated with working on unstructured datasets.
· Experience in supporting technical, managerial, or operational fields and mature judgement required to interface with external stakeholders and senior government personnel.
· Strong organizational, oral, and written communication skills are required.
· Ability to build processes that support data transformation, workload management, data structures, dependency, and metadata
· Strong interpersonal skills and the ability to build consensus, work effectively and independently, and demonstrate a consultancy mindset towards customer engagement.
· Experience navigating experimental environments and making recommendations on risk management while maximizing innovation.
· Thorough knowledge of research designs, particularly systems thinking and/or design thinking.
· Have a working knowledge and experience in working in an Organizational Program Maturity business model, with the ability to move between Agile and standard Defense Acquisition University (DAU) and Project Management Institute (PMI) Program Management processes and methodologies.
· Ability to effectively communicate complex, multi-disciplinary ideas, and insights.
· Ability to translate complex, technical findings into an easily understood narrative (i.e., tell story with data).
· Analytical and critical thinking skills, including superior ability to think strategically.
· Demonstrated experience using analytic methods and methodological tools in mathematics or statistics

The following qualifications are desired:
· Data Science Council of America (DASCA) Associate Big Data Engineer (ABDE).
· Databricks Certified Data Engineer Professional
· PMI Agile Certified Practitioner (PMI-ACP)
· PMI Project Management Professional (PMP) (or DAU Equivalent)

Travel: Some travel may be required
Other Requirements:
We seek:
· Highly motivated self-starter
· Resourceful individuals with extraordinary intellectual capability and the ability to rapidly learn and apply new concepts
· Individuals who have a “let me try” attitude and are resilient, present an opinion/position, justify it, and then accept whatever decision is made and charge forward
· Individuals who view criticism as an opportunity to improve (“let me try again”)
· Individuals who think and create, enhancing the company with a steady flow of fresh ideas, perspective, and energy.
Direct Inquiries and Resumes to:
Yashira Santiago
Corporate Recruiter
Gemini Industries Inc.
1408 N. Westshore Blvd. Ste. 909
Tampa, FL 33607
Telephone: (813) 286-4777
Jobs@gemini-ind.com
Gemini Industries Inc. is proud to be an Equal Opportunity / Affirmative Action Employer. We are committed to abiding by the requirements of 41 CFR §§ 60-1.4(a), 60-300.5(a) and 60-741.5(a). These regulations prohibit discrimination against qualified individuals based on their status as protected veterans or individuals with disabilities and prohibit discrimination against all individuals based on their race, color, religion, sex, or national origin. Moreover, these regulations require that covered prime contractors and subcontractors take affirmative action to employ and advance in employment individuals without regard to race, color, religion, sex, national origin, protected veteran status or disability and any other basis protected by applicable law."
469,"Senior Data Engineer, Data Orchestration",Salesforce,Colorado•Remote,"$156,800 - $215,600 a year","To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Products and Technology
Job Details
We’re Salesforce, the Customer Company. If you believe in bringing companies and customers together, in business as the greatest platform for change, in creating a more equitable and sustainable future for all – well, you’re in the right place. Through our #1 CRM, Customer 360, we help companies blaze new trails and connect with their customers in a whole new way. And, we empower you to be a Trailblazer, too — driving your performance and growth, charting new paths, and improving the state of the world.
Senior Data Engineer, Data Orchestration
Slack is looking for a senior data engineer to join the Data Orchestration Team. As part of the Data Engineering organization, we build and operate the platform that ingests data into our Data Warehouse. We write software to manage the ingestion for thousands of stateful hosts and stateless real time logging events. Currently, our infrastructure handles 65PB+ of storage, processes ~900B records a day, 400+ ETL pipelines and 900+ Active Airflow DAGs. As Slack’s data grows (along with the number of customers, features and employees), the goal of the team is to build a highly scalable and resilient orchestration capabilities for our downstream customers so that they can focus on their strengths.

You will build scalable services and tools to help partners implement, deploy and analyze data assets with a high level of autonomy and limited friction. You will play a meaningful role in making partner interactions with the Data Warehouse pleasant and productive. You will have deep technical skills, be a self-starter, detail and quality oriented, and passionate about driving data driven decisions and having a huge impact at Slack!

Here are a few blog posts that shed light into what we do here at Slack:
Data Lineage at Slack - https://slack.engineering/data-lineage-at-slack/
Reliably Upgrading Apache Airflow at Slack's Scale - https://slack.engineering/reliably-upgrading-apache-airflow-at-slacks-scale/
Introducing Data Residency for Slack - https://slack.com/blog/transformation/introducing-data-residency-for-slack
Data Wangling At Slack - https://slack.engineering/data-wrangling-at-slack/
What you will be doing
Design and develop highly scalable and resilient services/data pipelines for data ingestion and processing using modern big data technologies
Develop and maintain our real time analytics/low latency data access layer built on top of modern OLAP solutions
Optimize the end-to-end workflow for data users at Slack (from crafting libraries to schedule data pipelines and access data assets).
Automate and handle the lifecycle of data sets (schema evolution, metadata management, change and backfill management, deprecation and migration).
Improve the data quality and reliability of the pipelines through properly monitoring and failure detection.
Comfortably collaborate with cross functional partners and lead technical initiatives end to end.
Be a role model and a multiplier, coaching and mentoring other engineers across the org.
Write, review, or provide feedback on a technical design proposal from others.
What you should have
5+ years of software/data engineering experience, including experience with Big Data technologies, e.g. OLAP, Airflow, Spark, Kafka, Hadoop, etc.
Experience operating Airflow clusters and knowledge of Airflow best practices.
Experience working with cloud infrastructure (AWS preferred).
You have extensive experience of building and maintaining large scale ETL pipelines and in-depth knowledge of various big data frameworks and architectures
You are skilled at crafting and building robust distributed Microservices with tools like Docker, Kubernetes, AWS ECS/EKS etc.
Experience in real time analytics/low latency data access layer with OLAP stores such as Apache Pinot or Apache Druid is a huge plus.
You have strong dedication to code quality, automation and operational excellence: CI/CD pipelines, unit/integration tests.
You are proficient in object-oriented and/or functional programming languages: Python, Java/Scala, Chef, Terraform
You have excellent written and verbal communication and interpersonal skills; able to effectively collaborate with cross functional partners and explaining sophisticated technical concepts to non-technical stakeholders
You have high growth expectations for yourself and your team, and a willingness to push yourself and your team to achieve them
Bachelor's degree in Computer Science, Engineering or related field, or equivalent training, fellowship, or work experience.

Slack has a positive, diverse, and supportive culture—we look for people who are curious, inventive, and work to be a little better every single day. In our work together we seek to be smart, humble, hardworking and, above all, collaborative.
More details about our company benefits can be found at the following link: https://www.getsalesforcebenefits.com/
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form .
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at Salesforce and explore our benefits.
Salesforce, Inc . and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce, Inc . and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce, Inc . and Salesforce.org will not pay any third-party agency or company that does not have a signed agreement with Salesforce, Inc . or Salesforce.org .
Salesforce welcomes all.
For Colorado-based roles, the base salary hiring range for this position is $156,800 to $215,600.
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience.
Certain roles may be eligible for incentive compensation, equity, and benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com."
470,Data Software Engineer II,Tango,"Seattle, WA 98116•Remote","$134,000 - $154,000 a year","The Job
As Software Development Engineer II at Tango, you will deliver world-class rewards and service to our customers and also support the development of our internal teams. This role requires passion for our people, customers, business, and technology.
In this SDE II role, you'll get to work with passionate, dedicated Engineers to build value into an organization that is committed to being the best in the industry. Your enterprise experience will be valuable to the team; you will support building quality, secure functionality, as you learn and grow in your craft with guidance from experienced, passionate Engineers. You'll be given opportunities to apply and refine your skills and will be expected to deliver results, contributing to the Tango Card technical community.
Reports To: Director of Engineering

As Software Development Engineer II at Tango, you will:
Contribute to the realization of Tango Card's architecture and strategy to achieve business goals.
Evaluate and recommend libraries, tools, and vendors from the market, in collaboration with other stakeholders, to meet the needs of the department and business.
Contribute to the design and documentation of initiatives with guidance from technical leaders.
Integrate with internal and external services, tools, and vendors to provide functionality reliably and securely.
Implement solutions to provide best-in-class functionality to internal and external customers.
Deliver secure, performant, scalable, resilient, observable, cloud-based systems to stakeholders in an iterative methodology.
Measure and monitor new functionality for continuous feedback and improvement on performance.
Mentor team members with a focus on consistency and alignment with Tango Card practices.
Stay curious about technology, approaches, and trends that contribute to innovating our products and services.
Contribute to a collaborative culture within and outside of Engineering to foster transparency, rapport, and support.
To be effective in our Software Development Engineer II role, you must have:
Experience with PostgreSQL, SQL, DBT (or another transformation framework)
Expertise in Java 11 or higher, SpringBoot 2.1 or higher
Experience in Service Oriented Architecture and Structured Event-Driven Architecture
Experience in RDBMS, including familiarity with ACID properties and transactionality
Knowledge of Design Patterns, Object Oriented Development, and SOLID Principles
Programming experience with at least one software programming language
Experience with production-worthy distributed systems development
Our ideal Software Development Engineer II at Tango will have most of the following skills and experience:
Hands on experience with Vue, React, Angular, or another modern frontend framework
Experience with NoSQL data repositories
Familiarity with Python 3.7 or higher
Experienced with writing tests using tools such as Cypress, Jest, Karma, Junit, etc
5+ years experience in web-based architecture, Engineering discipline and working in an Agile environment
Experience launching secure, scalable, resilient services at Enterprise scale
Ability to handle multiple competing priorities in a fast-paced environment
Experience with cloud infrastructure (AWS preferred)
Salary: The targeted pay range for this position is between $134,000 and $154,000. Please note that the actual salary offer will carefully consider a wide range of factors, including your skills, qualifications, and experience. Certain positions are eligible for additional forms of compensation such as bonus.
Please note: visa sponsorship is not available for this position.

#Li-DNI #Li-Remote #BI-Remote

What You'll Get From Us
Competitive compensation package (money isn't everything, but it helps)
Medical, dental, and vision benefits (100% employer paid premium)
Flex PTO and a generous holiday schedule
401(k) matching and equity opportunities (401(k) is a 100% match of up to 6%)
Choice-First Plan allows employees to choose to Work from Home, at our Seattle HQ, or hybrid!
A Work from Home monthly stipend for all employees
Award-winning culture that fosters autonomy, creativity, inclusion, transparency, and ownership
Dog-friendly Seattle office!
What We're Up to at Tango
Tango believes that rewards, incentives, and payments are more than transactions. They're opportunities to fuel stronger relationships with people, and better performance for business. Whether you're focused on customer loyalty or employee engagement, wellness or research participation, we can help you seamlessly send smiles and drive results.
Why Work for Us
We care about each other and our customers. Our team is autonomous, collaborative, creative, and eager to learn. We are an inclusive and diverse company that offers excellent health, dental, and vision benefits, flexible PTO, and competitive compensation packages. We're doing incredible things with awesome people, and we'd love for you to join us!
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the Form I-9.
Tango is an equitable and equal opportunity employer; we're strong because we're diverse and prioritize inclusive practices. All applicants will be recruited and considered for employment regardless of their social identities, including but not limited to race, gender, color, religion, belief, national origin, age, sexual orientation, disability, and other protected classes. We thrive in the areas of recruiting and hiring when we have a growing awareness of the challenges faced by underrepresented candidates during the application and hiring process. We encourage all candidates to apply for roles they feel they have the skills to fulfill or a willingness to learn.
We at Tango take the protection of your personal information very seriously. We will never ask for financial information of any kind or for payment during the job application process. Please take extra caution while examining the email address of the individual you are communicating with, as scammers may misspell an official Tango email address or use a slightly modified version. Emails from us will come only from an official @tangocard.com email address. If you suspect a role or job ad is fraudulent, or that you are being contacted by a scammer pretending to be a representative of Tango, please contact us. If you believe you are the victim of fraud resulting from a job listing, please contact your local authorities."
471,Junior Data Engineer (Remote),"Lakeshore Learning Materials, LLC","Long Beach, CA 90810•Remote",$27 - $31 an hour,"Company Description

At Lakeshore, we create innovative learning materials and world-class guest experiences for teachers, parents and children. Since 1954, we’ve grown into a global community—with a thriving e-commerce business, multiple catalogs, 60+ retail stores, a peerless national sales force, plus international offices that support our preeminent supply chain division. But today we’re working better, smarter and faster than ever—and setting our sights even higher. We’re building an infrastructure designed for scalability, embracing data-driven decision-making and using technology to improve efficiency and ensure the best tools for the best work. Most importantly, we continue to invest in a diverse team of inquisitive top talent who fuel each other’s passions and curiosity, take risks, try new things, and believe that every new day brings opportunities for growth.

Job Description

In a time of unprecedented expansion, are seeking a Junior Data Engineer who will be responsible for enabling and enhancing data insights for internal stakeholders, with significant overlap and collaboration with different teams. In this role, you will design, build, enhance and maintain an enterprise business intelligence (BI) platform that supports advanced analytics. The position also requires data cleanup, requirements gathering, data modeling and transformation, reports/dashboard creation, data mining and analytics on complex data sets. The ideal candidate has proven experience working with complex data and has a passion for improving technology. And we’re here to help you succeed—in 2022 Lakeshore earned its Great Place to Work® Certification™ and is proud to put its associates first.
A day in the office looks like this:
Serving as the primary point of contact with business teams to provide actionable insights into current delivery performance, as well as ad hoc investigations into future improvements or innovations
Using BI and visualization software (SQL Server, Redshift, AWS Data Lake, Qlik, etc.) to empower nontechnical, internal customers to drive their own analytics and reporting
Providing complex analysis, conceptualization, design, implementation and development of solutions for critical BI components
Performing dataflow, system and data analysis; developing meaningful presentation of data in BI applications
Contributing to data analysis, design and development of new and ongoing BI projects
Collaborating closely with internal and external teams to understand modifications impacting data lake, visualization, etc.
Participating in the entire lifecycle of BI solution delivery
Helping plan, design, implement and manage the deployment of a self-service data visualization platform in Qlik
Building and maintaining data visualizations that inform and engage business stakeholders
Analyzing key performance indicators to discover root causes for various parameters

Qualifications

Got the skills and experience? Here’s what we’re looking for:
At least 1 year of experience in relevant business domains, including data warehousing and BI tools, techniques and technology
Knowledge of data warehouse platforms such as SQL Server, AWS Redshift and SSIS required
Knowledge of Qlik and Tableau as visualization tools required
Knowledge of ETL, presentation layer and design strategy reporting required
Knowledge of data mining and experience using large-scale, complex data sets in a business environment
Proficiency in SQL and performance optimization
Technical capability to query large data sets and apply statistical models
Knowledge of advanced statistics and experience with statistical data analysis systems (scikit-learn, Pandas) a plus

Additional Information

And here’s our end of the bargain!
Hourly: $27-$31 with an annual bonus of up to 10%
Excellent medical/dental and vision coverage—EPO, PPO and HSA
401(k) retirement plan with company contribution (because you will retire someday)
Flexible benefits—choose what you like, ignore the rest
On-site preschool for our employees’ children
On-site employee gym for all levels/fitness needs
Generous employee discount on products that make you smarter
Casual dress…and we really mean it
At Lakeshore, we know our diversity makes us stronger, and when everyone feels included and valued, we all win. We strive to embrace our differences and create an intentionally diverse and inclusive community that is representative of the teachers, families and children we serve.
We know we couldn’t do the extraordinary things we’re doing without the people on our team. Thanks to the passion and enthusiasm of this spectacular group, Lakeshore is more than a great place to work—it’s a great experience to be part of. Day in and day out, we give everything we’ve got to create products that instill a sense of wonder and foster a true love of learning. To help maintain this high bar for success, we’re constantly on the lookout for people to join us. So if you’re a down-to-earth professional who shares our desire for making a difference, we’d love to hear from you.

To learn more about Lakeshore, visit www.lakeshorelearning.com/careers

Equal Employment Opportunity Policy
People are selected to become members of the Lakeshore family based on skill, merit and mind-boggling talent—not based on race, color, creed, sexual orientation, gender or gender identity, marital status, domestic partnership status, military status, religion, age, national origin, ancestry, alienage, AIDS or AIDS-related complex status, genetic information, predisposition or carrier status, status as a victim of domestic violence, physical or mental disability, or any other characteristic protected by applicable law. If things aren’t equal, we all lose.
To learn about how we collect and use Applicant information, please visit our Employee/Applicant Privacy Policy. INDRLL10"
472,Data Load Engineer / SQL Developer - eviCore - Remote,Cigna,"St. Louis, MO 63110•Remote","$77,300 - $128,800 a year","Job Profile Summary:
The Data Support Engineer will be responsible for the overall Enterprise Client Data Exchange/Load environment.

This is a critical role as all of the data received by clients are loaded to the core business applications and are vital to the patient authorization and case build process.

Required to be working on-call in the evening as and when needed.

Technical

5+ years of experience with Data Exchange/Load, Data Warehouse and TSQL development (scripting, stored procedures, triggers, query development)

Strong understanding of SQL Server Database technology

Knowledge and Experience with Best Practices and Standards for TSQL and SDLC

IBM Sterling Control Center (ICC/SCC) Integration, monitoring, SI/Sterling File Gateway (SFG) instance sending notification via SMTP email server

Extensive Knowledge on EDI Applications like ITXA & ITX Mapping & Sterling Integrator, SFG – Sterling File Gateway

Extensive Knowledge on EDI X12 Transactions like 834, 278, 999, Delimited Files, Positions/Fixed-Width files, JSON file format

Should be able to Understand and Create Shell scripts, Perl scripts and Batch scripts to automate the FTP process, handling of data discards, data errors, compliance errors etc.

Highly Experienced in HIPAA (Health Insurance Portability and Accountability Act), Health Care industry providing Business Process Assessment, Requirements Gathering, Gap Analysis, Implementation and Testing.

Experienced in handling the windows servers Admin work

Experience in supporting any Post-Patching Validations end to end

Non-Technical

Familiar with ANSI/EDI standards

Documentation Development

Excellent Interpersonal and Communications Skills

Critical Thinking/Problem Solving and Innovative/Out of Box Thinking

Ownership and Responsibility

Ticket Management

Should be capable of handling Production failures and identify the root cause immediately and be able to provide appropriate solutions.

Ability to Read Business Processes (BP's) and understand existing ones and share that knowledge with co-workers.

Able to communicate with application developers and UX designers when needed.

Additional Skills

Healthcare/Medical/insurance data experience a plus

AS400 Knowledge a plus

If you will be working at home occasionally or permanently, the internet connection must be obtained through a cable broadband or fiber optic internet service provider with speeds of at least 10Mbps download/5Mbps upload.

For this position, we anticipate offering an annual salary of 77,300 - 128,800 USD / yearly, depending on relevant factors, including experience and geographic location.

This role is also anticipated to be eligible to participate in an annual bonus plan.

We want you to be healthy, balanced, and feel secure. That’s why you’ll enjoy a comprehensive range of benefits, with a focus on supporting your whole health. Starting on day one of your employment, you’ll be offered several health-related benefits including medical, vision, dental, and well-being and behavioral health programs. We also offer 401(k) with company match, company paid life insurance, tuition reimbursement, a minimum of 18 days of paid time off per year and paid holidays. For more details on our employee benefits programs, visit Life at Cigna Group .

About Evernorth Health Services
Evernorth Health Services, a division of The Cigna Group, creates pharmacy, care and benefit solutions to improve health and increase vitality. We relentlessly innovate to make the prediction, prevention and treatment of illness and disease more accessible to millions of people. Join us in driving growth and improving lives.

Qualified applicants will be considered without regard to race, color, age, disability, sex, childbirth (including pregnancy) or related medical conditions including but not limited to lactation, sexual orientation, gender identity or expression, veteran or military status, religion, national origin, ancestry, marital or familial status, genetic information, status with regard to public assistance, citizenship status or any other characteristic protected by applicable equal employment opportunity laws.

If you require reasonable accommodation in completing the online application process, please email: SeeYourself@cigna.com for support. Do not email SeeYourself@cigna.com for an update on your application or to provide your resume as you will not receive a response.

The Cigna Group has a tobacco-free policy and reserves the right not to hire tobacco/nicotine users in states where that is legally permissible. Candidates in such states who use tobacco/nicotine will not be considered for employment unless they enter a qualifying smoking cessation program prior to the start of their employment. These states include: Alabama, Alaska, Arizona, Arkansas, Delaware, Florida, Georgia, Hawaii, Idaho, Iowa, Kansas, Maryland, Massachusetts, Michigan, Nebraska, Ohio, Pennsylvania, Texas, Utah, Vermont, and Washington State."
473,Senior Data Engineer (Remote),CareFirst BlueCross BlueShield,"Owings Mills, MD 21117•Remote",N,"Resp & Qualifications
PURPOSE:
The Senior Data Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (Relational and NoSQL, distributed and converged) with emphasis on reliability, automation and performance. This role will focus on developing solutions and helping transform the company's platforms deliver data-driven, meaningful insights and value to company.

ESSENTIAL FUNCTIONS:
Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using multiple technologies.
Interprets data, analyzes results using statistical techniques, and provides ongoing reports. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.
Develops data models by studying existing data warehouse architecture; evaluating alternative logical data models including planning and execution tables; applying metadata and modeling standards, guidelines, conventions, and procedures; planning data classes and sub-classes, indexes, directories, repositories, messages, sharing, replication, back-up, retention, and recovery.
Creates data collection frameworks for structured and unstructured data.
Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies.
Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
Applies and implements best practices for data auditing, scalability, reliability and application performance.

SUPERVISORY RESPONSIBILITY:
Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.

QUALIFICATIONS:

Education Level: Bachelor's Degree in Computer Science, Information Technology or Engineering or related field OR in lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.

Experience: 5 years Experience with database design and developing modeling tools. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.

Knowledge, Skills and Abilities (KSAs)
Knowledge and understanding of at least one programming language (i.e., SQL, NoSQL, Python).
Knowledge and understanding of database design and implementation concepts.
Knowledge and understanding of data exchange formats.
Knowledge and understanding of data movement concepts.
Strong technical and analytical and problem solving skills to troubleshoot to solve a variety of problems.
Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities.
Department
Department:Actuarial Systems
Equal Employment Opportunity
CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
Hire Range Disclaimer
Actual salary will be based on relevant job experience and work history.
Where To Apply
PeopleSoft/Self Service/Recruiting
Closing Date
Please apply before: 03/21/2023
Federal Disc/Physical Demand
Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
PHYSICAL DEMANDS:
The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
Sponsorship in US
Must be eligible to work in the U.S. without Sponsorship.
#LI-CB1"
474,Junior Data Engineer,Small Batch Standard,Remote,"$58,000 - $70,000 a year","We're the premier, remote accounting, tax, and consulting firm built exclusively to serve the craft brewing industry.
Our mission is to help craft breweries grow profits and build deep successful relationships. And our team is filled with expert, autonomous, adaptable, technology-driven high performers.
Are you up for the challenge?
We're looking for a full-time, remote Junior Data Engineer to join our specialized team. The main objective of this role is to design, develop, implement, and improve both internal and external applications to support our brewery clients and team in accordance with the SBS Core Values.
About The Role
This role will report to our Technology/Product Manager and is accountable for fulfilling the following responsibilities:
Building our data pipeline and analysis applications. A key aspect of the consulting service we provide to clients involves the collection, aggregation, analysis, modeling, and usage of financial data and benchmarks. We use this data both internally to develop and inform strategy, as well as externally through our Benchmarks Assessment (https://sbstandard.com/assessment/) and Compass analysis product (https://sbstandard.com/levelup-compass/). You'll be responsible for working with our team to build out our data pipeline for these tools, and progressively increasing our ability to aggregate, analyze, query, and feed back this data into our reporting, analysis, and consulting work. Platforms we're building with include: SQL, Airflow, Excel Visual Basic for Applications (VBA), Google Apps Script, Intuit/QuickBooks Online.
New process and technology R&D. We're always looking for new opportunities to provide both our team and our clients access to additional tools that give them leverage, automate and streamline processes, and overall make work more efficient. Part of your time will be dedicated to researching, testing, and prototyping new tech and application options.
Participate and contribute to the overall success of our team. Each week the team meets to share wins, progress, and knowledge, as well as identify and solve issues at multiple levels (company, team, individual). Your full participation in this process is critical to ensure that we are operating as a cohesive, high-performance unit.
About You
We're looking for an individual who:
Is a problem solver through-and-through. Everywhere you look, you both (a) see problems to solve, and (b) see solutions and new ways of doing things that just haven't been done yet. You know how to think outside of the box, are willing to “go there” with new ideas and solutions that haven't been done before, and have the confidence to start building, testing, iterating, and making sh*t work.
Is a systems thinker. You understand both the big picture and how the functional components fit together, and have the ability to take a specific analysis outcome and generalize it to fit a wide range of scenarios through structure and sound system design.
Can fail fast, iterate, and learn. You're an independent, self-directed, learner who isn't afraid to “move fast and break stuff” knowing that failure is a prerequisite to success, ESPECIALLY in product development. You may not have traditional credentials, but what you do have is the ability to rapidly learn, adopt, test, and understand new languages, platforms, tools, and solutions.
Is a manager of one. Unlike working within a traditional firm, in this role you'll be in the driver's seat, managing your workflow and workload in order to meet the standard set of deliverables required for each client.
About Our Culture
We're fully remote, with team members and clients located all across the U.S. and have developed our own unique culture we call The SBS Way, within which we operate, evaluate performance, and make decisions using our core values as a guide:
Be Antifragile. Everything we do, good or bad, makes us better. And every experience is an opportunity for learning and continuous improvement.
Play The Long Game. We make decisions, to the best of our ability, in the long-term interest of our firm, our team, our clients, and our broader industry and community.
Embrace Technology. We welcome new technologies with open arms, and are always exploring, testing, and implementing them in the interest of enhancing both our internal capabilities and our client's outcomes.
Build and Trust The Process. Each member of the team is committed to building, following, and improving the processes we use to deliver exceptional results for our clients.
Act as A Team of Expert Knowledge Workers. We openly and willingly collaborate, communicate, and provide rapid, direct feedback in the interest of learning, improving and developing ourselves.
Working At SBS
What it's like working at our firm:
High flexibility. We believe in the ability of our team to determine the best way to complete their work. We measure outputs, not inputs. We don't have time sheets. We don't track hours. We don't pay attention to when and where our team works. Your schedule is yours to make.
High accountability. What we care about most is that we deliver on what we promise to our clients. In this respect, we measure and manage to our deliverable performance metrics and ensure each team member takes ownership over their accomplishment with a high level of quality that aligns with our core values
Great pay for great work. We pay based on the characteristics that matter: position (and its market value), level of mastery, and longevity with the firm. All of which aim to ensure each member of the team feels they are compensated well and can focus on great work.
Merit-based career progression. We have clearly established career tracks, performance benchmarks, and mastery levels set for all of our core positions. How quickly you progress is entirely under your control, with a quarterly review and bi-annual promotion consideration cycle in place to evaluate your progress.
Generous benefits. We offer a generous benefits package that includes medical, dental, and vision insurance enrollment; as well as an IRA match, tech stipend, 3 weeks of paid time off, and entry into our profit share bonus program after two years of service.
Personal and and team development. In addition to our overall continuous learning focus, we also provide support for personal development in the form of expense coverage for continuing education (books, courses, training, certifications, etc.) as well as experiential learning (brewery visits, industry events and conferences, etc.). Each year we also meet in person for an all-expenses-paid annual retreat as a team. No work. Lots of fun. Lots of client beer.
Job Requirements
The following basic requirements must be met:
Previous experience in SQL development and database management.
Previous experience building useful applications in scripting languages like VBA, Google Apps Script, Python, PHP, etc.
Can do effective cross-functional work in a remote environment.
Have crystal clear professional written and verbal communication skills.
Have exacting organizational standards and a calm and friendly attitude.
Available and responsive during normal business hours (9am-5pm Eastern Time, Monday-Friday).
Have a strong, consistent internet connection and a work environment conducive to video calls.
Preferred qualifications include:
Direct previous experience building data pipelines.
Direct previous experience building Airflow workflows and applications.
Experience building out and managing API connections.
Experience working with Quickbooks Online or similar accounting or finance platforms.
Experience using Podio or similar remote project management tools (e.g. Trello, Asana, etc.).
Next Steps
If the position, culture, values, and mission at Small Batch Standard sound like they're the right fit for you, please apply here."
475,Reservoir Data Engineer I/ II/ III (remote),EQT Corporation,"Pittsburgh, PA 15222•Remote",N,"Location: Pittsburgh, PA
Job Category: Asset Performance
Sub-Department: RSVR ENGR
Job ID: 2058
EQT is the United States largest producer of natural gas with a goal to reach Net Zero by or before 2025 (among the fastest in the industry).
From the office to the field, the #EQTeam is fueling the future. Power your potential with us.
At EQT, we are making strides toward becoming the best producer by creating long-term value for all stakeholders, including employees, landowners, communities, industry partners and investors. Our vision is to evolve EQT into a modern, connected, digitally enabled organization.
With an incredibly collaborative culture and a determined, progressive workplace, EQT was both named a National Top Workplace, as well as one of Pittsburgh’s Best Places to Work!
Join our Qrew!
Here is how the Reservoir Data Engineer role will impact our business:
The Reservoir Engineer I/II/III main goals is to realize the full value potential of our upstream assets by having a great understanding of the reservoir. This role will help the Reservoir Engineering team build, track, and maintain engineering tools, insights, and dashboards to deliver data driven type curves and perform well analysis lookback. Furthermore, this role with help ensure our well design is optimized and is evolving through a methodically planned science and technology program.
The Reservoir Data Engineer's responsibilities include but are not limited to:
· Provide petroleum reservoir data engineering support by leveraging analytics and programming skills to our digital oilfield environment
· Improve reservoir characterization, streamline asset performance insights, and optimize the way we work leveraging modern methods for data analytics
· Lead evolution of the petroleum reservoir data process management, insight electrification & department optimization
· Develop database programming and coding solutions to electrify our processes that follow IT guidelines for data modeling and data engineering tools and platforms
· Develop petroleum statistical computing methods to electrify the petroleum reservoir engineering analysis methods
· Support evolution efforts for evolving the way we use and consume data from enterprise apps
· Lead data-driven approaches and apply data security strategy to solve business problems
· Build and maintain type curves, ensure they are aligned with geology, are data-driven, reviewed regularly to ensure they are holding up to expectations
· Use reservoir engineering principles (pressure normalize rate, numerical modeling, RTA and similar technical expertise) to perform well performance reviews
· Evaluate our peers and non-op partners to feed the evolution of our science program, technology adoption and benchmark our well design performance
· Reservoir engineering support on A&D opportunities, New Ventures, and various corporate requests
Required Experience and Skills:
· Bachelor's degree in petroleum engineering or other engineering curriculum
· 1-5 years of related experience, in petroleum engineering or other data engineering or scientific roles
· Certificate or graduate degree in data analytics, data science, or data engineering with an emphasis on petroleum systems (preferred)
· Proficient in R and/or Python and SQL programming languages
· Strong coding experience in R/ RStudio Workbench, Azure Data Studio, Spotfire, Power BI, Alteryx, IronPython
· Databases Systems: Microsoft Azure and API integration methods
· Must love technology, science, and innovation. Always on the hunt for new tech to evolve the way we work
· Knowledge of basic engineering concepts, fundamentals, and theory
· Ability to think analytically and solve basic problems

The selected incumbent will be placed into the position that best suits their abilities and experience level.
Remote work is being considered for this role.
EQT Corporation and its subsidiaries is an Equal Opportunity Employer – Disabilities/Veterans"
476,Senior Data Engineer [ Remote ],"FFF Enterprises, Inc.",Remote,"$130,000 - $165,000 a year","Senior Data Engineer
Location:
[Remote]
Salary Range:
$130,000-$165,000 / Annual Salary
Position Summary:
The senior data Engineer will work in a highly motivated team environment and with all the best practices, worth ethics, and processes followed to execute various projects. The data engineer will be responsible for collecting, managing, and converting raw data into information that can be interpreted by data scientists, business intelligence developers, and business analysts. The goal of the senior data engineer is to help the organization to utilize data for performance evaluation and optimization.
Essential Functions and Duties:
Provide project and overall day to day support to the organization.
Supporting a high performing agile delivery team consisting of onsite and offshore members.
Use effective communication and presentation skills to communicate concepts, facilitate conflict resolution and recommended solutions.
Monitors and organizes the efforts of technical and business support staff.
Manages projects according to milestones and completes tasks assigned by more experienced analysts and managers. Guides the efforts of less experienced team members.
Developing data warehousing applications, familiarity with ETL architectures, leveraging multiple sources, (including SAP S4HANA Database and Non-SAP Databases)
Demonstrate strong knowledge of technology developments within healthcare domain.
Champion continual quality improvement consistent with company's core values.
Adheres specifically to all company policies and procedures, Federal and State regulations and laws.
Display dedication to position responsibilities and achieve assigned goals and objectives.
Represent the Company in a professional manner and appearance at all times.
Understand and internalize the Company's purpose; Display loyalty to the Company and its organizational values.
Display enthusiasm and dedication to learning how to be more effective on the job and share knowledge with others.
Work effectively with co-workers, internal and external customers and others by sharing ideas in a constructive and positive manner; listen to and objectively consider ideas and suggestions from others; keep commitments; keep others informed of work progress, timetables, and issues; address problems and issues constructively to find mutually acceptable and practical business solutions; address others by name, title, or other respectful identifier, and; respect the diversity of our work force in actions, words, and deeds.
Comply with the policies and procedures stated in the Injury and Illness Prevention Program by always working in a safe manner and immediately reporting any injury, safety hazard, or program violation.
Ensure conduct is consistent with all Compliance Program Policies and procedures when engaging in any activity on behalf of the company. Immediately report any concerns or violations.
Other duties as assigned.
Education, Knowledge, Skills and Experience:
Required Education:
Bachelor's Degree in or a related field of study.
Required Knowledge:
Experience working in Healthcare Life Sciences.
Informatica IICS experience.
Excellent SQL skills.
Google Cloud Platform Foundation (BigQuery/SQL experience).
Required Experience:
Must have at least three (3) years' experience as an Informatica Intelligent Cloud Services (IICS) Developer.
Data warehouse experience (At least 5 years).
SQL experience (At least 5 years).
Google BigQuery (Preferred).
Required Skills:
Forward thinking, independent, creative, and self-sufficient who can work with less documentation, has exposure working in complex multi-tiered integrated applications.
Strong SQL experience with the ability to develop, tune and debug complex SQL applications.
Experience with Google Cloud Platform, BigQuery and Informatica Intelligent Cloud Services (IICS).
Ability to interact well with user base, development teams and other technical team members.
Experience with database design, querying, stored procedures, views, joins, performance tuning etc.
Experience with load optimization for Informatica batch processes.
Ability to diagnose and resolve issues/problems in aggressive timeframes.
Strong analytical, problem-identification/solving skills, as well as strong written and oral communication skills.
Experience in querying and analyzing data coming from multiple sources and determine logic for transformations.
Ability to interact well with user base, development teams and other technical teams.
Providing production support for daily, weekly, and monthly production processes.
Provides data modeling where needed to enable data movement or improve data architecture.
Demonstrates advanced analytical and problem-solving skills in resolving complex discrepancies, by actively suggesting and assisting in implementing process improvements and procedures.
Excellent verbal and written communication skills and the ability to interact effectively with end users, co-workers, and management.
Ability to exercise discretion and maintain confidentiality to the level of required HIPAA standards.
Intermediate to advanced Microsoft Office skills.
Physical Requirements:
Vision, hearing, speech, movements requiring the use of wrists, hands and/or fingers. Must have the ability to view a computer screen for long periods and the ability to sit for extended periods. Must have the ability to work the hours and days required to complete the essential functions of the position, as scheduled. The employee occasionally lifts up to 20 pounds and occasionally kneels and bends. Working condition include normal office setting.
Mental Demands:
Learning, thinking, concentration and the ability to work under pressure, particularly during busy times. Must be able to pay close attention to detail and be able to work as a member of a team to ensure excellent customer service. Must have the ability to interact effectively with co-workers and customers, and exercise self-control and diplomacy in customer and employee relations' situations. Must have the ability to exercise discretion as well as appropriate judgments when necessary. Must be proactive in finding solutions.
Direct Reports:
Project resources (in-house).
EEO/AAP Statement:
If you are applying for a job and would like to make a request for a reasonable accommodation during any part of the employment process, submit an email to Human Resources at mmiller@fffenterprises.com or call (951) 296-2500 extension 1391. Please include your contact information along with the specifics of your request for a reasonable accommodation. Only inquiries regarding a reasonable accommodation request will receive a response via email or phone in a timely manner.
FFF Enterprises is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law.
Read about our excellent employee benefits package."
477,Senior Software Engineer - Data and API,Rehrig Pacific Company,"Richardson, TX 75081",N,"Here at Rehrig Pacific, we are all about our people. Since 1913, our organization has focused on sustainable supply chain solutions while creating a culture and atmosphere where amazing people, like you, are celebrated for doing their best work. Rehrig Pacific has grown to meet the needs of our industry consumers across the country and internationally. We are constantly creating innovative solutions to transcend the new standards set forth by our customers. We find true fulfillment in helping others, both within the Rehrig Pacific family and in our communities. As servant leaders, we lead by example.
Brief Role Description
The Senior Software Engineer with a specialty in data and APIs will be accountable for helping our customers integrate with our enterprise products and applications. The person will collaborate with both our client’s and Rehrig’s engineers to integrate data between systems. The desired skills of the senior software engineer involve high levels of understanding around database architecture, SQL queries, ETL, authentication, authorization, REST, and python. This position reports to the Technology Engineering Director in the New Product Development group.

Accountabilities
Ability to work closely with technical architects and our client’s architecture governance technical team for solution development and design reviews
Manages API lifecycle and release management
Manages data integrity
Manages how we do reporting
Manages the database architecture
Creates and maintains effective documentation on solutions: The Engineer will create documentation on the data flow, architecture, and daily operation and troubleshooting for all assigned solutions and maintain that documentation in an ongoing fashion
Work closely with Rehrig’s Product Owners, and must understand the short and medium-term strategy for the business
Function as an escalation point for technical support and troubleshooting: The Engineer will help troubleshoot API/ETL issues and problem resolution

General Responsibilities
Provide timely updates to management on open issues and projects
Ensure availability and security requirements are met consistently
Other duties as assigned

Qualifications
Bachelor’s Degree in Computer Science, Management Information Systems or a related field or equivalent in work experience.
Minimum of 5 years of software engineering experience
Experience with REST based APIs
Knowledge of HTTP and SOAP Protocols
Experience with source code management tool (git)
Demonstrated subject matter expertise in all of the following: Python, Agile Development, SQL Queries, ETL
Experience working cross functionally in disparate geographies required
Ability to travel approximately 10% of the time
Rehrig Pacific Company is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also here."
478,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
479,SQL Data Engineer,BPM LLP,United States,N,"BPM Overview :
What does BPM stand for? Innovation, opportunity, community, diversity, inclusivity, flexibility and so much more.

B-P-M stands for “Because People Matter,” because at our core, our people drive everything we do and how we do it.

We are a forward-thinking, full-service accounting firm providing modern solutions to businesses across the globe. We focus on comprehensive assurance, tax, and consulting services for our clients, and we provide our people and our community with the resources to lead meaningful and purposeful lives.

While we are one of the largest California-based accounting firms, our flexible work locations and schedules mean we have professionals across the continent. Our teams and our clients drive us to provide quality services and ignite unique insights and ideas that contribute to our continued success. Our clients come from different backgrounds and industries, which keep our people intellectually challenged every day.

Our initiatives and ideals lead to our continued recognition as one of the “Best Places to Work” in the Bay Area and beyond. We are dedicated to providing meaningful careers for all of our employees along with fostering an environment that allows an integrated lifestyle. Our flexible culture allows our professionals to live a balanced lifestyle between their work responsibilities and personal commitments.

Job Responsibilities:
To gain intimate knowledge of STAR data structures and code (stored procedures, functions, and triggers).
To understand reports that STAR has developed for BPM and be able to modify existing or create new ones.
Understand current pain points with STAR, research daily issues and work with STAR engineers to resolve them.
Learn BPA system within STAR. Be able to modify and create alerts and other aspects of it using STAR SQL Databases including modifying Stored Procedures.
Work closely with the Integration framework and be able to assist other developers in implementing integration modules.
Oversee STAR new releases and analyze compatibility with existing code and systems.
Take over regular update procedures currently performed by STAR such as Budget and Rates updates.
Creating and running ad-hoc SQL Queries of various complexity.
Assist with technical training and documentation on newly released STAR modules.

Qualifications / Skills:
Profound knowledge of RDBMS data design principles.
5-7 years of working with MS T-SQL writing queries and stored procedures.
Experience performing root cause analysis on data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with structured and unstructured datasets.
Expert knowledge of SQL queries performance, scaling, and optimization.
3-5 years of working experience with SQL Server Reporting Services and SSRS.
Solid experience working with SSMS or similar Integrated environments.
Excellent communication skills.

Highly Desired:
Experience with Azure SQL DB.
Familiarity working with .Net Framework, .Net Core, C#.
Development experience in working with Accounting Systems and Industries

Note: STAR will assist with all aspects of initial training related to their structures and systems
Wondering if you should apply?

BPM is powered by knowledgeable, enthusiastic, and forward-thinking people committed to developing a culture of inclusion. We recognize, develop, and empower talent and encourage diversity of thought. Your point of view, skillset and experience will only make us stronger, so if you're eager to share new ideas and try new things, we want to hear from you.

***************

BPM provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance.

Please note - this posting is for prospective candidates only. Unsolicited third party resume submissions will be considered property of BPM and will not be acknowledged or returned."
480,Senior Data Engineer - DataBricks,Grainger,Illinois,N,"About Grainger:
Grainger is a leading broad line distributor with operations primarily in North America, Japan and the United Kingdom. We achieve our purpose, We Keep the World Working®, by serving more than 4.5 million customers with a wide range of products that keep their operations running and their people safe. Grainger also delivers services and solutions, such as technical support and inventory management, to save customers time and money.
We're looking for passionate people who can move our company forward. As one of the 100 Best Companies to Work For, we have a welcoming workplace where you can build a career for yourself while fulfilling our purpose to keep the world working. We embrace new ways of thinking and recognize everyone you. Find your way with Grainger today.

Position Details:
The Data Engineering team at Grainger is focused on transforming data from Grainger’s key domains into reliable and real-time analytics products that address key needs. You will be focused on building and operating data pipelines that power analytics ranging from key financial reports to production models that define Grainger.com’s user experience. You will play an important part in defining the strategy of the team, evaluating and integrating data patterns and technologies, and building data products alongside domain experts. You are a thoughtful observer who enjoys investigating business problems and building data solutions that address them. You are an technical teacher that can guide teams to adopt the capabilities and products you build.

You Will:
Design efficient and scalable data processing systems and pipelines on Databricks, Airflow, APIs, and AWS Services.
Create technical solutions that solve business problems and are well engineered, operable, maintainable, and delivered.
Design and implement tools to detect data anomalies. Ensure that data is accurate, complete, and across all platforms.
Develop data models and mappings and build new data assets required by users. Perform exploratory data analysis on existing products and datasets.
Provide technical guidance to help data users adopt new data pipelines and tools.
Develop scalable and re-usable frameworks for ingestion and transformation of large datasets.
Understand trends and latest technologies. Evaluate the performance and applicability of potential tools for our requirements.
Work within an Agile delivery / DevOps methodology to deliver product increments in iterative sprints.
Design, and maintain efficient and scalable data processing systems and pipelines on Databricks, Airflow, APIs, and AWS Services.
Create technical solutions that solve business problems and are well engineered, operable, maintainable, and delivered on schedule.
Design and implement tools to detect data anomalies. Ensure that data is accurate, complete, and across all platforms.
Develop data models and mappings and build new data assets required by users. Perform exploratory data analysis on existing products and datasets.
Provide technical guidance to help data users adopt new data pipelines and tools.
Develop scalable and re-usable frameworks for ingestion and transformation of large datasets.
Understand trends and emerging technologies. Evaluate the performance and applicability of potential tools for our requirements.
Work within an Agile delivery / DevOps methodology to deliver product increments in iterative sprints.
Work with our AI, Platform, and Business Analytics teams to build useful pipelines and data assets.
You Have:
Experience in batch and streaming ETL using Spark, Python, Scala on Databricks for Data Engineering or Machine Learning workloads.
Familiarity with AWS Services not limited to Glue, Athena, Lambda, S3, and DynamoDB
Experience prepping structured and unstructured data for data science models
Demonstrated experience implementing data management life cycle, using data quality functions like standardization, transformation, rationalization, linking and matching.
Familiarity with containerization and orchestration technologies (Docker, Kubernetes) and experience with shell scripting in Bash, Unix or windows shell is preferable.
Rewards and Benefits:
With benefits starting day one, Grainger is committed to your safety, health and wellbeing. Our programs provide choice and flexibility to meet our team members' individual needs. Check out some of the rewards available to you at Grainger
Medical, dental, vision, and life insurance plans
Paid time off (PTO) and 6 company holidays per year
Automatic 6% 401(k) company contribution each pay period
Employee discounts, parental leave, 3:1 match on donations and tuition reimbursement
A comprehensive set of emotional, financial, physical and social wellbeing programs
DEI Statement
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace."
481,Sr Data Engineer,OscarMike,"Irving, TX 75063",N,"Sr Data Engineer (Python/Databricks)

Direct Hire 1099 or W2 only.
The client is not offering VISA sponsorship or subcontracting for this position.
This job is located onsite in Irving, TX. Local applicants only.

Our client needs highly enthusiastic Python/Databricks engineers to create, tune, maintain and build reporting for the Big Data platforms that drive their retail business. This role is key to the company's continued market dominance, and it requires people that are passionate about data and seeking perfection in their development.

Requirements:

KEY DUTIES AND RESPONSIBILITIES:

Performing systems integration design and development in cloud architecture design (Azure)
Building sizing and cost models for Cloud Services (Azure)
Review enhancement requests and recommend best solution to stakeholders as SME.
Containerizing ingestion processes using docker and kubernetes services.
Understand business requirements to design , build, develop and test data integration pipelines.
Plan, direct DevOps implementation and system hardware/software updates for better performance, stability.
Work on the BI dashboards to support retail stores and the Azure AI platform that runs the personalized consumer experiences used by millions each day.

EDUCATION AND EXPERIENCE:

EDUCATION: Bachelors/4 Yr Degree
YEARS OF RELEVANT WORK EXPERIENCE: 5+ years
YEARS OF MANAGEMENT EXPERIENCE: NA

SPECIFIC KNOWLEDGE AND SKILLS:

3+ years or demonstrated mastery of development with programming languages - Python, SQL, Unix.
2+ years experience with Apache Spark and Databricks cloud tool sets (AWS or Azure).
1+ years experience with GCP/Azure Cloud Big Data tools (PowerBI/DataLake/ Azure Databricks).
Self-motivated with excellent analytical, problem solving, verbal and communication skills.
Proficiency in API security frameworks, token management and UAC including OAuth, JWT, etc.
Basic knowledge of using Devops, Github, JIRA tools and ability to work in agile environment.
Bachelor's degree - Computer Science or equivalent."
482,Data Engineer (CosmosDB),Talent Hires,Remote,N,"Data Engineer (CosmosDB)
Fulltime
Remote
Requirements for this role:
Designing and implementing data models and data distribution, loading data into an Azure Cosmos DB database, and optimizing and maintaining the solution.
The professional should design, implement, and monitor solutions that consider security, availability, resilience, and performance requirements.
Must have solid experience developing apps for Azure and working with Azure Cosmos DB database technologies.
The professional should be proficient at developing applications that use the API for Azure Cosmos DB for NoSQL, and write efficient SQL queries for the API
Experience creating server-side objects and should be able to interpret JSON.
Optimizing query performance in Azure Cosmos DB, and designing and implementing change feeds for an Azure Cosmos DB,
Defining and implementing an indexing strategy for an Azure Cosmos DB, should be able to create appropriate index policies.
Job Type: Full-time
Pay: From $120.00 per year
Benefits:
Health insurance
Schedule:
8 hour shift
Experience:
Cosmos DB: 4 years (Required)
SQL: 3 years (Required)
Data warehouse: 3 years (Preferred)
Work Location: Remote
Speak with the employer
+91 (628) 500-1975

Health insurance"
483,Data Engineer,Tulip Interfaces,"561 Windsor St B402, Somerville, MA 02143",N,"Tulip, the leader in frontline operations, is helping companies around the world equip their workforce with connected apps, leading to higher quality work, improved efficiency, and end-to-end traceability across operations. Companies of all sizes and across industries have implemented composable solutions with Tulip's cloud-native, no-code platform to solve some of the most pressing challenges in operations: error-proofing processes and boosting productivity, capturing and analyzing real-time data, and continuous improvement.
A spinoff out of MIT, Tulip is headquartered in Somerville, MA, with offices in Germany and Hungary. Focused on composable, human-centric solutions for industrial environments, Tulip is disrupting the MES category and has been recognized as a World Economic Forum Global Innovator. Tulip has also been named one of Energage's Top Workplaces USA and one of Built In Boston's ""Best Places to Work"" and ""Best Midsize Places to Work"" for 2023.

About You:
You love a good challenge and learning new things.
You have gotten disparate systems working robustly together.
You have delivered complex data engineering solutions.
You're able to own a core part of the product and juggle the different requirements that come along with it.
You are comfortable moving around a large technology stack to understand how those features work and contribute to different parts of the platform.

What skills do I need?
4+ years of experience as a data engineer, fullstack or backend developer
Proficient in python for analytical computing
Highly proficient in SQL distributed data processing pipelines (Spark, Glue, Airflow)
Experience building and maintaining a data warehouse (Snowflake, Redshift)
Works well as a part of a team, with effective communication

Key Responsibilities:
Develop and maintain the data warehouse.
Develop new business metrics and improve existing ones.
Work directly with stakeholders to identify data needs/requirements.
Produce clean, efficient code based on specifications
Test and deploy programs and systems
Work with developers to design algorithms and flowcharts
Integrate software components and third-party libraries
Troubleshoot, debug and upgrade existing software
Gather and evaluate user feedback
Recommend and execute improvements
Create technical documentation for reference and reporting

Key Collaborators:
Engineering, Product Management, Hardware, Commercial Teams

Working At Tulip
We know even great candidates experience imposter syndrome. Even if you don't match every requirement, applying gives you the opportunity to be considered.
We're building a strong, diverse team that values hard work, families, and personal well-being. Benefits of working with us include:
Direct impact on product and culture
Company equity
Competitive benefits package including Health, Dental, Vision, Short-term Disability, Long-term Disability, Life Insurance, AD&D Insurance, Flexible Spending Account (FSA), Commuter Benefits, Parental Leave, and 401(K)
Flexible work schedule and unlimited vacation policy
Virtual company events and happy hours
Fitness subsidies
We are an equal opportunity employer. At Tulip, we celebrate all. Qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Help us build an inclusive community that will transform frontline operations."
484,Data Engineer,Quant Solutions,"Quant Solutions in Atlanta, GA 30303","$120,000 - $150,000 a year","Job Title: Data Engineer
Location: Atlanta, GA
Duration: Full Time
Skills Required:
Five or more years’ experience in software engineering.
Five or more years’ experience in large scale RDBMS environments or Google BigQuery
Two or more years of Exadata experience OR Google BigQuery
Four or more years’ experience with Informatica PowerCenter or IICS
One or more years’ experience in Erwin
Experience in code automation (e.g., pattern-based integration)
Experience in advanced SQL and PL/SQL techniques
Experience in building re-usable Utility packages.
Experience with testing the code.
Experience in Unix shell and Python scripting
Integration design & data modeling skills in Data Lake and Data Warehousing environments
Exposure to both on-prem and cloud Integration solutions
Familiarity with non-relational DB technologies is a plus.
Experience with automated testing
Experience with both batch and real-time patterns for integrations
Ability to build and analyze complex integration workflows from heterogeneous data sources.
Experienced in large Enterprise Data Warehouse & Integration projects.
Strong background in full lifecycle development using multiple platforms or languages.
Ability to interact at a technical and non-technical level with Infrastructure, Network, Development, BA, and QA teams.
Development experience in high transaction/high availability systems.
Experience with analyzing and recommending solutions for Production issues short-term and long term.
Job Type: Full-time
Salary: $120,000.00 - $150,000.00 per year
Work Location: One location"
485,Data Engineer,OUTFRONT media,"1500 Broadway, New York, NY 10036","$140,000 - $155,000 a year","The Data Engineer is part of the Platform and Data team within OUTFRONT and is responsible for enhancing and maintaining the infrastructure to answer questions with data, using software engineering best practices, data management fundamentals, and data storage principles. You will work with data to populate our audience platform, allowing our client facing organizations to better serve their clients.
Principal responsibilities
Receiving, validating, and loading data to be used within our audience and other platforms
Evaluating new data sources for structure and continuity of application
Ensuring data computations are as efficient as possible
Data analysis on large and small scales to meet company and client needs
The ideal person
1+ years of experience as a Data Engineer or in a similar role
Experience with data warehousing, and building ETL pipelines
Experience in SQL
Bachelor's degree in an engineering or technical field such as Computer Science, Physics, Mathematics, Statistics, Engineering or similar
1+ years of experience with data warehouse technical architectures, ETL/ ELT, reporting/analytic tools and, scripting
Advanced hands-on experience in ANSI SQL with working knowledge of Redshift, HiveSQL, Spark SQL, or Scala
Familiar with data warehouse schema architectures
Your skills
Hands on experience with AWS Redshift Database
Exposure to AWS ecosystem with products such as EMR, Athena, Glue etc.
Experience in scripting languages such as Python/Pyspark for parsing and data analysis.
Experience with AWS API Gateway
Experience building APIs and automation with Python
Proven ability to solve complex quantitative business challenges
Sound business judgment, strong attention to detail, excellent organization skills, and ability to manage multiple projects and responsibilities are critical to success in this position
Proven track record of identifying metric variances and delivering solutions to address the changes
Reporting
The position reports to the Platform and Data team within the Product organization.
For NY only, the salary range for this role is $140,000-$155,000 per year. Compensation is determined during our interview process by assessing a candidate’s experience and skills relative to internal peers and market benchmarks evaluated for the scope and responsibilities of the position. Please note that the foregoing compensation information is a good-faith assessment associated with this position only and is provided pursuant to the New York City Salary Transparency Law.
To all Recruitment Agencies: OUTFRONT Media LLC does not accept agency and unsolicited resumes. Please do not forward resumes to our OUTFRONT Media employees or any other company location.
OUTFRONT Media is not responsible for any fees related to unsolicited resumes.
OUTFRONT Media Is An Equal Opportunity Employer
All applicants shall receive equal consideration without regard to race, color, religion, gender, marital status, gender identity or expression, sexual orientation, national origin, age, veteran status or disability. Please refer to the
OUTFRONT Media Affirmative Action policy statement."
486,SENIOR DATA SCIENCE ENGINEER (ECONOMIST),DraftKings,"500 Boylston St, Boston, MA 02116",N,"BOSTON, MA
ENGINEERING
JR06102
FULL TIME
At DraftKings, we're inspired by our shared passion for developing creative solutions to complex challenges and empowering the people around us to do their best work. We are industry leaders in the digital entertainment and technology space and are propelled by constant curiosity and diverse perspectives.
BE THE STRATEGY BEHIND THE GAME:
Our team comprises algorithm experts, data science technologists, and causal thinkers, coming together to develop innovative data products that solve analytically challenging problems at DraftKings. As part of this role, you will be a creative thinker, utilizing data, machine learning, and causal inference skills to craft high-impact solutions that grow the business.
WHAT YOU'LL BE AS A SENIOR DATA SCIENCE ENGINEER (ECONOMIST):
Leverage sound modeling frameworks to automate critical business processes which unlock scale and efficiency.
Shape DraftKings’ consumer product strategy through a deep understanding of behavioral economics and research
Build robust causal models to support marketing and pricing decisions utilizing a variety of transactional and behavioral data.
Collaborate with product and analytics to design and execute rigorous field experiments and identify and analyze natural experiments.
WHAT YOU'LL BRING:
Deep understanding of econometric techniques, notably dealing with causal analysis of field and natural experiments.
Experience in a programming language like Python or R.
Experience with SQL.
Expertise in statistical modeling and machine learning techniques, including regression analysis, decision trees, and neural networks.
Excellent communication skills with the ability to effectively present complex data and technical concepts to non-technical stakeholders.
An intuitive sense of how quantitative work aligns with business priorities.
Ideally, you have experience as a core contributor on a data science team where you were responsible for multiple aspects of data science technical projects, including development and deployment, and then monitoring of how those applications perform in production.
JOIN US!
Our teams are fueled by innovation. We are looking ahead, building what’s next, and continuously reinventing the industry. We’re a publicly traded (NASDAQ: DKNG) technology company headquartered in Boston, with teams around the world and an expanding global presence.
We strive to create a place where all feel safe, empowered, engaged, championed, and inspired. DraftKings is proud to be an equal opportunity employer. This means we do not tolerate discrimination of any kind and are committed to providing equal employment opportunities regardless of your gender identity, race, nationality, religion, sexual orientation, status as a protected veteran, or status as an individual with a disability.
READY TO BUILD WHAT’S NEXT? APPLY NOW.
As a regulated gaming company, you may be required to obtain a gaming license issued by the appropriate state agency as a condition of employment.
The US base salary range for this full-time position is $120,800.00 - $181,200.00, plus bonus, equity, and benefits as applicable. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range and how that was determined during the hiring process."
487,Lead Systems Engineer - Flight Data Analytics,L3Harris Technologies,"Saint Petersburg, FL•Remote","$104,500 - $193,500 a year","Description:
Job Title: Lead Systems Engineer - Data Analytics
Job ID: IMS20231603-98449

Job Location: St. Petersburg FL (preferred), or Remote
Schedule: 9/80 every other Friday off - hybrid or Remote
Job Description:
We are currently seeking a Systems Engineer to work on the specification and creation of the next generation of cloud-based flight data analytics and AI/ML solutions to serve our global customer base. We are looking for a creative problem solver and team player - a candidate with a strong background in data analytics, solid project management skills, and cutting-edge technological know-how. This role requires that you have the energy and ability to work with a variety of functions and teams to collaboratively design modern, secure, and data-centric solutions - and then you must be comfortable implementing and deploying those solutions end-to-end. You must be dynamic and proactively communicate with your co-workers, management, and customers alike. Finally, we expect that you excel in managing your own time, prioritizing multiple requests and demands, and thriving in a global and fast-paced environment where you enjoy making contributions that positively impact the team and leverage the company’s success.
Essential Functions:
Capturing high-level business requirements from internal and external stakeholders and translating them into well-documented engineering specifications.
Developing new systems designs, writing systems specifications, performing system level tests, and ensuring applicable commissioning and deployment of Data Analytics solutions.
Supporting with platform and application architectural designs and decisions.
Interfacing with customers, suppliers, and internal functional groups (product owners, sales, marketing, customer support, etc.) ensuring the logical and systematic conversion of customer or product requirements into total systems solutions.
Creatively guiding the design, development, test, monitoring and maintenance of Data Analytics solutions throughout their lifecycle.
Performs functional analysis, timeline analysis, detail trade studies, requirements allocation and interface definition studies to translate customer requirements into software (and hardware) specifications.
Helps maintain and develop Data Analytics engineering processes.
Coordinates cross-process team issues both within and between teams.
Plans and executes system automation as needed for increased efficiency.
Performs technical planning, system integration, verification and validation, cost and risk and supportability and effectiveness analyses for total systems.
Implement and maintain strategies for optimal performance, backup, security, scalability and redundancy.
Qualifications:
Bachelor of Science Degree in Computer Science, Aerospace Engineering, Electrical Engineering or related engineering discipline.
9 years of prior relevant experience, preferably in the Technology and/or Aerospace & Defense Industries.
Experience with or knowledge of programming languages (e.g. Python), Linux operating systems, system performance and monitoring tools, containerization and orchestration, RESTful APIs, CI/CD, DevOps, cloud computing (AWS preferred), Software separation of concerns, and cybersecurity
Preferred Additional Skills:
MBA or advanced degree
Professional and Technology certifications
Proficiency in capturing high-level customer demands, translating them into engineering specifications, and writing systems requirements and documentation aligned with Engineering procedures while supporting the product/solution development lifecycle.
You are a distinguished engineer, great problem solver, and a team player ready to join our skilled group of engineers, data scientists, and product owners delivering mission-critical data analytics (infrastructure and automation)
Experience delivering Data Pipelines and AI/ML models from concept to production
Knowledge of Git version-control software and change management best practices
Experience in project management, application design and integration, and modern software development best practices and architectures
Experience in implementing data-centric, multi-platform applications, content delivery, and coaching
Experience with or knowledge of flight data
Strong mathematical & statistics background and knowledge of time series and predictive algorithms
In compliance with pay transparency requirements, the salary range for this role is $104,500 - $193,500. This is not a guarantee of compensation or salary, as final offer amount may vary based on factors including but not limited to experience and geographic location. L3Harris also offers a variety of benefits, including health and disability insurance, 401(k) match, flexible spending accounts, EAP, education assistance, parental leave, paid time off, and company-paid holidays. The specific programs and options available to an employee may vary depending on date of hire, schedule type, and the applicability of collective bargaining agreements.


#LI-Remote"
488,Data Engineer,Acutus Medical,"2210 Faraday Ave Ste 100, Carlsbad, CA 92008",N,"Acutus Medical is totally focused on the development and commercialization of solutions that improve the way complex cardiac arrhythmias are diagnosed and treated. Our passionate, driven team of innovative professionals are dedicated to providing better tools for clinicians and making life better for the millions of people who suffer from these challenging conditions around the world. Are you ready to be a part of a dynamic, innovative team with a shared purpose that truly matters? If so, we are currently looking for a Data Engineer to join us in our important mission.
Position Overview
The Data Engineer is a key contributor on the Medical Affairs team overseeing the collection, preparation, quality, accessibility, and organization of Acutus-supported research data. The Data Engineer, working closely with all internal stakeholders, will be responsible for the curation and quality control of high volumes of data. This role will drive the future of our data infrastructure, ensuring data is reliable, and accessible to stakeholders. The data curation platform will integrate data from a variety of internal and external sources to support quantitative data analytics. The Data Engineer will also optimize data flow and collection, ensure data delivery is efficient, robust, standardized, organized and well labeled. The ideal candidate is proficient in Scientific Computing languages (MATLAB, Python or C++), and has experience in populating and maintaining large databases (SQL). Effective performance in this role is demonstrated by delivering high-quality data and making an impact on our decision-making process while proactively enhancing current operational efforts.

Duties and Responsibilities
Collect and assemble large complex datasets that meet technical specifications for use in scientific and developmental purposes
Process and curate high volumes of data through internal/external software sources
Crafting, maintaining efficient data pipeline architecture and ensuring quality control of the data
Optimizing data flow, ensuring data robustness, organization and consistent self-explanatory labeling
Identifying, designing, and implementing internal process improvements, such as automating manual processes, optimizing standardized data delivery, re-designing infrastructure for greater scalability, etc.
Working with Field, Technical and Medical Affairs team members to assist with data retrieval, data-related technical issues and support their data infrastructure needs
Works under general guidance. Must understand overall project goals. Is a collaborative team player and demonstrates open-mindedness and flexibility. Creates innovative and creative solutions to keep projects on-track. Maintains a high degree of quality in all work performed.
Must demonstrate mutual respect, ongoing communication and a positive outlook with both internal and external team members.

Qualifications
Requires a Bachelor's degree in Information Technology or in another Scientific/Engineering discipline preferably with Research experience, or a Master's degree.
Experience in Scientific Computing languages (MATLAB, Python, C++)
Experience scraping data from various sources, centralizing in existing or to be developed databases
Working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Ability to learn usage of new software with limited guidance (e.g., quick capacity to learn how to use our software “the Frame”)
Demonstrate an aptitude and enthusiasm for learning. Must be able to understand job duties and responsibilities, have the necessary skills/knowledge and be willing and able to continue learning and growing within the field.
Must be accurate, have strong attention to detail, and demonstrated critical thinking
Responds positively to direction from leadership and guidance received on work assignments
Demonstrates strong technical ability at a fundamental level. Will regularly collaborate with senior Medical Affairs and R&D team members. Works on moderately complex problems where analysis of situations or data requires in-depth evaluation. Exercises judgment within generally defined practices in selecting methods and techniques for obtaining solutions.
Committed to high quality standards and proactive in finding solutions to achieve successful outcomes.
Strong verbal and written communication skills with the ability to produce accurate, punctual reports/information. Must be able to read, write and speak effectively. Must be able to effectively communicate to different audiences (no technical background) and skill levels within the organization
Strong listening skills with the ability to seek constructive feedback and remain flexible and open-minded. Able to quickly adapt to change.
Capable of working under pressure and in a timely manner.

Founded in 2011, Acutus Medical is headquartered in Carlsbad, CA. We pride ourselves on being an innovative company comprised of dedicated and talented industry leaders working together to make a distinctive mark within the Electrophysiology market. Our team works diligently to fulfill the mission of bringing advanced tools for physicians and hospitals to access, identify, diagnose and treat complex arrhythmias to in order to optimize and expand the success of cardiac ablation.

The anticipated salary range for candidates is $95,000 to $115,000. The final salary offered to a successful candidate will be dependent on several factors that may include but are not limited to the type and length of experience within the job, type and length of experience within the industry, education, etc.

Our employees enjoy working in a company that truly cares about them, their career and overall wellbeing. We offer competitive salaries, comprehensive benefits, paid time off, holidays and a variety of health and wellness programs. We are steadily growing and look forward to adding more talent to our team.

Acutus Medical provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Acutus uses E-Verify for all employment verifications.
We are not accepting resumes from 3 rd party headhunters or agencies."
489,Azure Data Engineer,Queryon,Remote,"$100,000 - $120,000 a year","Queryon is looking to fill an Azure Data Engineer role.
This position requires a high level of technical knowledge and experience to help us deliver innovative custom solutions for our clients. You will have the opportunity to design and implement Microsoft Azure cloud solutions and technologies across a variety of industries. The ideal candidate will be a motivated, collaborative, and innovative individual able to perform in a fast-paced, modern IT environment.
Qualifications
Prior consulting experience required.
Experience in designing and hands-on development in cloud-based analytics solutions.
Expert level understanding on Azure Data Factory, Azure Synapse, Azure SQL, Azure Data Lake, and Azure App Service is required.
Designing and building of data pipelines using API ingestion
Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code is essential.
Thorough understanding of Azure Cloud Infrastructure offerings.
Strong experience in common data warehouse modelling principles including Kimball, Inmon.
Knowledge in Power BI is essential.
Knowledge in Power Platform tools such as Power Apps is desirable.
Knowledge of Python and Hands-on with Powershell scripting is desirable.
Experience developing security models.
Soft Skill Qualifications
Strong customer engagement skills to understand customer needs for Analytics solutions fully.
Experience in leading small and large teams in delivering analytics solutions for customers.
Have demonstrated ability to define, develop and implement data models and supporting policies, standards, and guidelines.
Strong data analysis and analytical skills.
Have demonstrated the ability to guide the development of data requirements for projects and drive a user experience that is easy to use and delivers the right information at the right time.
Experience in working in a fast-paced environment.
Strong problem solving and troubleshooting skills.
Responsibilities
Provide technical leadership and thought leadership as a senior member of the Data Analytics team in areas such as data access & ingestion, data processing, data integration, data modelling, database design & implementation, data visualization, and advanced analytics.
Engage and collaborate with customers to understand business requirements/use cases and translate them into detailed technical specifications.
Lead quality assurance (QA) activities from design to implementation phase of the proposed solutions to customers.
Collaborate with project managers for project/sprint planning by estimating technical tasks and deliverables.
Engage with clients’ leadership and technology teams in strategic discussions to provide technical strategy and industry guidance to improve their long-term business goals.
Staying abreast of the latest developments in the modern data platform area (especially Azure analytics services)
Lead small teams in delivering solutions for customers.
Develop best practices including reusable code, libraries, patterns, and consumable frameworks for cloud-based data warehousing and ETL.
Maintain best practice standards for the development or cloud-based data warehouse solutioning including naming standards.
Benefits
Health insurance benefits
Generous unlimited PTO / sick leave
401(k) plan with employer match
Home office stipend
Yearly bonus structure
Work remotely from anywhere in the US!
Job Type: Full-time
Pay: $100,000.00 - $120,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Experience:
Business consulting: 3 years (Preferred)
Azure: 5 years (Preferred)
Work Location: Remote

Health insurance"
490,Real Estate Data Strategy & Analytics Engineer,Salesforce,"111 West Illinois Street, Chicago, IL 60654",N,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Products and Technology
Job Details
We’re Salesforce, the Customer Company. If you believe in bringing companies and customers together, in business as the greatest platform for change, in creating a more equitable and sustainable future for all – well, you’re in the right place. Through our #1 CRM, Customer 360, we help companies blaze new trails and connect with their customers in a whole new way. And, we empower you to be a Trailblazer, too — driving your performance and growth, charting new paths, and improving the state of the world.
At Salesforce, our unique office design is a powerful expression of who we are and what we care about. We want to make an impact on every person who enters our spaces by crafting and maintaining an inspiring environment. We dream up and build environments that create a sense of belonging. Together our team challenges the status quo to reimagine everyday experiences in Salesforce campuses and towers across the globe.

We are seeking an Analytics Engineer to join our team to help us manage and analyze real estate data including building occupancy, work orders, project management, and transactions. The ideal candidate will have a strong background in data engineering and analysis, and be familiar with dbt cloud and Snowflake.

Responsibilities:
Develop, maintain and optimize dbt data pipelines and data models to support real estate analytics
Work collaboratively with analysts to build the datasets required for our Tableau analytics
Work closely with cross-functional teams to identify business requirements and provide datasets
Ensure data accuracy, integrity, and availability by implementing appropriate data quality checks and monitoring processes
Continuously explore new technologies and data tools to improve analytics engineering processes and data analysis capabilities
Collaborate with data scientists and analysts to build predictive models and algorithms
Perform data extraction, transformation, and loading (ETL) processes on various data sources
Qualifications:
Minimum of 3 years of experience in data engineering, data analytics, or related roles
Strong experience with dbt cloud and Snowflake (or similar cloud warehouse)
Strong experience in SQL and data modeling
Degree or equivalent relevant experience required. Experience will be evaluated based on the core competencies for the role (e.g. extracurricular leadership roles, military experience, volunteer roles, work experience, etc.)
Strong communication skills and the ability to work collaboratively with cross-functional teams
Strong analytical and problem-solving skills
Experience with ETL processes, and data warehousing concepts
Knowledge of Python or other scripting languages
Experience working with real estate data including building occupancy, work orders, project management, and transactions
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form .
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at Salesforce and explore our benefits.
Salesforce, Inc . and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce, Inc . and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce, Inc . and Salesforce.org will not pay any third-party agency or company that does not have a signed agreement with Salesforce, Inc . or Salesforce.org .
Salesforce welcomes all."
491,Data Engineer,Wingsoft Consulting LLC,Remote,"$115,875 - $207,192 a year","Roles & Responsibilities:
Remotely Will Work with Clients To Develop the new components for data pipelines and data collection software.
Drive operational efficiency by maintaining their data ecosystems and providing As-a-Service offerings for continuous data collection improvements
Implement large-scale data ecosystems including data management, governance, and the integration of structured and unstructured data
Report to client on daily basis. Work you’ll do
Localize an in-house software to adapt the French speaking environment.
Implement at-scale data collection pipeline, include but not limit to developing the pipeline of data from raw to curation layers, such as cleansing, transformation, derivation, and aggregation of data.
Support in the development of technical solutions to business problems, such as design, develop new databases, leverage the dev-ops platforms such as Kubernetes, etc.
Communicate effectively (written and spoken) and work with the multi-location development teams and self-manage own work.
Key Qualifications & Skills required
Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
Fluent in French and English
5+years of hands-on experience as a Software Developer or Data Engineer
5+ years of experience in Python and SQL, foundational to advanced knowledge including relational database experience
3+ years of experience in Spark / Unix Shell Scripting, including performance tuning, working with Data frames, and code optimization
Experience defining, analyzing, and documenting functional and technical requirements
Ability to communicate effectively (written and spoken) and work with the multi-location development teams and self-manage own work
Strong customer facing skills with the ability to articulate business value and communicate features of the end-product Additional Requirements
Complex software development, test, maintenance experience is a big plus.
Hands on big data/ Hadoop performance tuning and optimization experience
Developed or applied Dev-Ops platform experience
If you are interested in this Data Engineer job in Cupertino, CA/ Remote, then please click APPLY NOW. For other opportunities available at Akkodis go to www.akkodis.com. If you have questions about the position, please contact Hema Nayak at hema.nayak@Akkodisgroup.com
Equal Opportunity Employer/Veterans/Disabled
Benefit offerings include medical, dental, vision, term life insurance, short-term disability insurance, additional voluntary benefits, commuter benefits and 401K plan. Our program provides employees the flexibility to choose the type of coverage that meets their individual needs. Available paid leave may include Paid Sick Leave, where required by law; any other paid leave required by Federal, State or local law; and Holiday pay upon meeting eligibility criteria. Disclaimer: These benefit offerings do not apply to client-recruited jobs and jobs which are direct hire to a client
To read our Candidate Privacy Information Statement, which explains how we will use your information, please visit https://www.modis.com/en-us/candidate-privacy/
The Company will consider qualified applicants with arrest and conviction records.
Job Type: Contract
Salary: $115,874.56 - $207,192.05 per year
Schedule:
8 hour shift
Application Question(s):
Dear Applicant It's a W2 Position . So are you agree on W2 ?
Are you Fluent in French ?
Work Location: Remote
Speak with the employer
+91 (609) 669-5240"
492,Data Engineer/Analyst,Labcorp,"Durham, NC 27709",N,"Labcorp is recruiting a Data Engineer/Analyst for a dynamic team in either Burlington or RTP, NC.
Get ready to redefine what’s possible and discover your extraordinary potential at Labcorp. Here, you’ll have the opportunity to personally advance healthcare and make a difference in peoples’ lives with your bold ideas and unique point of view. With the support of exceptional people from across the globe and an energized purpose, you’ll be empowered to own your career journey with mentoring, training and personalized development planning.

At Labcorp we believe in the power of science to change lives. We are a leading global life sciences company that delivers answers for crucial health questions because we know that knowledge has the potential to make life better for all. Through our unparalleled diagnostics and drug development capabilities, we provide insights and accelerate discoveries that not only empower patients and providers but help medical, biotech and pharmaceutical companies transform ideas into innovations.
Overview:
Reviews, evaluates, and maintains data for computer processing. Analyzes data for system performance and functionality. Works directly with users to resolve data conflicts. Recommends methods, tools, or software to maximize performance.
Skill Requirements
Position Responsibilities And Expected Outcomes:
Work on complex data initiatives with broad impact and act as key participant in large scale software planning for the Technology area
Perform data analysis and modelling tasks within a data warehouse environment.
Perform SQL queries to analyze and troubleshoot issues.
Discover problems in data and applications and design solutions with engineering and product leadership.
Strategically collaborate and consult with internal partners to resolve highly risky data engineering challenges
Demonstrated ability to solve complex data engineering problems; end to end problem resolution and continuous improvement mindset.
In-depth knowledge and experience with Data Engineering essentials such as scripting languages, source control, workflow scheduling, relational and non-relational SQL, and development of complex data solutions
Required Qualifications:
2+ years of Data Engineering experience, or equivalent demonstrated
2+ years of relevant Python development experience
Experience with Databricks , Spark, Hive, AWS EMR/S3, Data Stage or similar systems for performance.
Experience with AWS technologies like Lambda, S3
Familiarity with modern build pipelines, tools, CI/CD concepts
Experience in data modelling within a data warehouse environment
Desired qualifications:
ETL (Software agnostic) Experience
Databricks, Hive, Datastage and Oracle SQL Experience
Experience in query and/or SQL tuning
CI/CD Tool Experience
Experience working in an Agile Team
Understanding of SDLC Requirements
Knowledge on mainframe
License/Certification/Education:
Normally requires a B.S. Degree in Computer Science w/1-3 years of relevant experience.
Labcorp is proud to be an Equal Opportunity Employer:
As an EOE/AA employer, Labcorp strives for diversity and inclusion in the workforce and does not tolerate harassment or discrimination of any kind. We make employment decisions based on the needs of our business and the qualifications of the individual and do not discriminate based upon race, religion, color, national origin, gender (including pregnancy or other medical conditions/needs), family or parental status, marital, civil union or domestic partnership status, sexual orientation, gender identity, gender expression, personal appearance, age, veteran status, disability, genetic information, or any other legally protected characteristic. We encourage all to apply.
For more information about how we collect and store your personal data, please see our Privacy Statement."
493,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
494,Senior Data Engineer [ Remote ],"FFF Enterprises, Inc.",Remote,"$130,000 - $165,000 a year","Senior Data Engineer
Location:
[Remote]
Salary Range:
$130,000-$165,000 / Annual Salary
Position Summary:
The senior data Engineer will work in a highly motivated team environment and with all the best practices, worth ethics, and processes followed to execute various projects. The data engineer will be responsible for collecting, managing, and converting raw data into information that can be interpreted by data scientists, business intelligence developers, and business analysts. The goal of the senior data engineer is to help the organization to utilize data for performance evaluation and optimization.
Essential Functions and Duties:
Provide project and overall day to day support to the organization.
Supporting a high performing agile delivery team consisting of onsite and offshore members.
Use effective communication and presentation skills to communicate concepts, facilitate conflict resolution and recommended solutions.
Monitors and organizes the efforts of technical and business support staff.
Manages projects according to milestones and completes tasks assigned by more experienced analysts and managers. Guides the efforts of less experienced team members.
Developing data warehousing applications, familiarity with ETL architectures, leveraging multiple sources, (including SAP S4HANA Database and Non-SAP Databases)
Demonstrate strong knowledge of technology developments within healthcare domain.
Champion continual quality improvement consistent with company's core values.
Adheres specifically to all company policies and procedures, Federal and State regulations and laws.
Display dedication to position responsibilities and achieve assigned goals and objectives.
Represent the Company in a professional manner and appearance at all times.
Understand and internalize the Company's purpose; Display loyalty to the Company and its organizational values.
Display enthusiasm and dedication to learning how to be more effective on the job and share knowledge with others.
Work effectively with co-workers, internal and external customers and others by sharing ideas in a constructive and positive manner; listen to and objectively consider ideas and suggestions from others; keep commitments; keep others informed of work progress, timetables, and issues; address problems and issues constructively to find mutually acceptable and practical business solutions; address others by name, title, or other respectful identifier, and; respect the diversity of our work force in actions, words, and deeds.
Comply with the policies and procedures stated in the Injury and Illness Prevention Program by always working in a safe manner and immediately reporting any injury, safety hazard, or program violation.
Ensure conduct is consistent with all Compliance Program Policies and procedures when engaging in any activity on behalf of the company. Immediately report any concerns or violations.
Other duties as assigned.
Education, Knowledge, Skills and Experience:
Required Education:
Bachelor's Degree in or a related field of study.
Required Knowledge:
Experience working in Healthcare Life Sciences.
Informatica IICS experience.
Excellent SQL skills.
Google Cloud Platform Foundation (BigQuery/SQL experience).
Required Experience:
Must have at least three (3) years' experience as an Informatica Intelligent Cloud Services (IICS) Developer.
Data warehouse experience (At least 5 years).
SQL experience (At least 5 years).
Google BigQuery (Preferred).
Required Skills:
Forward thinking, independent, creative, and self-sufficient who can work with less documentation, has exposure working in complex multi-tiered integrated applications.
Strong SQL experience with the ability to develop, tune and debug complex SQL applications.
Experience with Google Cloud Platform, BigQuery and Informatica Intelligent Cloud Services (IICS).
Ability to interact well with user base, development teams and other technical team members.
Experience with database design, querying, stored procedures, views, joins, performance tuning etc.
Experience with load optimization for Informatica batch processes.
Ability to diagnose and resolve issues/problems in aggressive timeframes.
Strong analytical, problem-identification/solving skills, as well as strong written and oral communication skills.
Experience in querying and analyzing data coming from multiple sources and determine logic for transformations.
Ability to interact well with user base, development teams and other technical teams.
Providing production support for daily, weekly, and monthly production processes.
Provides data modeling where needed to enable data movement or improve data architecture.
Demonstrates advanced analytical and problem-solving skills in resolving complex discrepancies, by actively suggesting and assisting in implementing process improvements and procedures.
Excellent verbal and written communication skills and the ability to interact effectively with end users, co-workers, and management.
Ability to exercise discretion and maintain confidentiality to the level of required HIPAA standards.
Intermediate to advanced Microsoft Office skills.
Physical Requirements:
Vision, hearing, speech, movements requiring the use of wrists, hands and/or fingers. Must have the ability to view a computer screen for long periods and the ability to sit for extended periods. Must have the ability to work the hours and days required to complete the essential functions of the position, as scheduled. The employee occasionally lifts up to 20 pounds and occasionally kneels and bends. Working condition include normal office setting.
Mental Demands:
Learning, thinking, concentration and the ability to work under pressure, particularly during busy times. Must be able to pay close attention to detail and be able to work as a member of a team to ensure excellent customer service. Must have the ability to interact effectively with co-workers and customers, and exercise self-control and diplomacy in customer and employee relations' situations. Must have the ability to exercise discretion as well as appropriate judgments when necessary. Must be proactive in finding solutions.
Direct Reports:
Project resources (in-house).
EEO/AAP Statement:
If you are applying for a job and would like to make a request for a reasonable accommodation during any part of the employment process, submit an email to Human Resources at mmiller@fffenterprises.com or call (951) 296-2500 extension 1391. Please include your contact information along with the specifics of your request for a reasonable accommodation. Only inquiries regarding a reasonable accommodation request will receive a response via email or phone in a timely manner.
FFF Enterprises is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law.
Read about our excellent employee benefits package."
495,Data Engineer,N,"Charlotte, NC 28208",N,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language."
496,"VP, Market Data Engineer",Barclays,"745 7th Ave, New York, NY 10019","$210,000 a year","VP, Market Data Engineer
New York, New York
Barclays Services Corp.
What will you be doing?
On behalf of a global financial services organization, develop, create, and modify general computer applications software and specialized utility programs.
Analyze user needs and develop software solutions.
Design software with the aim of optimizing operational efficiency.
Develop monitoring and automation processes to manage real time market data feed handler estate.
Support a global work stream within specialized infrastructure to standardize automation across various technology disciplines including market data, networks, Unix, and middleware.
Assist in the integration of automation processes with monitoring frameworks including Elastic Logstash Kibana.
Review and monitor the health and quality of real-time market data environment.
Perform diagnostic work and joins major incident bridges as needed to investigate and remediate critical production issues affecting market data.
Develop and design Market Data products and support procedures where required.
Communicate with stakeholders to resolve any market data production issues and troubleshoots accordingly.
Undertake key business requests around value and applicability of market data cost saves and reducing of production.
Ensure that all activities and duties are carried out in full compliance with regulatory requirements, internal Barclays Policies, and Policy Standards.
What we’re looking for:
Minimum of Bachelor’s degree, or foreign equivalent, in Computer Science, Computer Engineering, Finance, Business Administration or related field and at least five (5) years of post-baccalaureate progressively responsible experience in a Market Data related occupation within the financial services industry.
Must have at least five (5) years of progressively responsible employment experience with each of the following required skills:
Programming and developing tools including BASH (Born Again Shell) and Python;
Subject Matter Expertise in Market Data concepts;
Knowledge of Configuration Management Tools, such as Ansible and Chef;
Experience operating various Linux command line utilities including Global Regular Expression Print (GREP), AWK, SORT, LS and PS;
Version Control Tools including Global Information Tracker (GIT) and STASH;
Computer Networks including Wireshark, Transmission Control Protocol Dump (TCPDump) and Transmission Protocol/ Internet Protocol Stack (TCP/IP Stack);
Open-source IT monitoring tools, including Elastic, Logstash, and Kibana (ELK) as well as SNMP (Simple Network Management Protocol) and syslog; and
Linux Operating Systems Fundamentals. Experience working with the Red Hat Enterprise Linux or CentOS distributions.
Where will you be working?
You will be working at our New York, New York location at 745 Seventh Avenue. This 37-story office tower is located in Times Square in the heart of Manhattan and features a cafeteria, fitness center and state-of-the-art LED signage on the façade of the building. The building is easily accessible to Restaurants, Shops and Public Transportation.
Interested and want to know more about Barclays? Visit home.barclays/who-we-are/ for more details.
Our Values
Everything we do is shaped by the five values of Respect, Integrity, Service, Excellence and Stewardship. Our values inform the foundations of our relationships with customers and clients, but they also shape how we measure and reward the performance of our colleagues. Simply put, success is not just about what you achieve, but about how you achieve it.
Our Diversity
We aim to foster a culture where individuals of all backgrounds feel confident in bringing their whole selves to work, feel included and their talents are nurtured, empowering them to contribute fully to our vision and goals.
It is the policy of Barclays to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, creed, religion, national origin, alienage or citizenship status, age, sex, sexual orientation, gender identity or expression, marital or domestic/civil partnership status, disability, veteran status, genetic information, or any other basis protected by law.
Dynamic working gives everyone at Barclays the opportunity to integrate professional and personal lives, if you have a need for flexibility then please discuss this with the hiring manager.
Our Benefits
Our customers are unique. The same goes for our colleagues. That's why at Barclays we offer a range of benefits, allowing every colleague to choose the best options for their personal circumstances. These include a competitive salary and pension, health care and all the tools, technology and support to help you become the very best you can be. We are proud of our dynamic working options for colleagues. If you have a need for flexibility then please discuss this with us.
Salary/Rate Minimum/yr: $210,000
Salary/Rate Maximum/yr: $210,000
The minimum and maximum salary/rate information above include only base salary or base hourly rate. It does not include any other type of compensation or benefits that may be available.
This position is eligible for incentives pursuant to Barclays Employee Referral Program."
497,Data Engineer,ZLLIUS INC.,"ZLLIUS INC. in Rothbury, MI 49452",N,"Hi,
Greetings from Zllius Inc!
We have an urgent opening for the role Data Engineer. If interested, revert me back with your Updated resume, along with contact details, work authorization.
Job Role: Data Engineer
Duration: Long-Term
Position Type: W2/ 1099
Visa: Canadian Citizen / TN
Job Description:
Technical/Functional Skills
Successful candidates will have:
Hands on experience in Dremio and Scripting
Hands on Scripting experience in Python and Flask
Hands on experience in Integrating data from multiple source systems (PostgreSQL and CSV)
Experience with Apache Superset
Ability to write complex SQL queries
Experience in building data pipelines using PostgreSQL, Oracle database, Dremio, AWS Open Search
Experience with Docker Containerization of Code
Work as a part of a scrum team in Agile methodology
ETL, Data Pipelines, PostgreSQL, SQL Queries, Scripting
Thanks & Regards
Job Type: Full-time
Schedule:
8 hour shift
Monday to Friday
Work Location: One location"
498,Senior Software Engineer - Data and API,Rehrig Pacific Company,"Richardson, TX 75081",N,"Here at Rehrig Pacific, we are all about our people. Since 1913, our organization has focused on sustainable supply chain solutions while creating a culture and atmosphere where amazing people, like you, are celebrated for doing their best work. Rehrig Pacific has grown to meet the needs of our industry consumers across the country and internationally. We are constantly creating innovative solutions to transcend the new standards set forth by our customers. We find true fulfillment in helping others, both within the Rehrig Pacific family and in our communities. As servant leaders, we lead by example.
Brief Role Description
The Senior Software Engineer with a specialty in data and APIs will be accountable for helping our customers integrate with our enterprise products and applications. The person will collaborate with both our client’s and Rehrig’s engineers to integrate data between systems. The desired skills of the senior software engineer involve high levels of understanding around database architecture, SQL queries, ETL, authentication, authorization, REST, and python. This position reports to the Technology Engineering Director in the New Product Development group.

Accountabilities
Ability to work closely with technical architects and our client’s architecture governance technical team for solution development and design reviews
Manages API lifecycle and release management
Manages data integrity
Manages how we do reporting
Manages the database architecture
Creates and maintains effective documentation on solutions: The Engineer will create documentation on the data flow, architecture, and daily operation and troubleshooting for all assigned solutions and maintain that documentation in an ongoing fashion
Work closely with Rehrig’s Product Owners, and must understand the short and medium-term strategy for the business
Function as an escalation point for technical support and troubleshooting: The Engineer will help troubleshoot API/ETL issues and problem resolution

General Responsibilities
Provide timely updates to management on open issues and projects
Ensure availability and security requirements are met consistently
Other duties as assigned

Qualifications
Bachelor’s Degree in Computer Science, Management Information Systems or a related field or equivalent in work experience.
Minimum of 5 years of software engineering experience
Experience with REST based APIs
Knowledge of HTTP and SOAP Protocols
Experience with source code management tool (git)
Demonstrated subject matter expertise in all of the following: Python, Agile Development, SQL Queries, ETL
Experience working cross functionally in disparate geographies required
Ability to travel approximately 10% of the time
Rehrig Pacific Company is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also here."
499,"Data Engineer, Power BI",Leading Edge Administrators LLC,United States,"$93,000 - $139,000 a year","Description:
This position is hybrid, working in Manhattan or Brooklyn, NY or Tampa or Boca Raton, FL.
Job Summary:
The ETL (extract, transform, load) Power BI Data Engineer will design, develop and support new and existing BI solutions for users and client reporting. The primary role is to transform data into meaningful and accurate information which the business can easily consume.
Duties and Responsibilities
Support implementation and ongoing maintenance, including data mapping, data gap analysis and remediation, configuration and changes as a result of data source updates.
Identify data requirement and data quality issues from data providers, account services teams and end-users.
Create robust reporting and issue tracking, such as data quality scorecards and dashboards based on key metrics and indicators to monitor progress.
Analyze requirements at sufficient level of detail to allow ETL solution to be developed.
Develop ETL job flows according to company standards for naming, performance, reliability.
Support testing and remediation of defects in newly-developed/modified workflows.
Create Power BI Datasets to support the Analytic Delivery team.
Stay current with industry trends and emerging technologies in data exploration and BI tools, and recommend tools and methodologies for data management, governance and PHI security.
Collaborate with the Data Architecture team to understand and implement load processes for reporting and analytic data structures (data warehouses, data marts and data lakes).
Tune and troubleshoot processes under development and in production; monitor data integration jobs and correct failures.
Work with Data Architects to augment ERD’s as changes are developed.
Develop, maintain, and extend reusable data components.
Provide timely project and task updates.
Monitor production data integration jobs and correct failures create and manage incident reports as they pertain to data integration processes.
Reverse engineer existing reports and rebuild using Power BI.
Write SQL queries for data extraction.
Execute unit tests, validate to expected results and ensure data quality and accuracy.
Coordinate and perform unit, system and user acceptance testing as well as code deployment / configuration changes to environments.
Follow change management stipulations.
Requirements:
Required Knowledge, Skills, and Abilities:
Bachelor’s degree in computer science, engineering or equivalent work experience
2+ years Azure exposure (Any Resources: databases, data factory, Synapse Studio, Storage Account, Power Platform) and ANSI SQL
1+ year data modeling
Preferred:
Experience with Azure connectivity/authentication (service principals, managed identities, certificates;
Power BI Dataset creation/maintenance;
Azure resources: DevOps, Logic Apps, Gen 2 Storace, Purview;
SQL Server, PostGre SQL
Solid working knowledge of standard computer applications including MS Word, Excel, Outlook and PowerPoint
Ability using a computer which includes expert keyboard and navigation skills and learning new programs
Communicate clearly and professionally with internal and external customers
Work effectively as part of a team to achieve established outcomes. Understand other’s roles and empower one another to take responsibility to be successful. Demonstrate a collaborative interaction with peers to reach a common goal.
Demonstrate a collaborative interaction with peers to reach a common goal as well as be a resource to team members and internal/external customers
Pay close attention to detail in all aspects of the job
Make decisions using available resources and sound judgment
Maintain confidentiality and discretion
Identify and resolve problems in a timely manner, gather and analyzes information skillfully
Share knowledge with associates by effectively communicating and providing follow-up
Open to other’s ideas and exhibits a willingness to try new things.
Demonstrate accuracy and thoroughness; monitor work to ensure quality.
Prioritize and plan work activities to use time efficiently.
Adapt to changes in the work environment, manage competing demands and is able to deal with frequent change, delays, or unexpected events.
Follows instructions, responds to direction, and solicits feedback to improve.
Act in such a way to instill trust from management, other associates, as well as customers.
Physical Demands - The physical demands described here are representative of those necessary for an employee to successfully perform the essential functions of this job. Reasonable accommodation can be made to enable individuals with disabilities to perform the essential functions.
While performing the duties of this job, the employee is required to talk and hear. The employee will have prolonged periods of sitting at a desk and working on a computer. The employee may be required to lift/carry objects of up to 20 pounds. The employee is also required to have better than average dexterity to use hands to, handle, or feel; and reach with hands and arms. Specific vision abilities required by this job include close vision, distance vision, color vision, depth perception, and ability to adjust focus.
Work Environment - The work environment described is representative of what must be met by an employee successfully perform the essential functions of this job.
The physical environment is indoors in a controlled climate, office setting. The noise level may be low to moderate.
The duties described are representative, but not restrictive of tasks that may be assigned or of the abilities required to do the job. The description is subject to change at any time. Other related duties may be assigned. This description does not alter the at-will status of employment."
500,171: Data Engineers (Sr & Jr),Dataspace,Massachusetts•Remote,"Up to $120,000 a year","Our client is a new startup in the domain of telecommunications, building and maintaining digital products that match customers with the right phones/service providers for their individual needs. They have asked us to help them find two Data Engineers to take the lead on optimizing the company-wide data environment - keeping both operations and analytics in mind.

Does this sound like you? If so, please submit your resume and fill out our questionnaire ASAP!

THE WORK: The work includes building and maintaining the data pipelines that feed the company’s digital products, as well as participating in the design and development of the company-wide data environment. The chosen candidates will work directly with the client-facing analytics teams in order to ensure that they have the data they need.

LOCATION: This client operates in a fully remote capacity. Candidates are welcome to work from anywhere in the United States.

MODE: These are permanent, full-time roles.

REQUIRED: Applicants must be very strong in the following skills:
SQL
Python
ETL data pipeline building/maintenance
Experience with automation and workflow management tools (Airflow, Watchdog, Jenkins)
Experience deploying applications in a cloud environment

DESIRED: In addition, it would be great if you had some background in:
CI/CD
Working with an analytic database (Bigquery, Spark SQL, Athena, etc.)
The telecommunications industry

COMPENSATION: This client is willing to consider candidates with a range of experience, and is looking to hire at both the Senior and Junior levels. Both roles offer equity and health insurance, in addition to the following compensation:
Senior: $160K+/yr
Junior: up to $120K/yr, depending on experience level

WHY THIS JOB IS SPECIAL: This role offers the opportunity to make a huge impact within a growing company, and help them expand their tech platform in novel ways!

INTERESTED? If you're interested and have the skills, we'd love to hear from you. Please answer our questionnaire and submit your resume right away! Thanks!Why data science job seekers work with Dataspace

NOTE: Dataspace performs background and drug screens on accepted candidates prior to their employment or contract start dates.

U.S. Citizens and all those authorized to work for any employer in the U.S. are encouraged to apply. We are unable to provide sponsorship at this time."
501,Software Engineer - Data Quality Cloud,Collibra,"New York, NY•Remote","$116,000 - $174,000 a year","Joining Collibra's Engineering - Data Quality team
We are looking for an experienced Engineer who will be working on evolving our new Data Quality Product alongside leading minds in Data Quality detection and monitoring. This role will focus on building the infrastructure to support our data quality offering in the Cloud. As one of the lead engineers on this project, you'll be able to flex your skills with Java, Spring framework, and containerization while having a heavy focus on overseeing that the designs and implementations are pragmatic and high quality. Collibra is building some new-age tech, and this role will be a key component of that exciting growth.
DQ Cloud Engineers at Collibra are responsible for:
Developing high quality code, including participating in testing and maintenance of that code
Collaborating with Product Management, Design, Architecture, and other Engineers to help estimate work, design scalable and performant solutions that will delight our customers, and produce high quality production-grade software
Work on bleeding edge computing practices, including Infrastructure as code, Edge Computing and Observability
Work over a containerized , multi-cloud, multi-region environment
Working closely with Data Science and Backend Engineering teams to ensure quality of product
Being seen as leading engineer who focuses and aligns on the team objectives
Play an active role in designing and building new, cutting-edge Data Quality management product features
You Have:
A Bachelor's degree or equivalent work experience
A great understanding of cloud-native engineering, infrastructure as code, and cloud computing platforms
At least 4 years of software engineering experience with some combination of the following: Java w/Spring, Docker, Kubernetes, Python, Postgres, Spark DataFrame, PySpark, and/or Database Modeling.
Experience with SQL and understanding of Snowflake, BigQuery and other similar Cloud databases processing big datasets in the cloud
Knowledge of enterprise level software architecture and infrastructure such as multi-tenant DB
Experience in designing relational database models and exposing the data to frontend as REST APIs
Experience with working with and safely modernizing existing, complex software
You Are:
Agile-minded, optimistic, passionate, and pragmatic about delivering valuable software to customers
Passionate about creating and finishing new things while exploring new problems
Obsessive about data and how it impacts customer's business processes
Able to work productively and independently with a geographically distributed remote team and to be able to mentor and advise other engineers you work with on the scrum team
Accustomed to a fast paced environment and able to identify and suggest solutions for gaps in software development process
Measures of success are:
Within your first month, you will get familiar with the environment we use, the way we implement software as an organization, simple business use cases and submit code to production
Within your third month, you will implement new user stories with quality and care, and understand what value they will bring to our customers. You will start providing input on how best to move the product forward and improve the product
Within your sixth month, you will be a lead engineer of the Data Quality team, regularly delivering quality code, participating in testing, and ensuring good, scalable, pragmatic engineering practices are followed by the team
Compensation for This Role
The standard base salary range for this position is $116,000 - $174,000 per year. This position is not eligible for additional commission-based compensation. Salary offers are based on a combination of factors, including, but not limited to, experience, skills, and location.
In addition to base salary, we offer equity ownership at every level, bonus potential, a Flex Fund monthly stipend, pension/401k plans, and more.
Benefits at Collibra
Collibra recognizes and values that everyone has different needs, interests, and life goals. We built our {Be}well benefits program with flexibility in mind to support you and your loved ones through a diverse range of circumstances and life events. These flexible offerings sit on a foundation of competitive compensation, health coverage, and time off.
Professional Development
Collibrians are ambitious and inventive, and we want to develop our skills individually and as a team. You'll have access to LinkedIn Learning and other development opportunities, as well as other rewards and recognition programs to help grow your career.
Health Coverage
We strive to remain locally competitive and globally equitable. This means comprehensive offerings including medical, dental, vision, and mental health benefits for you and your family.
Paid Time Off and Flexibility
We provide unlimited paid time off, global leave policies for a variety of personal and family circumstances, company-wide wellness days off throughout the year, meeting-free Wednesdays, and a flexible culture to help balance your work and your life.
Diversity, Equity, and Inclusion
We create inclusion and belonging through how we onboard, meet, connect, engage, and communicate. Learn more about diversity, equity, and inclusion at Collibra.
Learn more about Collibra's benefits.
At Collibra, we're proud to be an equal opportunity employer – which ties directly to our core value, ""open, direct, and kind."" We realize the key to creating a company with a world-class culture and employee experience comes from who we hire and creating a workplace that celebrates everyone.
With this, we proudly consider qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, sexual orientation, pregnancy, sex, gender identity, gender expression, genetic information, physical or mental disability, HIV status, registered domestic partner status, caregiver status, marital status, veteran or military status, citizenship status or any other legally protected category.
#LI-JW2"
502,Software Engineer - Data Platform - Remote,Splunk,"San Francisco, CA 94107•Remote","$136,000 - $187,000 a year","A little about us. Splunk is here to build a safer and more resilient digital world. We’re proud to say that we’re the key to enterprise resilience for more than 11,000 enterprise organizations that use our Unified Security and Observability Platform to keep their systems secure and reliable. We’re also especially proud of our award-winning culture and our regular appearance on those “Best Places to Work” lists.
If you end up joining us, we’ll only ask you one thing: bring your whole, authentic self, what we call your million data points. So bring your work experience, problem-solving skills and talent, of course, but also bring your joy, your passion and all the things that make you, you. Because when you feel free to be you, it makes it a lot easier for us to be us.
Role:
Are you interested in being part of building the next-generation and highly-scaled industry-leading data platform for machine data? The Splunk Data Platform Search Execution team is looking for a Software Engineer to join our backend distributed systems engineering team.
You will be working on the core of Splunk's Search technology and be part of the team to solve the most challenging and exciting problems in the Search backend area to delight our customers with high-performance, reliable, and efficient solutions.
We give our engineers an environment in which they can contribute from day one while also providing learning and growth opportunities. You'll learn how our entire stack works, from data ingestion and storage to searching, reporting, and building dashboards, all in distributed environments. The work you’ll do will directly impact the experience of our customers.
Responsibilities:
Design, develop, and maintain features for Splunk search infrastructure.
Build robust, fault-tolerant distributed systems in a multi-threaded/multi-process environment.
Analyze and improve the scalability of data collection, routing, storage, and retrieval.
Define and perform various search language layer optimizations/transformations.
Requirements:
5 years of related experience with a technical Bachelor’s degree; or equivalent practical experience; or 3 years and a technical Master’s degree; or equivalent practical experience.
Master knowledge of developing and debugging any object-oriented language like C++.
You have knowledge of backend systems, storage, filesystem, memory, and multithreading.
You have familiarity with any query language and processing like SQL, SPL, etc.
You have a proven foundation in operating systems, data structures, algorithms, and software design.
You have knowledge of modern distributed system design and implementation in the Unix/Linux environment.
Passion for solving hard problems and exploring new technologies.
You have knowledge and experience with AWS services, like EC2, S3, etc.
What We Offer You
A constant stream of new things for you to learn. We're always expanding into new areas, bringing in open source projects and contributing back, and exploring new technologies.
A set of exceptionally talented and dedicated peers, all the way from engineering and QA to product management and customer support.
A stable, collaborative and encouraging work environment.
We don't expect people to work 12-hour days. We want you to have a successful time outside of work too. Want to work from home sometimes? No problem. We trust our Colleagues to be responsible with their time and dedication, and believe that balance helps cultivate an outstanding environment.
We value diversity at our company. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or any other applicable legally protected characteristics in the location in which the candidate is applying.
For job positions in San Francisco, CA, and other locations where required, we will consider employment qualified applicants with arrest and conviction records.

Note: Splunk provides flexibility and choice in the working arrangement for most roles, including remote and/or in-office roles. We have a market-based pay structure which varies by location. Please note that the base pay range is a guideline and for candidates who receive an offer, the base pay will vary based on factors such as work location as set out below, as well as the knowledge, skills and experience of the candidate. In addition to base pay, this role is eligible for incentive compensation and benefits, and may be eligible for equity or long-term cash awards.
Benefits are an important part of Splunk's Total Rewards package. This role is eligible for a competitive benefits package which includes medical, dental, vision, a 401(k) plan and match, paid time off, an ESPP and much more! Learn more about our comprehensive benefits and wellbeing offering here.
Base Pay Range
SF Bay Area, Seattle Metro, and New York City Metro Area
Base Pay Range: $136,000.00 - 187,000.00 per year
California (excludes SF Bay Area), Washington (excludes Seattle Metro), Washington DC Metro, and Massachusetts
Base Pay Range: $124,000.00 - 170,500.00 per year
All other cities and states excluding California, Washington, Massachusetts, New York City Metro Area and Washington DC Metro Area.
Base Pay Range: $116,000.00 - 159,500.00 per year"
503,Data Engineer,Montecito Bank & Trust,"Goleta, CA 93117","$93,047 - $139,570 a year","Are you a talented professional who excels at personalized service, enjoys getting involved in your communities and has a desire to work for an award-winning Best Bank?

Montecito Bank & Trust is seeking a full-time Data Engineer to join our Technology department. The Data Engineer will deliver a World Class Experience by developing and maintaining solutions to transfer, ingest, transform, blend, model and store bank data, as well as any applications using or processing that data. The Data Engineer will apply broad technical knowledge of technology and industry trends to develop, implement and maintain high risk/critical data solutions in high complexity environments.

Minimum Requirements, Activities & Responsibilities
Education and Experience:
Bachelor's degree or equivalent work experience with an emphasis in data management or data science
Three to five years of experience as a data engineer ingesting, transforming, scheduling and cleaning of data in a data warehouse required
Minimum of two years experience in data visualization and data analysis required
Experience in data architecture
Experience in prepping and modeling data
Experience working with version control, adhering to SDLC, and implementing Data Governance best practices
Essential Functions:
Design and develop ETL Processes that will transform a variety of raw data, flat files, Excel spreadsheets into SQL Databases
Develop and document systems, processes and logic required to expose existing data sets to end users for reporting and analysis purposes
Provide database support for development and deployment of database scripts in Development, Test, Pre-Production and Production environments
Collaborate with lines of business and other departments to understand data reporting needs and maximize use of reporting tools
Troubleshoot and resolve database and reporting issues
Communicate requested changes to reported data fields with all affected departments to ensure concurrence with changes prior to implementation
Responsible for ensuring our warehouse and business intelligence tool has the most current, accurate and relevant data possible for our internal customers and determines and recommends enhancements for performance improvement
Create process/data flow maps of links between systems, current processes, areas of opportunity to improve process flows and new systems to integrate
Assist in maintaining documentation for internal procedures and inventory
Skills and Capabilities:
Strong skills in MS SQL Server
Strong skills in SQL queries, stored procedures, performance tuning, and error catching
Strong skills in scheduling and orchestration of data ingestion and movement
Strong skills in transforming and blending data into optimized schemas
Moderate skills in data visualization and reporting
Moderate skills working with data warehouses, data marts, data lakes and other repositories of data
Familiarity of machine learning, algorithms, python, data streaming, nosql, distributed sql, in-memory databases, multi-dimensional data, tabular data, unstructured data and cloud databases
Ability to communicate effectively, verbally and in writing, with all levels of staff and vendors
Ability to solve problems and work on multiple projects in a fast-paced, deadline-driven environment
Ability to initiate, evaluate, and influence projects from the perspective of the entire organization
Able to demonstrate highest levels of service and confidentiality with internal and external customers
Strong organizational, problem-solving, and time management skills
Must be able to perform this position safely, without endangering the health or safety to himself or herself or others
Interested in applying? Visit our website at www.montecito.bank/careers
Review opening using the “Apply Now” tab on our Careers web page
Once you've selected desired position, click “Apply"" on the top right-hand corner of the web page
Create a personal log-in to submit your expression of interest
A member of our Human Resources department will reach out to you to let you know whether or not you have been selected to move forward in the interview process"
504,Data Engineer,"Perficient, Inc","22 Cortlandt St, New York, NY 10007",N,"We currently have a career opportunity for a Data Engineer to join our Financial Services team. This role is located in Pittsburg, PA.
As a Data Engineer you will participate in all aspects of the software development lifecycle which includes estimating, technical design, implementation, documentation, testing, deployment and support of application developed for our clients. As a member working in a team environment you will take direction from Solution Architects and Leads on development activities.
Perficient is always looking for the best and brightest talent and we need you! We’re a quickly-growing, global digital consulting leader, and we’re transforming the world’s largest enterprises and biggest brands. You’ll work with the latest technologies, expand your skills, and become a part of our global community of talented, diverse, and knowledgeable colleagues."
505,Data Engineer - LATAM,N,Remote,N,"Who We Are:
Working at The Data Sherpas is like being part of a dynamic and collaborative team of talented individuals passionate about helping clients navigate the ever-changing information technology landscape. At The Data Sherpas, you can work with cutting-edge tools and technologies, tackle challenging data problems, and continuously develop your skills in a supportive environment. As a Data Sherpa, you'll be empowered to lead projects, take ownership of your work, and make meaningful contributions to our client's success.
What We Are Looking For:
We're seeking an ambitious and driven Data Engineer to join our team. As a Data Engineer, you can work on exciting, challenging projects supporting our client's needs. You will be part of a dynamic team of experts dedicated to improving business performance and driving data-driven results.
What You'll Do:
Design and implement data pipelines, ETL processes, and data warehousing solutions
Develop and maintain attribution and measurement models for ad campaigns
Perform data matching and segmentation techniques to create customer profiles and behavior patterns
Configure and maintain AWS infrastructure and services to support data engineering processes
Collaborate with cross-functional teams to identify data needs and develop data-driven solutions.
Stay up-to-date with the latest technologies and industry trends related to Data Engineering and Ad Tech.
What You Have:
Proficiency in data modeling, ETL development, and data warehousing
Ability to design and maintain data pipelines for large-scale data sets
Proven experience with Python programming language for data engineering solutions
Knowledge of data matching techniques for identifying duplicate data and inconsistencies
Experience with data deduplication techniques
Ability to work with large datasets to segment and group data
Experience with clustering algorithms and methodologies
Understanding of customer segmentation and persona development
Familiarity with data visualization tools to showcase segmentation results
Strong experience with data engineering in cloud-based environments, primarily in AWS
Experience using AWS data analytics services like EMR, Redshift, Kinesis, and Glue"
506,Data Engineer,Skyla Credit Union,"Charlotte, NC",N,"Pay Grade: 13
primary functions:
The Data Engineer is expected to design and code Enterprise Data Warehouse (EDW), Data modelling,
integration patterns (Extract transform load (ETL), ELT, etc.). Be responsible for the full life cycle of
development, from requirements gathering through ETL coding and report/dashboard design and
creation. Work with business users to establish reporting and analytic requirements. Translate business requirements into database design, ETL coding and report specifications. Provide technical and business knowledge support to the team. Compile ad-hoc data and report requests.
responsibilities:
1. The Data Engineer is expected to have deep knowledge of the EDW, Data modelling,
integration patterns (ETL, ELT, etc.) and may work with one or a range of tools depending on
project deliverables and team resourcing.
2. The Data Engineer will also be expected to understand traditional relational database systems
and be able to assist in administering these systems.
3. Responsibilities range from batch application\client integration, aggregating data from
multiple sources into a data warehouse, automate integration solution generation using reusable patterns\scripting, prototyping integration solutions, and security.
4. Develop batch integration solutions, including traditional Data Warehouse (DW) workloads
and nightly large extracts that are scheduled.
5. Design and Build Data models – star schema, snowflake dimension modeling.
6. Create pipelines to bring new data from various sources.
7. Create Data transformation.
8. Document all solutions as needed using standard documentation.
9. Plan, review, and perform the implementation of database changes for integrations/DW work.
10. Maintain integration documentation and audit tools (to include developing/updating the
integration dashboard)
11. Provide support for database and associated technology.
12. Work with project management and business analysis team to provide estimates and ensure
documentation of all requirements.
13. Provide logical layers (database views) for end-user access to data in database systems.
14. Build Semantic layer to deliver data across enterprise.
15. Other duties as assigned.
minimum requirements:
1. BA/BS Degree in Computer Science, Information technology, Business or similar.
2. Minimum of 5 years of experience with financial services in data engineering domain.
3. Excellent written and verbal communication skills. Must be able to articulate ideas and plans
clearly to a diverse audience.
4. Excellent knowledge of data design, SQL, and data warehousing.
5. Knowledge of cloud based and on-premises systems and capabilities.
6. Strong knowledge of BI technology & Semantic layer design.
Update: 02/05/2023
7. Preferred Experience: Jack Henry Symitar, Credit Union, Financial Services, and Database
Technologies.
8. Ability to work on a team in order to find solution to complex issues.
9. Ability to work independently under minimal supervision.
10. Able to receive and maintain confidentiality."
507,Senior Data Engineer,Sciata,United States,N,"5+ years of development experience in data architecture, design and engineering with hands-on scripting experience in SQL
5+ years' experience in Business Intelligence, relational data modeling, data analytics and advanced SQL programming, development using Big Query and Teradata in a data warehouse environment. Experience with Google Cloud Platform and Linux a plus
3+ years of professional experience working in big data ecosystems. Google BigQuery preferred.
Experience developing database objects focused on scalability, performance benchmarking, and data consistency guiding business strategy and decisions.
Experience working directly with business and technology partners to understand objectives and articulate the impact of Business Intelligence solutions.
Prior experience working with large data sets (> 100M records)
B.S. preferred in Computer Science, Information Systems, or related fields (foreign education equivalent accepted)"
508,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
509,Senior Data Engineer (Unannounced Project),NCSOFT,"Bellevue, WA•Remote","$112,000 - $165,000 a year","WHO WE ARE
We're ArenaNet—we've created a studio that makes online worlds infused with innovation, hand-crafted excellence, and creative passion. We're not the only ones who share this passion; gamers made Guild Wars 2 the fastest selling MMO game in the West with more than 3 million copies sold in its first 9 months, and players and press have consistently called it one of the best MMOs of all time.
Our studio is built on foundations of excellence with a focus on community. We constantly innovate, unlocking the true potential of online role-playing with game-changing titles like Guild Wars 2 and beyond. The status quo is not good enough for our dedicated worldwide community of players, which is why we question the de facto industry conventions and demand more from ourselves as developers.
As a Senior Data Engineer, you will be responsible for architecting best-in-class data structures for easily consumption and democratization. You will partner closely with fellow ArenaNet studio development teams to produce this high quality and meaningful player data. You will also collaborate closely with Data Scientists and Analysts to build an understanding of the downstream needs for data and the player problems we are solving.
WHAT YOU'LL DO
Building and maintaining the Data Telemetry Brief for Arena.net titles
Designing best-in-class data architecture for ease of consumption
Developing the ETL codebase in robust and self-healing manner
Managing data dependencies and troubleshooting live environment issues
Gathering data requirements from upstream and downstream stakeholders
Deep understanding of our games and how they operate
WHAT YOU'LL NEED TO BE SUCCESSFUL
Experience with programing languages like Python, Scala, Spark, or similar
Experience with task orchestration like Airflow, Jenkins, Databricks Jobs, or similar
Experience with data file types like Avro, Parquet, Protobuf, or similar
Experience with warehouse solutions like Redshift, Delta Lake, Snowflake, or similar
Experience with messaging brokers like Confluent, Kafka, Kinesis, Pub/Sub, or similar
Strong communication skills able convey complex data topics to non-technical audiences
Excellent self-management with the capability to balance incoming priorities
Great problem-solving skills alongside the ability to distill the best problem statement
BS in Computer Science or equivalent degree or experience
5+ years of hands-on working experience as a Data Engineer

We are open to remote work in AZ, CA, CO, FL, GA, ID, IL, MA, MD, NC, NV, NY OR, TX, UT and WA or onsite/hybrid/flex at our office in Bellevue, WA.

Total compensation is salary plus an annual bonus potential . Please note that the listed pay range is a general guideline only. We take into consideration the geography in which you live and your experience and qualifications when determining final salary. This role is also eligible for medical, dental, vision and life insurance, short and long term disability, 401(k), flexible and dependent care spending accounts, PTO, paid holidays and parental leave.
Remote pay range
$112,000—$165,000 USD"
510,Data Engineer,Current Job Openings at Revionics,"Atlanta, GA•Remote","$75,000 - $95,000 a year","Revionics guides retailers on the lifecycle pricing journey with leading AI solutions for pricing, promotions, and markdowns. We provide our customers with clarity and confidence to make optimal pricing decisions for powerful results. Retailers in all segments across the world adopt our solutions to improve top-line sales, demand, and margin.
Revionics is investing in their Data Platform and is growing the team. This role would be a junior role in the team that is responsible for driving the overall system design, architecture, scalability, reliability, and performance of end-to-end data systems.
Who you are:
Bachelor's degree in Computer Science, Computer Engineering, CIS/MIS, or a related field
You have 1+ years of development experience with one or more of the following, or another similar language: Python/Scala (or similar)
Some exposure to different non-relational (MongoDB or similar) and/or relational technologies (SQL)
Exposure to orchestration tools (e.g. Luigi, Airflow, Kubeflow etc.)
Interested in big data technologies likes Spark
Exposure to cloud platforms (AWS/GCP) a plus
Able to communicate, collaborate, and work effectively in a distributed team.
Can think about and write high quality code and can demonstrate that capability, be it through job experience, schoolwork, or contributions to community projects.
Solid understanding of software engineering concepts and methodologies
Familiarity with software testing principles
Enjoy tough technical challenges and are naturally intellectually curious

What you'll do:
In this role, you will be defining and developing modular data pipelining solutions to feed the AI/ML systems that power Revionics' pricing products
Work with Product and Science teams to build data products that enable some of the largest retailers in the world to consume and build upon Revionics' AI/ML platform.
Collaborate with data operations to build native observability services that meet the high bar in terms of visibility and precision for AI/ML software.
We also look for
Passion
Initiative and a Pioneering Spirit
Quality orientation
Resourcefulness and application
The pay range for this position is between $75,000 and $95,000 plus Annual; Incentive Bonus. Starting salary may vary based on a number of factors including, but not limited to, the position being offered, location, education, training, and/or experience.
Revionics can support work authorization and visa sponsorship.
#LI-TM1 #LI-HYBRID"
511,"Java Engineer - Kotlin, DevOps - Finance Data Engineering",Amex,"New York, NY","$85,000 - $150,000 a year","You Lead the Way. We’ve Got Your Back
With the right backing, people and businesses have the power to progress in incredible ways. When you join Team Amex, you become part of a global and diverse community of colleagues with an unwavering commitment to back our customers, communities, and each other. Here, you’ll learn and grow as we help you create a career journey that’s unique and meaningful to you with benefits, programs, and flexibility that support you personally and professionally.
At American Express, you’ll be recognized for your contributions, leadership, and impact—every colleague has the opportunity to share in the company’s success. Together, we’ll win as a team, striving to uphold our company values and powerful backing promise to provide the world’s best customer experience every day. And we’ll do it with the utmost integrity, and in an environment where everyone is seen, heard and feels like they belong.
Join #TeamAmex and let’s lead the way together.
As part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers’ digital lives. Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems. American Express offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source. And because we understand the importance of keeping your skills fresh and relevant, we give you dedicated time to invest in your professional development. Find your place in technology of #TeamAmex.
Who we are:
The Finance Data Engineering group at American Express is entering into a new phase of technology transformation driven by opportunities to simplify processes, deepen business intelligence, analytics and reporting, and raise operational efficiency. If you have the talent and desire to deliver innovative products and services at a rapid pace, with hands on experience and strategic thinking, in areas of data management and analytics, cloud computing, and modern software engineering, join our leadership team to be part of our transformation journey.
What we’re looking for:
You’re a talented, creative, and motivated engineer who loves developing powerful, stable, and intuitive apps – and you’re excited to work with a team of individuals with that same passion. You’ve accumulated years of experience, and you’re excited about taking your mastery of Java, Kotlin, Python, or Javascript to a new level. You enjoy challenging projects involving big data sets and are cool under pressure. You’re no stranger to fast-paced environments and agile development methodologies – in fact, you embrace them. With your strong analytical skills, your unwavering commitment to quality, your excellent technical skills, and your collaborative work ethic, you’ll do great things here at American Express.
Leadership attributes:
Demonstrate increased self-reliance to achieve team goals
Influence team members with creative changes and improvements by challenging status quo and demonstrating risk taking
Impact/Influence:
Communicates, collaborates, and proactively engages with teammates (and cross-functional teams alike) to ensure requirements are delivered on-time.
Accountable to team for delivery of quality work
Qualifications:
Minimum 3+ years experience coding in one or more of the following languages: Java, Kotlin.
Experience implementing highly-available, reliable, and secure APIs, services, and infrastructure to support large-scale initiatives, primarily in the AWS or GCP ecosystems.
Solid Engineering Principles (OOP/Func Paradigms, TDD, DDD, SoC, MicroServices, Caching Strategies)
Experience with CICD pipelines (ie: Jenkins/CircleCI/XLR/Git/GitHub Actions)
Experience deploying to cloud-based infrastructure resources (DevOps, CloudOps, SecOps, or SRE experience a plus)
3+ years experience with container technologies and runtimes (docker, containerd, Kubernetes, EKS, ECS, etc)
Experience with different architectures: (ie: mvc, microservices, async/event-based, task-based, workflow-based, ETLs, data pipelines)
Experience with relational database engines (ie: MySQL, Postgres, MemSQL)
Ability to effectively interpret technical and business objectives and challenges and articulate solutions
Strong analytical skills and programming skills, in production environment
Experience contributing to Open-Source Software (OSS) projects
Salary Range: $85,000.00 to $150,000.00 annually + bonus + benefits
The above represents the expected salary range for this job requisition. Ultimately, in determining your pay, we'll consider your location, experience, and other job-related factors.
American Express is an equal opportunity employer and makes employment decisions without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran status, disability status, age, or any other status protected by law.
We back our colleagues with the support they need to thrive, professionally and personally. That's why we have Amex Flex, our enterprise working model that provides greater flexibility to colleagues while ensuring we preserve the important aspects of our unique in-person culture. Depending on role and business needs, colleagues will either work onsite, in a hybrid model (combination of in-office and virtual days) or fully virtually.
US Job Seekers/Employees - Click here to view the “Know Your Rights” poster and supplement and the Pay Transparency Policy Statement.
If the links do not work, please copy and paste the following URLs in a new browser window: https://www.dol.gov/agencies/ofccp/posters to access the three posters.
Employment eligibility to work with American Express in the U.S. is required as the company will not pursue visa sponsorship for these positions."
512,Data Engineer General,Global Information Technology,"Dearborn, MI",N,"Job Title: Data Engineer General
Job Location: Dearborn, MI
Job Type: Contract

Job Description:
Experience with Alteryx, Qlikview, SQL, R, or Python a plus.
Experience creating Data Models and Data products.
Experience working with data in Informatica.
Experience working with data in SAP - S4HANA Strong collaboration and influencing skills, and the ability to energize a multi-functional team.
Proven problem formulation with the ability to take complex problems and break them down to build and implement an action plan.
Ability to communicate findings to make data analysis meaningful and understandable by Data Operations team members, business partners, including IT and analytic teams.
Ability to optimally communicate information and ideas in written and verbal formats, including process documentation.
Experience developing data standards.
Experience working in Hadoop, particularly with HDFS and Hive.
Experience Required:
3+ years of progressive responsibilities in managing data and data processes
Education Required:
Bachelor's degree in Business, Finance, Computer Science, Engineering, Statistics, Economics or equivalent experience.

Interested candidates can send their updated resumes at jobs@global-itech.com"
513,Data Engineer,Together Credit Union,Remote,N,"Position:
Data Engineer (Remote- Nationwide)
Position Summary:
As a Data Engineer at Together Credit Union, you will be responsible for developing and enhancing the various real time flow pipelines as well as enabling sophisticated data analysis from data in our data lake, while also maintaining strict high performance and throughput requirements. You will also work closely with Enterprise Architecture, Business Analysts and Security experts to bring new ideas in data exploration and bi analytics to fruition as deliverables that will enable new ways of furthering the member experience.
Job Description:
Role and Responsibilities:
Work with architects, business analysts, and other technical resources to understand data designs and implement them.
Coordinates, plans, and implements data solutions to meet needs of the business.
Create near real time pipeline with Spark jobs using PySpark scripting.
Build required infrastructure for extract load and transform (ELT) operations from various sources of data to Redshift Serverless.
Develop complex SQL.
Implement JDBC solutions with encryption in transit.
Execute pipelines and Spark jobs in AWS EMR Serverless.
Document pipelines solutions.
Understand the role of an Active Meta Data Repository in driving pipeline solutions.
Create shared and reusable code which can be used across PySpark scripting.
Schedule pipelines which execute in a cohesive manner to ensure timely data delivery and ingestion.
Demonstrate strong technical acumen when representing the data team to the business.
Appropriately escalate and mitigate risks for projects and initiatives to leadership.
Experience, Qualifications, and Skills:
Bachelor’s Degree in Computer Science, Management Information Systems, Information Technology, or related field preferred.
3+ years’ experience in Information Technology.
Expert level Python scripting skills and/or two plus years of PySpark development required.
Minimum of 2 years’ experience with AWS Cloud experience preferred.
Familiarity with additional cloud platforms would be a plus.
3 years of SQL experience required.
Understand relational databases and SQL technologies especially massively parallel solutions like AWS Redshift Serverless or Redshift.
2 years’ experience with ELT required.
2 years’ experience with AWS Redshift preferred.
Previous experience with Snowflake, Synaps, and BigQuery would be a plus.
2 years of AWS EMR experiences preferred.
Previous experience with Job scheduling software.
Possesses and applies a high degree of subject matter expertise; continually strives to build on this knowledge to produce results that meet customer needs and enterprise goals and objectives.
Outstanding customer service skills and demonstrated ability to interact with anticipated audiences in a courteous, service-oriented manner.
Excellent organizational, multi-tasking, and time-management skills.
Collaborative team player, capable of working well with others, but also autonomously with little direction."
514,Enterprise Data Warehouse Engineer,Commonwealth Fusion Systems,"Devens, MA 01434•Hybrid remote",N,"Commonwealth Fusion Systems (CFS) has the fastest, lowest cost path to commercial fusion energy.

CFS collaborates with MIT to leverage decades of research combined with groundbreaking new high-temperature superconducting (HTS) magnet technology. HTS magnets will enable compact fusion power plants that can be constructed faster and at lower cost. Our mission is to deploy these power plants to meet global decarbonization goals as fast as possible. To that end, CFS has assembled a team of leaders in tough tech, fusion science, and manufacturing with a track record of rapid execution. Supported by the world’s leading investors, CFS is uniquely positioned to deliver limitless, clean, fusion power to combat climate change. To implement this plan, we are looking to add dedicated people to the team who treat people well, improve our work by adding multifaceted perspectives and new ways of solving problems, have achieved outstanding results through a range of pursuits, and have skills and experience related to this role.

The Enterprise Data Warehouse Engineer is an exceptional data professional with a proven track record of excellent problem-solving skills. The Enterprise Data Warehouse Engineer will work in a variety of settings to build data pipelines and extract information from various systems and convert it into usable information for business needs. The ideal candidate is an experienced data enthusiast, has worked with various databases, and enjoys developing and optimizing methods to rapidly deploy quality data products.
This team member will:
Create and maintain end to end data pipeline products on serverless cloud data tech stack
Assemble large, complex data sets and deploy them for reusable and rapid data pipeline deployment
Create and maintain knowledge base documentation for data products
Build analytical tools that utilize the data pipeline to provide actionable insights to business users
Utilize ETL software like Informatica and Talend to create and maintain data pipeline jobs
Develop algorithms to transform data into useful, actionable information
Develop data jobs which enable central and open CFS Data Warehousing/Data Lake/Data Mesh solutions
Create needed data validation methods and data analysis tools
Ensure compliance with data governance and security policies adopted at CFS
Develop big data solutions using cloud technologies such as AWS EMR and GCP BigQuery
Perform all other duties as assigned
The ideal candidate will have most, if not all, of these requirements:
Bachelor's degree in an IT-related field or equivalent experience in data engineering.
Exceptional technical knowledge of cloud agnostic data pipeline tools and technologies
Proven experience as a data engineer in a fast paced, dynamic and growing environment
Experience with designing and building data pipelines using specialized languages such as R, SQL, Python and C
Experience with ETL tools
Knowledge of relational bases
AWS and GCP certification
Great numerical and analytical skills
Possess excellent problem-solving skills
Have attention to detail and excellent communication skills, both written and verbal to provide updates on Applications and projects status
Have an agile mindset to provide solutions quickly with an incremental value to customers
Additional experience and/or qualifications:
Experience with integration tools
Experience as data analyst/business analyst
Experience as a database administrator is welcome
Ability to lift up to 50lbs occasionally
Perform activities such as typing, standing, and sitting, for extended periods of time
Working with appropriate PPE in the presence of hazards such an exposure to heat,
Willingness to travel occasionally
#LI-Hybrid
CFS team members thrive in a fast-paced, dynamic environment and have demonstrated exceptional results through a range of different pursuits. We all tightly align with our company values of integrity, execution, impact and self-critique. As we grow, we are looking to add talented people who are mission driven and bring diverse perspectives and new ways of solving problems.

At CFS, we deeply value diversity and are an equal opportunity employer by choice. We consider all qualified applicants equally for employment. We do not discriminate on the basis of race, color, national origin, ancestry, citizenship status, protected veteran status, religion, physical or mental disability, marital status, sex, sexual orientation, gender identity or expression, age, or any other basis protected by law, ordinance, or regulation."
515,Data Visualization Engineer - Remote,Wells Enterprises,United States•Remote,N,"Use your knowledge of Business Intelligence (BI) technologies to help maintain and foster Wells� Enterprise Data Warehouse (EDW) and analytics/dashboard development towards a world class organization where the products and services from that team become ever critical to business information insights and decision making.
Create & inspire others to create visually appealing and impactful dashboards which help drive the business through information enlightenment.
Collaborate with business partners to understand strategy, challenges & opportunities, define & capture key data, and build world class dashboards driving self-service analysis & key decision-making processes.
Take lead in solving problems, presenting ideas, training, and play an active role in ideation/brainstorming sessions with business partners and within IT.
Lead team and organizational thinking around new and better ways to display information, driven from an insatiable passion for data visualization.
Maintain BI environment from user maintenance, data source and back-end server configuration, and lead data architecting for all size projects.
Champion visualization journey, driving user groups, and advising on opportunities going forward.
Collaborate with others in Information Services to support back-end server, cloud environment, and data architecture.

In addition to the responsibilities listed above, other duties may be assigned by your supervisor, as dictated by business necessity.
Bachelors degree in IT, Eng, or business area
5+ years of hands-on experience building Tableau (or related) dashboards and reports. Good understanding of performance tuning, security implementation, report refreshes, and Tableau server configuration. Proven track record of consistently architecting, designing, and implementing end- to-end Data Visualization / Reporting / Analytics projects. Background in CPG-Manufacturing business process is a plus. Familiarity with the Azure cloud storage technologies is a plus.
Knowledge - Expert level knowledge of relational & reporting database structure techniques, how to convert from one to the other, and the tools for manipulating & moving data. Understands advanced statistical techniques, information management, principles of design, and data modeling. Tableau Server (or related), Oracle DB, Azure Scripting tools such as Python or R helpful.
Skills - Expert knowledge of Tableau and Tableau Server or related technology/tools. Expert in modeling, creating charts and graphic depictions, gathering datasets, producing reports and dashboards. Expertise with SQL towards building data sources, as well as writing level of detail calculations and table calculations, expertise in applying filters and using parameters. Excellent interpersonal skills. Passionate and self-driven with positive attitude. Strong communication skills. Ability to build trust, collaborate and network with stakeholders outside and inside the organization. Strong team player with the ability to collaborate and advise others on possible paths forward. Knowledge of Azure data storage. Technologies used: - Azure Synapse Analytics (Data Warehouse) - Azure Analysis Services (Azure Cube) - Tableau Server (2020.1.6) - Tableau Desktop (2020.1.2) - Mindtree BlueSky Integration Studio (ETL version 11.2.1)
Abilities - Highly motivated, hard-working, and passionate about building better solutions Able to thrive in a very fast paced and dynamic environment with minimal supervision.
Ability to think through problems and challenge proposed / existing solutions Able to explore, analyze and propose BI Infrastructure architecture opportunities Able to join tables, and build data sets for BI consumption Comfortable around large data sets towards analytics and beautiful and highly functional dashboards Exhibit constant drive to learn more, ask questions, seek better outcomes Understands how people perceive data and information and incorporate into work Able to build world class data visualizations that facilitate storytelling and data exploration
Wells Enterprises is an EEO/AA employer M/F/Vet/Dis"
516,Senior Data Engineer (Python or OOP),Charles Schwab,"Westlake, TX","$99,000 - $220,000 a year","Your Opportunity

The Schwab Technology Sevrvices Data team in Data and Rep Technology (DaRT) at Schwab is looking for a Senior Data Engineer with skills in Python or OOP to enable the strategic use of technology data assets for the entire STS organization. Would you like to be part of a new team chartered to build the next generation data & analytics platform supporting Schwab Technology Services?

Our ideal candidate is enthusiastic about learning new technologies (Python or OOP) and delivering exceptional data solutions for our technology organization at Schwab. You need to have proven critical thinking skills and a focus on pragmatic problem solving. We require strong ethics and the ability to partner with business stakeholders and technologists across the organization. You should have backgrounds in both software development (Python or OOP) and data engineering, along with a strong passion for leveraging real-time data insights and analytics.
What you are good at

Collaborating directly with business and technology stakeholders to define future-state business capabilities & requirements, and translating those into transitional and target state data architectures
Partnering with platform architects to ensure implementations meet published platform principles, guidelines, and standards
Analyzing the current technology environment to detect critical deficiencies, and recommend solutions for improvement
Designing, implementing, and maintaining data services, interfaces, and real-time data pipelines via the practical application of existing, new, and emerging technologies and data engineering techniques
Developing continuous integration and continuous deployment for data pipelines that include automated unit & integration testing
Mentoring, motivating, and supporting the team to achieve objectives and goals
Advocating for agile practices to increase delivery throughput
Designing and maintaining solutions consistent with published standards
What you have

2+ years of experience using object-oriented languages (Python, .Net, Java) to deliver data for near real-time, streaming analytics
7+ years of experience using standard methodologies to design, build, and support near real-time data pipelines and analytical solutions using BigQuery, Hadoop, Teradata, MS SQL Server, Talend, Informatica, and/or SSIS
5+ years of experience working with partners documenting business requirements and translating those requirements into dimensional data models using Erwin
5+ years of experience working on agile teams delivering data solutions
4+ years of experience developing MDM solutions
3+ years of experience in delivering solutions on public cloud platforms (Google Cloud preferred)
Experience writing automated unit, integration, and acceptance tests for data interfaces & data pipelines
Ability to quickly comprehend the functions and capabilities of new technologies, and identify the most appropriate use for them
Exceptional interpersonal skills, including teamwork, communication, and negotiation
Why work for us?
Own Your Tomorrow embodies everything we do! We are committed to helping our employees ignite their potential and achieve their dreams. Our employees get to play a central role in reinventing a multi-trillion-dollar industry, creating a better, more modern way to build and manage wealth.
Benefits: A competitive and flexible package designed to empower you for today and tomorrow. We offer a competitive and flexible package designed to help you make the most of your life at work and at home—today and in the future.
TD Ameritrade, a subsidiary of Charles Schwab, is an Equal Opportunity Employer. At TD Ameritrade we believe People Matter. We value diversity and believe that it goes beyond all protected classes, thoughts, ideas, and perspectives."
517,SENIOR DATA SCIENCE ENGINEER (ECONOMIST),DraftKings,"500 Boylston St, Boston, MA 02116",N,"BOSTON, MA
ENGINEERING
JR06102
FULL TIME
At DraftKings, we're inspired by our shared passion for developing creative solutions to complex challenges and empowering the people around us to do their best work. We are industry leaders in the digital entertainment and technology space and are propelled by constant curiosity and diverse perspectives.
BE THE STRATEGY BEHIND THE GAME:
Our team comprises algorithm experts, data science technologists, and causal thinkers, coming together to develop innovative data products that solve analytically challenging problems at DraftKings. As part of this role, you will be a creative thinker, utilizing data, machine learning, and causal inference skills to craft high-impact solutions that grow the business.
WHAT YOU'LL BE AS A SENIOR DATA SCIENCE ENGINEER (ECONOMIST):
Leverage sound modeling frameworks to automate critical business processes which unlock scale and efficiency.
Shape DraftKings’ consumer product strategy through a deep understanding of behavioral economics and research
Build robust causal models to support marketing and pricing decisions utilizing a variety of transactional and behavioral data.
Collaborate with product and analytics to design and execute rigorous field experiments and identify and analyze natural experiments.
WHAT YOU'LL BRING:
Deep understanding of econometric techniques, notably dealing with causal analysis of field and natural experiments.
Experience in a programming language like Python or R.
Experience with SQL.
Expertise in statistical modeling and machine learning techniques, including regression analysis, decision trees, and neural networks.
Excellent communication skills with the ability to effectively present complex data and technical concepts to non-technical stakeholders.
An intuitive sense of how quantitative work aligns with business priorities.
Ideally, you have experience as a core contributor on a data science team where you were responsible for multiple aspects of data science technical projects, including development and deployment, and then monitoring of how those applications perform in production.
JOIN US!
Our teams are fueled by innovation. We are looking ahead, building what’s next, and continuously reinventing the industry. We’re a publicly traded (NASDAQ: DKNG) technology company headquartered in Boston, with teams around the world and an expanding global presence.
We strive to create a place where all feel safe, empowered, engaged, championed, and inspired. DraftKings is proud to be an equal opportunity employer. This means we do not tolerate discrimination of any kind and are committed to providing equal employment opportunities regardless of your gender identity, race, nationality, religion, sexual orientation, status as a protected veteran, or status as an individual with a disability.
READY TO BUILD WHAT’S NEXT? APPLY NOW.
As a regulated gaming company, you may be required to obtain a gaming license issued by the appropriate state agency as a condition of employment.
The US base salary range for this full-time position is $120,800.00 - $181,200.00, plus bonus, equity, and benefits as applicable. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range and how that was determined during the hiring process."
518,ETL Spatial Data Engineer,"ESRI, Inc.","380 New York St, Redlands, CA 92373","$72,800 - $124,800 a year","Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where ®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1"
519,Data Science Engineer,Wheels Up,United States,"$100,000 - $250,000 a year","Overview:
Wheels Up is on a mission to democratize the private aviation space through cutting-edge technology and innovative product and service offerings. With the Wheels Up App, anyone can search, book, and fly on private aircraft from the world's largest marketplace of safety-vetted and verified planes, totaling over 1,500 whenever they have a travel need. It's as easy as booking an Uber or an Airbnb.
In January 2020, we entered a groundbreaking partnership with Delta, which combined Delta Private Jets to expand our fleet and offer guaranteed nationwide coverage. This enabled us to provide unparalleled experiences across private and commercial air travel. We also acquired our longtime partner Gama Aviation in March 2020, making us the largest Part 135 operator in the U.S. and one of the world's largest private aviation operators and aircraft management companies. Additionally in January 2021, we acquired Mountain Aviation, the largest Part 135 operator of Citation X Super-Mid Jets in the U.S. These developments, coupled with our previous acquisitions of Avianis, a leading tech company in our space that has accelerated the development of our next-level digital platform, and TMC, the leading light jet charter wholesaler in the U.S., have made Wheels Up one of the fastest-growing and most innovative private aviation companies in the industry, capable of fulfilling any flight need. At Wheels Up, we realize our vision of making private flying and the accompanying lifestyle accessible to millions of individuals, families, and businesses in the U.S. and worldwide.
Highly complex business and operations problems at the forefront of Wheels Up require building data products to support and automate both short-term tactical and long-term strategic decision making. The Data Science Engineering Team within the Data organization at Wheels Up is essential in turning data science models and algorithms to production-grade scalable products and microservices for both internal and external customers. We are seeking Data Science Engineers at all levels to contribute to this group.
Responsibilities:
Collaborate with software engineers, data scientists, and product managers to develop, deploy and maintain scalable data products.
Leverage knowledge of software engineering principles to guide development and deployment of data science solutions in production.
Work closely with the business team to design and implement new features using a variety of platforms and technologies.
Work closely with senior engineering leadership to design, build and deliver high-quality technology solutions.
Own products/features end-to-end in all phases from development to production.
Qualifications:
A bachelor’s degree (preferably an advanced degree) in Computer Science, Engineering, or a related field.
3+ years of industry experience in building data products as well as backend services and APIs using Python; preferably with additional experience in a fully object-oriented language (e.g. C++, C#, or Java).
Strong understanding of computer science fundamentals, including algorithms, complexity analysis, data structures, problem-solving, and object-oriented design.
Database design and management experience with MySQL and/or SQL Server.
Knowledge of developing services for the cloud, specifically Azure services.
Wheels Up Story:
Wheels Up (NYSE:UP), a leading demand generator in private aviation, offers a total private aviation solution that includes world-class safety, service and flexibility through on-demand flights, membership programs, corporate solutions, aircraft management, whole aircraft sales and commercial travel benefits through a strategic partnership with Delta Air Lines.

Our employees’ passion for innovation, customer-centric mindset and collaborative spirit drives our mission to connect flyers to private aircraft – and one another – to deliver exceptional, personalized experiences and build an industry-leading marketplace.

Wheels Up attracts, empowers and retains diverse, talented employees who bring a range of experiences and are committed to taking Wheels Up to new heights. We recognize our employees’ unique perspectives and skills and take pride in helping them develop a meaningful and exciting career path with us.

Interested in joining our talented team, explore our career opportunities, today!

To learn more about Wheels Up, please visit WheelsUp.com.

The wage range for this position is $100,000 to $250,000 base salary determinded based on experience. This range takes into account a variety of factors that are considered in making compensation decisions, such as differences in skill sets; experience and training; licensure and certifications; and other business and organizational needs. This wage range has not been adjusted for any applicable geographic differential associated with where the position may be filled. At Wheels Up, it is not typical for an individual to be hired at or near the top of the range for their role. Your placement within the wage range is primarily based on your qualifications and experience.

You may also be eligible to participate in a discretionary annual incentive program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance. Wheels Up also offers a full range of healthcare (medical, dental, vision, mental health), a generous 401k plan, financial, and other benefits, including pet, life and disability insurance and a flexible paid time off policy including 14 paid holidays."
520,Senior Data Engineer,Global Atlantic Financial Group,"Boston, MA","$87,980 - $167,580 a year","All offices are currently open, and our employees are back 4 or 5 days a week in Hudson Yards, NY and 3 days a week in all other offices. If you have questions on this policy or the application process, please contact recruiting@gafg.com.

COMPANY OVERVIEW
Global Atlantic Financial Group is a leader in the U.S. life insurance and annuity industry, serving the needs of individuals and institutions. Global Atlantic is a majority-owned subsidiary of KKR, a leading global investment firm that offers alternative asset management across multiple strategies and capital markets solutions.

Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us.
We use Greenhouse as our scheduling tool and communicate through their systems. At times, your email may block our communications. Please be sure to check your SPAM so that you do not miss critical information about our process, including scheduling.
Job Summary:
In this role, you will be responsible for expanding and evolving the cloud-based enterprise data platform. You will ensure the technology components of the platform are correctly configured and optimized to meet the functional, non-functional, and operational requirements. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. The candidate must be self-directed with the ability to own and execute the platform improvement activities while collaborating with other team members and stakeholders.
Be willing to work non-standard business hours on an on-call basis in a 24x7 environment few days a month.
Responsibilities:
Triage problems across the data platform to help address development, test, and production issues
Able to identify and debug issues in Tableau dashboards.
Testing Data pipelines at staging/QA environments and help deploying them at production
Work with business, product and technical stakeholders on data or report issues.
Create and manage Incident and Change Requests as needed for data pipelines.
Assist with data architecture, pipeline development, automation and planning
Develop and maintain documentation, departmental technical procedures, and user guides
Maintain best practices to facilitate optimized software development and continuous improvement/continuous delivery (CI/CD)
Qualifications:
Undergraduate degree in computer science, engineering, or a related discipline preferred
8+ years of experience in building and maintaining data platforms, ETL processes and connected sources and targets required
5+ years' experience in data analytics on (Microsoft SQL Server, Redshift, HQL)
5+ years of professional experience in Python, Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc.
3+ years of Tableau or similar Visualization dashboards build debugging.
Excellent advanced SQL troubleshooting skills
Proficient in Python and Java programming languages.
Understanding of distributed compute frameworks like Hadoop, Spark .
Advanced structured query language (SQL) skills are key
Strong foundation in the AWS Cloud Services: S3, Redshift Spectrum, EC2
Solid understanding and experience working with Gitlab, JIRA and Confluence
#LI-AO1
Various jurisdictions have passed pay transparency laws that require companies provide salary ranges for any positions for which they are accepting applications. Global Atlantic has offices in Atlanta, Batesville, Bermuda, Berwyn, Boston, Des Moines, Hartford, Indianapolis, and New York City. The base salary range posted below is inclusive of the lowest cost of living geography to the highest in which we have a Global Atlantic office.
Global Atlantic's base salary range is determined through an analysis of similar positions in the external labor market. Base pay is just one component of Global Atlantic's total compensation package for employees and at times we hire outside the boundaries of the salary range. Other rewards may include annual cash bonuses, long-term incentives (equity), generous benefits (including immediate vesting on employee contributions to a 401(k)), as well as a company match on your contributions), and sales incentives. Actual compensation for all roles will be based upon geographic location, work experience, education, licensure requirements and/or skill level and will be finalized at the time of offer. Compensation for our more senior positions have a larger component of short-term cash bonus and long-term incentives. The base salary range for this role is $87,979.5 to $167,580.
TOTAL REWARDS STATEMENT
Global Atlantic's total rewards package is reflective of our corporate values, particularly diversity, excellence and innovation, with a focus on inclusion, pay equity, and flexibility. We are proud to support your personal and professional growth and well-being through programs such as educational assistance, virtual physical therapy, remote/onsite fitness reimbursement, a medical second opinion program, pet insurance, military leave, parental leave, adoption assistance, fertility and family planning coverage. We strive to foster a culture of total well-being through community outreach and charitable giving programs.
We are active in our communities-
New York: Red Hook Conservancy, Girls Who Invest and The Bowery Mission
Boston: Cradles to Crayons, Project Bread, Let's Get Ready, Rise Against Hunger, Salvation Army and many other local volunteer organizations in around the Boston area
Hartford: Habitat for Humanity, Foodshare, Humane Society, Hands on Hartford, Mercy Shelter and Dog Star Rescue
Indianapolis: Elevate Indianapolis, Gleaners Food Bank and the Juvenile Diabetes Research Foundation
Batesville: American Cancer Society Relay for Life, Angels of Giving, Margaret Mary Health Foundation, Ripley County Community Foundation, Safe Passage, Batesville High School Sponsorships, local area youth sports and food pantries, as well as many others
Des Moines: United Way, Central Iowa Shelter & Services, Junior Achievement of Central Iowa and Make a Wish Foundation
Berwyn: Food drive and will be planning an event to help a local family over the holidays
Atlanta: Packaged Good Organization, which helps the most vulnerable community members with providing personalized care packages for people in need including the elderly, our armed forces, the homeless and hospitalized kids
Bermuda: Sponsor of a weekly feeding program operated by The Hamilton Seventh-Day Adventist Church
Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family.
Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status.
Employees who require an accommodation to perform the essential functions of their job will participate in an interactive process which may include providing documentation. If you are hired and require an accommodation for any protected status, please email benefits@gafg.com.
Please click on the links below to learn more about Global Atlantic.
Global Atlantic Privacy Statement"
521,Data Engineer II,LeaseQuery,"Atlanta, GA•Hybrid remote",N,"LeaseQuery is looking for a Data Engineer II to join our Engineering team. We’re seeking someone with a passion for improvement; both in our processes and in yourself. We work in an Agile environment with development teams utilizing Scrum and Kanban. At LeaseQuery, a Data Engineer is a person who focuses on the design, scalability and malleability of our data infrastructure. They work on teams using agile approaches building artifacts needed to sustain the technical systems beyond data management/ETL (documentation, data dictionaries, designs and so on). The ideal candidate has deep knowledge of SQL database design, experience with data warehouses across multiple databases, and expertise in the design, creation, management, and business use of large datasets.

LeaseQuery's headquarters is located in Atlanta, GA, but this role can sit 100% remote.
What you will be doing:
Ensure scalable, reliable, and performant AWS PostGres databases and environments
Application and system monitoring and performance tuning
Interfacing daily with Development teams and other departments
Monitoring and providing feedback to developer driven data architecture changes
Participating in release and deployments
Identifying, designing, and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes
Working with stakeholders including engineering, product, and executive teams and assisting them with data-related technical issues and inquiries
Proposing new technologies and practices as solutions or improvements to existing systems
Research, assess, and enhance ETL pipeline
Developing architectural roadmap for the databases, potentially including a move towards sharding
Experience with designing data warehousing solutions and integrating systems
Tech you will work with:
Hosting and Administration: AWS RDS/Aurora
Communication: Atlassian Jira, Slack
Database: PostgreSQL, MS SQL Server
DataWarehouses & Integration Systems: Snowflake, FiveTran, Celigo
Analytics and Logging: DataDog, Tableau
Build and deployment tools: Git, Liquibase
Languages: SQL, Python, Shell Scripting
What experience and skills we need you to have:
2+ years in a data/database engineer role
4+ years of experience working with databases and data architectures in any IT role
Experience designing, building, deploying, and scaling databases
Solid understanding of data architecture
An attitude of success defined in team accomplishments
Benefits
Flexible PTO (including 11 holidays and your birthday off)
401(k) plan with employer matching
Great health benefits with multiple plan option
Option to choose between in office, fully remote, or a hybrid work environment for all employees
Sabbatical program (4 weeks after 5 years of service)
Casual dress environment (when in office)
Catered lunches every Tuesday and Thursday
Company events each quarter
Signing stipend for a work-from-home setup
Free gym membership at our office
Annual employee development program stipend of $2,000 for each employee
Flexible parental leave with 10 weeks paid leave for ALL new parents
Fertility/adoption assistance
Annual tutoring stipend for your children
Mentorship program available immediately
Regular team outings
Advancement opportunities based on results, not politics
Culture that emphasizes inclusiveness driven by our REDI Committee
About Us
LeaseQuery simplifies complex accounting with our innovative FinTech SaaS technology. Our products, used by more than 3000+ organizations in 87 countries, are top rated for user satisfaction and ease of use by G2, and our company has appeared on the Deloitte Technology’s Fast 500 list and Georgia’s Fast 40 list among many other recent accolades over the past several years.

During the past decade, our CEO, George Azih, grew LeaseQuery from a one-person company to a workforce of 350+ representing one of the fastest-growing FinTech companies today. As we move into our next phase of growth, we're looking for passionate and dedicated people who want to invest their energy to align with our company's long-term goals.

#LI-Remote

LeaseQuery is an equal opportunity employer to all persons, free from restrictions and prejudice based upon race, color, creed, religion, sex, domestic relationship status, parental status, family status, sexual orientation, national origin, gender identity, age, disability, and veteran status. LeaseQuery maintains a drug-free workplace."
522,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
523,Senior BI Data Engineer,Teletrac Navman,United States,"$125,000 - $145,000 a year","The Company
Teletrac Navman is a software-as-a-service (SaaS) provider leveraging location-based technology that empowers people managing mobile assets to move their business forward with certainty.
The Position
We’re looking to add a Senior BI Data Engineer to join Teletrac Navman! This role will design, implement, and maintain systems used to collect and analyze business intelligence data.
In this role, you’ll get to:
Create and maintain optimal data pipeline architecture.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Keep TN data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues.
Other duties and/or projects, as needed.
Qualifications
At Teletrac Navman, we believe in your potential to make an impact. And we believe in giving you the opportunity, accountability and visibility to do just that.
We are looking for people who have:
Bachelor degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
At least 10+ years of software development experience
At least 5+ years of experience in a Data Engineer role
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience using the following software/tools:
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Redshift.
Experience with data pipeline and workflow management tools: Amazon SWF, AWS Step Functions, Apache Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift, Glue, Lambda etc., with development in Node JS preferred.
Experience with stream-processing systems preferred: Kinesis, Storm, Spark-Streaming, etc.
Hands on coding experience with object-oriented/object function scripting languages: C#, Java, Python, Scala, etc.
Analytical, framework thinker.
Comfortable working in a fast-paced environment.
Perks!
Variety of medical, dental, and vision plans, including a wellness program
15 vacation days + 5 sick days + 2 floating holidays + 11 major holidays
Employee Assistance Program (EAP): Access to a range of support and resources such as counseling services and support for major life events, pet insurance, weight and health coaching, financial and legal concerns, and more
Family Planning: Paid parental/family leave of absence (LOA), adoption assistance up to $20,000, wide range of fertility care and support with the Maven Clinic, adult and elder care support, and more
Flexible work: Teletrac Navman is committed to providing a work environment that maximizes functionality, collaboration and work/life satisfaction with flexibility where available
401k: Pre- and Post-Tax (Roth) opportunities to save for retirement starting day one
Learning & Growth: Full access to LinkedIn Learning, product training, tuition reimbursement program, student debt repayment program with Fidelity
Community Impact: Charitable fundraising activities and a paid Day of Caring for volunteering
The base compensation range for this position is $125,000 to $145,000 per annum. Your actual base salary will be determined based upon a number of factors which may include relevant experience, skills, location (labor market data), credentials (education, certifications), and internal equity.
Vontier partners with you and your family on your health and wellness journey. Visit VontierBenefits.com to view our benefits. We offer a premium suite of health and wellness programs for you and your family. With programs for family planning from Maven Clinic to managing diabetes like Livongo, coverage for women's health, support for adult and elder care, paid parental leave, a generous 401(k) plan with matching company contributions and more. Vontier is here for all stages of life.
Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. The Company reserves the right to modify this information at any time, subject to applicable law.
Teletrac Navman is a leading software-as-a-service (SaaS) provider leveraging location-based technology that empowers people managing mobile assets to move their business forward with certainty. It tracks and manages more than 500,000 vehicles and assets for more than 40,000 companies around the world. With headquarters in Orange County, CA, we have an international presence with additional offices in the United States, United Kingdom, ANZ and Mexico. Check our website at www.teletracnavman.com.
Vontier is a $3B global industrial technology company focused on smarter transportation and mobility. Our six operating companies—Gilbarco Veeder-Root, Global Traffic Technologies, Teletrac Navman, Matco Tools, Hennessy Industries, and DRB Systems—are united by a powerful purpose: mobilizing the future to create a better world. Our portfolio of trusted brands includes market-leading expertise in mobility technologies, retail and commercial fueling, fleet management, telematics, vehicle diagnostics and repair, and smart cities. Vontier’s pioneering solutions advance safety, security, efficiency, and sustainability worldwide.
""Vontier Corporation and all Vontier Companies are equal opportunity employers that evaluate qualified applicants without regard to race, color, national origin, religion, ancestry, sex (including pregnancy, childbirth and related medical conditions), age, marital status, disability, veteran status, citizenship status, sexual orientation, gender identity or expression, and other characteristics protected by law."""
524,Azure Data Engineer,Devcare Solutions,"Devcare Solutions in Durham, NC 27701",$75 - $85 an hour,"Build data analytics solutions using Azure Synapse serverless SQL pools
Performing data engineering with Azure Synapse Apache Spark Pools
Working with Data Warehouses using Azure Synapse Analytics
Transferring and transforming data with Azure Synapse Analytics pipelines
Experience configuring, designing, developing and testing dash boards using Power BI
Experience in using Oracle, Oracle Utilities, ETL tools or any other relational database and tools
Experience with PL/SQL or other database scripting and Relational Database modeling.
Experience with structured system development methodologies
Ability to develop relationships/partnerships with customer by responding to needs and exhibiting an sense of urgency; independently identify options.
Ability to organize, prioritize, and follow complex and/or detailed technical procedures.
Experience with Agile (Scrum, Kanban, or SAFe) development
Data Warehouse experience
Experience with Business Intelligence tools such as Business Objects
Experience with Cloud SaaS and PaaS
Knowledgeable in DevOps practices and Shift Left testing
Experience developing applications using low code no code technologies like Salesforce, Microsoft dynamics and others
Job Type: Contract
Pay: $75.00 - $85.00 per hour
Benefits:
Dental insurance
Experience level:
10 years
11+ years
5 years
6 years
7 years
8 years
9 years
Schedule:
8 hour shift
Ability to commute/relocate:
Durham, NC 27701: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
PL/SQL: 5 years (Required)
Oracle: 5 years (Required)
ETL: 5 years (Required)
Business intelligence: 5 years (Required)
DevOps practices: 2 years (Required)
Agile: 2 years (Required)
Work Location: One location
Speak with the employer
+91 9106376210"
525,Software Engineer- Big Data,State Farm,"Dunwoody, GA 30346•Hybrid remote",N,"Overview:
Do you crave innovation and want to work for a company that is the BEST at what they do in the industry? Does the opportunity to work remote and maintaining a work life balance appeal to you? Then we have the perfect job for you! We are seeking software engineers who push the envelope and strive to create the best product possible. This position will allow you to utilize different technologies, languages, and frameworks to drive solutions while working on inclusive teams that foster diversity of thought. You will be provided opportunities via in house training programs for upskilling to support your development and career goals!
Check out what our Software Engineers have to say about working at State Farm: https://youtu.be/1t5y2PHDypI
Responsibilities:
Looking for something that is pushing the envelope at State Farm and moving Enterprise Technology to the next level? If so, the Claims Data Marketplace (CDM) product may be your answer!
CDM is building out the next generation of a data warehouse in the cloud called ‘Marketplace’. The focus is on migrating Claim’s analytic data and consumers to the cloud. Consumers of this data include Data Scientists, Power Users and BI Reporting to name a few. The future will also include the integration with the Enterprise Marketplace, Unstructured data and other consumers wanting to integrate Claim’s data.
We work closely with the Claims Business and Data Science teams to enable analytic initiatives using the AWS technology stack. Qualified candidates should be high performers, able to work on multiple priorities with a strong desire to learn and grow.
The person filling this position will be responsible for working with consumers of the Claims analytic data to determine their data requirements and building the data pipeline for aggregated domains of data. They will also work closely with other team members on ETL activities associated with data modeling and data design, and ingestion of the data into AWS redshift for consumption.
Qualifications:
Required Skills:
Solution development and deployment on AWS
Experience with various software development tooling and techniques, (e.g., GIT, CI/CD, data pipelines)
Python development experience
Experience with AWS commonly used services (i.e. S3, Lambda, SNS, Glue, Terraform)
Preferred Skills:
Data movement experience (i.e. ETL)
Java
Claims Data Domain knowledge
AWS QuickSight
Join State Farm!
As a Fortune 50 company, we hire the best employees to serve our customers, making us a leader in the insurance and financial services industry. State Farm embraces diversity and inclusion to ensure a workforce that is engaged, builds on the strengths and talents of all associates, and creates a Good Neighbor culture.
We offer competitive benefits and pay with the potential for an annual financial award based on both individual and enterprise performance. Our employees have an opportunity to participate in volunteer events within the community and engage in a learning culture. We offer programs to assist with tuition reimbursement, professional designations, employee development, wellness initiatives, and more!
Visit our Careers page for more information on our benefits, locations and the process of joining the State Farm team!
HYBRID: Qualified candidates (in or near hub locations listed below) should plan to spend time working from home and some time working in the office as part of our hybrid work environment.
HUB LOCATIONS: Dunwoody, GA or Bloomington, IL
SPONSORSHIP: Applicants are required to be eligible to lawfully work in the U.S. immediately; employer will not sponsor applicants for U.S. work authorization (e.g. H-1B visa) for this opportunity
#LI-BS1
SFARM
#LI-Hybrid"
526,Data Engineer,University of Texas at Austin,"1616 Guadalupe St, Austin, TX 78701","$110,000 a year","Job Posting Title:
Data Engineer
-
Hiring Department:
Office of the President
-
Position Open To:
All Applicants
-
Weekly Scheduled Hours:
40
-
FLSA Status:
Exempt
-
Earliest Start Date:
Immediately
-
Position Duration:
Expected to Continue Until Dec 31, 2025
-
Location:
UT MAIN CAMPUS
-
Job Details:
General Notes
The Data to Insights (D2I) Initiative at UT Austin is an investment to build a trusted, integrated, and scalable information infrastructure that transforms complex UT data into valued insights for data-informed decisions.
As a team member in the Data to Insights (D2I) Initiative, you will work with a cross-campus team using the latest cloud technologies to build a next-generation data ecosystem that will improve decision-making and advance the university’s mission. We believe the best ideas arise from collaborative work among colleagues from varied backgrounds and experiences. We actively seek diversity of viewpoint and perception in our student, faculty, and staff recruiting and retention practices. If you’re the type of person who loves to learn and wants to know your work has meaning, you may find your career home at UT Austin. Please note that this position is currently funded through December 31, 2025.
The University of Texas at Austin provides an outstanding benefits package to staff, including:
Competitive health benefits (Employee premiums covered at 100%; family premiums at 50%)
Vision, dental, life, and disability insurance options
Paid vacation, sick leave, and holidays
Teachers Retirement System of Texas (a defined benefit retirement plan)
Additional voluntary retirement programs: tax sheltered annuity 403(b) and a deferred compensation program 457(b)
Flexible spending account options for medical and childcare expenses
Training and conference opportunities
Tuition assistance
Athletic ticket discounts
Access to UT Austin's libraries and museums
Free rides on all UT Shuttle and Capital Metro buses with staff ID card
For more details, please see:
https://hr.utexas.edu/prospective/benefits
and
https://hr.utexas.edu/current/services/my-total-rewards
This position requires you to maintain internet service and a mobile phone with voice and data plans to be used when required for work.
This position provides life/work balance with typically a 40-hour work week and travel generally limited to training (e.g., conferences/courses).
Purpose
The Data Engineer for the UT Data Hub improves university outcomes and advances the UT mission to transform lives for the benefit of society by increasing the useability and value of institutional data. You will create complex data pipelines into UT’s cloud data ecosystem in support of academic and administrative needs. In collaboration with our team of data professionals, you will help build and run a modern data hub to enable advanced data-driven decision making for UT. You will leverage your creativity to solve complex technical problems and build effective relationships through open communication.
Responsibilities
Data Engineering:
Assist in designing and automating scalable data integration solutions between institutional data sources, including the UT source systems, Amazon Web Services, and vendor APIs.
Assure performance and reliability of integration processes through monitoring, performance analysis and development of automated alerting processes.
Participate in end to end delivery of technical projects involving design and development of data pipelines for complex datasets.
Work closely with business partners to design and manage data pipeline elements including load frequency, data delivery mechanisms, and transformations.
Ensure that data pipeline solutions align with industry best practices, and adhere to UT security guidelines.
Develop and maintain detailed technical documentation of data pipeline processes.
Collaborate with key stakeholders including enterprise data architect, data modelers, data stewards, and subject matter experts (SMEs).
Other responsibilities:
Participate in change management processes to coordinate and communicate team activity.
Other related functions as assigned.
Required Qualifications
BS degree in Computer Science, Information Systems, Engineering, or equivalent professional experience. One year of Data Engineering experience with Amazon Web Services. One year of experience implementing and monitoring complex data pipelines and automation of cloud-based workloads. Proficiency in one or more scripting languages and/or programming languages, preferably Python. Advanced SQL knowledge and experience working with relational databases. Demonstrated experience of developing and implementing Continuous Integration and Continuous Delivery (CI/CD) systems. Proficiency in systems analysis, design, and a solid grasp of development, quality assurance, and integration methodologies. Excellent problem solving and troubleshooting skills. Relevant education and experience may be substituted as required.
Relevant education and experience may be substituted as appropriate.
Preferred Qualifications
Three years of experience in Data Engineering or related field. One year of experience working in a cloud-based data warehouse environment. Two years of experience building and monitoring complex data pipelines. AWS Developer and/or Solutions Architect certification. Two years of experience with Agile software development methodologies. Two years of experience with issue tracking systems (JIRA). Experience with Spark, Kafka etc., is a plus.
Salary Range
$110,000 + depending on qualifications
Working Conditions
May work around standard office conditions
Repetitive use of a keyboard at a workstation
Use of manual dexterity
Up to 100% remote at employee discretion with office work environment also an option. Majority of team is located in Austin, TX, and working remotely with infrequent in-office presence.
Required Materials
Resume/CV
3 work references with their contact information; at least one reference should be from a supervisor
Letter of interest
Important for applicants who are NOT current university employees or contingent workers: You will be prompted to submit your resume the first time you apply, then you will be provided an option to upload a new Resume for subsequent applications. Any additional Required Materials (letter of interest, references, etc.) will be uploaded in the Application Questions section; you will be able to multi-select additional files. Before submitting your online job application, ensure that ALL Required Materials have been uploaded. Once your job application has been submitted, you cannot make changes.
Important for Current university employees and contingent workers: As a current university employee or contingent worker, you MUST apply within Workday by searching for Find UT Jobs. If you are a current University employee, log-in to Workday, navigate to your Worker Profile, click the Career link in the left hand navigation menu and then update the sections in your Professional Profile before you apply. This information will be pulled in to your application. The application is one page and you will be prompted to upload your resume. In addition, you must respond to the application questions presented to upload any additional Required Materials (letter of interest, references, etc.) that were noted above.
-
Employment Eligibility:
Regular staff who have been employed in their current position for the last six continuous months are eligible for openings being recruited for through University-Wide or Open Recruiting, to include both promotional opportunities and lateral transfers. Staff who are promotion/transfer eligible may apply for positions without supervisor approval.
-
Retirement Plan Eligibility:
The retirement plan for this position is Teacher Retirement System of Texas (TRS), subject to the position being at least 20 hours per week and at least 135 days in length.
-
Background Checks:
A criminal history background check will be required for finalist(s) under consideration for this position.
-
Equal Opportunity Employer:
The University of Texas at Austin, as an
equal opportunity/affirmative action employer
, complies with all applicable federal and state laws regarding nondiscrimination and affirmative action. The University is committed to a policy of equal opportunity for all persons and does not discriminate on the basis of race, color, national origin, age, marital status, sex, sexual orientation, gender identity, gender expression, disability, religion, or veteran status in employment, educational programs and activities, and admissions.
-
Pay Transparency:
The University of Texas at Austin will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information.
-
Employment Eligibility Verification:
If hired, you will be required to complete the federal Employment Eligibility Verification I-9 form. You will be required to present acceptable and original
documents
to prove your identity and authorization to work in the United States. Documents need to be presented no later than the third day of employment. Failure to do so will result in loss of employment at the university.
-
E-Verify:
The University of Texas at Austin use E-Verify to check the work authorization of all new hires effective May 2015. The university’s company ID number for purposes of E-Verify is 854197. For more information about E-Verify, please see the following:
E-Verify Poster (English)
[PDF]
E-Verify Poster (Spanish)
[PDF]
Right To Work Poster (English)
[PDF]
Right To Work Poster (Spanish)
[PDF]
-
Compliance:
Employees may be required to report violations of law under Title IX and the Jeanne Clery Disclosure of Campus Security Policy and Crime Statistics Act (Clery Act). If this position is identified a Campus Security Authority (Clery Act), you will be notified and provided resources for reporting. Responsible employees under Title IX are defined and outlined in
HOP-3031
.
The Clery Act requires all prospective employees be notified of the availability of the Annual Security and Fire Safety report. You may
access the most recent report here
or obtain a copy at University Compliance Services, 1616 Guadalupe Street, UTA 2.206, Austin, Texas 78701."
527,Data Software Engineer,Hudson River Trading,"1 Financial Sq, New York, NY 10005","$150,000 - $250,000 a year","Data is at the core of everything we do here at HRT. We excel at deriving deep insights from all types of data, allowing us to achieve consistent success in a competitive market. As we think about exciting new avenues for growth, we look forward to expanding the scope of data at HRT to new frontiers.
Being a Data Software Engineer at HRT means being a pioneer. In this role, you'll have the opportunity to work on problems that are pretty brand new to us. You will help onboard and study alternative datasets for use in our trading strategies. You will help design tools, processes, and an entirely new system for managing, storing, accessing, and exploring data at HRT. Your work will unlock new areas of research, enable new insights and signals, and help us expand the footprint of our trading. We're looking for someone who loves data (maybe even dreams about data) and wants to be a critical part of bringing a world-class data warehouse to life.
Responsibilities
Partner and support our business users in their interactions with the data platform and ensure smooth delivery of data - from across the landscape of alternative, reference, financial, and market data across all asset classes
Evaluate, clean, and normalize data; perform initial quantitative analysis to assess suitability of data under trail and develop pipelines to automate processes
Pair up with Algo Developers to onboard and study alternative datasets for use in our trading strategies; revisit and enhance the data workflows around existing datasets to make research more productive; normalize key metrics across multiple data vendors
Work closely with financial and alternative data vendors, including Bloomberg, Refinitiv and many others, and navigate their various delivery platforms (FTP, APIs, etc)
Build tools and automation capabilities for our data pipeline in order to ensure the overall quality of our data platform
Design and implement HRT's data storage strategy to help drive successful delivery and access of data in our data centers around the world
Skills
Strong programming experience in Python
Experience in numpy/pandas is a plus, but not required
Experience in at least one of the following:
Data infrastructure experience - you know how to really store data; and no, we don't mean just knowing not to save an Excel file on your desktop (everyone knows that's messy and it should be in a nicely named folder)
Experience managing ETL pipelines and working with financial data vendors and their various delivery methods
Experience in serving data in a large, distributed system
Experience working on financial datasets, such as fundamental, alternative, news or reference data (e.g. IBES, Refinitiv, Barra, Factset, Compustat)
Track record of working successfully in a collaborative environment
Top-notch communication skills
Profile
You have a minimum of 2-3 years of experience working in data infrastructure and/or with financial datasets
You possess a Bachelor's degree in computer science, engineering, math or a related field
You may pride yourself about that dataset quirk nobody else understands (or knows about), and have automated or normalized a data processing pipeline or two
You enjoy being part of an amazing team but don't mind working alone on a difficult problem
You can investigate and fix problems quickly
You really like to work with people who motivate you and make you better
Annual base salary range of $150,000 to $250,000. Pay (base and bonus) may vary depending on job-related skills and experience. A sign-on and discretionary performance bonus may be provided as part of the total compensation package, in addition to company-paid medical and/or other benefits.
Culture
Hudson River Trading (HRT) brings a scientific approach to trading financial products. We have built one of the world's most sophisticated computing environments for research and development. Our researchers are at the forefront of innovation in the world of algorithmic trading.

At HRT we welcome a variety of expertise: mathematics and computer science, physics and engineering, media and tech. We're a community of self-starters who are motivated by the excitement of being at the cutting edge of automation in every part of our organization—from trading, to business operations, to recruiting and beyond. We value openness and transparency, and celebrate great ideas from HRT veterans and new hires alike. At HRT we're friends and colleagues – whether we are sharing a meal, playing the latest board game, or writing elegant code. We embrace a culture of togetherness that extends far beyond the walls of our office.

Feel like you belong at HRT? Our goal is to find the best people and bring them together to do great work in a place where everyone is valued. HRT is proud of our diverse staff; we have offices all over the globe and benefit from our varied and unique perspectives. HRT is an equal opportunity employer; so whoever you are we'd love to get to know you."
528,Data Integration Engineer,Ellevation,Remote,"$70,000 - $80,000 a year","The Data Integration Team is responsible for ensuring data from school districts are loaded and maintained with a high level of fidelity into the Ellevation platform. This team plans and executes the solutions to integrate customer data into our SaaS educational platform, and designs solutions to simplify processes, save time and maximize the use of Ellevation by educators around the country. As a Data Integration Engineer, you will work directly with customers (data teams and administrators) and internal teams to support districts in the extraction of data, own the data transformation and import process, and ensure ongoing data fidelity within Ellevation’s platform.

This role is fast paced and will require one to take ownership, balance, and prioritize multiple projects at once. This role requires a combination of technical skills, organization, a service orientation, and the ability to communicate technical issues to non-technical partners. Just as important, we are looking for someone who is a self starter and motivated to continuously learn. This role requires one to be an excellent teammate who is willing to collaborate and provide input. Feedback is ingrained in our culture and feeling comfortable giving and receiving feedback is an integral part of the process to advance yours and Ellevation’s growth and sustainability.
Responsibilities
Writing and committing easy to read, reproducible code
Importing student, staff, schedule, and test files
Automating set up processes
Meeting Service Level Agreements for imports and troubleshooting tasks requested by partners
Prioritizing multiple projects at once
Designing solutions to automate and/or simplify integration processes
Within 1 month, you will:
Gain familiarity and knowledge of Ellevation, its team structure, and product/services
Know our mission, values, and product offering
Know teams and high-level responsibilities of each
Learn about our platform and have familiarity with our product offerings
Setup and Familiarize yourself with our internal tools and communication norms (Slack, GSuite, Jira, Wiki)
Attend PSO team overviews and shadow partner calls to further understand who the Data Integration Team (DIT) collaborates with on a daily basis and how we provide service to our partners
Set up a development environment on your computer to execute all Data Integration Engineer (DI Eng) tasks (python, SQL, git)
Start to become familiar with addressing DIT data processing and imports (Student, Schedule, Staff, ELP test, Grade file evaluations and uploads & Review and practice Python code development)
Within 3 months, you will:
Develop familiarity with our data model
Be able to navigate our DB and locate key tables relevant to DI Eng tasks
Develop proficiency in importing various data types, including standardized tests
Take on automated tasks
Learn how to commit code
Learn automation setup process
Familiarize with and begin contributing to DI Eng projects
Start to take on responsibility for Enterprise partner(s)
Take on more technical DI Eng project work
Within 6 months, you will:
Work independently
Troubleshoot data issues independently
Maintain SLA on your assigned work
Within 12 months, you will:
Continue to develop your technical skills
Maintain a high level of data fidelity for partners
Collaborate on new projects and initiatives
Work to improve our processes and code libraries
Automate manual tasks
About You
1-3 years experience working with many heterogeneous datasets and formats from a variety of vendors, providers and platforms.
Understanding complex data and translating issues into non-technical terms.
You have a knack for problem solving complex data discrepancies.
Demonstrated success and effectiveness working in and promoting a rapidly changing, collaborative, and time-critical work environment.
Interpersonal Skills
Attention to detail and strong written and verbal communication skills.
Customer service orientation, and an understanding of the importance of meeting customer needs and maintaining strong customer satisfaction.
Able to translate customer requests into technical solutions.
Commitment to continuous improvement and the mission; desire to focus talents on helping improve outcomes for others.
Collaborates well with others, and not afraid to ask for help when needed
Self Starter/Learner - Comfortable with ambiguity
Technical Skills
Proficiency with Python (Pandas Library or other scripting languages such as R)
Basic understanding of Coding/Programming Logic
Experience with Microsoft Office Excel or Google Sheets
Familiarity with SQL and RDBMS concepts (SQL Server, Oracle, MySQL, PostgreSQL, etc.)
Experience implementing prevention and detection controls to ensure data integrity
Statistics/data-study based experience
Working with, extending and defining data interchange standards and best practices
Experience with SFTP, FTPS and other file transfer protocols
Bonus: SIS/K-12 student data/Ed-Fi experience
Ellevation is transparent about its approach to pay which includes competitive base salary, annual performance-based raise, and bonus (or commission for sales positions). The salary range for this role is $70,000 to $80,000 with a 10% annual bonus potential and the budgeted target is the midpoint, commensurate with candidate experience and internal parity. Our philosophy is that the full range is indicative of growth during employment. In addition, we provide comprehensive benefits to all full-time employees.
#LI-AP1

About Us:
At Ellevation, we develop world-class software to help educators serve the fastest growing population of K-12 students: English Learners (ELs). Ellevation helps school districts transform their Multilingual Learner programs and ensure all students can achieve their highest aspirations. Our product suite includes a best-in-class data and instructional planning platform, resources to build teacher capacity, and student-facing programs to teach academic language.

We are a fast-growing, mission-driven technology company partnering with over 1,200 school districts and more than 3 million current and former English Learners. Over the next five years, we plan to double the number of ELs we serve and drive measurable outcomes for students. Ellevation merged with Curriculum Associates in 2021 to accelerate impact for Multilingual Learners. Our company continues to operate independently and is well-capitalized to support our ambitious social and financial objectives.

Why Ellevation?
In addition to our great benefits and competitive salaries, here are some things that make us unique:
+ Mission-driven organization where team members are empowered to make a significant impact
+ Opportunities to join DEIB-focused groups and support building a culture of belonging
+ Professional development and growth opportunities
+ Company and team offsites in various cities across the United States
+ Collaborative workspace in the heart of Boston - a stone's throw from many central T stops (Downtown Crossing, Government Center, and Aquarium). Free snacks, beverages, and local organic fruit provided
+ Remote and in-person engagement opportunities, including happy hours, themed events, and competitions
+ Remote and in-person wellness programming to support team’s mental and physical wellbeing

Learn more about our team in our Culture Deck .

Here at Ellevation, we champion diversity, inclusion, equity, and belonging. We strive to build a team that reflects the diverse communities we serve. We are committed to creating an inclusive workplace that promotes and values a range of ideas and opinions.

We believe in building a culture where productivity can flourish, one that is empathetic, respectful, and inclusive. We are proud to have been recognized in prior years as “Best Tech Workplace for Diversity” by the Timmy Awards for investing in training around inclusive behaviors, microaggressions, unconscious bias, and fostering a culture of continuous learning and feedback across the company. We are encouraged by our progress, but there’s more work to be done.

Benefits – Benefits eligible employees (and qualifying dependents) are covered by medical, dental, vision, and basic life insurance. Employees can enroll in our company’s 401(k) retirement plan and receive an employer match up to 7%. Employees have access to flexible paid time off for vacations, sick and disability policy, additional 10 paid company holidays, 2 floating holidays and a winter office closure between Christmas and New Year's. In addition, we provide a generous parental leave benefit, back-up childcare or eldercare, and a variety of other perks to support the health and well-being of our employees.

Additional Information: Ellevation operates under Curriculum Associates, LLC, an Equal Opportunity Employer. Curriculum Associates, LLC will not discriminate against any employee or applicant for employment because of race, color, creed, religion, sex, national origin, age, marital status, veteran status, sexual orientation, gender identity or expression, disability, genetic information, or any other category protected by law. Curriculum Associates, LLC will grant employment, without regard to race, color, creed, religion, sex, national origin, age, marital status, veteran status, sexual orientation, gender identity or expression, disability, genetic information, or any other category protected by law. Such action shall include, but not be limited to, the following: employment, upgrading, demotion, transfer, recruitment or recruitment advertising, layoff or termination, rates of pay or other forms of compensation.

Our company uses E-Verify to confirm the employment eligibility of all newly hired employees. To learn more about E-Verify, including your rights and responsibilities, please visit www.uscis.gov/e-verify.

Information that you provide when applying for employment with Curriculum Associates, LLC may be subject to the California Privacy Rights Act. Click here for more information about our data-collection practices and your rights related to that data.

#LI-REMOTE"
529,Senior Data Engineer - Remote U.S.,TTEC Digital,Remote,N,"At TTEC Digital, we coach clients to ensure their employees feel valued, and fully supported, because an amazing customer experience is an employee first process. Our vision is the same, a place where employees know they can thrive.

Looking to work with cutting edge technology and expand your skill set? Have a passion for data and analytics? You’ll be instrumental in building data products and big data solutions, that turn data into actionable insights, for our clients in automotive, healthcare, retail, and travel industry. Bring your curiosity to solve interesting engineering puzzles and develop tech-enabled analytics solutions that delivers personalization at-scale. Along the way, you’ll partner with the internal consulting, data science, and technology teams to provide feedback to improve the platform.

You’ll make an impact by building actionable solutions for our clients based on your analysis and subject matter expertise in data engineering discipline.
During a Typical Day, You’ll
Partner with leadership, engineers, data scientists, and strategic data consultants to refine data requirements and drive technical solutions
Be responsible for the end-to-end implementation of the technology stack from data collection to reporting, with a focus on data infrastructure and technical processes
Design, build and launch efficient and reliable data pipelines to move, ingest and process data from disparate applications including databases, APIs, message brokers and big data stores
Contribute to conceptualization, design and maintenance of data infrastructure and architecture
Implement and monitor quality control processes to ensure accuracy of data and reports
Proactively identify internal and external dependencies, issues, scope changes and progress against project plan
Stay on top of data trends; evaluate new solutions, technologies to evolve our data platform as new needs emerge
Effectively communicate and share your knowledge with global teams through mentoring, code reviews, pair programming and presentations
Develop and expand our knowledge-base and best practices for delivering data products
Provide technical support to assist clients during pre and post implementation
Initiate and foster partnerships with current clients, potential clients, and senior business executives
What You Bring to the Role
Bachelor's in Computer Science, Engineering, or other STEM fields required (Masters preferred)
3+ years professional experience building and supporting data and analytics solutions
Mastery of SQL, Python, PySpark
Experience with Scala, NoSQL, stream processing, SQL performance tuning, build/deploy automation and E2E process optimization
Skilled at designing & building APIs
Experience designing and building solutions on any public cloud environments and associated services (e.g., Data Factory, Airflow, Databricks, Serverless Functions, mlFlow, API gateways, load balancers, DevOps)
Knowledge of data structures, data validation, algorithms, and implications of architecture on software performance
Familiarity with automated testing frameworks and concepts
Exceptional problem-solving skills: demonstrated ability to understand business challenges, structure complex problems, develop solutions
Understanding of Agile Software Development Lifecycle and project planning/execution skills
Experience organizing large projects into manageable action items and communicating next steps to stakeholders
What you can expect
The anticipated range for individuals expressing interest in this position is $120,000 - $140,000 USD. Actual compensation offers to a candidate may vary based upon geographic location, work experience, education and/or skill levels. This position is eligible to participate in an incentive program.

Benefits available to eligible employees include
Medical, dental, and vision
Tax-advantaged healthcare accounts
Financial and income protection benefits
Paid time off (PTO) and wellness time o
#LI-MS3

About Us
TTEC Digital, and our 1,700+ employees, pioneer engagement and growth solutions that fuel the exceptional customer experience (CX). TTEC Engage is a 60,000+ employee service company, with customers in more than 80 countries. Together, we utilize a holistic approach, applying solutions from two centers of excellence, Engage and Digital.
TTEC is a proud equal opportunity employer where all qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, disability. TTEC has fully embraced and is committed to expanding our diverse and inclusive workforce. We strive to reflect the communities we serve while delivering amazing service and technology centered around humanity.
Rarely do applicants meet all desired job qualifications, so if you feel you would succeed in the role above, please take a moment and share your qualifications.

#LI-RemoteUS"
530,Software Engineer - Data,SpyCloud,"Austin, TX•Remote",N,"Software Engineer - Data
Austin TX, Remote
SpyCloud is on a mission to stop criminals from profiting off stolen information. Our solutions thwart cyberattacks that originate from the use of stolen credentials & PII, including account takeover, ransomware, and online fraud, and protect more than 2 billion accounts worldwide. We are looking for a Software Engineer. Cybersecurity is an exciting, evolving space, but being at the forefront of the fight to disrupt the cybercriminal economy makes SpyCloud a special place to work. If you're driven to create an experience that allows you to grow your career while connecting with a fantastic mission, look no further!
Reporting to the Engineering Manager, Data. The ideal candidate has professional experience working on scalable, data-heavy distributed systems, along with an excellent understanding of data modeling, data pipelines and big data processing and building solutions using Python. This is a remote role supporting a hybrid workforce in an estimated 40 hours per week.
We are looking for a passionate developer who can join our Data Team and contribute to our data parsing and ingestion platform.
What You'll Do
Create new features for our data processing platform that support business needs
Build and improve automation
Maintain and improve existing supporting infrastructure and code components
Assist with scheduled data ingest runs
Collaborate to develop stable, innovative data solutions
Participate in a fast-paced environment
Our Stack
Python
Bash or Shell scripting
NoSQL, SQL data stores
Linux
AWS (EC2, RDS, SQS, S3, Lambda, API Gateway, and much more)
Requirements
3 - 5 years of professional experience as a Python software developer
Experience with AWS - Compute, Storage, Database
Experience with designing scalable, data-heavy distributed systems
Strong understanding of computer science fundamentals (data structures, algorithms, basic cryptography)
Experience with databases (relational or NoSQL)
Familiarity with Bash or Shell scripting
Experience with Linux
Excellent communication skills
Highly self-directed, empathetic, curious, and flexible
Be self-motivated and be able to switch contexts as business needs chang
Nice to Have
Familiarity with a version control system. We use Git.
Prior big data experience
Familiarity with CI/CD
Familiarity with building REST APIs
We are not currently sponsoring Visas for candidates.
Benefits + Perks
As a mission-driven organization, we are committed to working alongside individuals who are equally passionate about preventing cybercrime regardless of what department they are in or role they may have. Being led by our core values in all business decisions, we absolutely lean into being united on this front, and in doing so we also look to ensure all SpyCloud employees have the support and benefits they need to stay focused on the mission. In addition to our engaging workspace in South Austin, flexible and remote-friendly work options, and competitive salary package, we also offer our full/part-time employees a comprehensive benefits package that includes:
401(k)
Generous PTO Plan
In-office meals provided
Who We Are
SpyCloud is a place for innovative, collaborative, and problem-solvers to thrive. Individually we're amazing but together, we're unstoppable. We celebrate diversity and various perspectives and aim to create an inclusive and supportive environment for all. We are proud to be an Equal Employment Opportunity and Affirmative Action ""at will"" employer of choice. All aspects of employment decisions will be based on merit, performance, and business needs. We do not discriminate on the basis of any status protected under federal, state, or local law. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics. Women, minorities, individuals with disabilities and protected veterans are encouraged to apply. SpyCloud complies with applicable state and local laws governing nondiscrimination in employment. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
SpyCloud expressly prohibits any form of workplace harassment. Improper interference with the ability of SpyCloud's employees to perform their job duties may result in discipline up to and including discharge.
SpyCloud shares the right to work and participates in the E-Verify program in all locations.
If you need assistance or accommodation due to a disability, you may contact us.
Our culture is something really special. We're all driven to disrupt the cybercriminal economy as we keep customer accounts safe from compromise. We support a truly worthy and serious mission, but we have fun doing it together. If you are driven, inventive, and collaborative, you'll fit right in.
SpyCloud's Recruitment Policy: We will never ask an applicant for sensitive or personal financial information during the recruitment process. We advise all applicants seeking employment with SpyCloud to review available information on recruitment fraud. Anyone who suspects that they have been contacted by someone falsely representing SpyCloud should email careers@spycloud.com.
About SpyCloud
SpyCloud transforms recaptured darknet data to protect businesses from cyberattacks. Its products operationalize Cybercrime Analytics (C2A) to produce actionable insights that allow enterprises to proactively prevent ransomware and account takeover, protect their business from consumer fraud losses, and investigate cybercrime incidents. Its unique data from breaches, malware-infected devices, and other underground sources also powers many popular dark web monitoring and identity theft protection offerings. SpyCloud customers include half of the ten largest global enterprises, mid-size companies, and government agencies around the world. Headquartered in Austin, TX, SpyCloud is home to nearly 200 cybersecurity experts whose mission is to make the internet a safer place.
Learn more and apply: https://spycloud.com/company/careers/"
531,Data Quality Engineer,Coca-Cola Bottling Company Consolidated,"Charlotte, NC 28211",N,"Requisition ID: 148590
Posting Locations: Charlotte

Click here to view a Day in the Life of our Teammates!

Our Secret Ingredient is our Teammates.
We offer great rewards, competitive pay, career advancement and growth opportunities.
Full Time Teammates are also eligible for:
Paid Training
Paid Time Off plus paid holidays
401(k) with Company matching on a dollar-for-dollar basis
Employee Stock Purchase Plan (ESPP)
Group Health Insurance – Medical, Dental, Vision & Disability
Basic and Supplemental Life Insurance

Refresh and Grow your Career with Us!
Job Overview
The Data Quality Engineer is responsible for defining, designing, and implementing the working components of a data quality program; performing data acquisition, manipulation, distribution, and analysis, while demonstrating an understanding of what data is within context and how it should be maintained; building automated rules and workflows to check the quality of the data; continuously monitor the quality of the data; and provide insights on how to prevent data quality errors in the future; and identifying process improvements and opportunities for automation.
Duties & Responsibilities
Documents and maintain data standards - Collaborates with process owners, subject matter experts, and data stewards to define and document what data means and the lifecycle it undergoes within the respective systems and processes for non-technical audiences.
Designs and builds data quality objects - Uses data governance systems to articulate business logic and continuously monitors the quality of the data and prevent data quality errors in the future.
Develops and builds training modules – Educates the organization to understand what data is and why it is used.
Facilitates process improvement design ( as it applies to data quality ) – Analyzing existing processes, proposing solutions, and handling change management with internal stakeholders.
Provides reporting and analytics related to data quality and governance
Knowledge, Skills, & Abilities
Experience working directly with non-technical audience with strong customer service skills and relationship-building skills. Strong communication skills, both written and verbal
Inquire and document data standards documentation ( Data Ownership, Data Profiles, Business Definitions, ERD diagrams, Data Flows, Business Process Flow Diagrams, Security Implications, Data Quality rules )
Must have a solid understanding of how to join and manipulate data within a database using SQL. Preferred experience with Databases (MS SQL, Snowflake, SalesForce, SAP, et al ), Approval Workflows, Data Governance software ( e.g. Talend / Palantir / MS Purview / Informatica )
Ability to dive into technical details with IT partners to communicate requirements, test, and implement technical infrastructure
Must be able to balance attention to detail/principle with delivering value to the business/action.
Team player
Minimum Qualifications
High school diploma or GED
Knowledge acquired through 1 to 3 years data work experience
Preferred Qualifications
Bachelor’s degree in Computer Science or Business Administration
Work Environment
Office environment
Coca-Cola Consolidated, Inc. is an Equal Opportunity Employer. Coca-Cola Consolidated, Inc. also participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S."
532,Data Engineer (Full Remote Role) – DE222-000691,"Evergreen Technologies, LLC.","New York, NY•Remote",N,"Title: Data Engineer
Position: Full Remote Role
Job Description:
In this role, you will provide the enterprise data design, models, tools and guidance to provide the business and IT with a view of how data is being utilized.
This starts with a strong semantic foundation all the way to identifying potential business process improvements.
As a Segment Data Architect, your key deliverables include Critical Data List, Data Affinity Maps, Enterprise Data Catalogues, Conceptual & Logical Data Models, Mastership Matrices, Enterprise Data Flow Diagrams, Data Principles, and Standards & Handrails.
In this role, you will be leveraging on your desire to bring enterprise data design to IT and business functions.
You will also require the passion to drive IT related business activity towards value-based outcomes built on a foundation of well-described quality data.
The Segment Data Architect brings together both business and IT stakeholders to capture and model a data landscape that supports business interests whilst enabling the effective IT management of data.
You will have full ownership of data domains, including being a key focal point for the Lead Segment Data Architect, Segment Architects, Solution Architects and Solution Data Architects.
Furthermore, your accountabilities are as follows:
Work with Solution and Segment Architects to review/approve data provisioning from a simplified access and governance perspective and adopt Data-as-a-Service (DaaS) approach to enable effective re-use, provisioning and sharing of data across the business.
Provide Solution Architecture an understanding of how the project fits into the segment architecture. Ensure compliance and alignment of the project design.
Maintain the Enterprise Conceptual, Logical, Integration Models and work with Solution Data Architects to ensure that project-related Logical and Physical Models remain aligned.
Understand the importance of Data Mastership and Data Flow mapping for the business, capturing both As-Is and To-Be states, and help identify the right data for the business.
Provide Solution Data Architects with Enterprise Conceptual/Logical Models, Mastership Matrices and Data Flows as input to projects as part of the delivery lifecycle We're keen to hear from IT professionals who has substantial experience in Data Architecture and Data Modelling.
Beyond that, we'd want to hear from individuals who have the following:
Proven Core Data Architecture skills including Data Analysis, Data Modelling (ERD), Big Data Design, Master Data Management patterns & implementation, Data Landscape Design (Data Flow, Mastership, API).
Extensive understanding of technologies involved in data management (Database management systems, data services, data lakes, etc.).
Proven knowledge of all architecture aspects and the ability to stay up to date with key developments in IT - affinity with the other Enterprise Architecture domains (process, applications, infrastructure, security, etc.).
A delivery and result oriented mindset with a hands-on attitude with a vision for the long-term implications of short to intermediate term decisions.
Ability to initiate, plan and develop according to plan, along with the ability to adapt to the circumstances on the ground that may have a level of uncertainty and risk.
Experience in Data Modelling tools such as PowerDesigner. While you experience in working with Enterprise Architecture tools such as BiZZDesign is not required, it will certainly be beneficial in your application.
Skills:
Ability to build and optimize data sets, 'big data' data pipelines and architectures
Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
Excellent analytic skills associated with working on unstructured datasets
Ability to build processes that support data transformation, workload management, data structures, dependency and metadata
Additional Skills:
PowerDesigner"
533,Senior Data Engineer - Remote,Mass General Brigham(PHS),"Somerville, MA 02145•Remote",N,"Senior Data Engineer - Remote
- (3235191)

Summary
As a member of the Enterprise Operations Insight team, the individual will be part of a fast-paced environment committed to delivering critical strategic and operating insights to key business executives and decision makers across the Mass General Brigham enterprise. This new analytics department within the MGB Office of the COO will be responsible for the delivery of data, analysis, and insights necessary to meet the patient clinical care delivery mission, vision, strategy, goals, and objectives of the MGB system, and sites and affiliates. The person will leverage Data Integration tools such as SQL, SSIS, Informatica, Snowflake and others to integrate data into MGB’s data resources for use with internal dashboards, reports and scorecards, as well as for adhoc analyses. The person will have responsibility for building and managing ETL jobs, job scheduling, and monitoring system performance. The incumbent will have a good understanding of data warehouse implementations as well as possess good data analysis, and SQL skills. This person will take on project management responsibilities to manage the development and implementation of new features and functions for projects that are small to medium in size. This person will need to work independently as well as part of a collaborative team.

Principle Duties and Responsibilities
Work with end users, EOI analysts, technical staff, and project team members to plan, design, develop, implement, and enhance business analytics capabilities through data integration of various Mass General Brigham and external data sources.
Perform data analysis, including data profiling into MGB data warehouses, to obtain a good understanding of data availability, relationships and nuances for consideration of ETL solution design.
Develop and maintain data integration/ETL jobs in order to deliver information requests and deepen the analytics capabilities of MGB operations, fiscal, and administrative staff.
Triage, troubleshoot and resolve data issues from end users and internal team members.
Develop and maintain standards pursuant to MGB’s architecture, report documents, dashboards, cubes, and other data warehousing development practices.
Partner with MGB Digital (IS) teams to leverage existing tools, methods, and processes in order to deliver timely information to operations stakeholders across MGB, including partnering on an overall platform redesign, leveraging Microsoft Azure, Snowflake, and Informatica cloud tools.
Create and/or perform peer review of Business ETL code from other analysts and programmers to ensure that all code and processes conform to MGB and EOI standards and guidelines.
Create and execute test plans and test scripts for unit and system testing of ETL code and reports, based upon requirements specifications.
Develop and enforce change management and versioning processes surrounding new releases of ETL jobs and evaluate and recommend new tools and methodologies to assist with change and version management.
Migrate ETL job changes between development, test, training, and production environments and maintain a proper development cycle.
Assist with performing upgrades of MGB’s data environment when necessary.
Assist with performing change impact analysis on code changes in order to assist with estimating level of effort required for code changes and testing.
Review current operational data structures and data flows and recommend optimizations and opportunities for automation.
Evaluate and recommend new tools and methodologies to assist in meeting business requirements through data integration and reporting.
Provide training to EOI team and end-users on key analytic and reporting systems and databases when necessary.
May represent EOI on internal and external committees or task forces as needed. The incumbent may play a role in presenting data and analyses to key stakeholder groups independently, as requested.
Performs all other related duties as required.

Skills and Abilities

Two (2) or more years of experience as an ETL developer or similar role working with complex SQL queries and extracting data.
Proven analytical and problem-solving skills, along with strong written and verbal communication skills.
Experience with relational database management systems (RDBMS), and an understanding of relational database design.
Experience with cloud environments such as Microsoft Azure and Snowflake preferred
Experience with data integration tools such as Informatica preferred
Excellent written and oral communication skills
Proven ability to pay attention to details in a fast-paced environment with multiple initiatives. Keeps track of various deadlines and is consistent in meeting them.
Ability to be flexible, versatile and adaptable in day to day activities conducted in a fast-paced environment.

Qualifications
Must have a bachelor’s degree preferably in the area of management information systems or similar; masters degree is beneficial.
Must possess strong oral and written communication skills.
Must be capable of working independently with limited to no supervision.
Must possess a strong background in data warehousing projects relative to data integration, showing progressive experience in this area.
Must be able to identify, triage, and resolve or dispatch issues.
Must be willing to contribute to and foster a team player culture where all are encouraged and willing to share information accurately.
Experience with cloud data warehousing environments such as Microsoft Azure and Snowflake are a plus
Experience with ETL tools such as Informatica, Data Stage, and Ab Initio are a plus
Must possess strong data analysis skills and be able to perform data analysis using SQL, SAS or similar query languages
Experience in Health Care a plus, but not required

EEO Statement

Mass General Brigham (formerly Partners HealthCare) is an Equal Opportunity Employer & by embracing diverse skills, perspectives and ideas, we choose to lead. All qualified applicants will receive consideration for employment without regard to race, color, religious creed, national origin, sex, age, gender identity, disability, sexual orientation, military service, genetic information, and/or other status protected under law.

Primary Location MA-Somerville-MGB Assembly Row
Work Locations MGB Assembly Row 399 Revolution Drive Somerville 02145
Job Business and Systems Analyst
Organization Mass General Brigham(PHS)
Schedule Full-time
Standard Hours 40
Shift Day Job
Employee Status Regular
Recruiting Department MGB Digital
Job Posting Mar 6, 2023"
534,Senior Data Engineer,Luxury Presence,United States•Remote,"$165,000 - $185,000 a year","About Us
Luxury Presence is a rapidly growing Bessemer-backed SaaS startup that enables real estate agents to manage their online presence. We are moving fast to be a one stop shop for everything from websites to marketing services. If building scalable data platforms that will be used by dozens of engineers and tens of thousands of customers interests you, then you would be a great fit for our data engineering teams.

Who is the Data Squad?
The Data team's primary focus is to provide accurate and consistent data to our platform and customers. We own the ingestion, transformation, and normalization of home listing data from Multiple Listing Services (MLS) and other sources. We maintain the “source of truth” for Luxury Presence to provide valuable and accurate listing data to our customers. We also support building out the data infrastructure for various BI/Data Science needs of the company & empower business leaders to make data-driven decisions.

Strategic projects you may work on in your first year include:
Design & build a platform to maintain high-quality listing data & drive operational excellence for Luxury Presence’s MLS data ingestion
Build tools and services to support data discovery, lineage & governance across the data platform and enable self-service analytics for all the stakeholders
Responsibilities
Build and maintain modern data pipelines that make data available and accessible to the entire company and our clients.
Partner with architecture and external development teams to design reusable frameworks and technical solutions.
Participate in the evaluation of emerging technology and tools.
Participate in code reviews, squad ceremonies, and team events (lunch & learn, brainstorms, etc.)
Skills And Qualifications:
5+ years of programming experience, preferably with one of Python, Java, NodeJS
Exposure to data warehousing concepts, ETL/ELT patterns & SQL query optimization
Familiarity with modern CI/CD practices and tools
Experience working with AWS or any other cloud providers
Proven success working in Agile environments (Scrum, Kanban, etc.)
Nice to Have:
Experience working with data frameworks such as Spark, Dbt, Iceberg & Hive
Experience with data warehouse technologies such as Snowflake/Athena/Redshift & orchestration tools such as Airflow
Familiarity or interest in establishing and leveraging data quality tools such as Great Expectations & Data Catalog/Observability tools
Experience working with messaging platforms such as Kafka/Kinesis/SQS
Applicants must be authorized to work for ANY employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.

Who we are: Luxury Presence is the real estate industry's most powerful marketing platform, providing award winning websites and cutting edge tech to the world’s top agents.
Founded in 2016 by Stanford Business School alumni Malte Kramer, Luxury Presence currently serves over 5000 clients in the U.S. and Canada with its SaaS model — including over 20 of the top 100 WSJ real estate agents and teams. In addition, Luxury Presence is the official website partner to some of the industry's most powerful brokerages.
The Los Angeles-based SaaS company raised $25.9 million for its Series B round. Bessemer Venture Partners led the round alongside fellow existing investors Toba Capital and Switch Ventures. Former Dallas Mavericks basketball player Dirk Nowitzki also participated in the round, along with other angel investors.
Its solutions include stunning website design, an engaging home search tool, an agent-to-agent listing referral network, powerful content & SEO strategies, expert-lead social media management, and digital advertising for lead generation. In 2020, Luxury Presence was recognized as a Best Place to Work by BuiltinLA and by Inc. as the 322nd fastest growing private company in America and then again in 2021 — LP ranked 598th.
Luxury Presence is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin."
535,Senior Data Engineer,Affine Inc,Remote,N,"Key Responsibilities:
Maintain and expand upon our paid marketing data by maintaining our ETL process, consuming data from third parties
Work with cross-functional stakeholders in defining and documenting single sources of truth to ensure consistent and high-quality data
Reconcile data issues and alerts between various systems, finding opportunities to innovate and drive improvements
Own and document data pipelines and data lineage
Work with the internal data team ETL production
Requirements:
3+ years of experience as a Data Engineer or in a similar role
3+ years in programming pipelines in languages like Python, Scala, Java
3+ years of broad software engineering experience including infrastructure and application development
Experience with data modelling, data warehousing, and building ETL pipelines from data available through external APIs
Expert-level knowledge of SQL.
Experience with Cloud technologies like AWS, GCP, Containers, Kubernetes. (GCP Preferred)
MS/BS in Computer Science, Systems Engineering or Electrical Engineering or Applied Mathematics or equivalent.
Job Type: Full-time
Benefits:
Health insurance
Paid time off
Compensation package:
Performance bonus
Yearly pay
Experience level:
4 years
Schedule:
Monday to Friday
Experience:
SQL: 3 years (Required)
Python: 3 years (Required)
ETL: 3 years (Required)
Cloud infrastructure: 2 years (Required)
Application development: 3 years (Required)
Work Location: Remote

Health insurance"
536,Real Estate Data Strategy & Analytics Engineer,Salesforce,"111 West Illinois Street, Chicago, IL 60654",N,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Products and Technology
Job Details
We’re Salesforce, the Customer Company. If you believe in bringing companies and customers together, in business as the greatest platform for change, in creating a more equitable and sustainable future for all – well, you’re in the right place. Through our #1 CRM, Customer 360, we help companies blaze new trails and connect with their customers in a whole new way. And, we empower you to be a Trailblazer, too — driving your performance and growth, charting new paths, and improving the state of the world.
At Salesforce, our unique office design is a powerful expression of who we are and what we care about. We want to make an impact on every person who enters our spaces by crafting and maintaining an inspiring environment. We dream up and build environments that create a sense of belonging. Together our team challenges the status quo to reimagine everyday experiences in Salesforce campuses and towers across the globe.

We are seeking an Analytics Engineer to join our team to help us manage and analyze real estate data including building occupancy, work orders, project management, and transactions. The ideal candidate will have a strong background in data engineering and analysis, and be familiar with dbt cloud and Snowflake.

Responsibilities:
Develop, maintain and optimize dbt data pipelines and data models to support real estate analytics
Work collaboratively with analysts to build the datasets required for our Tableau analytics
Work closely with cross-functional teams to identify business requirements and provide datasets
Ensure data accuracy, integrity, and availability by implementing appropriate data quality checks and monitoring processes
Continuously explore new technologies and data tools to improve analytics engineering processes and data analysis capabilities
Collaborate with data scientists and analysts to build predictive models and algorithms
Perform data extraction, transformation, and loading (ETL) processes on various data sources
Qualifications:
Minimum of 3 years of experience in data engineering, data analytics, or related roles
Strong experience with dbt cloud and Snowflake (or similar cloud warehouse)
Strong experience in SQL and data modeling
Degree or equivalent relevant experience required. Experience will be evaluated based on the core competencies for the role (e.g. extracurricular leadership roles, military experience, volunteer roles, work experience, etc.)
Strong communication skills and the ability to work collaboratively with cross-functional teams
Strong analytical and problem-solving skills
Experience with ETL processes, and data warehousing concepts
Knowledge of Python or other scripting languages
Experience working with real estate data including building occupancy, work orders, project management, and transactions
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form .
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at Salesforce and explore our benefits.
Salesforce, Inc . and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce, Inc . and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce, Inc . and Salesforce.org will not pay any third-party agency or company that does not have a signed agreement with Salesforce, Inc . or Salesforce.org .
Salesforce welcomes all."
537,"Senior Data Analytics Engineer, Remote Americas",Grafana Labs,Remote,"$116,000 - $140,000 a year","The Role
The Sr. Data Analytics Engineer will be responsible for empowering analysts and data-driven stakeholders by owning the architecture and infrastructure by which we ingest, transform, test, document, and connect data across Grafana Labs. Critical to the success of this role will be consistent communication with end stakeholders (analysts & business partners) in order to build a data stack that is flexible enough to meet the needs of our business as we grow.
Key Responsibilities
Own, operate and maintain the architecture & infrastructure of our data stack (e.g. BigQuery, dbt, Fivetran)
Ensure that the data we need to understand and serve our stakeholders is available, accurate, and accessible
Partner with GTM, Self-serve, Marketing, Finance, and Bizapps (and other stakeholders) to understand their data needs and identify places where the analytics stack can improve on what they’re doing
Become the go-to expert on our data pipeline, data tools, and what our data means
Create process and standard operating procedures for the Data team
Create and own best practices for version control, documentation, testing, etc.
Requirements
3-5 years experience contributing to data modeling
Experience with dbt
3-5 years experience in developing dashboards and visualizations
A passion for understanding business questions and making data driven insights. Excellent analytical skills.
Be able to understand the business at a level to influence VP priorities and company strategy.
Consistent track record of using quantitative analysis to impact key business decisions
Ability to present data concisely through written and oral communication. Expert at influencing business stakeholders
BS degree in Economics, Mathematics, Engineering, Statistics or relevant degree
Experience working in cloud environments such as AWS, GCP is a plus!
Familiarity with the monitoring, observability, log aggregation, or APM market is a plus!
What you’ll bring to the role
Strong analytical, data processing and problem-solving skills
Exceptional attention to detail
Strong verbal and written communications skills with the ability to effectively communicate with all levels of employees and provide support globally
Experience implementing best practices for analytics, dashboarding, documentation, testing
Experience building collaboration and rapport within an organization with both technical and non-technical members
Experience working in a “remote first” environment
Empathetic, understanding and humble
Strong work ethic
Experience working well in both a team environment and independently
Inquisitive demeanor with willingness to learn new technologies and responsibilities
In the United States, the Base (OTE for commission positions) compensation range for this role is $116,000 - $140,000. Actual compensation may vary based on level, experience, and skillset as assessed in the interview process. Benefits include equity, bonus (if applicable) and other benefits listed here.

About Grafana Labs: There are more than 950,000 active installations of Grafana around the globe, monitoring everything from beehives to climate change in the Alps. The instantly recognizable dashboards have been spotted everywhere from a NASA launch and Minecraft HQ to Wimbledon and the Tour de France. Grafana Labs also helps companies including Bloomberg, JPMorgan Chase, and eBay manage their observability strategies with full-stack offerings that can be run fully managed with Grafana Cloud, or self-managed with Grafana Enterprise Stack. The Grafana stack has grown to include four other open source projects, Grafana Loki (for logs), Grafana Tempo (for traces), Grafana Mimir (for metrics), and Grafana OnCall (for on-call management).
Benefits: For more information about the perks and benefits of working at Grafana, please check out our careers page.
A note about covid-19: All Grafanistas who wish to attend in-person events or travel for Grafana Labs must be fully-vaccinated.
Equal Opportunity Employer: At Grafana Labs we’re building a company where a diverse mix of talented people want to come, stay, and do their best work. We know that our company runs on the hard work and the dedication of our passionate and creative employees. If you're excited about this role but your experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways.
We will recruit, train, compensate and promote regardless of race, religion, colour, national origin, gender, disability, age, veteran status, and all the other fascinating characteristics that make us different and unique. We believe that equality and diversity builds a strong organisation and we’re working hard to make sure that’s the foundation of our organisation as we grow.
For information about how your personal data is used once you’ve applied to a job, check out our privacy policy ."
538,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
539,Data Engineer II,Cloudflare,"Austin, TX•Remote","$118,000 - $144,000 a year","About Us
At Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world's largest networks that powers approximately 25 million Internet properties, for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazine's Top Company Cultures list and ranked among the World's Most Innovative Companies by Fast Company.
We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!
About the team
The BI team builds and operates the cloud data analytics platform for Cloudflare. We are responsible for building a centralized cloud data analytics platform using open source technologies that will be used by our internal Business Partners and Machine Learning teams. Our goal is to democratize data, support Cloudflare's critical business needs, provide reporting and analytics self-service tools to fuel existing and new business critical initiatives.
About the role
As part of this initiative, we are looking for a Data Engineer and help us build a scalable petabyte scale data lake and Cloud Enterprise Data Warehouse (EDW) using modern tech stack from the ground up using open source technologies. Success in this role comes from marrying a strong data engineering background with product and business acumen to deliver scalable data pipelines and analytics solutions that can enable advanced analytics via a self-service user interface.
What you will do
Partner closely with internal stakeholders to gain a strong understanding of business and product data needs
Design, build and support scalable and reliable data solutions that can enable self-service reporting and advanced analytics using open source technologies
Develop technical tools and programming that leverage machine learning and big-data techniques to cleanse, organize and transform data and to maintain and update data structures and integrity on an automated basis
Design application components and evolve architecture: API/Services, data access, integration, application components, etc.
Analyze and support platform requirements for Data Science team
Implement automation tools and frameworks (CI/CD pipelines)
Build tools to automate the monitoring or workload and take proactive measure to scale the platform or to fix the problem
Mentor junior data engineers
Examples of desired skills, knowledge, and experience
Proven ability to work closely with business and product teams to ensure data solutions are aligned with business initiatives and are of high quality
2-5 years of development experience in Big Data space working with Petabytes of data and building large scale data solutions using any Cloud Platform, Apache Spark, Airflow, Scala, Python, etc
Experience with multiple POCs for different tools and platforms such as data lake solutions, data validation frameworks, etc
Experience with environment and deployment automation, IaaS, deployment pipeline specification and development
Experience with API design and development of RESTful web services or GraphQL
Working experience in Kubernetes, Docker etc.
Knowledge of and experience with backend frameworks like NodeJS or Golang
Bachelor's or Master's Degree in Computer Science or Engineering or related experience required
Compensation
Compensation may be adjusted depending on work location.
For Colorado-based hires: Estimated annual salary of $118,000 - $144,000.
For New York City-based and California (excluding Bay Area) and Washington hires: Estimated annual salary of $131,000 - $161,000.
For Bay Area-based hires: Estimated annual salary of $139,000 - $169,000.
Equity
This role is eligible to participate in Cloudflare's equity plan.
Benefits
Cloudflare offers a complete package of benefits and programs to support you and your family. Our benefits programs can help you pay health care expenses, support caregiving, build capital for the future and make life a little easier and fun! The below is a description of our benefits for employees in the United States, and benefits may vary for employees based outside the U.S.
Health & Welfare Benefits
Medical/Rx Insurance
Dental Insurance
Vision Insurance
Flexible Spending Accounts
Commuter Spending Accounts
Fertility & Family Forming Benefits
On-demand mental health support and Employee Assistance Program
Global Travel Medical Insurance
Financial Benefits
Short and Long Term Disability Insurance
Life & Accident Insurance
401(k) Retirement Savings Plan
Employee Stock Participation Plan
Time Off
Flexible paid time off covering vacation and sick leave
Leave programs, including parental, pregnancy health, medical, and bereavement leave
What Makes Cloudflare Special?
We're not just a highly ambitious, large-scale technology company. We're a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.
Project Galileo: We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare's enterprise customers-at no cost.
Athenian Project: We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.
Path Forward Partnership: Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.
1.1.1.1: We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here's the deal - we don't store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.
Sound like something you'd like to be a part of? We'd love to hear from you!
This position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.
Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.
Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107."
540,Data Engineer,Uplift,Remote,N,"Uplift’s mission is to help people get more out of life, one thoughtful purchase at a time. Our enterprise Buy Now, Pay Later solution is used by the world’s most loved brands including Southwest Airlines, Carnival Cruise Line, Universal Studios, and more. With flexible pay over time installments, we empower consumers to buy what matters most while unlocking higher conversions and customer lifetime value for our partners.

Our team is rapidly growing and comes from diverse backgrounds of leading technology and financial brands. Our HQ is in Sunnyvale, California with offices in Toronto, Ontario, New York, Reno, Nevada, and Guadalajara, Mexico.

Working at Uplift allows you to push your limits, challenge the status quo, and collaborate with some of the brightest minds in the industry. We’re committed to building a diverse team and inclusive culture and believe your potential should only be limited by how big you can dream. We make this a reality by empowering you with the tools, resources, and support you need to grow your career.

Uplift is seeking a Data Engineer to join our Data Engineering team. You will be responsible for constructing a new data lake on Databricks and optimizing existing data use cases, from Business Intelligence to Machine Learning operations. As a member of our team, you'll have the opportunity to work with cutting-edge technologies like Spark, Delta Lake, and ML flow. We invite you to come help us build an outstanding data platform for all of our business needs.
What you will do and achieve
Design, build, and maintain the ETL using AWS and Databricks
Develop and maintain near real-time data pipelines
Build Python libraries, tools, serverless applications and workflows
Creates applications using test-driven development and agile methodologies
Internal process improvements such as automating manual processes, building alerting/monitoring bots
Support daily operations of troubleshooting of Databricks and Snowflake jobs
Collaborate closely with product and business teams to influence technological decision-making and to support reporting needs
Work with analysts and data scientists to extract actionable insights from data that shape the direction of the company
Actively engage in design and code reviews - learn from your peers and teach your peers
Who you are
3 years of related experience with a Bachelor’s degree; or 2 years and a Master’s degree
Experience with Big Data, ETL, and data modeling
Solid coder with Python
Strong SQL knowledge and experience working with a variety of databases
Experience with modern cloud data warehouses (preferably Databricks or Snowflake)
Knowledge of Linux, AWS, and Docker
Experience in developing and operating high-volume, high-available and scalable environments
Working knowledge of API and stream based data extraction processes
Ability to align with rapid business changes: new requirements, evolving goals and strategies and technological advancements
Experience supporting and working with cross-functional teams in startup culture
Life at Uplift
Health Insurance and 401k plan: some plans cover 99-100% premiums for medical, dental, and vision insurance and a 401k plan
Work/Life Harmony: Flexible, remote-first work culture. Uplift fosters a culture where employees can achieve both their professional and personal goals. This balance is especially true for our working parents
Shared Success: competitive salary and Pre-IPO stock options
Health and Wellness Perks: Uplift is proud to reimburse our employees for exercise, wellness products and activities as well as free counseling and coaching for physical, mental and emotional support
Professional Development: We are committed to the growth and development of all of our employees. Uplift invests in professional conferences, certifications, and training for employees who want to grow in their careers
Pick-A-Perk: money that can go towards something of your choosing within tuition reimbursement, student loan payment reimbursement, vacation savings account, charitable donations, or home office expenses
At Uplift, in accordance with the U.S pay transparency laws across various states, we provide the minimum and maximum pay range for each of our levels. This reflects the minimum and maximum ranges for new hire salaries. There are two ranges that reflect positions across all US locations. The first range includes: New York and California whereas the second range includes all other US states. Within each range, there are several different factors that are considered when determining compensation including but not limited to the candidate's location, title, years of experience, job related skills and relevant education. In addition to the base salary, we offer competitive benefits including medical, dental and vision and pre-ipo stock options.

$120k to $168k
$104k to $145k

We want you!

If you made it this far, chances are you’re as excited about working to change how people experience BNPL as we are — and we love that. Please apply even if you’re unsure about whether you meet every single requirement in this posting. Uplift is looking for smart, intellectually curious people who are invested in our mission, not just those who can “check all the boxes”.

Uplift is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.

Note: Uplift does not accept agency resumes. Please do not forward resumes to any recruiting alias or employee. Uplift is not responsible for any fees related to unsolicited resumes."
541,Data Engineer I,HEARTLAND BUSINESS SYSTEMS LLC,Remote,N,"Description:
Position Summary:
This position would require a candidate to possess a strong technical background in developing and delivering BI solutions along with a strong understanding of SQL Server environments. Business intelligence (BI) is a set of technologies and practices for transforming business information into actionable reports and visualizations. The BI Data Engineer transforms data into a useful format for analysis and is focused on the design and architecture.
A BI Data Engineer is the data professional who prepares the data infrastructure to be leveraged by the HBS BI Data Developers. The BI Data Engineer will design, build, integrate data from various resources and manage big data. The BI Data Engineer ensures the operations of the data pipeline follow a consistent process of Ingestion, Processing, Storage and Access. The work involves tuning databases for fast analysis and creating table schemas.
The BI Data Engineer is responsible for making data easily accessible, ensuring the process works smoothly and is optimized. The BI Data Engineer is a critical firm member of the Data Team, The BI Data Engineer will run Extract, Transform and Load (ETL) on top of datasets and create data warehoused that can be used for reporting and analysis. The BI Data Engineer ensures the operations of the data pipeline follow a consistent process of Ingestion, Processing, Storage and Access.
Roles and Responsibilities/ Essential Functions:
Meet with clients to understand their current business processes and needs to provide consulting services and direction on how to build or grow their current data strategy.
Work with HBS Sales Solutions consultants to identify and grow opportunities within HBS client environments.
Support and administer the underlining infrastructure and layout of a client data environment.
Develop and design the process for the customer data collection process.
Develop policies and procedures for the collection and analysis of data.
Review customer sources to ensure integrity of the data collection process.
Collaborate with the BI Data Developers to ensure the requirements are being met to build the right solution needed.
Estimate development effort required to deliver data customer needs and requests.
Use business analysis skillset to identify development needs for the purpose of streamlining and improving the operations of the organization for efficiency and profitability.
Ability to work independently or as a team on project-based solutions for clients.
Work with team mates to continue to grow and mature data services and delivery options for HBS clients.
Based on experience, one may mentor other engineers in developing scalable, secure, high-performance BI and Data solutions.
Minimum of 1,238 hours billed per fiscal year prorated based on start date. These charge hour requirements will be balanced against professional development and on the job training.
Requirements:
Competencies
Accuracy – Ability to produce high quality work deliverables leveraging industry best practices.
Analytical Skills - Strong abilities require to effectively interpret customer business needs and translating them into application and operational requirements, resolving complex technical and business problems.
Communication – strong written, verbal, and non-verbal communication skills, especially conveying complex information in an understandable manner.
Leadership – Ability to motivate and guide others to ensure performance is an accordance with clear expectations and goals.
Learning – Ability to quickly learn new technologies to deliver solutions.
Presentation Skills – Ability to effectively conduct formal and informal presentations in both small and large group settings within all levels of a company.
Project Management – Ability to demonstrate an understanding of process engineering, planning, organizing, staffing, directing, and controlling work tasks.
Time Management – Ability to effectively utilize available time for managing multiple tasks/projects simultaneously.
Required Experience:
Experience with needs analysis, software evaluation and selection, customization, and implementation
Microsoft BI Suite Experience
Microsoft Excel
Programming and Processing Experience
T-SQL
ETL
Azure Experience
Azure SQL Database
Strong knowledge of SQL utilizing MS SQL Server
Preferred Experience:
Expertise in Professional Services or similar client facing roles
Experience in multiple industry (Education, Healthcare, Retail, Manufacturing) verticals
PowerShell knowledge and understanding
Understanding of report writing and visualization
Data Warehousing systems and architecture experience in 'real world', practical, successful implementations
Understand multi-dimensional database structures and schemas
Strong knowledge of system design, development, and deployment
Azure Experience
Azure SQL Manage Instance
Azure Architecture
Azure Data Factory
Microsoft BI Suite Experience
Microsoft SQL Server Reporting Services (SSRS)
Microsoft SQL Server Analysis Services (SSAS)
Microsoft SQL Server Integration Services (SSIS)
Microsoft Power BI
Required Skills, Education and/ or Certifications:
Bachelor’s degree in business or I.T. related discipline accepted
Equal Opportunity Employer - Including Disabled and Veterans
#HBS"
542,Associate Data Engineer,Tailored Brands,United States•Hybrid remote,N,"At Tailored Brands, we help people love the way they look and feel for their most important moments. Our Technology team loves the way they feel and thrive at work, with:
Flexible work opportunities, including remote and hybrid options
Small, empowered teams that have fun delivering real value for our customers
A culture that values a 50-year legacy while eagerly embracing the future
Want to be part of this? We currently have an exciting opportunity for a Associate Data Engineer to join our Data Engineering team consisting of developers from a wide array of backgrounds.
What you’ll do
Maintain current ETL processes and resolving daily ETL job failures as they arise and also building new ETL jobs as needed.
Resolve Ad-hoc data questions from the organization in a timely and accurate manner
Improve continuously current ETL processes to ensure accuracy, timeliness and scalability as data volumes grow
Work with end users and collect the requirements.
Automate manual data flows for repeated use and scalability
Develop data-intensive applications with API’s and streaming data pipelines
Prepare and transform data into a usable state for analytics
Work with DataStage, Tableau, Microstratgey, Uc4 and Airflow
Document and maintain source-to-target mappings and data lineage
Identify opportunities for data improvements and presents recommendations to management
Perform data profiling and data modelling
Create, schedule and maintain automation jobs using Uc4 and Airflow


What you’ll bring
Bachelor's degree or higher in Computer Science, or related technical field
0-2 years’ experience with dimensional modelling architecture and with at least one Business Intelligence tool such as Tableau or MicroStrategy
Programming experience with Python and Shell Scripting
Hands-on experience with SQL Queries
Great numerical and analytical skills
Excellent communication skills
If you see yourself in this role, but fall short in any areas above, apply anyway! We love to invest in our team members!
The Best Kept Secret in Retail
Tailored Brands is North America’s leading specialty retailer of menswear, famous for world-class customer service and unique offerings like tailoring and custom suits. We are best known by our brands, including Men’s Wearhouse, Jos. A. Bank, K&G Fashion Superstore, and Moores Clothing for Men in Canada.
We are emerging from the pandemic with incredible momentum. We own a unique and special market segment experiencing record-level demand. This has inspired our 3-year journey to become Legendary; an exciting strategy that leans heavily upon a strong technology foundation.
With over 1,000 stores across the US and Canada, 15,000 employees, and over 35 million customers, we’re the perfect size! Large enough to demand leading-edge enterprise-worthy technology, but small enough to move quickly and empower our teams with significant scope & autonomy.
Apply now to power your career journey to Legendary!
Benefits
This role is eligible for health, dental and vision insurance, prescription drug, retirement savings (401k, employer-funded retirement plans, deferred compensation, and other defined-benefit or defined-contribution), life insurance, accident and disability, paid time off for sick leave, vacation (80 hours per year), bereavement, jury duty, holidays (9 days per year), wellbeing program, commuter, adoption assistance, legal services, and employee merchandise discounts.
Work-Life Balance
We understand the demands of work, school, family, and personal responsibilities. Through our work-life resources and programs we offer services for every stage of life to help you manage the day-to-day needs. We offer programs such as:
Meeting-Free Fridays (encouraged) | so you can catch up on work and self-development
Summer Fridays | from Memorial Day to Labor Day so you can enjoy a head-start to the weekend
Holiday Early Departure | close out early the business day before a company observed holiday
Work Environment, Physical & Mental Demands
Ability to sit and work at a computer keyboard for extended periods of time
Ability to stoop, kneel, bend at the waist, and reach daily
Able to lift and move up to 25 pounds occasionally
Must utilize visual acuity, speech and hearing, hand and eye coordination and manual dexterity necessary to operate a computer and office equipment
Hours regularly 40 hours per week, as work dictates, from a hybrid location near our Houston Corporate Office.
Note: To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed are representative of the knowledge, skill, and/or ability required and are not intended to be an exhaustive list of all duties, responsibilities or qualifications associated with this job.


Work Locations: 01099I IT Dept. 6380 Rogerdale Rd Houston 77072
Job: Information Technology (IT)
Organization: Tailored Shared Services
Shift: Day Job"
543,DATA ENGINEER (REMOTE OPPORTUNITY),N,"Chicago, IL•Remote",N,"Summary
The Opportunity
At Hyatt, we’re working to Advance Care through data-driven decisions and automation. This mission serves as the foundation for every decision as we create the future of travel. We can’t do that without the best talent – the talent that is innovative, curious, and driven to create exceptional experiences for our guests, customers, owners, and colleagues. Hyatt seeks an experienced Data Engineer who will be an exceptional addition to our growing engineering team. The Data Engineer will work closely with engineering, product managers, and data science teams to meet the data requirements of various initiatives in Hyatt. As a Data Engineer, you will take on big data challenges in an agile way. In this role, you will build data pipelines that enable engineers, analysts, and other stakeholders across the organization. You will build data models to deliver insightful analytics while ensuring the highest standard of data integrity. You will integrate different data sources, improve the efficiency, reliability, and latency of our data system, help automate data pipelines, and improve our data model and overall architecture. You will be part of a highly visible, collaborative, and passionate data engineering team and will be working on all the aspects of design, development, and implementation of scalable and reliable data products and pipelines. Applying the latest techniques and approaches across the domains of data engineering, and machine learning engineering isn’t just a nice to have, it’s a must. This candidate builds fantastic relationships across all levels of the organization and is recognized as a problem solver who looks to elevate the work of everyone around them.

Who We Are
At Hyatt, we believe in the power of belonging and creating a culture of care, where our colleagues become family. Since 1957, our colleagues and our guests have been at the heart of our business and helped Hyatt become one of the best, and fastest growing hospitality brands in the world. Our transformative growth and the addition of new hotels, brands and business lines can open the door for exciting career and growth opportunities to our colleagues.
As we continue to grow, we never lose sight of what’s most important: People. We turn trips into journeys, encounters into experiences and jobs into careers.

Why Now?
This is an exciting time to be at Hyatt. We are growing rapidly and are looking for passionate changemakers to be a part of our journey. The hospitality industry is resilient and continues to offer dynamic opportunities for upward mobility, and Hyatt is no exception.

How We Care for Our People
What sets us apart is our purpose—to care for people so they can be their best. Every business decision is made through the lens of our purpose, and it informs how we have and will continue to support each other as members of the Hyatt family. Our care for our colleagues is the key to our success. We’re proud to have earned a place on Fortune’s prestigious 100 Best Companies to Work For® list for the last ten years. This recognition is a testament to the tremendous way our Hyatt family continues to come together to care for one another, our commitment to a culture of inclusivity, empathy and respect, and making sure everyone feels like they belong.

We’re proud to offer exceptional corporate benefits which include:
Annual allotment of free hotel stays at Hyatt hotels globally
Flexible work schedule and location
Work-life benefits including well-being initiatives such as a complimentary Headspace subscription, and a discount at the on-site fitness center
A global family assistance policy with paid time off following the birth or adoption of a child as well as financial assistance for adoption
Paid Time Off, Medical, Dental, Vision, 401K with company match

Our Commitment to Diversity, Equity, and Inclusion
Our success is underpinned by our diverse, equitable and inclusive culture and we are committed to diversity across the board—from who we hire and develop, organizations we support, and who we buy from and work with.
Being part of Hyatt means always having space to be you. Our global teams are a mosaic of cultures, ethnicities, genders, ages, abilities, and identities. We constantly strive to reflect the world we care for with teams that achieve and grow together. To learn more about our commitments to DE&I, please visit the Why Hyatt section of the Hyatt career page.

Who You Are
As our ideal candidate, you understand the power and purpose of our Culture of Care and embody our core values of Empathy, Inclusion, Integrity, Experimentation, Respect, and Well-being. You enjoy working with others, are results driven and are looking for a variety of opportunities to develop personally and professionally.

The Role
Collaborate with product managers, data scientists, engineering, and program management teams to define product features, business deliverables, and strategies for data products
Collaborate with business partners, operations, senior management, etc on day-to-day operational support
Support operational reporting, self-service data engineering efforts, production data pipelines, and business intelligence suite
Interface with multiple diverse stakeholders and gather/understand business requirements, assess feasibility and impact, and deliver on time with high quality
Design appropriate solutions and recommend alternative approaches when necessary
Work with high volumes of data, fine-tuning database queries and able to solve complex technical problems
Contribute to multiple projects/demands simultaneously
Work in a fast-paced, collaborative, and iterative environment
Exercise independent judgment in methods and techniques for obtaining results
Work in an agile/scrum environment
Use state-of-the-art technologies to acquire, ingest and transform big datasets
The ideal candidate demonstrates a commitment to Hyatt's core values: respect, integrity, humility, empathy, creativity, and fun.
Qualifications
Qualifications
2 to 5+ years of experience within the field of data engineering or related technical work including business intelligence, analytics
Experience and comfort in solving problems in an ambiguous environment where there is constant change. Have the tenacity to thrive in a dynamic and fast-paced environment, inspire change, and collaborate with a variety of individuals and organizational partners
Experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business
Very good understanding of the full software development life cycle
Very good understanding of Data warehousing concepts and approaches
Experience in building Data pipelines and ETL approaches
Experience in building Data warehouses and Business intelligence projects
Experience in data cleansing, data validation, and data wrangling
Hands-on experience in AWS cloud and AWS native technologies such as Glue, Lambda, Kinesis, Lake Formation, S3, Redshift
Experience using Spark EMR, RDS, EC2, Athena, API capabilities, CloudWatch, and CloudTrail is a plus
Experience with Business Intelligence tools like Tableau, Cognos, ThoughtSpot, etc is a plus
Hands-on experience building complex business logic and ETL workflows using Informatica PowerCenter is preferred
Experience in one of the scripting languages: Python or Unix Scripting
Proficient in SQL, PL/SQL, relational databases (RDBMS), database concepts, and dimensional modeling
Strong verbal and written communication skills
Demonstrate integrity and maturity, and a constructive approach to challenges
Demonstrate analytical and problem-solving skills, particularly those that apply to Data Warehouse and Big Data environments
Open-minded, solution-oriented and a very good team player
Passionate about programming and learning new technologies; focused on helping yourself and the team improve skills
Effective problem-solving and analytical skills.
Ability to manage multiple projects and report simultaneously across different stakeholders
Rigorous attention to detail and accuracy
Bachelor’s degree in Engineering, Computer Science, Statistics, Economics, Mathematics, Finance, or a related quantitative field
The position responsibilities outlined above are in no way to be construed as all-encompassing. Other duties, responsibilities, and qualifications may be required and/or assigned as necessary"
544,Project Engineer/Data Science,"Hottinger, Bruel & Kjaer","Southfield, MI",N,"This job will provide you with an opportunity to further your career alongside some of the best and most passionate technology experts from around the world in a leading company within the test, measurement and data analytics industry. You will be a strong contributor collaborating closely with colleagues from various business functions.
At HBK, we live up to our three values: Be True, Own It and Aim High. We believe in absolute integrity – it’s how we win for stakeholders, the environment and each other. We believe in teamwork and keeping our promises – to ourselves and others. Finally, we believe in being bold and positive. This is how we perform at our best and achieve greater success.
The position
Hottinger Bruel & Kjaer Solutions LLC is looking for a high caliber individual with a passion for solving engineering problems using engineering principles, data analytics and computer programming. This individual should possess solid engineering and data analytics skill sets. In this position, you will be working with a highly energetic team of engineers, researchers, developers, and sales teams to develop and deploy solutions focused on improving reliability, durability and availability of our customer assets (vehicles, aircrafts, machines, structures, etc.), delivering actionable information and providing solutions to customer problems. If you are eager to demonstrate your potential, solve problems for world-class customers and work with a team of subject matter experts, software developers, engineers and data scientists we would like to hear from you.
Primary responsibilities
You will be responsible for:
Analyze asset data (usage, failures, maintenance, test, cost, etc.) using engineering, mathematics, statistics, machine learning (ML), etc. for design, operations and maintenance
Apply methodologies associated with reliability and/or durability to solve customer problems
Collect and prepare data sets, including data processing, cleansing, verifying data integrity, and combining data from multiple sources
Research, develop and apply advanced analytics/algorithms using common open source tools
Interpret results to draw conclusions and provide recommendations
Prepare reports/presentations for project deliverables and internal documentation
Other duties as needed
Professional qualifications
US Citizen or Green card holder (Requirement - no exceptions)
Advanced Degree in Engineering
Experience with engineering concepts and data preparation
Excellent analytical, problem solving, and communication and collaboration skills
Experience with computer programming, including python, in developing analytical solutions
Working knowledge of ML concepts, including data preparation and visualization, with experience in ML model development (including feature engineering, training and prediction), deployment and management
Desire to continuously learn ML core concepts, application, and deployment skills as this technology evolves
We offer
The job will provide you with an opportunity to further your career alongside some of the best and most passionate technology experts from around the world in a leading company within the test, measurement and data analytics industry. You will be a strong contributor collaborating closely with colleagues from various business functions.
At HBK, we live up to our three values: Be True, Own It and Aim High. We believe in absolute integrity – it’s how we win for stakeholders, the environment and each other. We believe in teamwork and keeping our promises – to ourselves and others. Finally, we believe in being bold and positive. This is how we perform at our best and achieve greater success.
One company – HBK
Hottinger Brüel & Kjaer (HBK) is a global leader in the fields of sensors, data acquisition, analytics and collaboration for various R&D, production and in-operation applications.
Until the end of July 2020, the companies were known as Hottinger Baldwin Messtechnik GmbH (HBM) and Brüel & Kjær Sound & Vibration Measurement A/S respectively.
HBK is a subsidiary of Spectris plc and employs around 3000 people worldwide. Our product eco system covers all layers from sensors, electronics, to software and collaboration. Our customers range from end users of the entire tool chain focusing on analytics and results in virtual testing, physical testing, and monitoring, to our OEM and system integrator partners and customers integrating our products into their own offering and solution. The product portfolio is as versatile and varied as our customer base covering many industries.
We have engineering and production facilities in Germany, Denmark, UK, Portugal, USA and China and are represented in over 80 countries worldwide. We are proud to be one of the top three suppliers worldwide in our market segments served, thanks to our high-quality products and the commitment of our employees.
Application deadline
Please submit your application and CV by using the direct application link.
Please note that we will be conducting interviews on an ongoing basis."
545,Human Factors Design Engineer/ Data Science,Apple,"Austin, TX",N,"Summary
Posted: Feb 21, 2023
Weekly Hours: 40
Role Number: 200464393
Apple is currently seeking an enthusiastic team player to join a dynamic and collaborative group as a data scientist on a human factors design team. You’ll have the unique opportunity to contribute to the development and continuing improvement of Apple hardware products by distilling data into actionable and intuitive visualizations from human interaction perspective. As an Austin-based team member, you will collaborate closely with Cupertino-based cross-functional team members.
Key Qualifications
Candidates must be able to work independently and excel in a dynamic, hands-on, cross-functional and creative corporate environment. Ideal candidates will possess an energetic personality with excellent communication and interpersonal skills, attention to detail, patience, and aesthetic sensibility.
This position requires strong programming skills in Python with experience working with multivariate datasets to import, clean, analyze and visualize data to enable data-driven decisions. Knowledge of population studies, dimensionality reduction, and and big data analysis is desired. Datasets include human anthropometric and biometric data as well as user study research data.
Ideal candidates have experience analyzing and visualizing data sets in prediction and probabilistic models (e.g., Bayesian network models, Markov random field models). Candidates are enthusiastic about delving deeper into 3D data analysis and visualization. Most importantly, a great passion for creating informative visualizations and presentations to communicate results to all levels of the company.
Description
As a member of this team-oriented, collaborative group, you will be working on the latest, most innovative Apple products together with many cross-functional teams. Your responsibilities will include the following activities: Write scripts to pre-process data from various sources and formats. Process, clean, and verify the integrity of data. Investigate new sources that can extend and improve insights. Conduct data exploration on datasets, QA/QC, visualization, statistical and data analyses, and mathematical modeling. Create custom visualizations that help decision makers quickly understand the data. Collaborate with cross-functional teams to help identify trends in their data or to build tools that will enable easier analysis and visualizations in the future. The role will require close collaboration with human factor design engineers, computer vision scientists, data scientists, and product engineers with many opportunities to practice and grow communication skills. You’ll have a lot of latitude to define how you work; specific tools matter less than the lateral thinking they enable. This is a truly unique opportunity that exposes you to many different teams at Apple.
Education & Experience
Masters degree in an analytical field such as (but not limited to) Computer Science, Engineering, Statistics, Applied Math, Physics, Biological Sciences, Robotics, Climate or Environmental Science, along with 2+ years relevant work experience. Ph.D. degree preferred.
Additional Requirements"
546,Data Engineer,Ideal Business Advisors,"McKinney, TX","$130,000 - $180,000 a year","The Data Engineer will construct and leverage a unified data and analytic platform to deliver consistently valuable business insights. The Data Engineer implements domain driven designs of Data Models, Data APIs and Data Products. Responsible for collecting, translating, and validating data for analysis.
RESPONSIBILITIES
Utilizes a broad set of skills ranging from programming to database design and system architecture
Develops solutions in conformity with the enterprise architecture priorities and direction to enable business imperatives
Executes modern data engineering principals and data modernization practices, following target state architecture and roadmaps
Develops enterprise solutions related to operational data stores, data warehouses, analytics, and business intelligence
Implements and maintains the database systems, ensuring that the systems provide the upmost efficiency, accuracy, and availability in support of the company’s goals and strategies
Provides expertise across the Data & Analytics domain including data storage (Data Lake & Data Warehouse), data integration, business intelligence, artificial intelligence & machine learning
Prepares technical design specifications and other documentation
Designs and develops Data Models, Data Pipelines, Data Warehouses, Data Lakes, ETL/ELT processes
Ensures data is secure and compliant under regulations
Continuously improves efficiency, accuracy, and effectiveness of the data
REQUIREMENTS
Education, Experience & Minimum Requirements
Bachelor’s degree in computer science, data science, or relevant field: Master’s degree preferred.
A minimum of 10 years of designing, building and managing modern enterprise data.
Banking, insurance, or other similar financial services experience strongly preferred.
Technical Competencies
Excellent knowledge of best practices, industry trends and compliance requirements in Data Modeling, Governance and Data Management
Strong knowledge of data profiling, Data standardization, transformation, Data cleansing
Big Data architecture and knowledge of tools and technology (Big Data / Hadoop HDFS, MapReduce, Hive, HBase, Sqoop, Flume, Spark) /AWS /Bigdata platforms products)
Strong experience building big data solutions using Data Lake, Data Warehouse, Data Products
Expert level understanding of different data storage technologies, and appropriate use cases for each – Relational, NoSQL, Time-Series, Graph, etc.
Expert level understanding of different data-integration technologies – ETL/ELT, Streaming, Replication, etc.
Experience in data streaming technologies
Azure (SQL, Data Factory, Data Hub, etc.)
GCP (AI/ML, Big Query, etc.)
Containerization (i.e., Kubernetes)
Graphite, Splunk, Kafka, Snowflake
API management/gateway, JSON, Python, Query tuning
Functional Competencies
Excellent interpersonal, written, and verbal skills
Strong strategic mindset in connecting business drivers and a foundational understanding of company data in support of data driven approaches to analytics and insights
An agile, adaptable mindset that enables effective complex problem solving
Ability to balance competing priorities while maintaining professional discretion
Strong attention to detail
Job Type: Full-time
Pay: $130,000.00 - $180,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
McKinney, TX: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person

Health insurance"
547,Data Engineer,GEMINI INDUSTRIES,"Fairfax, VA",N,"POSITION DESCRIPTION
Gemini Industries Inc. provides technical, management and operations services to support National Security projects. We provide rapid response to the critical needs of our customers and those they serve. We perform analyses and develop operations plans to anticipate and prepare for the future. And we deliver advanced technology to improve our customer’s success in executing its mission.
Gemini seeks innovative, results-oriented individuals with the creativity, initiative, and intelligence to overcome any challenge and succeed. Members of the Gemini team thrive in a culture that is anticipatory, agile, and schedule-driven; with a sense of urgency and a drive to succeed. Our culture involves:
· The best and brightest personnel
· Work at a high operations tempo
· Integrated teams delivering rapid solutions
· An attitude that balances “I can make it better” with “As long as we succeed”
Position: Data Engineer
Location: Fairfax, VA
Clearance: TS/SCI
Education: Bachelor’s Degree in Applied Data Engineering or Science, and Computer Science fields of study, such as statistics, mathematics, data analytics, data science, computer science or other technology related fields.
Outcomes:
Develop and implement data engineering eco-systems and architectures that enable data Extraction, Transformation and Loading (ETL) operations for predictive and prescriptive data modeling.
Will have the ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions.
Build networking connections and opportunities for data acquisition.
Responsibilities:
The Combatant Command Intelligence Enterprise Management Support Office (CCI EMSO) requires a Data Engineer to implement methods to improve data reliability and quality. The Data Engineer will combine raw data from different data sources to create consistent and machine-readable formats that will be passed to the Data Science Team for Data Science Analysis and Studies, with a specific focus on creating and engineering data pipelines that can transfer the necessary Data Sets via Application Programming Interfaces (APIs), Resilient Distributed Datasets (RDD) and direct interface with the data source to the CCI EMSO data repository. The Data Engineer shall develop and implement data engineering eco-systems and architectures that enable data Extraction, Transformation and Loading (ETL) operations for predictive and prescriptive data modeling. Data Engineering builds will include, but not be limited to, establishment of a Data Lake and Eco-System which can move data from the DevSecOps to Data Science Production and Analysis Environment. The primary responsibilities of the CCI EMSO Data Engineer will be analyzing raw data, developing, and maintaining a repository of CCI EMSO specific data sets, and improving data quality and efficiency. CCI EMSO provides OSD senior leaders with data driven analyses using the scientific method to enable senior leadership decision making to deliver modernized capabilities that ensure the Combatant Commands are equipped with modern systems and the Joint Force is aligned with future Joint Warfighting Concepts.
Specific responsibilities include but are not limited to:
· Develop, construct, and deploy data lake and ecosystems infrastructure that will include a DevSecOps and production environment.
· Analyze and organize raw data for Data Science analysis.
· Build Data Systems and data transfer pipelines to ensure the CCI EMSO Data Science Team has the requisite data to answer Combatant Command, ISREC and ISPR Data Science Requests for Information (RFI).
· Evaluate CCI EMSO mission and vision needs and objectives and codify a data governance plan that supports them.
· Prepare data for prescriptive and predictive modeling.
· Build algorithms and prototypes to meet the objectives and requirements set by the Combatant Commands, ISREC and ISPR.
· Combine raw data from different sources and normalize into import ready data sets to pass to the CCI EMSO Data Science Team for data science analysis.
· Draft, develop and formalize a data governance plan, in conjunction with the CCI EMSO data scientists, that enhances data quality and reliability.
· Identify networking connections and opportunities for data acquisition.
· Work with stakeholders including data, design, product, and executive teams and assisting them with data-related technical issues.
· Identifying, designing, and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.

The candidate must have the following qualifications:
· A minimum of 10 years of experience (including demonstrated expert ability to use statistical software programs (e.g., R, Python, Weka, Apache Spark, and SQL).
· Ability to build and optimize data sets, ‘big data’ data pipelines and architectures as required by the CCI EMSO mission and vision and ISREC and ISPR Strategic Guidance.
· Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions.
· Excellent analytic skills associated with working on unstructured datasets.
· Experience in supporting technical, managerial, or operational fields and mature judgement required to interface with external stakeholders and senior government personnel.
· Strong organizational, oral, and written communication skills are required.
· Ability to build processes that support data transformation, workload management, data structures, dependency, and metadata
· Strong interpersonal skills and the ability to build consensus, work effectively and independently, and demonstrate a consultancy mindset towards customer engagement.
· Experience navigating experimental environments and making recommendations on risk management while maximizing innovation.
· Thorough knowledge of research designs, particularly systems thinking and/or design thinking.
· Have a working knowledge and experience in working in an Organizational Program Maturity business model, with the ability to move between Agile and standard Defense Acquisition University (DAU) and Project Management Institute (PMI) Program Management processes and methodologies.
· Ability to effectively communicate complex, multi-disciplinary ideas, and insights.
· Ability to translate complex, technical findings into an easily understood narrative (i.e., tell story with data).
· Analytical and critical thinking skills, including superior ability to think strategically.
· Demonstrated experience using analytic methods and methodological tools in mathematics or statistics

The following qualifications are desired:
· Data Science Council of America (DASCA) Associate Big Data Engineer (ABDE).
· Databricks Certified Data Engineer Professional
· PMI Agile Certified Practitioner (PMI-ACP)
· PMI Project Management Professional (PMP) (or DAU Equivalent)

Travel: Some travel may be required
Other Requirements:
We seek:
· Highly motivated self-starter
· Resourceful individuals with extraordinary intellectual capability and the ability to rapidly learn and apply new concepts
· Individuals who have a “let me try” attitude and are resilient, present an opinion/position, justify it, and then accept whatever decision is made and charge forward
· Individuals who view criticism as an opportunity to improve (“let me try again”)
· Individuals who think and create, enhancing the company with a steady flow of fresh ideas, perspective, and energy.
Direct Inquiries and Resumes to:
Yashira Santiago
Corporate Recruiter
Gemini Industries Inc.
1408 N. Westshore Blvd. Ste. 909
Tampa, FL 33607
Telephone: (813) 286-4777
Jobs@gemini-ind.com
Gemini Industries Inc. is proud to be an Equal Opportunity / Affirmative Action Employer. We are committed to abiding by the requirements of 41 CFR §§ 60-1.4(a), 60-300.5(a) and 60-741.5(a). These regulations prohibit discrimination against qualified individuals based on their status as protected veterans or individuals with disabilities and prohibit discrimination against all individuals based on their race, color, religion, sex, or national origin. Moreover, these regulations require that covered prime contractors and subcontractors take affirmative action to employ and advance in employment individuals without regard to race, color, religion, sex, national origin, protected veteran status or disability and any other basis protected by applicable law."
548,Data Engineers,Enable Data,"Minneapolis, MN",N,"Data engineers develop, maintain, test and evaluate big data solutions within client organizations using technologies such as Spark, MapReduce, or NoSQL. A big data engineer builds large-scale data processing systems, is an expert in data pipeline and warehousing solutions.
Candidates interested in this position should have sufficient experience in software engineering such as object-oriented design, coding and testing patterns as well as experience in engineering software platforms and large-scale data infrastructures using commercial and open source technologies. Examples would include integration tools like Informatica and Talend, or orchestration solutions like Airflow or Oozie.
Data engineers should have extensive knowledge in different programming or scripting languages like Java, Linux, C++, PHP, Ruby, and Python. Also expert knowledge should be present regarding different (NoSQL or RDBMS) databases such as MongoDB or HBase. Building data processing systems with Hadoop and Hive using Java or Python should be common knowledge to a data engineer.

Benefits
Enable Data offers employees the following benefits:
Health Insurance
401k Contribution
Paid Time Off (PTO)
Training Reimbursement
Certification Reimbursement"
549,Data Analytics Site Reliability Engineer,Apple,"San Diego, CA",N,"Summary
Posted: Mar 17, 2023
Weekly Hours: 40
Role Number: 200469008
At Apple, our Data Analytics team focuses on improving the user experience by improving operating system stability, capturing feature usage telemetry, and evaluating device performance. This requires assembling data from customers who have given consent, applies strong privacy preserving techniques, and entails aggregating information, all to help advise direction. We develop and operate a variety of Big Data infrastructure products and applications in support of these goals.
Key Qualifications
5+ years of production experience managing and supporting large scale distributed Big Data applications (from development to production)
Experience with at least one of the following languages and related development tools: (Python, Ruby, Java, Scala)
Experience with one or more of the following platforms: HDFS, Yarn, Spark, Impala, Hbase, MapReduce, Kubernetes, or other cloud services
Passion for quality and attention to detail
Excellent written and verbal communication skills
Description
We are looking for Site Reliability Engineer to be a member of our team. If working on large scale problems excites you then we’re excited to talk to you! Our team helps Apple engineers answer critical questions about their hardware, firmware, and software. We work with engineers across Apple to help keep our suite of analytics applications available and to ensure the integrity of their data. The successful candidate will write code to automate our processes to ensure reliability and handle thousands of compute and storage instances across large heterogeneous infrastructure. You'll dive into sophisticated data and application issues to drive root cause analysis of large scale problems. You'll partner with your teammates and peers to address problems, applying critical thinking skills and understanding of sophisticated distributed systems. The candidate will have to opportunity to give to the development of Apple's applications such as Mail and Safari, in addition to help to improve the performance of macOS and iOS.
Education & Experience
Bachelor's degree or equivalent experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $136,000 and $242,000, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program."
550,Data Engineer,LendKey Technologies,Remote,N,"Description:

Remote- United States · Engineering
DESCRIPTION
LendKey is solving a complex challenge – to improve lives with lending made simple – by helping financial institutions compete in the digital age and provide a delightful customer experience, while providing borrowers with the simple, transparent, digital borrowing experience they have come to expect and desire. LendKey works with hundreds of credit unions and banks to conduct their education finance and home improvement loan programs.
We are looking for an enthusiastic data engineer to help us build out our data lake using the lakehouse architecture. Our data capabilities and culture are still in the early stages, so this is an opportunity to build a modern, near real-time Spark-based data platform from the ground up.
Please note- this is a permanent role and we are not entertaining contract to contract at this time.
What you’ll do:
Contribute to building out the data lake; including the architecture, design, development, implementation, and support of data warehouse
Contribute to the development of data governance policies, including data normalization and anonymization, chain of custody, etc.
Design, manage, and support self-healing ETL processes, including ongoing data quality and testing
Identify gaps and opportunities in data infrastructure and design and build appropriate solutions
Partner with the product and engineering teams to develop scalable, extensible systems
Work with end users and power users across the organization to translate business questions and requirements into appropriate data structures to be used by reporting tools
Help in monitoring and troubleshooting system performance, reliability, availability, and recoverability of data across the data platform
Build proofs of concept to evaluate visualization platforms
Assist and lead migration efforts from legacy platform to the data lake
Requirements:
What we’re looking for:
Culture Fit:
Strong desire to work for a mission-based organization that emphasizes the importance of providing exceptional customer service and aligned with our core values: Truthful at all times; Helpful to teammates, clients, and customers; Present, committed & engaged to their teams and work; Driven to be courageous to make an impact; and Diligent & conscientious in executing every element of work.
Technical/Business Experience
Bachelor’s degree in Computer Science or related field
5+ years of experience in the data engineering field with deep experience in at least one of the major relational databases like SQL Server, Oracle, and/or MySQL, including creating and maintaining stored procedures and functions
A minimum of 2+ years of experience in:
Designing, building, and maintaining Star Schemas
Building data processing pipelines in Python or PySpark with a firm understanding of data load strategies for dimensions and facts
Experience and background in building data platforms for financial services is a plus
Experience in building data visualization with one or more of the following is a plus: Power BI, Tableau, Looker
Experience with one or more of the following is a big plus: Hadoop/Hive/Spark

Why work for us?
We have a lot to offer those that are looking to take the next step in their career, including:
Opportunity to join a growing fintech
Creative and transparent company culture
Growth potential
Competitive salary and bonuses
Comprehensive medical, dental, vision, and life insurance benefits
Generous vacation and holidays
Company stock options for eligible employees
Flexible/remote work arrangements
LendKey is an equal opportunity employer and values diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or any other basis prohibited by applicable state or federal law. LendKey offers reasonable accommodations to candidates with physical or mental disabilities. If you need accommodations to participate in the job application or interview process due to a disability, please let us know at HR@lendkey.com.
If you are a resident of Colorado or New York City, please email HR@lendkey.com to receive compensation information for this role. Please be sure to include this posting’s job title in the subject line of the email to help ensure a timely response. Base pay may vary depending on job-related knowledge, skills, and experience. This information is provided in accordance with the Colorado Equal Pay Act and New York City's Pay Transparency Law. It is specific to Colorado and New York City residents may not be applicable to other locations.
About Us
LendKey delivers high quality consumer loans directly to financial institutions through its digital lending-as-a-service model and indirectly through ALIRO by LendKey, an innovative deal network designed to enable financial institutions to buy, sell and broker loans. The platform delivers the technology, servicing, and support that financial institutions need to establish a strong digital lending presence, acquire loans for their balance sheets, and build lifetime relationships with their partners and customers. Lenders optimize their offerings by drawing upon LendKey’s fintech platform for demand generation, online decisioning, loan origination, loan servicing, compliance expertise, risk analytics, and proprietary balance sheet options. LendKey currently services more than $2 billion in loans and has helped community-capital providers deploy over $4.5 billion in loans since 2009. Visit LendKey for more information."
551,Data Engineer (L2),Collegis Education,Remote,N,"Description:
Collegis Education is a marketing and technology education solutions company that offers industry-leading services for colleges and universities of every size in every sector. Using a proactive and data-driven approach, Collegis Education empowers institutions to make a broader impact by providing insights that help grow enrollments, improve student outcomes and optimize expenses. With several decades of experience working within the higher education industry, the team at Collegis Education was founded within the walls of a college and expanded to help change more lives through education. Currently, the infrastructures established by Collegis Education support more than 40,000 students nationwide. For more information about Collegis Education, please visit www.CollegisEducation.com.

The Data Enablement team at Collegis Education is seeking an experienced and highly engaged Data Engineer (L2). As a Data Engineer (L2), you will be responsible for crafting, developing, and maintaining data integration and transformation solutions to support delivery of professional services, including building and managing scalable data pipelines, ensuring data quality and governance, and providing technical expertise in cloud-native big data architecture. The ideal candidate will have 2-5 years of experience in data engineering, sophisticated SQL skills, bash and Python competency, as well as hands-on experience with cloud-native data integration and transformation tools.

Primary Responsibilities, Essential Functions and Requirements:

Design, develop and maintain data integration and transformation solutions using cloud-native tools such as Apache Beam, Google Cloud Data Fusion, dbt, serverless functions, and cloud-based database and data storage solutions.
Build and run scalable and reliable data pipelines using cloud data pipeline orchestration tools and approaches.
Ensure data quality and governance by implementing data quality processes and workflows.
Collaborate with cross-functional teams to identify and integrate new data sources and to ensure alignment to data governance and security standards.
Provide technical expertise in executing cloud-native big data architecture, including cloud data warehouse, data lake, and lakehouse approaches.
Understand cloud and legacy RDBMS and NoSQL design patterns and trade-offs.
Design, develop and deploy data solutions using common API integration and consumption patterns.
Ensure adherence to development and security processes and workflows.
Chip into team competency and cohesion with other data engineers through code reviews, working sessions, knowledge sharing sessions, and team activities.
Document data engineering activities and processes.
Lead technical partner engagements.
Adhere to and enforce the appropriate information security policies based on the sensitivity of company data and report any security related issues.
Reduce risk of theft, fraud, or misuse of information assets by acting as the data steward for the application(s) you administer.
Requirements:
Bachelor's or Master's Degree in MIS, IS, Business Analytics, Computer Science, a related field, or equivalent experience.
2-5 years of experience in data engineering.
Advanced SQL, plus bash and Python competency.
Experience with cloud-native data integration and transformation tools such as Apache Beam, Google Cloud Data Fusion, dbt, serverless functions.
Experience with cloud-native data storage solutions.
Experience with common API integration and consumption patterns.
Experience with cloud data pipeline orchestration tools and approaches.
Experience working with cloud-native big data architecture, including cloud data warehouse, data lake, and lakehouse approaches.
Ability to understand cloud and legacy RDBMS and NoSQL design patterns and trade-offs.
Strong proficiency with data quality processes and workflows.
Working experience with DevOps processes and workflows.
Working experience with data governance and security standards.
Strong organizational skills.
Strong documentation skills.
Ability to independently prioritize tasks with and without manager feedback.
Ability to manage technical partner engagements.
Education, Certifications and Licensures:
Bachelor's or Master's Degree in MIS, IS, Business Analytics, Computer Science, a related field, or equivalent experience.
Relevant certifications in data engineering or cloud computing is a plus (GCP preferred).
Collegis Education is committed to the policy that all persons shall have equal access to its programs, facilities, and employment without regard to race, color, creed, religion, national origin, sex, age, marital status, disability, public assistance status, veteran status, or sexual orientation."
552,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
553,Data Engineer,Gray Tier Technologies,"Crystal City, VA",N,"Data Engineer – Role Description & Requirements
Data is absolutely the lifeblood of ADVANA and our data engineers are the doctors administering it! Our data engineers support data collection, ingestion, validation, and loading of optimized data in the appropriate data stores. They work on a team made up of analyst(s), developer(s), data scientist(s), and a product lead, and everyone on the team collaborates in support of a specific mission. Working directly with the analyst(s) and the product lead, the data engineer identifies and implements solutions for the data requirements, including building pipelines to collect data from disparate, external sources, implementing rules to validate that expected data is received, cleansed, transformed, massaged and in an optimized output format for the data store. The Data Engineer performs validation and analytics corresponding with client requirements and evolves solutions through automation, optimizing performance with minimal human involvement. As pipelines are executed, the data engineer monitors their status, performance, and troubleshoots issues while working on improvements to ensure the solution is the very best version to address the customer's need.
As a Data Engineer, this role focuses specifically on the development and maintenance of scalable data stores that supply big data in forms needed for business analysis. The best athlete candidate for this position will be able to apply advanced consulting skills, extensive technical expertise and has full industry knowledge to develop innovative solutions to complex problems. This candidate is able to work without considerable direction and may mentor or supervise other team members.
What we're looking for:
Someone with a solid background developing solutions for high volume, low latency applications and can operate in a fast-paced, highly collaborative environment.
A candidate with distributed computer understanding and experience with SQL, Spark, ETL.
A person who appreciates the opportunity to be independent, creative and challenged.
An individual with a curious mind, passionate about solving problems quickly and bringing innovative ideas to the table.
Basic Qualifications:
4+ years of experience with SQL
4+ years of experience developing data pipelines using modern Big Data ETL technologies like NiFi or StreamSets.
4+ years of experience with a modern programming language such as Python or Java
4 years of experience working in a big data and cloud environment
Secret Clearance or higher
Additional Qualifications:
2 years of experience working in an agile development environment
Ability to quickly learn technical concepts and communicate with multiple functional groups
Ability to display a positive, can-do attitude to solve the challenges of tomorrow
Possession of excellent verbal and written communication skills
Preferred experience at the respective command with an understanding of analytical and data paint points and challenges across the J-Codes."
554,Senior Data Engineer,Ensemble Health Partners,Remote,N,"Thank you for considering a career at Ensemble Health Partners!
Ensemble Health Partners is a leading provider of technology-enabled revenue cycle management solutions for health systems, including hospitals and affiliated physician groups. They offer end-to-end revenue cycle solutions as well as a comprehensive suite of point solutions to clients across the country.
Ensemble keeps communities healthy by keeping hospitals healthy. We recognize that healthcare requires a human touch, and we believe that every touch should be meaningful. This is why our people are the most important part of who we are. By empowering them to challenge the status quo, we know they will be the difference
The Opportunity:
Job Competencies
Valuing Differences - Works effectively with individuals of diverse cultures, interpersonal styles, abilities, motivations, or backgrounds; seeks out and uses unique abilities, insights, and ideas. Considers the collective.
Collaboration - Works cooperatively within teams and partners with others, both internally and externally as needed, to achieve success; focuses on the results of the team, not the achievements of one person. It’s “All for One and One for All”
Accountability - Accepts personal responsibility and/or consequences of failure and successes, delivering on commitments and refocusing effort when needed. Someone who is willing to step up and own it.
Time Management - Effectively manages personal time and resources to ensure that work is completed efficiently.
Developing Trust - Gains others’ confidence by acting with integrity and following through on commitments; treats others and their ideas with respect and supports them in the face of challenges.
Takes Initiative - Takes prompt action to accomplish goals and achieve results beyond what is required; is proactive and pursues relentlessly.
Essential Job Functions
Develop, test, deploy, monitor, maintain, and continuously improve scalable data pipelines and builds our new API integrations to support continuing increases in data volume and complexity.
Translate product concepts into project commitments that deliver incremental value to our customers frequently and with high quality.
Review and recommend architectural patterns for data access and performance that align with the company’s best practices and platforms.
Validate, maintain, and enhance current data pipelines. Partner with internal and external clients to ensure company data needs are met.
This document is not an exhaustive list of all responsibilities, skills, duties, requirements, or working conditions associated with the job. Associates may be required to perform other job related duties as required by their supervisor, subject to reasonable accommodation.
Join an award-winning company
Three-time winner of “Best in KLAS” 2020-2022
2022 Top Workplaces Healthcare Industry Award
2022 Top Workplaces USA Award
2022 Top Workplaces Culture Excellence Awards
Innovation
Work-Life Flexibility
Leadership
Purpose + Values
Bottom line, we believe in empowering people and giving them the tools and resources needed to thrive. A few of those include:
Associate Benefits – We offer a comprehensive benefits package designed to support the physical, emotional, and financial health of you and your family, including healthcare, time off, retirement, and well-being programs.
Our Culture – Ensemble is a place where associates can do their best work and be their best selves. We put people first, last and always. Our culture is rooted in collaboration, growth, and innovation.
Growth – We invest in your professional development. Each associate will earn a professional certification relevant to their field and can obtain tuition reimbursement.
Recognition – We offer quarterly and annual incentive programs for all employees who go beyond and keep raising the bar for themselves and the company.
Ensemble Health Partners is an equal employment opportunity employer. It is our policy not to discriminate against any applicant or employee based on race, color, sex, sexual orientation, gender, gender identity, religion, national origin, age, disability, military or veteran status, genetic information or any other basis protected by applicable federal, state, or local laws. Ensemble Health Partners also prohibits harassment of applicants or employees based on any of these protected categories.
Ensemble Health Partners provides reasonable accommodations to qualified individuals with disabilities in accordance with the Americans with Disabilities Act and applicable state and local law. If you require accommodation in the application process, please contact
TA@ensemblehp.com
.
EEOC – Know Your Rights

FMLA Rights - English
La FMLA Español"
555,Data Engineer lead,princeton it services,"Boston, MA",From $65 an hour,"Data Engineer Lead
Job Description:
Position: Data Engineer Lead
Location: Raleigh, NC or Boston, MA
Job Length: Long term
Position Type: C2C/W2
Qualifications:
9+ years Experience in Alation, Collibra, Snowflake
9+ years Experience in Java , Spring boot , spark , Scala.
Stays current with technology trends in order to provide best options for solutions • Self-directed and is able to decompose work into problem sets for self and project team.
Equally capable working as part of a team or independently.
Responsibilities:
Designs, develops, tests, and delivers software solutions using one or more commercial languages as well as, open-source tools. Data processing and analysis using Snowflake.
Data management and Stewardship using Collibra.Alation
Data warehouse using Data Pipelines along with data transformation and optimization.
Comfortable working within a culture of accountability and experimentation
Work closely with internal stakeholders to implement solutions and generate reporting to meet business goals.
Demonstrate critical thinking for potential roadblocks; comprehends bigger picture of the business and effectively communicates these issues to greater news digital organization.
Collaborates with reporting teams and business owners to turn data into actionable business insights using self-service analytics and reporting tools.
Skills Required :Alation, Collibra, Snowflake
Job Type: Contract
Salary: From $65.00 per hour
Schedule:
8 hour shift
Experience:
collibra: 5 years (Preferred)
snowflake: 5 years (Preferred)
aliation: 4 years (Preferred)
Work Location: On the road
Speak with the employer
+91 6093006906"
556,Data Engineer,"Perficient, Inc","22 Cortlandt St, New York, NY 10007",N,"We currently have a career opportunity for a Data Engineer to join our Financial Services team. This role is located in Pittsburg, PA.
As a Data Engineer you will participate in all aspects of the software development lifecycle which includes estimating, technical design, implementation, documentation, testing, deployment and support of application developed for our clients. As a member working in a team environment you will take direction from Solution Architects and Leads on development activities.
Perficient is always looking for the best and brightest talent and we need you! We’re a quickly-growing, global digital consulting leader, and we’re transforming the world’s largest enterprises and biggest brands. You’ll work with the latest technologies, expand your skills, and become a part of our global community of talented, diverse, and knowledgeable colleagues."
557,Data Engineer (Full Remote Role) – DE222-000691,"Evergreen Technologies, LLC.","New York, NY•Remote",N,"Title: Data Engineer
Position: Full Remote Role
Job Description:
In this role, you will provide the enterprise data design, models, tools and guidance to provide the business and IT with a view of how data is being utilized.
This starts with a strong semantic foundation all the way to identifying potential business process improvements.
As a Segment Data Architect, your key deliverables include Critical Data List, Data Affinity Maps, Enterprise Data Catalogues, Conceptual & Logical Data Models, Mastership Matrices, Enterprise Data Flow Diagrams, Data Principles, and Standards & Handrails.
In this role, you will be leveraging on your desire to bring enterprise data design to IT and business functions.
You will also require the passion to drive IT related business activity towards value-based outcomes built on a foundation of well-described quality data.
The Segment Data Architect brings together both business and IT stakeholders to capture and model a data landscape that supports business interests whilst enabling the effective IT management of data.
You will have full ownership of data domains, including being a key focal point for the Lead Segment Data Architect, Segment Architects, Solution Architects and Solution Data Architects.
Furthermore, your accountabilities are as follows:
Work with Solution and Segment Architects to review/approve data provisioning from a simplified access and governance perspective and adopt Data-as-a-Service (DaaS) approach to enable effective re-use, provisioning and sharing of data across the business.
Provide Solution Architecture an understanding of how the project fits into the segment architecture. Ensure compliance and alignment of the project design.
Maintain the Enterprise Conceptual, Logical, Integration Models and work with Solution Data Architects to ensure that project-related Logical and Physical Models remain aligned.
Understand the importance of Data Mastership and Data Flow mapping for the business, capturing both As-Is and To-Be states, and help identify the right data for the business.
Provide Solution Data Architects with Enterprise Conceptual/Logical Models, Mastership Matrices and Data Flows as input to projects as part of the delivery lifecycle We're keen to hear from IT professionals who has substantial experience in Data Architecture and Data Modelling.
Beyond that, we'd want to hear from individuals who have the following:
Proven Core Data Architecture skills including Data Analysis, Data Modelling (ERD), Big Data Design, Master Data Management patterns & implementation, Data Landscape Design (Data Flow, Mastership, API).
Extensive understanding of technologies involved in data management (Database management systems, data services, data lakes, etc.).
Proven knowledge of all architecture aspects and the ability to stay up to date with key developments in IT - affinity with the other Enterprise Architecture domains (process, applications, infrastructure, security, etc.).
A delivery and result oriented mindset with a hands-on attitude with a vision for the long-term implications of short to intermediate term decisions.
Ability to initiate, plan and develop according to plan, along with the ability to adapt to the circumstances on the ground that may have a level of uncertainty and risk.
Experience in Data Modelling tools such as PowerDesigner. While you experience in working with Enterprise Architecture tools such as BiZZDesign is not required, it will certainly be beneficial in your application.
Skills:
Ability to build and optimize data sets, 'big data' data pipelines and architectures
Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
Excellent analytic skills associated with working on unstructured datasets
Ability to build processes that support data transformation, workload management, data structures, dependency and metadata
Additional Skills:
PowerDesigner"
558,Data Engineer,Simatree,"Washington, DC•Remote","$80,000 - $90,000 a year","Simatree is searching for a Data Engineer Consultant to support our client’s operational requirements on a mission focused program within a challenging, dynamic setting. Successful candidates in this position will collaborate with a diverse group of individuals across the homeland security community and directly contribute to management and analysis of operational data using data science tools.
The Role
The Data Engineer will work alongside a team to generate management tools that effectively control programs and generate reports while supporting the programmatic development of operational applications of the tools or analysis that are focused on operations research and document findings. They will conduct research into management, organizational, and business practices to implement improvements to existing processes. They will utilize commercial or open-source software to manage tasks and present findings to a broad audience, including agency executives. They will maintain a database to produce monthly financial reports, tracks processes, distribute deliverables, and monitor comment status. The Data Engineer will be required to use Microsoft Office to perform queries, analyses, and produce reports. Other responsibilities include:
Use R and Python to perform analyses and develop new tools using popular data visualization libraries such as ggplot2, plotly, matplotlib, or bokeh.
Use Databricks to Extract, Transform, and Load (ETL) data from different sources and transform it into a usable, reliable resource.
Manage data collection and transformation from federal law enforcement and other databases to be used in a wide spectrum of analyses including statistical modeling, performance metrics analysis, crosschecking data with other federal agencies, and risk analysis.
Use ArcGIS to perform analyses and develop new web map applications that summarize results from deep learning/AI algorithms.
Maintain and enhance business intelligence (BI) dashboards using best practices in UI/UX design
Develop and test new methodologies and algorithms which would increase the efficiency and accuracy of existing models, including pilot deployments of automated scripts
Requirements
This role supports a US Government contract. US Citizenship is required.
Strong Experience with SQL programming
Experience with R statistical programming and Python programming
Experience with BI software such as Qlik, Tableau or PowerBI
Demonstrated experience in performing data analysis and modeling on an independent basis
Demonstrated experience in developing and applying quantitative methods to find patterns and relationships in large data sets using statistical and graphical packages
Ability to translate complex, technical, or analytic findings into an easily understood narrative—tell a story with graphical, verbal or written form
1-3 years experience in supporting quantitative and analytical efforts[ES1]
Bachelors degree in relevant fields such as statistics, mathematics, computer science, physical science, economics or engineering
Job Type: Full-time
Pay: $80,000.00 - $90,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Experience level:
3 years
Schedule:
Monday to Friday
Application Question(s):
This position supports a US Government contract which requires a US Citizenship. Are you a US Citizen?
Work Location: Remote

Health insurance"
559,Data Engineer,University of Texas at Austin,"1616 Guadalupe St, Austin, TX 78701","$110,000 a year","Job Posting Title:
Data Engineer
-
Hiring Department:
Office of the President
-
Position Open To:
All Applicants
-
Weekly Scheduled Hours:
40
-
FLSA Status:
Exempt
-
Earliest Start Date:
Immediately
-
Position Duration:
Expected to Continue Until Dec 31, 2025
-
Location:
UT MAIN CAMPUS
-
Job Details:
General Notes
The Data to Insights (D2I) Initiative at UT Austin is an investment to build a trusted, integrated, and scalable information infrastructure that transforms complex UT data into valued insights for data-informed decisions.
As a team member in the Data to Insights (D2I) Initiative, you will work with a cross-campus team using the latest cloud technologies to build a next-generation data ecosystem that will improve decision-making and advance the university’s mission. We believe the best ideas arise from collaborative work among colleagues from varied backgrounds and experiences. We actively seek diversity of viewpoint and perception in our student, faculty, and staff recruiting and retention practices. If you’re the type of person who loves to learn and wants to know your work has meaning, you may find your career home at UT Austin. Please note that this position is currently funded through December 31, 2025.
The University of Texas at Austin provides an outstanding benefits package to staff, including:
Competitive health benefits (Employee premiums covered at 100%; family premiums at 50%)
Vision, dental, life, and disability insurance options
Paid vacation, sick leave, and holidays
Teachers Retirement System of Texas (a defined benefit retirement plan)
Additional voluntary retirement programs: tax sheltered annuity 403(b) and a deferred compensation program 457(b)
Flexible spending account options for medical and childcare expenses
Training and conference opportunities
Tuition assistance
Athletic ticket discounts
Access to UT Austin's libraries and museums
Free rides on all UT Shuttle and Capital Metro buses with staff ID card
For more details, please see:
https://hr.utexas.edu/prospective/benefits
and
https://hr.utexas.edu/current/services/my-total-rewards
This position requires you to maintain internet service and a mobile phone with voice and data plans to be used when required for work.
This position provides life/work balance with typically a 40-hour work week and travel generally limited to training (e.g., conferences/courses).
Purpose
The Data Engineer for the UT Data Hub improves university outcomes and advances the UT mission to transform lives for the benefit of society by increasing the useability and value of institutional data. You will create complex data pipelines into UT’s cloud data ecosystem in support of academic and administrative needs. In collaboration with our team of data professionals, you will help build and run a modern data hub to enable advanced data-driven decision making for UT. You will leverage your creativity to solve complex technical problems and build effective relationships through open communication.
Responsibilities
Data Engineering:
Assist in designing and automating scalable data integration solutions between institutional data sources, including the UT source systems, Amazon Web Services, and vendor APIs.
Assure performance and reliability of integration processes through monitoring, performance analysis and development of automated alerting processes.
Participate in end to end delivery of technical projects involving design and development of data pipelines for complex datasets.
Work closely with business partners to design and manage data pipeline elements including load frequency, data delivery mechanisms, and transformations.
Ensure that data pipeline solutions align with industry best practices, and adhere to UT security guidelines.
Develop and maintain detailed technical documentation of data pipeline processes.
Collaborate with key stakeholders including enterprise data architect, data modelers, data stewards, and subject matter experts (SMEs).
Other responsibilities:
Participate in change management processes to coordinate and communicate team activity.
Other related functions as assigned.
Required Qualifications
BS degree in Computer Science, Information Systems, Engineering, or equivalent professional experience. One year of Data Engineering experience with Amazon Web Services. One year of experience implementing and monitoring complex data pipelines and automation of cloud-based workloads. Proficiency in one or more scripting languages and/or programming languages, preferably Python. Advanced SQL knowledge and experience working with relational databases. Demonstrated experience of developing and implementing Continuous Integration and Continuous Delivery (CI/CD) systems. Proficiency in systems analysis, design, and a solid grasp of development, quality assurance, and integration methodologies. Excellent problem solving and troubleshooting skills. Relevant education and experience may be substituted as required.
Relevant education and experience may be substituted as appropriate.
Preferred Qualifications
Three years of experience in Data Engineering or related field. One year of experience working in a cloud-based data warehouse environment. Two years of experience building and monitoring complex data pipelines. AWS Developer and/or Solutions Architect certification. Two years of experience with Agile software development methodologies. Two years of experience with issue tracking systems (JIRA). Experience with Spark, Kafka etc., is a plus.
Salary Range
$110,000 + depending on qualifications
Working Conditions
May work around standard office conditions
Repetitive use of a keyboard at a workstation
Use of manual dexterity
Up to 100% remote at employee discretion with office work environment also an option. Majority of team is located in Austin, TX, and working remotely with infrequent in-office presence.
Required Materials
Resume/CV
3 work references with their contact information; at least one reference should be from a supervisor
Letter of interest
Important for applicants who are NOT current university employees or contingent workers: You will be prompted to submit your resume the first time you apply, then you will be provided an option to upload a new Resume for subsequent applications. Any additional Required Materials (letter of interest, references, etc.) will be uploaded in the Application Questions section; you will be able to multi-select additional files. Before submitting your online job application, ensure that ALL Required Materials have been uploaded. Once your job application has been submitted, you cannot make changes.
Important for Current university employees and contingent workers: As a current university employee or contingent worker, you MUST apply within Workday by searching for Find UT Jobs. If you are a current University employee, log-in to Workday, navigate to your Worker Profile, click the Career link in the left hand navigation menu and then update the sections in your Professional Profile before you apply. This information will be pulled in to your application. The application is one page and you will be prompted to upload your resume. In addition, you must respond to the application questions presented to upload any additional Required Materials (letter of interest, references, etc.) that were noted above.
-
Employment Eligibility:
Regular staff who have been employed in their current position for the last six continuous months are eligible for openings being recruited for through University-Wide or Open Recruiting, to include both promotional opportunities and lateral transfers. Staff who are promotion/transfer eligible may apply for positions without supervisor approval.
-
Retirement Plan Eligibility:
The retirement plan for this position is Teacher Retirement System of Texas (TRS), subject to the position being at least 20 hours per week and at least 135 days in length.
-
Background Checks:
A criminal history background check will be required for finalist(s) under consideration for this position.
-
Equal Opportunity Employer:
The University of Texas at Austin, as an
equal opportunity/affirmative action employer
, complies with all applicable federal and state laws regarding nondiscrimination and affirmative action. The University is committed to a policy of equal opportunity for all persons and does not discriminate on the basis of race, color, national origin, age, marital status, sex, sexual orientation, gender identity, gender expression, disability, religion, or veteran status in employment, educational programs and activities, and admissions.
-
Pay Transparency:
The University of Texas at Austin will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information.
-
Employment Eligibility Verification:
If hired, you will be required to complete the federal Employment Eligibility Verification I-9 form. You will be required to present acceptable and original
documents
to prove your identity and authorization to work in the United States. Documents need to be presented no later than the third day of employment. Failure to do so will result in loss of employment at the university.
-
E-Verify:
The University of Texas at Austin use E-Verify to check the work authorization of all new hires effective May 2015. The university’s company ID number for purposes of E-Verify is 854197. For more information about E-Verify, please see the following:
E-Verify Poster (English)
[PDF]
E-Verify Poster (Spanish)
[PDF]
Right To Work Poster (English)
[PDF]
Right To Work Poster (Spanish)
[PDF]
-
Compliance:
Employees may be required to report violations of law under Title IX and the Jeanne Clery Disclosure of Campus Security Policy and Crime Statistics Act (Clery Act). If this position is identified a Campus Security Authority (Clery Act), you will be notified and provided resources for reporting. Responsible employees under Title IX are defined and outlined in
HOP-3031
.
The Clery Act requires all prospective employees be notified of the availability of the Annual Security and Fire Safety report. You may
access the most recent report here
or obtain a copy at University Compliance Services, 1616 Guadalupe Street, UTA 2.206, Austin, Texas 78701."
560,Data Integration Engineer,Ellevation,Remote,"$70,000 - $80,000 a year","The Data Integration Team is responsible for ensuring data from school districts are loaded and maintained with a high level of fidelity into the Ellevation platform. This team plans and executes the solutions to integrate customer data into our SaaS educational platform, and designs solutions to simplify processes, save time and maximize the use of Ellevation by educators around the country. As a Data Integration Engineer, you will work directly with customers (data teams and administrators) and internal teams to support districts in the extraction of data, own the data transformation and import process, and ensure ongoing data fidelity within Ellevation’s platform.

This role is fast paced and will require one to take ownership, balance, and prioritize multiple projects at once. This role requires a combination of technical skills, organization, a service orientation, and the ability to communicate technical issues to non-technical partners. Just as important, we are looking for someone who is a self starter and motivated to continuously learn. This role requires one to be an excellent teammate who is willing to collaborate and provide input. Feedback is ingrained in our culture and feeling comfortable giving and receiving feedback is an integral part of the process to advance yours and Ellevation’s growth and sustainability.
Responsibilities
Writing and committing easy to read, reproducible code
Importing student, staff, schedule, and test files
Automating set up processes
Meeting Service Level Agreements for imports and troubleshooting tasks requested by partners
Prioritizing multiple projects at once
Designing solutions to automate and/or simplify integration processes
Within 1 month, you will:
Gain familiarity and knowledge of Ellevation, its team structure, and product/services
Know our mission, values, and product offering
Know teams and high-level responsibilities of each
Learn about our platform and have familiarity with our product offerings
Setup and Familiarize yourself with our internal tools and communication norms (Slack, GSuite, Jira, Wiki)
Attend PSO team overviews and shadow partner calls to further understand who the Data Integration Team (DIT) collaborates with on a daily basis and how we provide service to our partners
Set up a development environment on your computer to execute all Data Integration Engineer (DI Eng) tasks (python, SQL, git)
Start to become familiar with addressing DIT data processing and imports (Student, Schedule, Staff, ELP test, Grade file evaluations and uploads & Review and practice Python code development)
Within 3 months, you will:
Develop familiarity with our data model
Be able to navigate our DB and locate key tables relevant to DI Eng tasks
Develop proficiency in importing various data types, including standardized tests
Take on automated tasks
Learn how to commit code
Learn automation setup process
Familiarize with and begin contributing to DI Eng projects
Start to take on responsibility for Enterprise partner(s)
Take on more technical DI Eng project work
Within 6 months, you will:
Work independently
Troubleshoot data issues independently
Maintain SLA on your assigned work
Within 12 months, you will:
Continue to develop your technical skills
Maintain a high level of data fidelity for partners
Collaborate on new projects and initiatives
Work to improve our processes and code libraries
Automate manual tasks
About You
1-3 years experience working with many heterogeneous datasets and formats from a variety of vendors, providers and platforms.
Understanding complex data and translating issues into non-technical terms.
You have a knack for problem solving complex data discrepancies.
Demonstrated success and effectiveness working in and promoting a rapidly changing, collaborative, and time-critical work environment.
Interpersonal Skills
Attention to detail and strong written and verbal communication skills.
Customer service orientation, and an understanding of the importance of meeting customer needs and maintaining strong customer satisfaction.
Able to translate customer requests into technical solutions.
Commitment to continuous improvement and the mission; desire to focus talents on helping improve outcomes for others.
Collaborates well with others, and not afraid to ask for help when needed
Self Starter/Learner - Comfortable with ambiguity
Technical Skills
Proficiency with Python (Pandas Library or other scripting languages such as R)
Basic understanding of Coding/Programming Logic
Experience with Microsoft Office Excel or Google Sheets
Familiarity with SQL and RDBMS concepts (SQL Server, Oracle, MySQL, PostgreSQL, etc.)
Experience implementing prevention and detection controls to ensure data integrity
Statistics/data-study based experience
Working with, extending and defining data interchange standards and best practices
Experience with SFTP, FTPS and other file transfer protocols
Bonus: SIS/K-12 student data/Ed-Fi experience
Ellevation is transparent about its approach to pay which includes competitive base salary, annual performance-based raise, and bonus (or commission for sales positions). The salary range for this role is $70,000 to $80,000 with a 10% annual bonus potential and the budgeted target is the midpoint, commensurate with candidate experience and internal parity. Our philosophy is that the full range is indicative of growth during employment. In addition, we provide comprehensive benefits to all full-time employees.
#LI-AP1

About Us:
At Ellevation, we develop world-class software to help educators serve the fastest growing population of K-12 students: English Learners (ELs). Ellevation helps school districts transform their Multilingual Learner programs and ensure all students can achieve their highest aspirations. Our product suite includes a best-in-class data and instructional planning platform, resources to build teacher capacity, and student-facing programs to teach academic language.

We are a fast-growing, mission-driven technology company partnering with over 1,200 school districts and more than 3 million current and former English Learners. Over the next five years, we plan to double the number of ELs we serve and drive measurable outcomes for students. Ellevation merged with Curriculum Associates in 2021 to accelerate impact for Multilingual Learners. Our company continues to operate independently and is well-capitalized to support our ambitious social and financial objectives.

Why Ellevation?
In addition to our great benefits and competitive salaries, here are some things that make us unique:
+ Mission-driven organization where team members are empowered to make a significant impact
+ Opportunities to join DEIB-focused groups and support building a culture of belonging
+ Professional development and growth opportunities
+ Company and team offsites in various cities across the United States
+ Collaborative workspace in the heart of Boston - a stone's throw from many central T stops (Downtown Crossing, Government Center, and Aquarium). Free snacks, beverages, and local organic fruit provided
+ Remote and in-person engagement opportunities, including happy hours, themed events, and competitions
+ Remote and in-person wellness programming to support team’s mental and physical wellbeing

Learn more about our team in our Culture Deck .

Here at Ellevation, we champion diversity, inclusion, equity, and belonging. We strive to build a team that reflects the diverse communities we serve. We are committed to creating an inclusive workplace that promotes and values a range of ideas and opinions.

We believe in building a culture where productivity can flourish, one that is empathetic, respectful, and inclusive. We are proud to have been recognized in prior years as “Best Tech Workplace for Diversity” by the Timmy Awards for investing in training around inclusive behaviors, microaggressions, unconscious bias, and fostering a culture of continuous learning and feedback across the company. We are encouraged by our progress, but there’s more work to be done.

Benefits – Benefits eligible employees (and qualifying dependents) are covered by medical, dental, vision, and basic life insurance. Employees can enroll in our company’s 401(k) retirement plan and receive an employer match up to 7%. Employees have access to flexible paid time off for vacations, sick and disability policy, additional 10 paid company holidays, 2 floating holidays and a winter office closure between Christmas and New Year's. In addition, we provide a generous parental leave benefit, back-up childcare or eldercare, and a variety of other perks to support the health and well-being of our employees.

Additional Information: Ellevation operates under Curriculum Associates, LLC, an Equal Opportunity Employer. Curriculum Associates, LLC will not discriminate against any employee or applicant for employment because of race, color, creed, religion, sex, national origin, age, marital status, veteran status, sexual orientation, gender identity or expression, disability, genetic information, or any other category protected by law. Curriculum Associates, LLC will grant employment, without regard to race, color, creed, religion, sex, national origin, age, marital status, veteran status, sexual orientation, gender identity or expression, disability, genetic information, or any other category protected by law. Such action shall include, but not be limited to, the following: employment, upgrading, demotion, transfer, recruitment or recruitment advertising, layoff or termination, rates of pay or other forms of compensation.

Our company uses E-Verify to confirm the employment eligibility of all newly hired employees. To learn more about E-Verify, including your rights and responsibilities, please visit www.uscis.gov/e-verify.

Information that you provide when applying for employment with Curriculum Associates, LLC may be subject to the California Privacy Rights Act. Click here for more information about our data-collection practices and your rights related to that data.

#LI-REMOTE"
561,Senior Data Engineer with DBT experience,Zelis,Remote,"$90,693 - $136,606 a year","Summary of Job:
Looking for Lead – BI & DW, who will support business partners with advanced data reporting insights and to serve as a team lead. The best candidates for this role will exhibit strong communications skills, problem-solving abilities, and experience working collaboratively with different teams. This role further requires experience in leadership or mentorship of junior employees and calls for a deep technical understanding of DW & BI.
The BI-Tech Lead should be able to:
Perform data analysis, profiling, and quality assessment to ensure the integrity of business data to necessary levels.
Provide consultancy to business for detailed information on data availability, location, lineage, and quality.
Ensure that existing governance and process documents are up to date based on changes in requirements or business needs
Develop foundational domain specific operational processes and reporting structures related to Master Data.
Experienced in defining of MDM processes and data governance procedures.
Experience with BI development, SQL development, data warehousing, ETL, database integration and reporting services
Design, develop and implement scalable batch/real time data pipelines (ETLs) to integrate data from a variety of sources into Data Warehouse and Data Lake
Design and implement data model changes that align with warehouse dimensional modeling standards.
Proficient in Data Lake, Data Warehouse Concepts and Dimensional Data Model.
Responsible for maintenance and support of database environments, design and develop data pipelines, workflow, ETL solutions on both on-prem and cloud-based environments.
Ability to perform Data Analysis and Data Quality tests and create audit for the ETLs.
This role will lead daily operations of data integration, data processes and delivering dashboards and automated reporting
Creates, adheres to, and enforces processes and procedures for controlling data accuracy, changes to data models/flows, ultimately ensuring the quality and accuracy of data
Contribute to implement robust KPI metrics to ensure continuous data quality improvement
Deliver data-driven insights and tell the story that enables strategic and operational decisions
Act as the primary liaison between Operations, Build Team, and Business to evaluate BI support requests and to proactively propose solutions
Work with adjacent technical leads and architects to implement new standards, new technology, and tactical solutions driving the business strategy forward.
Technical Skills / Knowledge:
Must have Experience with at least one Columnar MPP Cloud data warehouse (Snowflake /Azure Synapse / Redshift) (2+ years)
Experience in ETL tools like Informatica, DataStage or DBT. (5 Years)
Has demonstrated proficiency in Designing, Developing and Administering Power BI end to solutions (5 Years)
Experience of working in at least one of the Cloud environments (Azure/AWS)(4 Years)
Hands on development experience in Python (1 Years)
Strong Experience designing, implementing, and optimizing Data Warehouse.
Experience of defining and supporting MDM processes and data governance.
Working knowledge of managing data in the Data Lake.
Experience in Agile, Jira and Confluence.
Solid understanding of programming SQL objects (procedures, triggers, views, functions) in SQL Server. Experience optimizing SQL queries a plus.
Advanced understanding of indexes, stored procedures, triggers, functions, views, etc.
Independence/ Accountability:
Requires minimal daily supervision
Receives detailed instruction on new assignments and determines next steps with guidance
Regularly reviews goals and objectives with supervisor
Demonstrates competence in relevant job responsibilities which allows for increasing level of independence
Ability to manage and prioritize multiple tasks
Ability to work under pressure and meet deadlines
Problem Solving:
Makes logical suggestions of likely causes of problems and independently suggests solutions
Excellent organizational skills are required to prioritize responsibilities, thus completing work in a timely fashion
Outstanding ability to multiplex tasks as required
Excellent project management skills.
Attention to detail and concern for impact is essential
Job Type: Full-time
Pay: $90,692.96 - $136,606.37 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
No sponsorship of any kind will be provided.
Experience:
designing, implementing, and optimizing Data Warehouse: 3 years (Required)
developing and Administering Power BI end to solutions: 4 years (Required)
DBT: 2 years (Required)
designing and building with Snowflake: 3 years (Required)
Work Location: Remote

Health insurance"
562,Principal Data Engineer,Amwell,"Boston, MA•Remote","$171,900 - $210,100 a year","Company Description
Amwell is a leading telehealth platform in the United States and globally, connecting and enabling providers, insurers, patients, and innovators to deliver greater access to more affordable, higher quality care. Amwell believes that digital care delivery will transform healthcare. We offer a single, comprehensive platform to support all telehealth needs from urgent to acute and post-acute care, as well as chronic care management and healthy living. With over a decade of experience, Amwell powers telehealth solutions for over 150 health systems comprised of 2,000 hospitals and 55 health plan partners with over 36,000 employers, covering over 80 million lives.
Brief Overview:
The Principal Data Engineer will play a key role in the Data Platform team within Engineering at an exciting time as we build out our new Converge platform and migrate from legacy solutions to cloud data solutions. They will assess data architecture maturity using industry-accepted principles and standards, identify gaps and drive standardization and adoption across the organization. They will identify Master Data Management needs in concert with the company-wide MDM initiative. The Principal Data Engineer will define and be a Subject Matter Expert in our data classification strategy and will ensure adherence across development squads. They will utilize their expertise to inform and guide others as we strive to capture, classify, secure and report on corporate data in our own product and other systems used across the business.
Core Responsibilities:
Evaluate and assess data and application integration framework
Identify master data management opportunities and needs
Define the target state and drive implementation of data catalog to enable effective data governance
Establish maps of business capability to data, services, users-roles, IT product model mapping
Develop and analyze various viewpoints, mappings to show relationships between data artifacts and business capability/attributes
Act as a liaison across Data, Engineering, Product, Hosting, Analytics and Delivery teams to drive standardization across data architecture principles
Analyze process, and provide guidance to streamline and optimize data enablement, operational, communication and training or literacy processes
Qualifications:
12+ years hands-on experience within data engineering organization
5+ years demonstrated experience in data architecture, data management, data governance and analytics within growing, agile, global organizations. Healthcare experience is a plus.
5+ years of experience in multiple database technologies such as distributed processing big Data platforms like AWS, GCP and tools like Spark, Kafka, Snowflake, Redshift, Athena, AWS Glue, Python/PySpark. etc.
5 + Years of experience building architecture to transform legacy to modern data platforms (Oracle to Cloud)
Experience in BigQuery and Informatica IICS.
Proficiency in data visualization tools (Tableau, Looker, etc.)
Knowledge of agile software development process and familiarity with performance metric tools
Distinct customer focus and quality mindset
Strong analytical and problem-solving skills; ability to drive creative, efficient solutions
Ability to work at an abstract level and build consensus across multiple viewpoints
Ability to set priorities, guide your own learning and contribute to domain knowledge
Excellent interpersonal, leadership, and communication skills
Bachelor's degree in computer science or other technically focused degree; master's degree preferred
Additional information
Working at Amwell
Amwell is changing how care is delivered through online and mobile technology. We strive to make the hard work of healthcare look easy. To make this a reality, we look for people with a fast-paced, mission-driven mentality. We're a culture that prides itself on quality, efficiency, smarts, initiative, creative thinking, and a strong work ethic.
Our Core Values include One Team, Customer First, and Deliver Awesome. Customer First and Deliver Awesome are all about our product and services and how we strive to serve. As part of One Team, we operate the Amwell Cares program, which brings needed assistance to our communities, whether that be free healthcare for the underserved or for people affected by natural disasters, support for equality, honoring doctors and nurses, or annual Amwell-matched donations to food banks. Amwell aims to be a force for good for our employees, our clients, and our communities.
Amwell cares deeply about and supports Diversity, Equity, and Inclusion. These initiatives are highlighted and reflected within our Three DE&I Pillars - our Workplace, our Workforce, and our community.
Amwell is a ""virtual first"" workplace, which means you can work from anywhere, coming together physically for ideation, collaboration and client meetings. We enable our employees with the tools, resources and opportunities to do their jobs effectively wherever they are!
The typical base salary range for this position is $171,900 - $210,100. The actual salary offer will ultimately depend on multiple factors including, but not limited to, knowledge, skills, relevant education, experience, complexity or specialization of talent, and other objective factors. In addition to base salary, this role may be eligible for an annual bonus based on a combination of company performance and employee performance. Long-term incentive and short-term variable compensation may be offered as part of the compensation package dependent on the role. Some roles may be commission based, in which case the total compensation will be based on a commission and the above range may not be an accurate representation of total compensation.
Further, the above range is subject to change based on market demands and operational needs and does not constitute a promise of a particular wage or a guarantee of employment. Your recruiter can share more during the hiring process about the specific salary range based on the above factors listed.
Unlimited Personal Time Off (Vacation time)
401K match
Competitive healthcare, dental and vision insurance plans
Paid Parental Leave (Maternity and Paternity leave)
Employee Stock Purchase Program
Free access to Amwell's Telehealth Services, SilverCloud and The Clinic by Cleveland Clinic's second opinion program
Free Subscription to the Calm App
Tuition Assistance Program
Pet Insurance"
563,Senior Data Engineer,Affine Inc,Remote,N,"Key Responsibilities:
Maintain and expand upon our paid marketing data by maintaining our ETL process, consuming data from third parties
Work with cross-functional stakeholders in defining and documenting single sources of truth to ensure consistent and high-quality data
Reconcile data issues and alerts between various systems, finding opportunities to innovate and drive improvements
Own and document data pipelines and data lineage
Work with the internal data team ETL production
Requirements:
3+ years of experience as a Data Engineer or in a similar role
3+ years in programming pipelines in languages like Python, Scala, Java
3+ years of broad software engineering experience including infrastructure and application development
Experience with data modelling, data warehousing, and building ETL pipelines from data available through external APIs
Expert-level knowledge of SQL.
Experience with Cloud technologies like AWS, GCP, Containers, Kubernetes. (GCP Preferred)
MS/BS in Computer Science, Systems Engineering or Electrical Engineering or Applied Mathematics or equivalent.
Job Type: Full-time
Benefits:
Health insurance
Paid time off
Compensation package:
Performance bonus
Yearly pay
Experience level:
4 years
Schedule:
Monday to Friday
Experience:
SQL: 3 years (Required)
Python: 3 years (Required)
ETL: 3 years (Required)
Cloud infrastructure: 2 years (Required)
Application development: 3 years (Required)
Work Location: Remote

Health insurance"
564,Software Engineer- Big Data,State Farm,"Dunwoody, GA 30346•Hybrid remote",N,"Overview:
Do you crave innovation and want to work for a company that is the BEST at what they do in the industry? Does the opportunity to work remote and maintaining a work life balance appeal to you? Then we have the perfect job for you! We are seeking software engineers who push the envelope and strive to create the best product possible. This position will allow you to utilize different technologies, languages, and frameworks to drive solutions while working on inclusive teams that foster diversity of thought. You will be provided opportunities via in house training programs for upskilling to support your development and career goals!
Check out what our Software Engineers have to say about working at State Farm: https://youtu.be/1t5y2PHDypI
Responsibilities:
Looking for something that is pushing the envelope at State Farm and moving Enterprise Technology to the next level? If so, the Claims Data Marketplace (CDM) product may be your answer!
CDM is building out the next generation of a data warehouse in the cloud called ‘Marketplace’. The focus is on migrating Claim’s analytic data and consumers to the cloud. Consumers of this data include Data Scientists, Power Users and BI Reporting to name a few. The future will also include the integration with the Enterprise Marketplace, Unstructured data and other consumers wanting to integrate Claim’s data.
We work closely with the Claims Business and Data Science teams to enable analytic initiatives using the AWS technology stack. Qualified candidates should be high performers, able to work on multiple priorities with a strong desire to learn and grow.
The person filling this position will be responsible for working with consumers of the Claims analytic data to determine their data requirements and building the data pipeline for aggregated domains of data. They will also work closely with other team members on ETL activities associated with data modeling and data design, and ingestion of the data into AWS redshift for consumption.
Qualifications:
Required Skills:
Solution development and deployment on AWS
Experience with various software development tooling and techniques, (e.g., GIT, CI/CD, data pipelines)
Python development experience
Experience with AWS commonly used services (i.e. S3, Lambda, SNS, Glue, Terraform)
Preferred Skills:
Data movement experience (i.e. ETL)
Java
Claims Data Domain knowledge
AWS QuickSight
Join State Farm!
As a Fortune 50 company, we hire the best employees to serve our customers, making us a leader in the insurance and financial services industry. State Farm embraces diversity and inclusion to ensure a workforce that is engaged, builds on the strengths and talents of all associates, and creates a Good Neighbor culture.
We offer competitive benefits and pay with the potential for an annual financial award based on both individual and enterprise performance. Our employees have an opportunity to participate in volunteer events within the community and engage in a learning culture. We offer programs to assist with tuition reimbursement, professional designations, employee development, wellness initiatives, and more!
Visit our Careers page for more information on our benefits, locations and the process of joining the State Farm team!
HYBRID: Qualified candidates (in or near hub locations listed below) should plan to spend time working from home and some time working in the office as part of our hybrid work environment.
HUB LOCATIONS: Dunwoody, GA or Bloomington, IL
SPONSORSHIP: Applicants are required to be eligible to lawfully work in the U.S. immediately; employer will not sponsor applicants for U.S. work authorization (e.g. H-1B visa) for this opportunity
#LI-BS1
SFARM
#LI-Hybrid"
565,Senior BI Data Engineer,Teletrac Navman,United States,"$125,000 - $145,000 a year","The Company
Teletrac Navman is a software-as-a-service (SaaS) provider leveraging location-based technology that empowers people managing mobile assets to move their business forward with certainty.
The Position
We’re looking to add a Senior BI Data Engineer to join Teletrac Navman! This role will design, implement, and maintain systems used to collect and analyze business intelligence data.
In this role, you’ll get to:
Create and maintain optimal data pipeline architecture.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Keep TN data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues.
Other duties and/or projects, as needed.
Qualifications
At Teletrac Navman, we believe in your potential to make an impact. And we believe in giving you the opportunity, accountability and visibility to do just that.
We are looking for people who have:
Bachelor degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
At least 10+ years of software development experience
At least 5+ years of experience in a Data Engineer role
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience using the following software/tools:
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Redshift.
Experience with data pipeline and workflow management tools: Amazon SWF, AWS Step Functions, Apache Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift, Glue, Lambda etc., with development in Node JS preferred.
Experience with stream-processing systems preferred: Kinesis, Storm, Spark-Streaming, etc.
Hands on coding experience with object-oriented/object function scripting languages: C#, Java, Python, Scala, etc.
Analytical, framework thinker.
Comfortable working in a fast-paced environment.
Perks!
Variety of medical, dental, and vision plans, including a wellness program
15 vacation days + 5 sick days + 2 floating holidays + 11 major holidays
Employee Assistance Program (EAP): Access to a range of support and resources such as counseling services and support for major life events, pet insurance, weight and health coaching, financial and legal concerns, and more
Family Planning: Paid parental/family leave of absence (LOA), adoption assistance up to $20,000, wide range of fertility care and support with the Maven Clinic, adult and elder care support, and more
Flexible work: Teletrac Navman is committed to providing a work environment that maximizes functionality, collaboration and work/life satisfaction with flexibility where available
401k: Pre- and Post-Tax (Roth) opportunities to save for retirement starting day one
Learning & Growth: Full access to LinkedIn Learning, product training, tuition reimbursement program, student debt repayment program with Fidelity
Community Impact: Charitable fundraising activities and a paid Day of Caring for volunteering
The base compensation range for this position is $125,000 to $145,000 per annum. Your actual base salary will be determined based upon a number of factors which may include relevant experience, skills, location (labor market data), credentials (education, certifications), and internal equity.
Vontier partners with you and your family on your health and wellness journey. Visit VontierBenefits.com to view our benefits. We offer a premium suite of health and wellness programs for you and your family. With programs for family planning from Maven Clinic to managing diabetes like Livongo, coverage for women's health, support for adult and elder care, paid parental leave, a generous 401(k) plan with matching company contributions and more. Vontier is here for all stages of life.
Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. The Company reserves the right to modify this information at any time, subject to applicable law.
Teletrac Navman is a leading software-as-a-service (SaaS) provider leveraging location-based technology that empowers people managing mobile assets to move their business forward with certainty. It tracks and manages more than 500,000 vehicles and assets for more than 40,000 companies around the world. With headquarters in Orange County, CA, we have an international presence with additional offices in the United States, United Kingdom, ANZ and Mexico. Check our website at www.teletracnavman.com.
Vontier is a $3B global industrial technology company focused on smarter transportation and mobility. Our six operating companies—Gilbarco Veeder-Root, Global Traffic Technologies, Teletrac Navman, Matco Tools, Hennessy Industries, and DRB Systems—are united by a powerful purpose: mobilizing the future to create a better world. Our portfolio of trusted brands includes market-leading expertise in mobility technologies, retail and commercial fueling, fleet management, telematics, vehicle diagnostics and repair, and smart cities. Vontier’s pioneering solutions advance safety, security, efficiency, and sustainability worldwide.
""Vontier Corporation and all Vontier Companies are equal opportunity employers that evaluate qualified applicants without regard to race, color, national origin, religion, ancestry, sex (including pregnancy, childbirth and related medical conditions), age, marital status, disability, veteran status, citizenship status, sexual orientation, gender identity or expression, and other characteristics protected by law."""
566,Data Engineer III,Central California Alliance for Health,"Scotts Valley, CA 95066•Remote","$91,694 - $146,723 a year","We have an opportunity to join the Alliance in the Data Analytics Services Department.
Note: Data Engineer III is a working title, however internally the position is currently referred to a Software Developer III - EDW. Throughout the recruitment, these titles may be used inter-changeably.
There are two positions available and can be filled in any office (Salinas, Merced, Scotts Valley).

WHAT YOU'LL BE RESPONSIBLE FOR
The Data Engineer III will:
Create, develop, document, test, implement, and maintain enterprise data warehouse and databases in an Agile environment to support the needs and requirements of the Alliance reporting systems
Plan, schedule, and coordinate the design, development, testing, implementation, and documentation of the data warehouse and ETL/ELT applications using appropriate design techniques and modification of existing software
Participate in sprint planning of software and ETL/ELT design and development as part of the Agile development lifecycle

ABOUT THE TEAM
We integrate data from many sources to create information marts that provide the basis for business intelligence, reporting, and advanced analytics.

WHAT YOU'LL NEED TO BE SUCCESSFUL
To read the full position description, and list of requirements click here.
Knowledge of:
Structured Query Language
Dimensional Modeling and Data Warehouse Design
Microsoft SQL Server Database
Oracle Database or Microsoft Azure
ETL (Extract, Translate, and Load) processing and SSIS packages
Ability to:
Develop, document, apply and provide guidance on EDW development best practices in Wherescape systems, including 3D and RED, in modern enterprise data warehouse systems
Gather and analyze complex data and information, troubleshoot issues, and develop practical and effective solutions
Provide advice and recommendations on projects, tasks and issues, including the impact of proposed solutions
Quickly learn new business concepts and the modern methodologies
Ability to understand and document business users' requirements
Education and Experience:
Bachelor's degree in Management Information Systems or Computer Science
A minimum of five years of enterprise data warehouse experience (a Master's degree may substitute for two years of the required experience); or an equivalent combination of education and experience may be qualifying

OTHER DETAILS
While this position is connected to one of our Alliance offices, we are in hybrid telecommute work environment right now and we anticipate that the interview process will take place remotely.
Our Alliance office locations have officially re-opened as of May 2, 2022 and while some employees may work in full-time telecommute schedules, attendance at quarterly company-wide events or department meetings may be expected.
Based on the nature of work, this position may require onsite presence, which is dependent on business need. Details about this can be reviewed during the interview process

COVID-19 Vaccine Requirement: At this time, the Alliance is requesting the vaccination status of its workforce for COVID-19. While this position doesn't require vaccination as a condition of employment, all employees are required to report their vaccination status, and, if vaccinated, provide proof of vaccination.

The full compensation range for this position is listed by location below.

The actual compensation for this role will be determined by our compensation philosophy, analysis of the selected candidate's qualifications (direct or transferrable experience related to the position, education or training), as well as other factors (internal equity, market factors, and geographic location).
Scotts Valley pay range
$100,537—$160,867 USD
Merced pay range
$91,694—$146,723 USD
Salinas pay range
$100,537—$160,867 USD
Additionally, all positions at the Alliance are required to meet these minimum qualifications.
OUR BENEFITS
Medical, Dental and Vision Plans
Ample Paid Time Off
12 Paid Holidays per year
401(a) Retirement Plan
457 Deferred Compensation Plan
Robust Health and Wellness Program
Onsite EV Charging Stations
And many more
ABOUT US
We are a group of over 500 dedicated employees, committed to our mission of providing accessible, quality health care that is guided by local innovation. We feel that our work is bigger than ourselves. We leave work each day knowing that we made a difference in the community around us.
Join us at Central California Alliance for Health (the Alliance), where you will be part of a culture that is respectful, diverse, professional and fun, and where you are empowered to do your best work. As a regional non-profit health plan, we serve members in Merced, Monterey and Santa Cruz counties. To learn more about us, take a look at our Fact Sheet.
At this time the Alliance does not provide any type of sponsorship. Applicants must be currently authorized to work in the United States on a full-time, ongoing basis without current or future needs for any type of employer supported or provided sponsorship."
567,Data Warehouse Engineer,Tradefull,Remote,N,"Data Warehouse Engineer ( Business Intelligence )

A rare opportunity that has all the ingredients to make it big. Tradefull is a powerful e-commerce SaaS platform with integrated marketplaces, rooted in building blocks our founder created to power a multimillion-dollar e-commerce retailer. We provide a unique software and services model, built on solid ideas and smart, motivated people.
We like risk takers, we like to fail fast, we like to work as a team, and most importantly, we like to put ourselves out there, and give it all. We are growing fast, and so does our work. We must solve complex problems, quickly, and for scale. If this sounds like a place that suits, come-on, what are you waiting for? Apply right away!

Opportunity

Are you excited by the prospect of optimizing or even re-designing a company’s data architecture? Tradefull is looking for a Data Warehouse Engineer to help us build the next generation of e-commerce SaaS platforms and bring our ideas to life! In this role you can look forward to expanding our data resources, data pipeline architecture, and optimizing data flow in our organization. Your work will enhance and accelerate our internal and external reporting capabilities and help us to deliver on data science initiatives. Our candidate should be self-directed, hands-on, and understand that scale and efficiency are critical while they build innovative solutions.

What you’ll accomplish

Build required infrastructure for optimal data extraction and transformation
Creat e and manage data pipeline architecture to support internal and external business intelligence initiatives
Design, maintain and tune BigQuery databases
Establish and protect trust of Tradefull’s reporting data and data pipelines
Assembl e large, complex data sets to meet functional/non-functional business needs
Automate manual processes, optimizing data delivery, re-designing infrastructure
Utiliz e a wide variety of data sources using SQL and ‘big data’ technologies
Collaborat e with executives, management, and partners to support data infrastructure needs
Formulat e data tools to support Tradefulll becoming an innovative industry leader
Brainstorm with a team of highly skilled professionals to achieve greater data system functionality
Construct and implement technology best practices, guidelines, and repeatable processes"
568,Data Engineer (MarTech),Productive Edge,Remote,N,"The Marketing Technology Data Engineer at Productive Edge will be responsible for building and delivering marketing technology solutions for the e-commerce division of a key Fortune 500 Healthcare and Wellness client. You will work with the e-commerce and marketing teams to understand their requirements and design, implement and maintain data pipelines, data models and analytics infrastructure to support their needs. The ideal candidate should have strong experience in data engineering, data modeling, data warehousing, ETL, and analytics technologies.

Responsibilities:
Work closely with the eCommerce and marketing teams to identify data requirements
Develop and maintain data models and pipelines to integrate various sources of data into a central data platform
Develop and maintain frameworks to ensure the accuracy and completeness of data
Build analytics infrastructure to support data-driven marketing initiatives
Develop and integrate monitoring, alerting, and ops functionality within platform
Collaborate with cross-functional teams including data scientists, analysts and developers to build end-to-end solutions
Stay up-to-date with emerging trends and technologies in data engineering and marketing technology
Qualifications:
Bachelor's or Master's degree in Computer Science, Engineering, or a related field
3+ years of experience in data engineering or related roles
Strong experience in data warehousing, modeling, pipelines, and analytics technologies
Experience with cloud-based technologies such as Azure, AWS, or GCP
Experience with distributed technologies such as Hadoop, Spark, Hive, or Presto
Experience with streaming or event-driven platforms such as Kafka
Proficiency with object-oriented programming languages such as Python, Scale, or Java
Proficiency with SQL for querying and transforming data
Strong analytical and problem-solving skills
Excellent communication and collaboration skills
Productive Edge offers competitive compensation packages, comprehensive benefits, and opportunities for career advancement. If you are a highly motivated data engineer with a passion for marketing technology and e-commerce, please apply today!

About Productive Edge
Productive Edge is a Chicago-based leader in digital transformation. As digital business consultants, we focus on digital strategy, customer experience, operations improvement, and product & services enablement. How do we do this? Through launching solutions for Data and Artificial Intelligence (AI), Intelligent Automation (IA), Internet of Things (IoT), and Cloud Native Solutions. Sounds fun, right? It gets better...and that’s where you come in!

Productive Edge offers the dynamic opportunity to use your talents in a progressive, highly-collaborative environment that values innovation and creativity. We are a culture that stays on the cusp of today’s developing and emerging technologies. Our passion, ingenuity, and dedication sets us apart from other consultants (as does our modern work atmosphere with fun outings, foosball, stocked fridge and other activities). Because we work with and develop solutions for some of the world’s most exciting brands, we are actively searching for individuals who are ready to change the way the world operates.

And if that’s not enough, Productive Edge has been featured as one of Chicago’s 101 Best and Brightest Companies to Work For, included in Crain’s Fast 50, and is a regular on Inc. 5000."
569,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
570,Data Engineer I,HEARTLAND BUSINESS SYSTEMS LLC,Remote,N,"Description:
Position Summary:
This position would require a candidate to possess a strong technical background in developing and delivering BI solutions along with a strong understanding of SQL Server environments. Business intelligence (BI) is a set of technologies and practices for transforming business information into actionable reports and visualizations. The BI Data Engineer transforms data into a useful format for analysis and is focused on the design and architecture.
A BI Data Engineer is the data professional who prepares the data infrastructure to be leveraged by the HBS BI Data Developers. The BI Data Engineer will design, build, integrate data from various resources and manage big data. The BI Data Engineer ensures the operations of the data pipeline follow a consistent process of Ingestion, Processing, Storage and Access. The work involves tuning databases for fast analysis and creating table schemas.
The BI Data Engineer is responsible for making data easily accessible, ensuring the process works smoothly and is optimized. The BI Data Engineer is a critical firm member of the Data Team, The BI Data Engineer will run Extract, Transform and Load (ETL) on top of datasets and create data warehoused that can be used for reporting and analysis. The BI Data Engineer ensures the operations of the data pipeline follow a consistent process of Ingestion, Processing, Storage and Access.
Roles and Responsibilities/ Essential Functions:
Meet with clients to understand their current business processes and needs to provide consulting services and direction on how to build or grow their current data strategy.
Work with HBS Sales Solutions consultants to identify and grow opportunities within HBS client environments.
Support and administer the underlining infrastructure and layout of a client data environment.
Develop and design the process for the customer data collection process.
Develop policies and procedures for the collection and analysis of data.
Review customer sources to ensure integrity of the data collection process.
Collaborate with the BI Data Developers to ensure the requirements are being met to build the right solution needed.
Estimate development effort required to deliver data customer needs and requests.
Use business analysis skillset to identify development needs for the purpose of streamlining and improving the operations of the organization for efficiency and profitability.
Ability to work independently or as a team on project-based solutions for clients.
Work with team mates to continue to grow and mature data services and delivery options for HBS clients.
Based on experience, one may mentor other engineers in developing scalable, secure, high-performance BI and Data solutions.
Minimum of 1,238 hours billed per fiscal year prorated based on start date. These charge hour requirements will be balanced against professional development and on the job training.
Requirements:
Competencies
Accuracy – Ability to produce high quality work deliverables leveraging industry best practices.
Analytical Skills - Strong abilities require to effectively interpret customer business needs and translating them into application and operational requirements, resolving complex technical and business problems.
Communication – strong written, verbal, and non-verbal communication skills, especially conveying complex information in an understandable manner.
Leadership – Ability to motivate and guide others to ensure performance is an accordance with clear expectations and goals.
Learning – Ability to quickly learn new technologies to deliver solutions.
Presentation Skills – Ability to effectively conduct formal and informal presentations in both small and large group settings within all levels of a company.
Project Management – Ability to demonstrate an understanding of process engineering, planning, organizing, staffing, directing, and controlling work tasks.
Time Management – Ability to effectively utilize available time for managing multiple tasks/projects simultaneously.
Required Experience:
Experience with needs analysis, software evaluation and selection, customization, and implementation
Microsoft BI Suite Experience
Microsoft Excel
Programming and Processing Experience
T-SQL
ETL
Azure Experience
Azure SQL Database
Strong knowledge of SQL utilizing MS SQL Server
Preferred Experience:
Expertise in Professional Services or similar client facing roles
Experience in multiple industry (Education, Healthcare, Retail, Manufacturing) verticals
PowerShell knowledge and understanding
Understanding of report writing and visualization
Data Warehousing systems and architecture experience in 'real world', practical, successful implementations
Understand multi-dimensional database structures and schemas
Strong knowledge of system design, development, and deployment
Azure Experience
Azure SQL Manage Instance
Azure Architecture
Azure Data Factory
Microsoft BI Suite Experience
Microsoft SQL Server Reporting Services (SSRS)
Microsoft SQL Server Analysis Services (SSAS)
Microsoft SQL Server Integration Services (SSIS)
Microsoft Power BI
Required Skills, Education and/ or Certifications:
Bachelor’s degree in business or I.T. related discipline accepted
Equal Opportunity Employer - Including Disabled and Veterans
#HBS"
571,"Data Engineer, Data Platform",Grammarly,"San Francisco, CA 94104•Hybrid remote","$134,000 - $162,000 a year","Grammarly is excited to offer a remote-first hybrid working model. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, Poland, or Portugal. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly's hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków. This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.
Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.
The opportunity
Every day, tens of millions of people and 50,000 professional teams rely on Grammarly's AI-enabled communication assistance to help them communicate confidently and achieve their goals. Our team members have the autonomy to take on exciting challenges in pursuit of our mission to improve lives by improving communication. Together, we're building on more than a decade of steady growth and profitability. We're defining the communication assistance category for individuals, enterprises, and developers with tailored service offerings: Grammarly Premium, Grammarly Business, Grammarly for Education, and Grammarly for Developers. All of this begins with our team collaborating in an inclusive, values-driven, and learning-oriented environment.
To achieve our ambitious goals, we're looking for a Data Engineer to join our Data Platform team. The Data Platform team is responsible for all aspects of the data lifecycle here at Grammarly.
As a rapidly growing company, we offer opportunities to join a team and learn from experienced peers, as well as opportunities to build product offerings and teams from the ground up. Within our growing engineering organization, there is plenty of room for ownership and direct impact on users.
Grammarly's engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.
Your impact
As a Data Engineer, you will create engineering and analytical efficiencies as the owner of our company-wide data lake. Our cutting-edge data lake is the central hub for all data producers and data consumers, so your impact will span all of Grammarly. You will have the opportunity to design and code highly optimized algorithms and datasets.
Data engineers are responsible for making disparate data centrally available to Grammarly's team members while ensuring query efficiency and efficacy. The technical requirements of this role include a deep understanding of the querying engine of Spark, the performance (read/write) optimization of Delta tables, data modeling, data transformation at scale, Python or Scala coding abilities, and advanced SQL skills. The backbone of a Data Engineer at Grammarly's success will be communication, stakeholder management, and a passion for data.
We invite you to share your knowledge, experience, and goals with us to help us find the best team match for you. Tell us your superpower!
In this role, you will:
Start building and pushing code in your first week and ship meaningful features in your first few months.
Build algorithms that allow for the quick and efficient retrieval of large amounts of data.
Build datasets that create fact-based insight, influence operational decision-making, and provide data-driven business solutions.
Have the opportunity to mentor new hires.
Shape and build a technical and data-driven culture at Grammarly.
We're looking for someone who
Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has at least 2 years of experience using large-scale data-analytics engines like Spark.
Displays excellent software-engineering fundamentals, including knowledge of algorithms and data structures.
Demonstrates perseverance when faced with tough technical issues.
Has some experience with AWS or other cloud offerings (GCP, Azure, etc.).
Has experience building, deploying, and debugging production systems at scale.
Cares about the end-user experience and strives to ensure high quality.
Support for you, professionally and personally
Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.
A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.
Compensation and benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more:
Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)
Disability and life insurance options
401(k) and RRSP matching
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region's cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.
United States:
Zone 1: $167,000 - $202,000/year (USD)
Zone 2: $150,000 – $182,000/year (USD)
Zone 3: $142,000 – $172,000/year (USD)
Zone 4: $134,000 – $162,000/year (USD)
We encourage you to apply
At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).
Please note that EEOC is optional and specific to US-based candidates.
#NA
#LI-DT1
Please note that Grammarly's COVID-19 vaccination policy requires that all team members in North America be vaccinated against COVID-19 to meet in person for Grammarly business or to work from a North America hub location. Qualified candidates in North America who cannot be vaccinated for medical reasons or because of a sincerely held religious belief may request a reasonable accommodation to this policy. In Europe, all team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated and provide proof of COVID-19 vaccination.
#LI-Hybrid"
572,Data Engineer,Lynx Analytics,"San Francisco, CA•Remote",N,"COMPANY OVERVIEW
Founded in 2010, Lynx Analytics is a predictive analytics company run by world-class quantitative marketing scientists and industry-experienced data scientists. Our focus is to become a leading analytics solution provider in our chosen fields of expertise (telecom, retail, life sciences, and financial services) while advancing graph analytics technology.
Lynx is headquartered in Singapore with operations in Hong Kong, Germany, USA, Hungary, South Africa, Indonesia, and several other Southeast Asian countries. We work with some of the world's largest companies and are constantly looking to expand our knowledge base and geographical footprint. Lynx Analytics' technology is deployed with various Clients across Asia and has significant growth potential.
We have a diverse and inclusive global team comprising Professors, PhDs, MSc's, and MBAs from Ivy Leagues, INSEAD and NUS with a broad spectrum of experience in start-ups and blue-chip companies (Google, SAP, Vodafone, GE, Morgan Stanley, Barclays, HSBC to name but a few). It is the combination of our industry insight and experience, scalable proprietary technology, and highly qualified people that drives our compelling value proposition.
We are looking for ambitious, innovative, empathetic and relentless team players to explore the career opportunities that we offer as we continue to scale our operations.
We are looking for a Data Engineer to work on automating and productizing advanced big data transformation and analytics pipelines. You would be working with standard big data technologies (Hadoop, Spark, etc,), as well as our proprietary big graph analysis framework.
KEY RESPONSIBILITIES
A Data Engineer's responsibility is to implement and deploy data analysis pipelines at various clients of Lynx Analytics. This includes participating in the activities below:
Understand deeply the business problem that we are trying to solve by our analytical solution
Through continuous consultations with employees of our client, discover the client's existing data sources that are relevant to the problem we try to solve. This includes discussions with client IT, data owners, future business users, etc.
Working together with the IT teams of the client, define the technical architecture for the analytical solution that we are to deploy for the client.
Implement the data ingestion subsystem: this is the system responsible for moving all the necessary data sources to a single location where the actual analysis will happen.
Implement the data analysis pipelines.
Integrate the results into business UIs developed by Lynx or pre-existing client software systems
REQUIREMENTS
Relevant tertiary qualification, preferably at Masters level or above, in Engineering or another relevant discipline with strong academic results
Strong programming skills
Experience in GCP, Airflow and Spark
Solid experience in Python and SQL
Good problem-solving skills
Good communication skills
DESIRABLE
Experience in Big Data
A minimum of 3 years of experience in Data Science or Analytics
Industry experience in working for a big enterprise (like our clients)
WHAT WE OFFER
Opportunity to work on creating innovative, leading-edge data science pipelines using our state of the art, in-house built big graph tool
Work closely with the developers of the (big graph) tool you will be building upon
Be a member of a very strong team with mathematicians, ex-Googlers, Ivy League professors, MBA alumni and telecommunications industry experts
Startup atmosphere"
573,Data Engineer,LendKey Technologies,Remote,N,"Description:

Remote- United States · Engineering
DESCRIPTION
LendKey is solving a complex challenge – to improve lives with lending made simple – by helping financial institutions compete in the digital age and provide a delightful customer experience, while providing borrowers with the simple, transparent, digital borrowing experience they have come to expect and desire. LendKey works with hundreds of credit unions and banks to conduct their education finance and home improvement loan programs.
We are looking for an enthusiastic data engineer to help us build out our data lake using the lakehouse architecture. Our data capabilities and culture are still in the early stages, so this is an opportunity to build a modern, near real-time Spark-based data platform from the ground up.
Please note- this is a permanent role and we are not entertaining contract to contract at this time.
What you’ll do:
Contribute to building out the data lake; including the architecture, design, development, implementation, and support of data warehouse
Contribute to the development of data governance policies, including data normalization and anonymization, chain of custody, etc.
Design, manage, and support self-healing ETL processes, including ongoing data quality and testing
Identify gaps and opportunities in data infrastructure and design and build appropriate solutions
Partner with the product and engineering teams to develop scalable, extensible systems
Work with end users and power users across the organization to translate business questions and requirements into appropriate data structures to be used by reporting tools
Help in monitoring and troubleshooting system performance, reliability, availability, and recoverability of data across the data platform
Build proofs of concept to evaluate visualization platforms
Assist and lead migration efforts from legacy platform to the data lake
Requirements:
What we’re looking for:
Culture Fit:
Strong desire to work for a mission-based organization that emphasizes the importance of providing exceptional customer service and aligned with our core values: Truthful at all times; Helpful to teammates, clients, and customers; Present, committed & engaged to their teams and work; Driven to be courageous to make an impact; and Diligent & conscientious in executing every element of work.
Technical/Business Experience
Bachelor’s degree in Computer Science or related field
5+ years of experience in the data engineering field with deep experience in at least one of the major relational databases like SQL Server, Oracle, and/or MySQL, including creating and maintaining stored procedures and functions
A minimum of 2+ years of experience in:
Designing, building, and maintaining Star Schemas
Building data processing pipelines in Python or PySpark with a firm understanding of data load strategies for dimensions and facts
Experience and background in building data platforms for financial services is a plus
Experience in building data visualization with one or more of the following is a plus: Power BI, Tableau, Looker
Experience with one or more of the following is a big plus: Hadoop/Hive/Spark

Why work for us?
We have a lot to offer those that are looking to take the next step in their career, including:
Opportunity to join a growing fintech
Creative and transparent company culture
Growth potential
Competitive salary and bonuses
Comprehensive medical, dental, vision, and life insurance benefits
Generous vacation and holidays
Company stock options for eligible employees
Flexible/remote work arrangements
LendKey is an equal opportunity employer and values diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or any other basis prohibited by applicable state or federal law. LendKey offers reasonable accommodations to candidates with physical or mental disabilities. If you need accommodations to participate in the job application or interview process due to a disability, please let us know at HR@lendkey.com.
If you are a resident of Colorado or New York City, please email HR@lendkey.com to receive compensation information for this role. Please be sure to include this posting’s job title in the subject line of the email to help ensure a timely response. Base pay may vary depending on job-related knowledge, skills, and experience. This information is provided in accordance with the Colorado Equal Pay Act and New York City's Pay Transparency Law. It is specific to Colorado and New York City residents may not be applicable to other locations.
About Us
LendKey delivers high quality consumer loans directly to financial institutions through its digital lending-as-a-service model and indirectly through ALIRO by LendKey, an innovative deal network designed to enable financial institutions to buy, sell and broker loans. The platform delivers the technology, servicing, and support that financial institutions need to establish a strong digital lending presence, acquire loans for their balance sheets, and build lifetime relationships with their partners and customers. Lenders optimize their offerings by drawing upon LendKey’s fintech platform for demand generation, online decisioning, loan origination, loan servicing, compliance expertise, risk analytics, and proprietary balance sheet options. LendKey currently services more than $2 billion in loans and has helped community-capital providers deploy over $4.5 billion in loans since 2009. Visit LendKey for more information."
574,Senior Data Engineer,"Rocket Travel, Inc.","Chicago, IL•Remote","$125,000 - $170,000 a year","About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes."
575,Data Engineer,CareMetx,Remote,N,"Hey!

Are YOU passionate about applying cutting-edge technology to improve the human experience? Are you passionate about fixing a broken healthcare system that is difficult to navigate, with barriers to access and afford life-changing medicine and treatment? Are you passionate about technical excellence and deploying software that makes people happier and healthier? If so, CareMetx wants you to be a part of our growing engineering team!

At CareMetx, data teams own an outcome - that means teams are both accountable AND empowered for a unit of business value. We create an environment for learning opportunities and believe there is no such thing as “that’s not my job.” Our vision is to generate valuable insights from the data and drive strategic decision-making. All Caremetx data team members have the opportunity to explore new technologies and data trends. That means you will have the opportunity to grow deeper in the skills you’re passionate about and expand your breadth by learning skills that will help the team succeed.

As a Data engineer/analyst/scientist you will constantly deliver business value. You will also function as a catalyst for innovation and new ideas through creative problem-solving, elegant engineering, and the application of new technology and architectural patterns. You will help shape a performance-oriented learning culture by sharing your knowledge and skill depth within the team.


Manage large data sets and model complex problems that impact patient outcomes. Discover insights and identify opportunities using statistical, algorithmic, mining, and visualization techniques.


The core of the role is…
Define and build an industry-standard pipeline and Data Warehouse for a variety of data sources (No SQL, Relational, Text)
Enhance data collection procedures to include information that is relevant for building analytic systems
Model front end and backend data sources to help draw a more comprehensive picture of user flows throughout our system and enable powerful data analysis
Processing, cleansing, and verifying the integrity of data used for analysis.
Performing ad-hoc analysis and presenting results in a clear manner
Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.
Reformulating existing frameworks to optimize their functioning.
Testing such structures to ensure that they are fit for use.
Preparing raw data for manipulation by data scientists
Relevant experience…
Degree(s) in Engineering, Computer Science, Math, Statistics, Economics, or related fields
5+ years of professional experience either in Big Data, Data Engineering, or Business Intelligence. This might include ETL, data warehousing, or data visualization.
Experience with Talend is preferred
Experience with API is preferred
Understanding of CI/CD, data governance, and data quality framework (great expectations) is preferred
5+ years of hands-on experience applying principles, best practices, and trade-offs of schema design to various types of database systems: relational (Oracle, MSSQL, Postgres, MySQL), NoSQL (HBase, Cassandra, MongoDB), and in-memory (e.g., VoltDB). Understanding data manipulation principles.
Deep understanding of NoSQL databases like MongoDB/Dynamo DB.
Understanding of data flows, data architecture, ETL, Star vs Snowflake schema, and processing of structured and unstructured data
Minimum 3 years of designing and building production data pipelines from ingestion to consumption within a hybrid big data architecture, using Java, Python, Scala, etc."
576,Data Engineer,APLOMB Technologies,"1 3 Acre Ln, Princeton, NJ 08540","$70,000 - $75,000 a year","We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location"
577,"Data Engineer - Hybrid Onsite - King of Prussia, PA",OPENMIND TECHNOLOGIES INC,"460 N Gulph Rd, King of Prussia, PA 19406",$75 - $85 an hour,"Details:
JOB DESCRIPTION:
We are working with a client in need of a talented Data Engineer to support our client located in King of Prussia, PA. This contractor will be working on a high-level corporate initiative to build an enhanced customer service model for a leading utilities organization. This contractor will be working with our client on Data Engineering topics, including creating relevant data models, developing powerful data pipelines, exposing them through various mechanisms including APIs, and using data visualization tools to efficiently present data.
Responsibilities:
· Partner with our client’s leadership teams, engineers, program managers and data analysts to understand data needs.
· Design, build and launch efficient and reliable data pipelines transforming data into useful report ready datasets.
· Communicate at scale, through multiple mediums: presentations, dashboards, client-wide datasets, bots and more.
· Use your data and analytics experience to ‘see what’s missing,’ identifying and addressing data gaps, build monitors to detect data quality issues and partner to establish a self-serve environment.
· Broad range of partners equates to a broad range of projects and deliverables, including ML Models, datasets, measurements, services, tools and process.
· Leverage data and business principles to automate data flow, detect business exceptions, build diagnostic capabilities, and improve both business and data knowledge base.
· Build data expertise and own data quality for your areas.
QUALIFICATIONS:
· At least 4+ years' of advanced SQL experience (including at least one SQL DBMS and one no SQL).
· 4+ years' of Python development experience.
· 3+ years' experience with Data Modeling.
· Experience analyzing data to discover opportunities and address gaps.
· Experience working with cloud or on-prem Big Data/MPP analytics platform (i.e. SnowFlake, Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).
· BSc/BA in Data Science, Computer Science, Engineering.
· Familiarity with SAP order generation and invoicing modules
Job Types: Full-time, Contract
Salary: $75.00 - $85.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
advanced SQL (including at least one SQL DBMS, one no SQL: 4 years (Preferred)
Python development: 4 years (Preferred)
Data Modeling: 3 years (Preferred)
analyzing data to discover opportunities and address gaps: 1 year (Preferred)
cloud or on-prem Big Data/MPP analytics platform: 1 year (Preferred)
SnowFlake, Netezza, Teradata, AWS Redshift, Google BigQuery,: 1 year (Preferred)
Azure Data Warehouse: 1 year (Preferred)
SAP order generation and invoicing modules: 1 year (Preferred)
Work Location: One location"
578,Senior Data Engineer,DONIA LLC,Remote,$50 - $68 an hour,"Location: Remote
Work Schedule: Monday-Friday 9AM-5PM
Target Duration/End Date: up to 7 months
Specific Roles and Responsibilities include but are not limited to:
· Create and maintain optimal data pipeline architecture that is coherent and scalable, based on best practices of integrating data into a consolidated repository
· Perform the technical design, development, and component testing of repository changes
· Build analytics tools that utilize the data pipeline to provide actionable insights into customer engagement and experience, operational efficiency, and other key business performance metrics
· Build the infrastructure required for optimal extraction, transformation, and loading (ETL) of data from a wide variety of data sources using SQL, cloud, and ‘big data’ technologies
· Develop ETLs to move data securely from source to target systems
· Create, update, and maintain system documentation
· Develop new or build against existing APIs for data access or landing data as output for further downstream consumption in the appropriate target data store
· Perform special projects and initiatives as assigned
Education:
Bachelor's degree preferred. And at least 8 years of relevant experience
Skills and Experience Requirements:
· 8+ years of experience in writing SQL
· 8+ years of experience in copying, transferring, manipulating, and automating data operations that were manual processes
· Experience with tools and components of data architecture such as Informatica Power Center, IICS, SSIS, or similar ETL tools
· Experience working with Amazon Web Services or Microsoft Azure cloud computing platform and services
· In-depth knowledge of SQL and other database solutions
· Experience with data warehousing (Snowflake, Redshift etc.)
· Knowledge of modeling database schemas for large datasets
· Experience developing cloud-ready applications
· Experience working with programming languages like Python, Java, and Perl
Preferred Qualifications:
· Hands on experience developing Microsoft PowerBI solutions
· 5+ years hands-on experience in development with the suite of tools from Informatica Power Center and B2B Data Transformation
· Experience using Oracle 10g/11g, SQL Server and/or a database appliance
· Knowledge of metadata-driven enterprise reporting platforms
(Resume must stand alone in demonstrating qualifications and experience)
Benefits:
· 401(k)
· Dental insurance
· Employee assistance program
· Health insurance
· Life insurance
· Vision insurance
Why work with us:
At Donia, we foster a culture of growth. We recognize that everyone brings unique skills and experiences to their work and that the company is better for such diversity. We encourage you to pursue your interest and grow within the company. We promote from within because we value and reward contributions
About us:
We are a Business Management Consulting and Staffing firm
Our founder is an engineer and seasoned business management consultant. The values of precision, quality, and reliability are embodied in all our work
We aim to delight our clients through the high quality of our service. So much so, that almost all our contracts are through Word of mouth and referrals. Because for over fifteen years, we have worked with Non-profits, City, State, and Federal Agencies – strategizing and putting together specialized teams that can tackle any project
Donia's team of collaborative solution-finders will meet you where you are and build the system that will get you to your goals. Let us know how we can use our Business Consulting and Staffing expertise to help you
(DPSI85823Y0490)
Job Types: Full-time, Contract, Temporary
Pay: $50.00 - $68.00 per hour
Schedule:
8 hour shift
Day shift
Application Question(s):
You reviewed and are okay with the job details - including qualification requirements, location (In-Person), duration, and pay rate W2?
Work Location: Remote"
579,Data Engineer,Booz Allen Hamilton,"Washington, DC•Hybrid remote","$58,400 - $133,000 a year","The Opportunity:
Ever-expanding technology like IoT, machine learning (ML), and artificial intelligence (AI) means that there’s more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it’s gathered from disparate sources. We need a data professional like you to help our clients find answers in their big data to impact important missions from fraud detection to cancer research, to national intelligence.
As a Data Engineer at Booz Allen, you’ll use your skills and experience to implement data engineering activities on some of the most mission-driven projects in the industry. You’ll develop and deploy the pipelines and platforms that organize and make disparate data meaningful. Here, you’ll work with a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You’ll sharpen your skills in analytical exploration and data examination while you support the assessment, design, developing, and maintenance of scalable platforms for your clients. Work with us to use big data for good.
Join us. The world can’t wait.
You Have:
3+ years of experience with the entire data pipeline, including data acquisition, data prep, and database architecture
Experience with data engineering projects supporting data science and AI/ML implementations
Experience with Python and SQL
Experience creating software for retrieving, parsing, and processing structured and unstructured data
Experience creating solutions within a collaborative, cross-functional team environment
Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor, and operate data platforms
TS/SCI Clearance
Bachelor’s degree
Nice If You Have:
Experience with a public cloud, including AWS, Microsoft Azure, or Google Cloud
Experience with NoSQL
Experience with API development
Experience with containerization tools like Docker and Kubernetes
Experience with distributed data and computing tools such as Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka
Experience working on real-time data and streaming applications
Experience developing scalable ETL/ELT workflows for reporting and analytics
Experience with data warehousing, including AWS Redshift, MySQL, or Snowflake
Experience with UNIX and Linux, including basic commands and Shell scripting
TS/SCI clearance with a polygraph
Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; TS/SCI clearance is required.
Create Your Career:
Grow With Us
Your growth matters to us—that’s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.
A Place Where You Belong
Diverse perspectives cultivate collective ingenuity. Booz Allen’s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you’ll develop your community in no time.
Support Your Well-Being
Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we’ll support you as you pursue a balanced, fulfilling life—at work and at home.
Your Candidate Journey
At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we’ve compiled a list of resources so you’ll know what to expect as we forge a connection with you during your journey as a candidate with us.
Compensation
At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen’s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.
Salary at Booz Allen is determined by various factors, including but not limited to location, the individual’s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $58,400.00 to $133,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen’s total compensation package for employees.
Work Model
Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.
If this position is listed as remote or hybrid, you’ll periodically work from a Booz Allen or client site facility.
If this position is listed as onsite, you’ll work with colleagues and clients in person, as needed for the specific role.
EEO Commitment
We’re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change – no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law."
580,Senior Data Engineer (Spark),The Coca-Cola Company,"Atlanta, GA 30313",N,"You will work in the Global Data Engineering organization responsible for building cloud-based analytics products for APAC, EMEA, Americas, and Corporate that directly impact Coca-Cola’s business growth globally.
What You’ll Do
We are looking for a Senior Data Engineer who has good experience in data modeling and data integration using Spark and SQL.
Specialize in data integration from multiple external and internal sources in batch and real-time using Spark.
Design and implement the platform and frameworks required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Design, implement and operationalize data pipelines, data warehouses and semantic models in data marts using Azure Data Services (ADF, Synapse, Spark/Databricks etc.).
Coordinate and enhance data quality and availability across the Coca-Cola data estate.
Automate and optimize existing analytic workloads by recognizing patterns of data and technology usage.
Manage own learning and contribute to technical skill building of the team.
Embrace the engineering mindset and systems thinking.
Gain deep technical expertise in the data movement patterns, practices and tools.
Required Qualifications
5+ years experience in building and performance tuning Spark code (Scala preferred).
Highly proficient in SQL.
2+ years of designing and deploying data analytics pipelines in cloud using Azure Data Services.
3+ years using relational databases running ETL process and managing large data transformations.
2+ years of experience in designing data modles (STAR, Vault, Tabular) for analytics.
Polyglot development: Capable of developing in Scala and Python with good understanding of functional programming, SOLID principles, concurrency models and modularization.
Experience with workflow scheduling / orchestration such as ADF, Airflow or Oozie.
Passion for software engineering and craftsman-like coding prowess.
DevOps: Appreciates the CI and CD model and always builds to ease consumption and monitoring of the system. Experience with Maven (or Gradle or SBT) and Git preferred.
Believe in “Build, Ship, Monitor” Philosophy.
Personal qualities such as creativity, tenacity, curiosity, and passion for deep technical excellence.

Our Purpose and Growth Culture:
We are taking deliberate action to nurture an inclusive culture that is grounded in our company purpose, to refresh the world and make a difference. We act with a growth mindset, take an expansive approach to what’s possible and believe in continuous learning to improve our business and ourselves. We focus on four key behaviors – curious, empowered, inclusive and agile – and value how we work as much as what we achieve. We believe that our culture is one of the reasons our company continues to thrive after 130+ years. Visit Our Purpose and Vision to learn more about these behaviors and how you can bring them to life in your next role at Coca-Cola.

We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class. When we collect your personal information as part of a job application or offer of employment, we do so in accordance with industry standards and best practices and in compliance with applicable privacy laws."
581,Data Quality Engineer,Coca-Cola Bottling Company Consolidated,"Charlotte, NC 28211",N,"Requisition ID: 148590
Posting Locations: Charlotte

Click here to view a Day in the Life of our Teammates!

Our Secret Ingredient is our Teammates.
We offer great rewards, competitive pay, career advancement and growth opportunities.
Full Time Teammates are also eligible for:
Paid Training
Paid Time Off plus paid holidays
401(k) with Company matching on a dollar-for-dollar basis
Employee Stock Purchase Plan (ESPP)
Group Health Insurance – Medical, Dental, Vision & Disability
Basic and Supplemental Life Insurance

Refresh and Grow your Career with Us!
Job Overview
The Data Quality Engineer is responsible for defining, designing, and implementing the working components of a data quality program; performing data acquisition, manipulation, distribution, and analysis, while demonstrating an understanding of what data is within context and how it should be maintained; building automated rules and workflows to check the quality of the data; continuously monitor the quality of the data; and provide insights on how to prevent data quality errors in the future; and identifying process improvements and opportunities for automation.
Duties & Responsibilities
Documents and maintain data standards - Collaborates with process owners, subject matter experts, and data stewards to define and document what data means and the lifecycle it undergoes within the respective systems and processes for non-technical audiences.
Designs and builds data quality objects - Uses data governance systems to articulate business logic and continuously monitors the quality of the data and prevent data quality errors in the future.
Develops and builds training modules – Educates the organization to understand what data is and why it is used.
Facilitates process improvement design ( as it applies to data quality ) – Analyzing existing processes, proposing solutions, and handling change management with internal stakeholders.
Provides reporting and analytics related to data quality and governance
Knowledge, Skills, & Abilities
Experience working directly with non-technical audience with strong customer service skills and relationship-building skills. Strong communication skills, both written and verbal
Inquire and document data standards documentation ( Data Ownership, Data Profiles, Business Definitions, ERD diagrams, Data Flows, Business Process Flow Diagrams, Security Implications, Data Quality rules )
Must have a solid understanding of how to join and manipulate data within a database using SQL. Preferred experience with Databases (MS SQL, Snowflake, SalesForce, SAP, et al ), Approval Workflows, Data Governance software ( e.g. Talend / Palantir / MS Purview / Informatica )
Ability to dive into technical details with IT partners to communicate requirements, test, and implement technical infrastructure
Must be able to balance attention to detail/principle with delivering value to the business/action.
Team player
Minimum Qualifications
High school diploma or GED
Knowledge acquired through 1 to 3 years data work experience
Preferred Qualifications
Bachelor’s degree in Computer Science or Business Administration
Work Environment
Office environment
Coca-Cola Consolidated, Inc. is an Equal Opportunity Employer. Coca-Cola Consolidated, Inc. also participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S."
582,Data Engineer,Bright Horizons Family Solutions,"Newton, MA 02459","$125,000 a year","Primary Purpose
The Data Engineer will be responsible for leading and developing ETL solutions, maintaining data integrity, data quality and development of API integrations using services from the IICS platform. This position will also develop and manage integrations with cloud service provider like Microsoft Azure and cloud data platforms. This position will be responsible for building/updating existing Data Warehouse using SSIS packages. The ideal candidate will also have proven experience in data analysis and management, with excellent analytical and problem-solving abilities.
Major Functions/Responsibilities
Design, develop and implement robust and extensible ETL solutions to support key business data/data warehouse using SQL Server Integration Services (SSIS), Informatica & Qlik.
Partner with business and subject matter experts to identify, document & design based on the business requirements, functional specs and translate them into appropriate technical solutions
Understand the Software Development Lifecycle and ability to apply the SDLC to existing and future development initiatives.
Responsible for estimating, planning and managing the user stories, tasks and reports on Agile Projects.
Performance tuning and optimizations of ETL Jobs, SQL Queries and Scripts.
Develop & maintain UNIX/PowerShell scripts.
Configure and maintain scheduled ETL jobs
Manage and Implement CICD for Informatica Intelligent Cloud Services ETL pipelines.
Develop advanced PLSQL Procedures, Functions and SQL jobs.
Support exploratory data analysis, statistical analysis and predictive analytics.
Develop data integrations between internal and external/third party sources.
Support production issues and maintain existing data systems by researching and trouble-shooting any issues/problems in a timely manner.
Participate in the development of corporate-wide strategies, processes, and standards for data architecture, storage and reporting.
Proactive, great attention to detail, results-oriented problem solver.
Communications
Facilitate project team meetings effectively.
Effectively communicate relevant project information to superiors
Deliver engaging, informative, well-organized presentations that are effectively tailored to the intended audience.
Ensure that feedback from key business owners is fully acknowledged and incorporated into functional and technical specifications, process flow diagrams and project plans to give business leaders the confidence that they are being heard and that their needs will be met.
Serve as a technical liaison with development partner.
Serve as a communication bridge between business leaders, applications team, developers and infrastructure team members to facilitate understanding of initial requirements, questions and changes across these groups.
Resolve and/or escalate issues in a timely fashion.
Understand how to communicate difficult/sensitive information tactfully.
Decision Making Authority
This position needs to possess the ability to act quickly with independent thinking and sound judgment.
Works under the direction of data architect or manager of data analytics.
Ability to make required and immediate decisions on his/her own and escalate complex issues as needed.
Education/Experience/Skills
Bachelor’s Degree or higher in Engineering, Technology or related field experience required.
6+ years of experience in performing data profiling, designing and implementing logical and physical data models, including star schema design and ER models, on MSS.
5+ years of experience in writing stored procedures and SQL query scripts on MSSQL to extract, manipulate and load data.
5+ years of experience in designing ETL solutions, using SSIS, Informatica.
5+ years’ experience of data warehouse design (e.g., dimensional modeling)
5+ years’ experience with design, build, test, and maintain data integrations for data marts and data warehouses.
3+ years’ experience with Azure Cloud platform and services.
In depth understanding of database management systems, online analytical processing (OLAP) and ETL (Extract, transform, load) framework.
Ability to effectively prioritize and execute tasks in a high-pressure environment and react to project adjustments and alterations promptly and efficiently.
Demonstrated experience with various BI technologies.
Proven abilities to take initiative and be innovative.
Analytical mind with a problem-solving aptitude
Excellent verbal and written communication skills
Experience managing all phases of the project life cycle, including running project prioritization meetings, post-installation project post-mortem meetings and a proven track record of managing multiple tasks and meeting deadlines.
Collaboration with both onshore and offshore development teams.
Preferred skills:
Experience in Cloud Data Platform like Snowflake, Data Bricks or Azure Synapse.
Experience in CDI/CAI Integration Services in Informatica Cloud platform.
Exposure to Qlik Sense or any Visualization tools, Dynamics CRM & Salesforce Systems.
This role has the following employment benefits:
Health Care
Retirement
Sick Leave
Vacation
Paid Holidays
Disability Insurance
Life Insurance
Tuition Reimbursement
The compensation for this role is $125,000 annual base.
Disclaimer
The above statements are intended to describe the general nature of work performed, not an exhaustive list of all essential functions and responsibilities.
HAVING TECHNICAL ISSUES WITH YOUR APPLICATION?
Contact us at bhrecruit@brighthorizons.com or 855-877-6866
Bright Horizons is dedicated to creating a workforce that promotes and supports diversity and inclusion. We provide equal employment opportunities to all individuals without discrimination. Bright Horizons complies with the laws and regulations set forth in the following EEO is the Law Poster: EEO – English and EEO – Spanish along with information on the Family and Medical Leave Act (FMLA) and Employee Polygraph Protection Act (EPPA).
Applicants requiring a reasonable accommodation for any part of the application and hiring process should contact the recruitment helpdesk at 855-877-6866 or bhrecruit@brighthorizons.com. Determinations on requests for reasonable accommodation will be made on a case-by-case basis."
583,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
584,Data Engineer,Commonwealth Care Alliance,"Boston, MA 02108•Remote",N,"Work closely with Data Scientists to rapidly convert prototype analyses to production data pipelines. Work closely with customer/clinician-facing colleagues and directly with customer/clinician directly. Collaborate with IT Architect and IT Security and Privacy teams to architect and deploy data pipeline solutions that are secure and performant. Core contributor to the creation of an analytics platform and data products that will drive core business processes and be used by analysts across the organization. Day to day management and reliability of analytics pipelines deployed securely on the public cloud. Contribute ideas and code for analytics, feature engineering, and modeling. Serve as technical lead for 4-5 employees.

Requirements: Minimum of bachelor’s degree (or foreign equivalent) in Computer Science, Mathematics, Statistics, Economics or related. Two years’ experience in Data Engineering/Administration or related. Experience/skill set must include BI tools (Looker, Tableau, PowerBI); object oriented design and programming languages (Java, C++); Linux and bash scripting; Git-based version control systems; Databases (Teradata Vantage, SQL Server, Oracle); Cloud Technologies (Azure, GCP); ETL (Talend, MuleSoft, Alteryx, Informatica); distributed systems and distributed data processing; Spark, Python and the PyData analytics stack (sklearn, pandas, numpy, seaborn, etc); SQL and exposure to a variety of relational and non-relational databases; data pipeline and workflow management tools such as Airflow, Luigi; conducting non-trivial data transformations on real world messy datasets including building validation and alert. Will only consider applications from “US Workers” (e.g., US Citizen, Lawful Permanent Resident). Will not provide sponsorship for employment-based visa status.

Work schedule: 8:30am-5:30pm. Job location: Boston, MA (working remotely presently allowed). Send your resume to recruit@commonwealthcare.org (refer to #100KE).. This position is covered by the Commonwealth Care Alliance Employee Referral Policy which has been provided to all CCA employees.

Equal Opportunity Employer Minorities/Women/Protected Veterans/Disabled"
585,Data Engineer,Farm Credit East,"240 South Rd, Enfield, CT 06082",N,"Be part of a team focused on the success of our customers, the success of our communities, and the success of each other. Farm Credit East is the leading provider of loans and farm advisory services to farm, forest product, fishing, and other agricultural business owners across the northeast. We are One Team Working Together with a focus on our five pillars: Outstanding Customer and Employee Experience, Quality Growth, Operational Excellence, Commitment to our Communities, and Protecting Customer Information.

Position Summary
The Data Engineer is responsible for cleaning, managing, and sharing data that guides business decisions. Using ETL tools you will gather data from a variety of sources, checking for anomalies, automating processes, and generally making it easier for business stakeholders to generate valuable insights. This position will collaborate with internal and external organization to capture requirements, design, create, document, manage, and fulfill requests for on-going and/or ad-hoc reports, dashboards, and scorecards.

Duties and Responsibilities
Work with product stakeholders to implement, maintain, and enhance data models and solutions used to define and measure quality of data domains.
Design data models to meet requirements
Perform ETL (Extract, Transform, and Load) on data to meet stakeholder specifications.
Design and develop data access methods, datasets, views etc.
Develops data modeling and is responsible for data acquisition, access analysis, archive, recovery, load design and implementation.
Coordinates new data developments to ensure consistency with existing warehouse structure.
Collaborates with internal customers to capture requirements, design, create, document, manage and fulfill requests for on-going and/or ad-hoc reports, dashboards, and scorecards.
Assists with the development, implementation, and maintenance of front-end presentation (dashboards), automated report solutions and other BI solutions to support tactical and strategic reporting needs of the organization.
Assists in identification of data integrity problems and recommends solutions.
Work collaboratively with key stakeholders both internally and externally, including but not limited to Senior Management, Business Unit Leaders, Knowledge Exchange, and Farm Credit Financial Partners (FPI).

Job Qualifications/Requirements
Bachelor’s Degree in Computer Science, Business, Finance, or other related field from an accredited University.
Experience with MSFT SQL Server
Microsoft Azure (Data Bricks, Data Factory, Logic Apps, Functions, etc.)
2 plus years of experience in Finance related informatics, performance measurement, or analysis with strong relational database SQL skills.
1 + years of experience using Microsoft Azure product to perform ETL
Familiar with Databricks Unity catalog

Farm Credit East is an Equal Opportunity Employer. As an Equal Opportunity Employer, we do not discriminate on the basis of race, color, religion, national origin, sex, sexual orientation, gender identity or expression, age, marital status, parental status, political affiliation, disability status, protected veteran status, genetic information or any other status protected by federal, state or local law. It is our goal to make employment decisions that further the principle of equal employment opportunity by utilizing objective standards based upon an individual's qualifications for a specific job opening. In compliance with the Americans with Disabilities Act (“ADA”), if you have a disability and would like a reasonable accommodation in order to apply for a position with Farm Credit East, please call 1-800-562-2235 or e-mail FarmCreditCareers@farmcrediteast.com ."
586,Data Engineer,Zuma,Remote,N,"About Zuma
Zuma makes an automated sales agent that converses with 100% of inbound leads, ultimately improving the way consumers interact with businesses and organizations. We've built this from the ground up using AI, ML, and human support which helps increase sales conversion and support capacity for businesses of all kinds. Zuma is one of the fastest-growing startups in Los Angeles, and is well-funded and backed by world-class investors such as Andreessen Horowitz (a16z), Y-Combinator, Joe Montana's fund (Liquid 2 Ventures), Day One Ventures, Soma Capital, and other notable angel investors including Austen Allred (from Lambda School), YC's former-COO Qasar Younis, among others.
Headquartered in Los Angeles, USA, we operate nationally and have plans to grow rapidly over the next few years. To do that, we need great people committed to our vision in a big way. We're looking to build a team of rockstars that are equally excited about the opportunity to leverage technology to improve the way customers interact with businesses!
Job Overview
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives.
Responsibilities
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data' technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data' data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data' data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc."
587,Senior Data Engineer,Agilon Health,Remote,N,"Position Summary:
agilon health is transforming healthcare by empowering community-based physicians with the resources and expertise they need to innovate the payment and delivery of care for seniors. The agilon health Total Care Model is powered by our purpose-built platform and enabled through a growing national network of like-minded physician partners.
With agilon health, physicians are freed from the constraints of the transactional fee-for-service reimbursement model and are able to practice team-based, coordinated care to serve the individual needs of their older adult patients and to transition to a sustainable and predictable, long-term business model.
As a member of agilon health’s Technology Group, step into a key role on an expanding data engineering team to build our data platforms, data pipelines and data transformation capabilities all running on AWS.
This is a great opportunity to help define and implement our data platform strategy on Cloud, have a meaningful impact on our customers and working in our high energy, innovative, fast paced Agile culture, in a leading tech company in healthcare space.
Job Functions:
· Develop and implement cloud native services to improve current data acquisition / ingestion and enrichment on all data layers of the SaaS platform.
· Construct, implement, and maintain extract load and transform (ELT) workflows with cloud native technology or open-source technologies (Snowflake, dbt, Airflow)
· Partner with Data Science team to define and implement best practices for data acquisition and transformation of data sets for ML training workflows.
· Leverage internal reporting tools to help validate and automate the data validation and data quality process.
· Management/Troubleshoot of all data pipeline services.
· Work with front-end developers / staff and customers to support reporting needs of business by restructuring data, creating tables/views, or virtual warehouses accessible in the cloud platform.
Qualifications:
· 4+ years of hands-on experience with designing and implementing data services in a cloud environment.
· Strong software engineering experience and deep familiarity programming languages such as Python, Java, Node.js, or equivalent.
· Experience working with at least one public cloud solution AWS, Azure, or GCP.
· Preferred experience working in scrum or agile teams & understanding of CI/CD pipelines.
· Experience working with cloud data warehouse / data lake solutions (Snowflake, Redshift, Google Big Query or equivalent platform.
· Undergraduate degree from accredited 4-year institution.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Retirement plan
Vision insurance
Compensation package:
Yearly pay
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you a US Citizen or Permanent Resident?
Education:
Bachelor's (Preferred)
Experience:
SQL: 4 years (Required)
Data warehouse: 3 years (Required)
Cloud architecture: 4 years (Required)
Software Engineering: 4 years (Required)
Agile: 2 years (Preferred)
Cloud computing: 3 years (Preferred)
Data lake: 2 years (Required)
Healthcare Industry: 2 years (Required)
Work Location: Remote

Health insurance"
588,Data Engineer,Slumberland Furniture,"3505 High Point Drive North, Oakdale, MN 55128",N,"Slumberland is embarking on a journey to leverage our data to drive key business decisions. We are seeking an individual responsible for collecting, managing, and converting raw data into information that can be interpreted by data scientists and business analysts for performance evaluation and optimization.
The Data Engineer is responsible for working with all areas across the business to develop and operationalize data structures, create data models and consolidate multiple sources of data across the organization. By leveraging data, insights and metrics, this position is a key driver in creating innovation and data-driven business strategies within our company.
Essential Functions and Accountabilities of the Data Engineer:
Design and build data models and data transformations efficiently and reliably to fit the analytical needs of the business
Develop and maintain ETL processes utilizing Azure Cloud tools such as Azure Data Factory, FiveTran, DataBricks as well as efficient cloud data strategies to minimize costs
Engage with teams to push the boundaries of analytical insights, creating new product features using data, and powering data models
Anticipate, Identify and solve issues concerning data management to improve data quality
Understand and use continuous integration, test driven development and production deployment frameworks using DevOps and GIT
Develop and champion modern Data Engineering concepts to technical audience and business stakeholders
Manage and maintain relational databases, including but not limited to SQL Server
Maintain data integrity and transparency to increase data, reporting and dashboard confidence and consistency across all departments
Advocate for new sources of data to create actionable insights and recommendations
Define and communicate business rules and terms governing use of the data including security and data lifecycle management
Communicate technical and non-technical information clearly, concisely, and effectively both orally and in writing. Present findings, recommendations, and specifications in formal reports and/or oral presentations
Slumberland is a family-owned company, ranking within the top 20 for both furniture and bedding retailers in the nation. At Slumberland, you'll find a kind, friendly, and supportive environment that values love, dignity, and respect. We share a common goal of providing an exceptional customer experience and quality furniture for a great price.
Data Engineer Benefits:
Up to 3 weeks of paid time off in your first year, plus 24 hours of PTO given on first day
Full Benefits Package: Medical, Dental, Vision, and more
401K Retirement and Profit Sharing
Tuition Reimbursement
Generous Merchandise Discount
Onsite fitness facilities
Requirements:
Knowledge & Education Requirements:
BA/BS degree in Computer Science, Database Administration or similar work experience
3-5+ years of experience in Data Engineering
2 or more years of experience in data modeling, data architecture and ETL
Experience with tools and concepts related to data and analytics, such as dimensional modeling, reporting tools, data governance, data warehousing, structured and unstructured data
Proficient in at least one major programming language (e.g.. Java Script, Python) and comfortable working with SQL
Advanced understanding of analytics tools and methods, best practices and awareness of new techniques of analytics
Experience working with relational databases like SQL, Postgres and Data Bricks
Experience working with ETL tools like Azure Data Factory, Fivetran and SSIS"
589,Data Engineer,Uplift,Remote,N,"Uplift’s mission is to help people get more out of life, one thoughtful purchase at a time. Our enterprise Buy Now, Pay Later solution is used by the world’s most loved brands including Southwest Airlines, Carnival Cruise Line, Universal Studios, and more. With flexible pay over time installments, we empower consumers to buy what matters most while unlocking higher conversions and customer lifetime value for our partners.

Our team is rapidly growing and comes from diverse backgrounds of leading technology and financial brands. Our HQ is in Sunnyvale, California with offices in Toronto, Ontario, New York, Reno, Nevada, and Guadalajara, Mexico.

Working at Uplift allows you to push your limits, challenge the status quo, and collaborate with some of the brightest minds in the industry. We’re committed to building a diverse team and inclusive culture and believe your potential should only be limited by how big you can dream. We make this a reality by empowering you with the tools, resources, and support you need to grow your career.

Uplift is seeking a Data Engineer to join our Data Engineering team. You will be responsible for constructing a new data lake on Databricks and optimizing existing data use cases, from Business Intelligence to Machine Learning operations. As a member of our team, you'll have the opportunity to work with cutting-edge technologies like Spark, Delta Lake, and ML flow. We invite you to come help us build an outstanding data platform for all of our business needs.
What you will do and achieve
Design, build, and maintain the ETL using AWS and Databricks
Develop and maintain near real-time data pipelines
Build Python libraries, tools, serverless applications and workflows
Creates applications using test-driven development and agile methodologies
Internal process improvements such as automating manual processes, building alerting/monitoring bots
Support daily operations of troubleshooting of Databricks and Snowflake jobs
Collaborate closely with product and business teams to influence technological decision-making and to support reporting needs
Work with analysts and data scientists to extract actionable insights from data that shape the direction of the company
Actively engage in design and code reviews - learn from your peers and teach your peers
Who you are
3 years of related experience with a Bachelor’s degree; or 2 years and a Master’s degree
Experience with Big Data, ETL, and data modeling
Solid coder with Python
Strong SQL knowledge and experience working with a variety of databases
Experience with modern cloud data warehouses (preferably Databricks or Snowflake)
Knowledge of Linux, AWS, and Docker
Experience in developing and operating high-volume, high-available and scalable environments
Working knowledge of API and stream based data extraction processes
Ability to align with rapid business changes: new requirements, evolving goals and strategies and technological advancements
Experience supporting and working with cross-functional teams in startup culture
Life at Uplift
Health Insurance and 401k plan: some plans cover 99-100% premiums for medical, dental, and vision insurance and a 401k plan
Work/Life Harmony: Flexible, remote-first work culture. Uplift fosters a culture where employees can achieve both their professional and personal goals. This balance is especially true for our working parents
Shared Success: competitive salary and Pre-IPO stock options
Health and Wellness Perks: Uplift is proud to reimburse our employees for exercise, wellness products and activities as well as free counseling and coaching for physical, mental and emotional support
Professional Development: We are committed to the growth and development of all of our employees. Uplift invests in professional conferences, certifications, and training for employees who want to grow in their careers
Pick-A-Perk: money that can go towards something of your choosing within tuition reimbursement, student loan payment reimbursement, vacation savings account, charitable donations, or home office expenses
At Uplift, in accordance with the U.S pay transparency laws across various states, we provide the minimum and maximum pay range for each of our levels. This reflects the minimum and maximum ranges for new hire salaries. There are two ranges that reflect positions across all US locations. The first range includes: New York and California whereas the second range includes all other US states. Within each range, there are several different factors that are considered when determining compensation including but not limited to the candidate's location, title, years of experience, job related skills and relevant education. In addition to the base salary, we offer competitive benefits including medical, dental and vision and pre-ipo stock options.

$120k to $168k
$104k to $145k

We want you!

If you made it this far, chances are you’re as excited about working to change how people experience BNPL as we are — and we love that. Please apply even if you’re unsure about whether you meet every single requirement in this posting. Uplift is looking for smart, intellectually curious people who are invested in our mission, not just those who can “check all the boxes”.

Uplift is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.

Note: Uplift does not accept agency resumes. Please do not forward resumes to any recruiting alias or employee. Uplift is not responsible for any fees related to unsolicited resumes."
590,Senior Data Platform Engineer,Enlitic,"Fort Collins, CO 80528•Remote","$115,000 - $135,000 a year","Job Information
Industry
Health Care
Work Experience
5+ years
Salary
$115,000-$135,000
City
Fort Collins
State/Province
Colorado
Country
United States
Zip/Postal Code
80528

About us
Enlitic is shaping the next generation of diagnostic healthcare tools to help patients around the globe. Using AI and deep learning, we help Radiologists identify disease and medical issues more accurately and earlier. We believe we are addressing one of the largest social impact opportunities in healthcare with our technology deploying around the globe. Enlitic is attacking the hard, high-value challenges facing healthcare by developing solutions others are afraid to touch. We are dedicated to the development of impactful healthcare solutions.

Job Description
Position Responsibilities
Design and manage large scale data systems with serverless, event-driven cloud technology, such as AWS Glue, Batch and Lambda
Work closely with software engineers to build ETL pipelines around our data sources, applications, deployments, and integrations
Support Machine Learning Engineers and Data Scientists to automate and optimize data discovery, access, and feature enrichment
Support remote data ETL for deployments to partners and clients
Collaborate with clinicians, product managers, researchers, regulatory experts, and IT stakeholders to tackle complex problems on a mission driven product team
Participate in data modeling, schema design, and SQL development
Ingest and aggregate data from both internal and external data sources to build our world class datasets
Be involved in testing and fixing of new or enhanced solutions for data products and reports, including automating ETL testing
Assist with the development and review of technical and end user documentation including ETL workflows, research, and data analysis
Build monitoring dashboards and automate data quality testing
Own meaningful parts of our service, have an impact, grow with the company

Requirements
Location
Remote or Fort Collins, Northern Colorado Area preferred
Relocation assistance available
Education
Bachelor's Degree in computer science or related field, or equivalent practical experience
Experience
5+ years’ experience in data analytics and/or data management
Proficient in Python
Experience in SQL, data modeling and managing databases
Experience in building and optimizing ETL pipelines
Experience with AWS or other cloud technology
Proficient in data analysis and visualization
Experience working with medical data (DICOM, HL7, EHR)
Experience with Apache Spark + other big data tools
Experience working with Apache Airflow
Experience working with Postgres, Oracle, MS SQL
ISO/QMS Requirements
Understand and practice all requirements of EN ISO 13485:2016, ISO 13485:2016 MDSAP including 21 CFR 820, QMS Manual, Process Flows and Work Instructions. Experience with EN ISO 62304:2006 a plus.
Comply with applicable regulatory requirements (including but not limited to MDSAP participating countries and CE Marking).
Support Internal and External audits.




Benefits
Our people love working here because they are challenged by unique and difficult problems in a nimble startup environment that is backed by five plus years of success in performance.
Health,vision, and dental insurance
Company paid Short Term Disability
Company paid Long Term Disability
Company paid employee Life Insurance
Up to 6% 401k match
Unlimited PTO
$150 a month health and fitness stipend



Enlitic is an Equal Opportunity Employer and does not discriminate on the basis of race or ethnicity, religion, sex, national origin, age, veteran disability or genetic information or any other reason prohibited by law in employment."
591,Data Engineer US,Sails Software,"Novi, MI",N,"Novi, Michigan
Work Type: Full Time

Would you be interested in exploring career opportunities with us?

Please find the job description below.

Job Title: Data Engineer (On-site)

Client: Equifax
Position: C2C or 1099
Location: Atlanta GA Onsite
St. Louis MO Onsite
Working Hours: 8 Hours/Per Day

What you’ll do
Conduct data wrangling and data analyses in a big data environment; Create business insights and KPI reporting using Equifax’s credit data and alternative data assets.
Working with D&A, IT, and Google Cloud Migration team, automate data reporting and scoring processes on Google Cloud
Collaborate with D&A Data Scientists and analytical consultants executing compelling analytical projects that demonstrate the value of Equifax data assets and products.
Articulate and educate various internal stakeholders on Equifax data and analytical solutions.
Use Airflow to perform a variety of data aggregation, data quality, and integrity checks.
Use Airflow for automating multiple jobs with dependencies, parallelizing jobs, monitoring run status, failures and troubleshooting, and more.
What experience you need
Master's or higher degree in Computer Science, Information Technology, Data Science, MIS, or other quantitative disciplines
4+ Years’ experience with data engineering in Financial Services or credit reporting companies
2 years experience with structured and unstructured data and accessing data using languages such as SQL
2 years of GCP and Python scripting experience
Expert knowledge of SQL and Python, or equivalent data analytics tools used for large-scale data analysis and modeling
Hands-on experience in data wrangling, data cleaning, data automation, and subsequent data monitoring and analysis
Hands-on experience in using Apache Airflow and Dataproc to programmatically author, schedule, and monitor workflows

What could set you apart
Knowledge of various sources of data such as credit bureau, consumer credit, demographics, and income data is preferred."
592,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
593,Business Intelligence Data Engineer,N,"33 East 17th Street, New York, NY 10003","$140,000 - $160,000 a year","Title: Business Intelligence Data Engineer
Category: Information Systems & Technology
EmploymentType: Full-Time
Location: NY-New York (Union Square)
LocationType: corporate
JobLocation: New York, New York 10003
JobSummary: The Business Intelligence Data Engineer will be a key member of the Barnes & Noble IT organization. The Business Intelligence Data Engineer will be responsible for expanding and optimizing our data and data pipeline architecture, support cross functional teams in generating timely insights. The ideal candidate is a data specialist experienced in designing, developing, and deploying complex data pipelines in Azure Cloud platform

The Business Intelligence Data Engineer will support our software developers, database architects, data analysts, dashboard developers and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. The role requires solid technical skills in designing and delivering large-scale enterprise data platforms on Azure Cloud combined with very strong communication skills.

An employee in this position can expect an annual starting rate between $140,000 and $160,000, depending on experience, seniority, geographic locations, and other factors permitted by law.
WhatYouDo:
Deploy new solutions and configurations to meet business and compliance requirements.
Participate in 24x7 on call rotations.
Discover current technical standards and best practices (R&D).
Deploy security patches, updates, and configuration changes.
Manage consultants to ensure compliance with Barnes & Noble engineering and business standards.
Knowledge&Experience:
Work with multiple business stakeholders in defining the right data requirements to fulfill growing analytics / insights needs across the enterprise
Create and maintain optimal data pipeline architecture
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, optimal cost and performance.
Design right infrastructure / compute configuration for optimal extraction, transformation, and loading of data from a wide variety of data sources into ADLS, Databricks and Synapse.
Develop data pipelines using PySpark, Python and DB SQL in Databricks in Lakehouse architecture
5+ years of experience in a Data Engineering environment with hands on experience developing ADF (Azure Data Factory) pipelines for an enterprise solution.
3+ years of experience in writing code in Databricks using Python to transform, manipulate (ETL/ELT) data, along with managing objects in Notebooks, Data Lake, ADLS, Azure Synapse.
Experience with writing complex SQL Queries, User Defined Function, Stored procedures and Materialized views. Someone who comes from database development background and have transitioned to Azure Cloud/Data Lake/Synapse.
Working experience with Azure DevOps and Source controls.
Experience working in a large Retail enterprise and understanding of Retail based data and reporting models.
Experience with reporting tools like PowerBI/Tableau/MicroStrategy
Strong analytic skills related to working with different types of datasets from wide variety of data sources
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
Understanding of ELT and ETL patterns and when to use each.
Undergraduate degree required (Graduate degree preferred) in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field.
Experience using the following software/tools/services: Azure Data Factory, Azure Data Lake Storage, Azure Databricks, Azure Synapse, SQL, PySpark
Experience with relational SQL and NoSQL databases
Experience with data pipeline and workflow management tools
EeoStatement: Barnes & Noble is an equal opportunity and affirmative action employer. All qualified applicants will receive consideration for employment without regard to age, race, color, ancestry, national origin, citizenship status, military or veteran status, religion, creed, disability, sex, sexual orientation, marital status, medical condition as defined by applicable law, genetic information, gender, gender identity, gender expression, hairstyle, pregnancy, childbirth and related medical conditions, reproductive health decisions, or any other characteristic protected by applicable federal, state, or local laws and ordinances."
594,Principal Data Engineer,Aimpoint Digital,"Atlanta, GA","$140,000 - $200,000 a year","Equivalent of Senior / Principal Data Engineer, Analytics Engineer, Sales Engineer
Are you an accomplished Data Engineer looking to apply your expertise to solving complex and interesting data and analytics challenges using the best modern tools?
Are you passionate about SQL and Data Modeling and interested in growing your dbt expertise, developing technical content and presentations, supporting & networking within the Atlanta community, and liaising with dbt Labs to promote analytics engineering principles and trends to your networks?
Aimpoint Digital is a fast-growing data and analytics consultancy. We partner with the most innovative software-providers in the data engineering space to solve our clients' toughest business problems. Our approach to data engineering blends modern tools and techniques with a respect for the foundations of our craft. As preferred partners with dbt, Aimpoint Digital was invited to host the Atlanta dbt Meet-up events in our new, state of the art office space!
You will:
Become a trusted advisor working together with our clients, from data owners and analytic users to C-level executives
Engage and lead multi-disciplinary teams to solve complex use-cases across a variety of industries
Assess existing analytics infrastructure and business processes and advise on best-in-class modern solutions
Design and develop the analytical layer, building cloud data warehouses, data lakes, ETL/ELT pipelines, and orchestration tools
Work with modern tools such as Snowflake, Databricks, Fivetran, and dbt
Write code in SQL, Python, and Spark, and use software engineering best-practices such as Git and CI/CD
Serve as a dbt subject matter expert for our internal community of technical analytics consultants and as a company ambassador externally
Plan and host in-person meetups to educate and engage the dbt community in Atlanta, coordinating the meetup agenda and technical speakers
Create and deliver engaging technical and thought leadership content (e.g. blogs, webinars, whitepapers, presentations) on the latest dbt features and best practices
Build and deliver end-to-end technical tool demonstrations of dbt functionality & interoperability for prospective clients/partners and general knowledge sharing
Foster community engagement and interaction by encouraging participation in discussions and Q&A sessions
Work with the dbt team to ensure meetups align with the company's overall goals and initiatives
Collaborate with other dbt thought leaders to enhance the reach and impact of the meetups
Develop dbt packages to improve consulting delivery excellence or to target a common industry need
Who you are:
We are building a diverse team of talented and motivated people who deeply understand business problems and enjoy solving them. As a Principal Data Engineer, you will be expected to contribute as a member of a client engagement delivery team, take part in the development of our practice, aid in business development, and contribute innovative ideas and initiatives to our company. You are also a self-starter who loves working with data to build analytical tools that business users can leverage daily to do their jobs better. You are passionate about contributing to a growing team and establishing best practices.
Requirements:
Degree-educated in Computer Science, Engineering, Mathematics, or equivalent experience
Strong communication and presentation skills, with the ability to engage and educate a technical audience
Ability to work independently and collaboratively in a fast paced, remote setting
Passion for technology and a desire to grow and contribute to the dbt community
Experience working with cloud data warehouses (Snowflake, Databricks, Google BigQuery, AWS Redshift, Microsoft Synapse)
3+ years working with relational databases and query languages
2+ years developing SQL or building dbt pipelines in production and ability to work across structured, semi-structured and unstructured data
2+ years data modeling (e.g. data vault, star schema, entity-relationship)
Expertise in software engineering or DevOps concepts & best practices is preferred
This position is based in Atlanta, GA, due to the frequency of events in that location. However, going to the office every day is optional."
595,Data Engineer,Plaxonic,"Trevose, PA",From $75 an hour,"Job Summary: Design, Develop and Deploy data models and programmables for SQL Server, AWS RDS and Non-relational databases. Work with data by analyzing & transforming raw data in meaningful data structures used for Insights and Data Analytics. Work closely with other data engineers, architects, tech leads & scrum teams in product development. Key Responsibilities: • Designs and develops operational and reporting database systems utilizing the latest techniques in data modeling and ETL concepts. • Models database solutions and ensure quality assurance and timely delivery of data. • Investigates, troubleshoots and corrects data and user related system errors, and ensures data integrity. • Translates product requirements from formal requirements document/discussions into database design following Enterprise database model. • Hands on development of advanced database concepts • Develop solutions that consider future concepts, products or technologies in alignment with long-range goals and objectives • Contributes to support an IT roadmap that is forward thinking and sets the groundwork for reducing maintenance cost while meeting or exceeding business objectives. • Promotes a constructive, positive team environment and maintains a spirit of openness, information sharing inquisitiveness, problem-solving and support for common goals. • Other duties as assigned. Required Skills and Knowledge: • Relevant 8+ years technical experience • Experience with SQL Server, Enterprise DB PPAS and/or Oracle databases • Proficient at complex database design and optimization in a cloud SaaS environment • Knowledge of SQL scripts including Stored Procedures, PSQL scripting, SQL Monitoring tools, T-SQL and Performance monitoring and tuning. • Experience with JDBC connection pools • Knowledge of .Net application development is preferred. • Knowledge of AWS RDS solutions & Redshift is a plus. • Practical experience in managing the internal and external database security • Experience using Agile software development cycle tools (JIRA, Confluence, Git, Jenkins) • Analytical and diligent with great attention to detail • Resilient: ability to work successfully in a fast-paced environment with shifting priorities • Strong collaborative skills • Excellent verbal and written communication Qualifications: • Bachelor’s degree in Computer Science, Information Technology (or similar) Required Languages: • English to a very high standard written and oral Travel/Rotation Requirements: • Occasional travel will be required as dictated by business need
Job Type: Contract
Salary: From $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Data Engineer: 10 years (Required)
Work Location: On the road"
596,Data Engineer,Pipe Technologies,"Miami, FL•Remote",N,"About Pipe
Pipe is a new kind of trading platform that enables entrepreneurs to grow their businesses on their terms. By treating recurring revenue streams as an asset, Pipe allows companies to transform their recurring revenue into up-front capital, instantly. For entrepreneurs, that means more cash flow for scaling a business without dilution or restrictive debt. For investors, Pipe has unlocked a previously untapped asset class. Whether you're an entrepreneur or an investor, Pipe is growth on your terms.
We're a fully distributed, remote-first, fast-growing startup. Our engineering and data teams are spread from UTC-8 to UTC+6 and we rely heavily on our written communication skills in order to make it work. We believe in giving our team agency and control over their schedules: we avoid standing meetings, and default to asynchronous communication. There are no core working hours, we just ask our team to communicate clearly about their schedules and be considerate to their coworkers if plans change. You will occasionally need to be flexible in order to meet synchronously with colleagues in different time zones.
The Role
This is a full-time, fully-remote position as a Data Engineer. In this role, you will:
Build data systems and pipelines to enable high-velocity model development, and self-serve analytics and reporting for teams across the company.
Own and improve various data infrastructure, including the data warehouse, distributed compute clusters, and model deployment tools.
Build tooling, testing and processes to drive data reliability, integrity and availability for applications across the company.
Help define flexible and scalable schemas across the data stack.
Independently and proactively find opportunities in our data model to unlock business value.
Ensure data safety, security, and regulatory compliance.
Data is at the heart of what we do. This is a highly visible role in which you will have the opportunity to transform data science, engineering and business operations workflows at the company.
Qualifications
We are looking for talented data engineers with past experience in a similar role. Ideal candidates will have:
Experience building ELT/ETL pipelines
Expert-level proficiency in SQL and at least one programming language
Experience with distributed computing frameworks such as Spark
Familiarity with building model deployment tooling and model hosting infrastructure
Bachelor's degree in Computer Science, or another technical field, or equivalent working experience
Strong written and verbal communication skills
In general, you will be successful at Pipe if:
You want to join a quickly growing startup and make an impact.
You want to be part of a team that holds each other to high standards.
You have a strong technical foundation, and are passionate about using your toolkit to help our customers succeed.
You take end-to-end ownership of your work and enjoy working with different functions across the company.
You have strong written and verbal communication skills.
You want to work in an environment that values and rewards excellence.
Many good candidates do not fit job descriptions perfectly. If you believe you are a good fit, we encourage you to apply. Pipe is an equal opportunity employer: we do not discriminate. Inclusion is important to us and we hope it is to you, too.
Tech Stack
We are committed to using the right tools for the problems we are trying to solve. Currently our stack is mostly comprised of:
PostgreSQL, dbt, BigQuery, Spark, Superset, Baseten, Airflow
We are primarily a Python and Go shop, but experience in other languages will translate. As an early member of the data engineering function at the company, you will have the opportunity to define our stack for the future as well.
Compensation and Benefits
We want you to feel like an owner and that will be reflected in your salary and equity.
The best equipment: if you want it, and it helps you do your job, we'll provide it. Computers, monitors, desks, chairs, headphones, speakers, microphones, webcams, keyboards, mice, etc.
A good work-life balance: we do our best work when we regularly can step away from it and live our lives.
Flexible vacation and work hours. We don't adopt conventional work practices that are meaningless for the type of work we do.
Parental leave for anyone who is growing their family, regardless of gender.
Very good health, dental, and vision insurance.
Great colleagues: we value a culture of authenticity, humility, and excellence."
597,Sr Data Engineer,Invitae,"San Francisco, CA 94107•Remote","$138,400 - $173,000 a year","Invitae (NYSE: NVTA) is a leading medical genetics company trusted by millions of patients and their providers to deliver timely genetic information using digital technology. We aim to provide accurate and actionable answers to strengthen medical decision-making for individuals and their families. Invitae's genetics experts apply a rigorous approach to data and research, serving as the foundation of their mission to bring comprehensive genetic information into mainstream medicine to improve healthcare for billions of people.
By joining Invitae, you'll work alongside some of the world's experts in genetics and healthcare at the forefront of genetic medicine. We've crafted a culture that empowers our teams and our teammates to have the biggest impact and to explore their interests and capabilities. We prize freedom with accountability and offer significant flexibility, along with excellent benefits and competitive compensation in a fast-growing organization!
We are looking for a reliable and motivated Senior Data Engineer to join our Patient Data Network team who can support our Data Solutions Team in developing the data ingestion pipelines and data platform architecture that supports the analytical and reporting needs of internal stakeholders, data scientists, and our machine learning team, as well as externally facing products.

What you'll do:
Understand our complex data ecosystem
Be hands-on with the technical design and implementation of reliable, scalable and efficient data processing framework (batch and streaming), data driven products and software solutions for external and internal customers
Identifies, prioritizes, and solves for ambiguous, open-ended problems
Collaborate with multiple teams; Owns and delivers data solutions from end-to-end with high quality
What you bring:
Typically requires a minimum of 8 years of related experience with a Bachelor's degree; or 6 years and a Master's degree; or a PhD with 3 years experience. Any equivalent combination of training, education, and experience that provides the required skills, knowledge and abilities.
Extensive hands-on experience working with large datasets, pipelines, and modern warehouse technologies
Self-starter attitude and ability to work towards a larger goal with minimal guidance
Advanced experience in SQL queries and performance tuning
Understanding of functional programing paradigms
Proficiency in Scala, Java, Python and a demonstrable ability to quickly learn
Focus on high quality code, including automated testing and coding best practices
Experience with messaging/queuing systems or stream processing systems
Experience in building distributed systems with infrastructure automation, monitoring and alerting
Track record of working with cross functional teams and stakeholders,

Additional Preferred but not Required Skills:
Experience with Snowflake as a warehouse technology
Experience using Kafka for implementing streaming application
Experience with CI/CD pipelines (e.g. GitHub Actions)
Experience with maintaining and administering Kubernetes clusters
Interest in working on related but separate projects in parallel
Experience with DBT as data transformation tool
Experience with data lineage/data governance tools like Atlan
Experience with data modeling/dimensional modeling
#LI-Remote
This salary range is an estimate, and the actual salary may vary based on a wide range of factors, including your skills, qualifications, experience and location. This position is eligible for benefits including but not limited to medical, dental, vision, life insurance, disability coverage, flexible paid time off, Spring Health, Carrot Fertility, participation in a 401k with company match, ESPP, and many other additional voluntary benefits. Invitae also offers generous paid leave programs so you can spend time with your new child, recover from your own illness or care for a sick family member.
USA National Pay Range
$138,400—$173,000 USD
Please apply even if you don't meet all of the ""What you bring"" requirements noted. It's rare that someone checks every single item, it's ok, we encourage you to apply anyways.
Join us!
At Invitae, we value diversity and provide equal employment opportunities (EEO) to all employees and applicants without regard to race, color, religion, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance.

We truly believe a diverse workplace is crucial to our company's success and to better serve our diverse patients. Your input is especially valuable. We'd greatly appreciate it if you can take a quick moment to make your selection(s) below. Submissions will be anonymous.
You can find a detailed explanation of our privacy practices here."
598,Principal Data Engineer,Blueprint Technologies,"Bellevue, WA•Remote","$127,000 - $211,600 a year","Principal Date Engineer Remote
Who is Blueprint?
We are a technology solutions firm headquartered in Bellevue, Washington, with a strong presence across the United States. Unified by a shared passion for solving complicated problems, our people are our greatest asset. We use technology as a tool to bridge the gap between strategy and execution, powered by the knowledge, skills, and the expertise of our teams, who all have unique perspectives and years of experience across multiple industries. We're bold, smart, agile, and fun.
What does Blueprint do?
Blueprint helps organizations unlock value from existing assets by leveraging cutting-edge technology to create additional revenue streams and new lines of business. We connect strategy, business solutions, products, and services to transform and grow companies.
Why Blueprint?
At Blueprint, we believe in the power of possibility and are passionate about bringing it to life. Whether you join our bustling product division, our multifaceted services team or you want to grow your career in human resources, your ability to make an impact is amplified when you join one of our teams. You'll focus on solving unique business problems while gaining hands-on experience with the world's best technology. We believe in unique perspectives and build teams of people with diverse skillsets and backgrounds. At Blueprint, you'll have the opportunity to work with multiple clients and teams, such as data science and product development, all while learning, growing, and developing new solutions. We guarantee you won't find a better place to work and thrive than at Blueprint.
What will I be doing?
Blueprint is looking for a Principal Data Engineer to join us as we build cutting-edge technology solutions! The ideal candidate will have a solid background in consulting, with demonstrated experience leading clients through the process of building modern data estates. As a Principal Data Engineer, you will spend a majority of your time working directly with clients to develop their advanced modern data estates, warehouses, and analytical environments. You will also be responsible for overseeing and mentoring junior developers within the organization.
Responsibilities:
Develop and implement effective data architecture solutions using Databricks and Lakehouse
Optimize and tune data pipelines for performance and scalability
Monitor and troubleshoot data pipelines to ensure data availability and reliability
Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and build solutions that enable them to extract insights from data
Implement best practices for data governance, data security, and data quality to ensure data integrity across all data sources
Create and maintain documentation related to data architecture, data pipelines, and data models
Stay up to date with emerging technologies and best practices in data engineering and big data processing
Mentor and train other data engineers on best practices for data engineering and Databricks usage
Provide thought leadership in the Databricks and Lakehouse space, both within the organization and externally
Qualifications:
Bachelor's or Master's degree in Computer Science, Computer Engineering, or a related field
8+ years of experience in data engineering
3+ years of experience working with Databricks and PySpark
6-8+ years of experience with SQL
Appreciation for the Lakehouse medallion data architecture – bronze, silver, gold – and how those data stages are used
Working knowledge of DLT(Delta Live Tables) and Unity Catalog a plus
Strong understanding of ETL and ELT data ingestion, acquisition, and data processing patterns
Experience with cloud-based data warehousing platforms such as Synapse, AWS Redshift, Google BigQuery, or Snowflake
Strong understanding of data engineering, data warehousing, data modeling, data governance, and data security best practices
Excellent problem-solving and troubleshooting skills
Strong communication and collaboration skills, with the ability to work effectively in a team environment
Experience mentoring and training other data engineers
Salary Range
Pay ranges vary based on multiple factors including, without limitation, skill sets, education, responsibilities, experience, and geographical market. The pay range for this position reflects geographic based ranges for Washington state: $127,000 to $211,600 USD/annually. The salary/wage and job title for this opening will be based on the selected candidate's qualifications and experience and may be outside this range.

Equal Opportunity Employer
Blueprint Technologies, LLC is an equal employment opportunity employer. Qualified applicants are considered without regard to race, color, age, disability, sex, gender identity or expression, orientation, veteran/military status, religion, national origin, ancestry, marital, or familial status, genetic information, citizenship, or any other status protected by law.
If you need assistance or a reasonable accommodation to complete the application process, please reach out to: recruiting@bpcs.com
Blueprint believe in the importance of a healthy and happy team, which is why our comprehensive benefits package includes:
Medical, dental, and vision coverage
Flexible Spending Account
401k program
Competitive PTO offerings
Parental Leave
Personal paid Volunteer time to support our community
Opportunities for professional growth and development
Location: Remote"
599,Data Engineer (ETL & System Administration concentration),"E-Logic, Inc.","Washington, DC 20019•Remote",$75 - $81 an hour,"The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population.
DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team.
The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public.
The Data Management Project team is responsible for meeting the project goals:
(1) establish a data governance program,
(2) perform a comprehensive data gap analysis,
(3) design a master data architecture,
(4) create a data warehouse for all data assets,
(5) develop a front-end for program staff to quickly access workforce information and visualize program status,
(6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and
(7) foster relations with other DC agencies and improve inter-agency data integration.
The Data Management Project Data Engineer serves as an ETL developer and system administrator for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff.
Specific duties for the position include: include:
Specific Duties:
1. Develop, test, and maintain extraction, transformation, and load (ETL) processes.
2. Support the System Administration of associated tools and software of data and analytics landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, and analytic products.
3. Support the Data Management Project team to develop and maintain data quality controls.
4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.
5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.
6. Support the data stewards to troubleshoot and resolve data issues.
7. Support business users to obtain requirements for enhancements and/or new analytic assets.
8. Assist in the Development of data asset training and documentation.
9. Participate in the development and implementation of a DOES data standard.
10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.
Job Type: Contract
Pay: $75.00 - $81.00 per hour
Benefits:
401(k)
401(k) matching
Schedule:
8 hour shift
Application Question(s):
11 years Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts.
11 years Modeling Tools: proficiency in the use and administration of Erwin Modeling Suite (Data Modeler Navigator, Data Modeler WGE, Mart Server, Web Portal
8 years Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing.
11 years Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions.
11 years Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives
Education:
Bachelor's (Preferred)
Experience:
Operating Systems: Windows and Linux.: 10 years (Preferred)
Familiarity with CKAN, DKAN, and/or ArcGIS is an added: 8 years (Preferred)
ETL processes development: 10 years (Preferred)
Work Location: Remote"
600,Senior Data Engineer - Remote U.S.,TTEC Digital,Remote,N,"At TTEC Digital, we coach clients to ensure their employees feel valued, and fully supported, because an amazing customer experience is an employee first process. Our vision is the same, a place where employees know they can thrive.

Looking to work with cutting edge technology and expand your skill set? Have a passion for data and analytics? You’ll be instrumental in building data products and big data solutions, that turn data into actionable insights, for our clients in automotive, healthcare, retail, and travel industry. Bring your curiosity to solve interesting engineering puzzles and develop tech-enabled analytics solutions that delivers personalization at-scale. Along the way, you’ll partner with the internal consulting, data science, and technology teams to provide feedback to improve the platform.

You’ll make an impact by building actionable solutions for our clients based on your analysis and subject matter expertise in data engineering discipline.
During a Typical Day, You’ll
Partner with leadership, engineers, data scientists, and strategic data consultants to refine data requirements and drive technical solutions
Be responsible for the end-to-end implementation of the technology stack from data collection to reporting, with a focus on data infrastructure and technical processes
Design, build and launch efficient and reliable data pipelines to move, ingest and process data from disparate applications including databases, APIs, message brokers and big data stores
Contribute to conceptualization, design and maintenance of data infrastructure and architecture
Implement and monitor quality control processes to ensure accuracy of data and reports
Proactively identify internal and external dependencies, issues, scope changes and progress against project plan
Stay on top of data trends; evaluate new solutions, technologies to evolve our data platform as new needs emerge
Effectively communicate and share your knowledge with global teams through mentoring, code reviews, pair programming and presentations
Develop and expand our knowledge-base and best practices for delivering data products
Provide technical support to assist clients during pre and post implementation
Initiate and foster partnerships with current clients, potential clients, and senior business executives
What You Bring to the Role
Bachelor's in Computer Science, Engineering, or other STEM fields required (Masters preferred)
3+ years professional experience building and supporting data and analytics solutions
Mastery of SQL, Python, PySpark
Experience with Scala, NoSQL, stream processing, SQL performance tuning, build/deploy automation and E2E process optimization
Skilled at designing & building APIs
Experience designing and building solutions on any public cloud environments and associated services (e.g., Data Factory, Airflow, Databricks, Serverless Functions, mlFlow, API gateways, load balancers, DevOps)
Knowledge of data structures, data validation, algorithms, and implications of architecture on software performance
Familiarity with automated testing frameworks and concepts
Exceptional problem-solving skills: demonstrated ability to understand business challenges, structure complex problems, develop solutions
Understanding of Agile Software Development Lifecycle and project planning/execution skills
Experience organizing large projects into manageable action items and communicating next steps to stakeholders
What you can expect
The anticipated range for individuals expressing interest in this position is $120,000 - $140,000 USD. Actual compensation offers to a candidate may vary based upon geographic location, work experience, education and/or skill levels. This position is eligible to participate in an incentive program.

Benefits available to eligible employees include
Medical, dental, and vision
Tax-advantaged healthcare accounts
Financial and income protection benefits
Paid time off (PTO) and wellness time o
#LI-MS3

About Us
TTEC Digital, and our 1,700+ employees, pioneer engagement and growth solutions that fuel the exceptional customer experience (CX). TTEC Engage is a 60,000+ employee service company, with customers in more than 80 countries. Together, we utilize a holistic approach, applying solutions from two centers of excellence, Engage and Digital.
TTEC is a proud equal opportunity employer where all qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, disability. TTEC has fully embraced and is committed to expanding our diverse and inclusive workforce. We strive to reflect the communities we serve while delivering amazing service and technology centered around humanity.
Rarely do applicants meet all desired job qualifications, so if you feel you would succeed in the role above, please take a moment and share your qualifications.

#LI-RemoteUS"
601,Data Engineer,Fuge Technologies Inc,"918 North Gardner Street, West Hollywood, CA 90046",$60 - $65 an hour,"Data engineer
Location: Hollywood, CA (onsite)
Key skills:
9+ years of overall experience with expertise in data mining and data lake experience.
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
West Hollywood, CA 90046: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location"
602,Data Engineer or Staff Data Engineer,Vida Health,United States•Remote,N,"ABOUT US

At Vida, we help people get better — and we’re helping the healthcare system get better, too.

Vida provides expert, personalized, on-demand health coaching and programs through a network of experienced health care providers — like dietitians, therapists, and health coaches and leading medical institutions — coupled with an easy-to-use app with award-winning content.

We focus on chronic conditions — like diabetes, depression, and hypertension — which account for 80% of the $3 trillion spent on healthcare in the US.

By combining advanced technology with the top-notch healthcare providers, Vida is breaking down the barriers that have historically kept people from getting the best care. Vida’s cloud-based platform captures real-time data from 100+ devices and apps and delivers AI-driven insights back to employers, health plans, and providers to improve care. We are trusted by Fortune 1000 companies, major national payers, and large providers to enable their employees to live their healthiest lives.

**Vida is authorized to do business in many, but not all, states. If you are not located in or able to work from a state where Vida is registered, you will not be eligible for employment. Please speak with your recruiter to learn more about where Vida is registered.

ABOUT THE ROLE

We are considering candidates ranging from mid to senior level.

We are looking for a Data Engineer who can help us build, manage, and optimize Vida Health’s data pipelines within our Google Cloud Platform (GCP) environment. From measuring Vida’s impact on healthcare costs to correlating member activities with successful health outcomes, you’ll have a substantial impact on projects whose results can lead to changes in product design and member experience. Our data strategy is Polychronic by Design, meaning we leverage population data insights to help members with multiple interrelated chronic conditions throughout their lifetime.

Because Vida is a startup, you’ll have the opportunity to work on a diverse set of projects that involve multiple parts of our data infrastructure. One of your responsibilities will be to connect all our sources of data such as claims data and app data while making the data structured and accessible for analysis and other ELT pipelines. You’ll also work on optimizing our ELT pipelines, including our ML pipelines which deliver recommendations and predictions that influence the member and health provider’s experience on the platform.

You’ll have the pleasure of working with a team who is excited about providing care to people living with chronic conditions. We also have experts in health care who are an invaluable resource for learning more about the health domain. We hope that you’ll consider embarking on this journey into polychronic health care with us.
RESPONSIBILITIES
Design the foundational layer of Vida Health’s data environment to make data standardized and reusable
Manage the infrastructure of our Data Platform alongside your peers.
Manage data pipelines that collect and transform data from multiple sources to support ML models, analysis, and reportingBuild out the infrastructure to serve Machine Learning models and recommendations for our Application and business users.
Work with Data Analysts and Data Scientists to design data architectures that will improve the productivity and velocity of both teams’ workflows
Optimize the costs of data pipelines and data products
Define and meet service-level agreements (SLAs) for data pipeline processes and ML-powered APIs
Work with serving Large Language Models (LLMs) in a healthcare environment.
QUALIFICATIONS
Experience with cloud platforms such as AWS or Google Cloud
Advanced knowledge of: Data Warehouses, SQL, Python, REST APIs
Track record of standardizing data for analysis on BI tools or delivering ML applications
Experience managing enterprise data exchanges, Analytics ETL, or ML Ops.
Has a relentless focus on delivering maximum value to the end user
Has an ownership mindset and is excited about monitoring and alerting on their systems
Has at least 5+ years of relevant work experience in Data Engineering or similar roles
BONUS SKILLS
Database tools such as BigQuery, Datastore, and Firestore
Asynchronous systems such as Kafka, RabbitMQ, PubSub, and Kinesis
Workflow orchestration services such as Apache Airflow
ETL Tools such as DBT, Matillion, Snowplow, Fivetran, Stitch, and Talend
Self-service BI tools such as Datastudio, Looker, Amplitude, and Tableau
HIPAA and healthcare data such as Medical and RX claims
Management and/or team lead experience preferred
Manage infrastructure with tools like Terraform, puppet, or chef
BENEFITS
Competitive compensation with meaningful stock options
Medical, Dental, Vision, Disability and Life Insurance (We cover 100% of your premium and 80% for your dependents)
Healthcare FSA Plan
Dependent Care FSA Plans
Commuter and Parking Benefits
401K Program
Flexible PTO Policy
Paid Parental Leave
10 Paid Company Holidays
PERKS
We’re a distributed company, so you can work from most US states (We still have a HQ office in San Francisco)
Access to a Vida Health Coach and the full Vida App
New hire home office stipend
Monthly wellness benefit
Training and leadership development programs
Weekly meetups with team members across the country through our #connectandcommit program
Quarterly All Company Events
Quarterly Team Based Connection Opportunities
Significant opportunities for growth and development as the business grows

Vida is proud to be an Equal Employment Opportunity and Affirmative Action employer.

Diversity is more than a commitment at Vida—it is the foundation of what we do. All qualified applicants will receive consideration for employment without regard to race, color, ancestry, religion, gender, gender identity or expression, sexual orientation, marital status, national origin, genetics, disability, age, or Veteran status. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.

We seek to recruit, develop and retain the most talented people from a diverse candidate pool. We don’t just accept differences — we celebrate them, we support them, and we thrive on them for the benefit of our employees, our platform and those we serve. Vida is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures.

We do not accept unsolicited assistance from any headhunters or recruitment firms for any of our job openings. All resumes or profiles submitted by search firms to any employee at Vida in any form without a valid, signed search agreement in place for the specific position will be deemed the sole property of Vida. No fee will be paid in the event the candidate is hired by Vida as a result of the unsolicited referral.

#LI-remote"
603,Senior Data Engineer,Flexport,"Bellevue, WA•Remote",N,"Build the platform that powers global trade
The opportunity
Flexport's Ocean Consolidation Services Data Intelligence team is building datasets, insights, and platforms to define, plan and operate logistics solutions to enable global trade for everyone. We move fast, iterating quickly on big business problems. We work smart, applying technology to unlock insights and provide outsized value to our customers. We swing big, knowing our customers won't benefit from micro optimizations. Leveraging the largest data set for logistics products and services sold in the industry, this team treats data as an asset and determines how to maximize its business value and extend our competitive advantage.
The ideal candidate will have analytics experience, and should be comfortable working across multiple projects / programs with the capability to influence external stakeholders. The candidate should be comfortable interfacing with technology systems, and be able to analyze data and gather actionable conclusions. Operating in a rapidly changing environment will require the candidate to be adept at dealing with ambiguous, new and challenging situations. The candidate will have advanced skills in using/building datasets, as well as building visualizations in various forums. The candidate must be capable of working with non-technical individuals, gathering input, and making independent decisions based on that input to ensure that Operations have the tool sets they need to be successful.
At their core, Data Engineers at Flexport are strong technical individuals who enjoy working with large data sets. They are comfortable working with both business users and other engineers to develop tools and datasets enabling self-service analytics. There are significant opportunities for new team members to emerge as leaders, taking on additional projects and responsibilities with strong performance.
You will:
Effectively manage customer expectations and resolve conflicts that balance client and company needs.
Develop processes to effectively maintain and disseminate project information to stakeholders.
Be successful in a delivery-focused environment and determine the right processes to make the team successful.
This opportunity requires excellent technical, problem-solving, and communication skills. The candidate is not just a policy maker/spokesperson but drives to get things done.
Possess superior analytical abilities and judgment. Use quantitative and qualitative data to prioritize and influence, show creativity, experimentation, and innovation, and drive projects with urgency in this fast-paced environment.
Partner with key stakeholders to develop the vision and strategy for customer experience on our platforms. Influence product roadmaps based on this strategy along with your teams.
Support the scalable growth of the company by developing and enabling the success of the Small and Medium Businesses (SMB) leadership team.
Actively seek to implement and distribute best practices across the operation.
You should have:
7+ years of experience as an analyst or engineer in the data/BI space.
Experience leading and influencing the BI strategy of your team or organization.
Experience working directly with business stakeholders to translate between data and business needs.
Experience with basic data analytics and science skills using SQL, Python, R or similar tools.
Experience with data visualization using Looker, Tableau, PowerBI, Quicksight, or similar tools.
Basic knowledge of statistics such as hypothesis testing and data distributions.
About Flexport:
At Flexport, we believe global trade can move the human race forward. That's why it's our mission to make it easy and accessible for everyone. We're shaping the future of a $8.6T industry with solutions powered by innovative technology and exceptional people. Today, companies of all sizes—from emerging brands to Fortune 500s—use Flexport technology to move more than $19B of merchandise across 112 countries a year.
The recent global supply chain crisis has put Flexport center stage as we continue to play a pivotal role in how goods move around the world. At a valuation of $8B, we're experiencing record growth and are proud to have the support of the best investors in the game who believe in our mission, solutions and people. Ready to tackle global challenges that impact business, society, and the environment? Come join us.
Worried about not having any logistics experience?
Don't be! Our mission is to make global trade easy for everyone. That's why it's important to bring people from diverse backgrounds and experiences together with our industry veterans to help move the global logistics industry forward.
We know this industry is complex. That's why we invest in education starting day one with Flexport Academy, a one week intensive onboarding program designed specifically to set every new Flexport employee up for success.
At Flexport, our ability to fulfill our mission of making global trade easy for everyone relies on having a diverse, dedicated and engaged workforce. That is why Flexport is committed to creating and nurturing an environment where anyone can be their authentic self. All qualified applicants will receive consideration for employment regardless of race, color, religion, sex, national origin, age, physical and mental disability, health status, marital and family status, sexual orientation, gender identity and expression, military and veteran status, and any other characteristic protected by applicable law. To learn more about what our tech teams have been up to, head to the Engineering Blog.
#LI-Hybrid
The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Our salary ranges are determined by role, level, and location. Within the range displayed, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education and / or training.
The US base salary range for this full-time position: (exclusive of bonus, equity and benefits.)
$221,484—$270,703 USD"
604,Data Engineer (L2),Collegis Education,Remote,N,"Description:
Collegis Education is a marketing and technology education solutions company that offers industry-leading services for colleges and universities of every size in every sector. Using a proactive and data-driven approach, Collegis Education empowers institutions to make a broader impact by providing insights that help grow enrollments, improve student outcomes and optimize expenses. With several decades of experience working within the higher education industry, the team at Collegis Education was founded within the walls of a college and expanded to help change more lives through education. Currently, the infrastructures established by Collegis Education support more than 40,000 students nationwide. For more information about Collegis Education, please visit www.CollegisEducation.com.

The Data Enablement team at Collegis Education is seeking an experienced and highly engaged Data Engineer (L2). As a Data Engineer (L2), you will be responsible for crafting, developing, and maintaining data integration and transformation solutions to support delivery of professional services, including building and managing scalable data pipelines, ensuring data quality and governance, and providing technical expertise in cloud-native big data architecture. The ideal candidate will have 2-5 years of experience in data engineering, sophisticated SQL skills, bash and Python competency, as well as hands-on experience with cloud-native data integration and transformation tools.

Primary Responsibilities, Essential Functions and Requirements:

Design, develop and maintain data integration and transformation solutions using cloud-native tools such as Apache Beam, Google Cloud Data Fusion, dbt, serverless functions, and cloud-based database and data storage solutions.
Build and run scalable and reliable data pipelines using cloud data pipeline orchestration tools and approaches.
Ensure data quality and governance by implementing data quality processes and workflows.
Collaborate with cross-functional teams to identify and integrate new data sources and to ensure alignment to data governance and security standards.
Provide technical expertise in executing cloud-native big data architecture, including cloud data warehouse, data lake, and lakehouse approaches.
Understand cloud and legacy RDBMS and NoSQL design patterns and trade-offs.
Design, develop and deploy data solutions using common API integration and consumption patterns.
Ensure adherence to development and security processes and workflows.
Chip into team competency and cohesion with other data engineers through code reviews, working sessions, knowledge sharing sessions, and team activities.
Document data engineering activities and processes.
Lead technical partner engagements.
Adhere to and enforce the appropriate information security policies based on the sensitivity of company data and report any security related issues.
Reduce risk of theft, fraud, or misuse of information assets by acting as the data steward for the application(s) you administer.
Requirements:
Bachelor's or Master's Degree in MIS, IS, Business Analytics, Computer Science, a related field, or equivalent experience.
2-5 years of experience in data engineering.
Advanced SQL, plus bash and Python competency.
Experience with cloud-native data integration and transformation tools such as Apache Beam, Google Cloud Data Fusion, dbt, serverless functions.
Experience with cloud-native data storage solutions.
Experience with common API integration and consumption patterns.
Experience with cloud data pipeline orchestration tools and approaches.
Experience working with cloud-native big data architecture, including cloud data warehouse, data lake, and lakehouse approaches.
Ability to understand cloud and legacy RDBMS and NoSQL design patterns and trade-offs.
Strong proficiency with data quality processes and workflows.
Working experience with DevOps processes and workflows.
Working experience with data governance and security standards.
Strong organizational skills.
Strong documentation skills.
Ability to independently prioritize tasks with and without manager feedback.
Ability to manage technical partner engagements.
Education, Certifications and Licensures:
Bachelor's or Master's Degree in MIS, IS, Business Analytics, Computer Science, a related field, or equivalent experience.
Relevant certifications in data engineering or cloud computing is a plus (GCP preferred).
Collegis Education is committed to the policy that all persons shall have equal access to its programs, facilities, and employment without regard to race, color, creed, religion, national origin, sex, age, marital status, disability, public assistance status, veteran status, or sexual orientation."
605,Data Scientist Engineer in NYC,Capital Management Fund,"New York, NY",$97 - $115 an hour,"ABOUT CFM
Founded in 1991, we are a global quantitative and systematic asset management firm applying a scientific approach to finance to develop alternative investment strategies that create value for our clients.
We value innovation, dedication, collaboration and the ability to make an impact and together we create an environment for talented and passionate experts in research, technology and business to explore new ideas and challenge assumptions.
ABOUT THE ROLE
As pioneers in scientific, quantitative trading, we explore more and more datasets in order to shape and consolidate our trading decisions. The IT Data Financial team works closely with Research to make statistical analyses, drawing on new exploratory data.

You will join our New York office.
Base salary 97-115 K-USD

You will take part, from the exploratory phase up to production launches, in analyzing new structured datasets for Alpha research and will extract relevant features from these datasets to assist the Research teams. You will be owning end-to-end data workflows and developing data domain expertise in relation with the Research teams. You will be working at reducing the TTI (Time to Test an Idea).
We expect you to properly understand business issues in order to identify new modelling pathways.
You will be the point of contact with our local data providers.
The role also requires contribution to the development and maintenance of our Data Platform and ingest pipelines, as well as data quality assurance and application monitoring.

SKILLSET REQUIREMENTS/QUALIFICATIONS
You have a Master Degree from an engineering school or university equivalent (preferably a Master’s in Data Science)
You have at least one experience in data analysis or similar role
You are proficient in programming and visualization in Python
You have coding skills with of data science tools and libraries (Numpy, Pandas, Matplotlib, Jupyter) and a good knowledge of data pipelines construction
You feel at ease with distributed computing tools and cloud technology (Spark, AWS)
You are proficient in financial markets
You are versatile, autonomous and rigorous, with a strong team spirit and good communication skills
You are fluent in written and spoken in English

Your “plusses”
You are inquisitive & production and result oriented
You have a good aptitude for synthesizing information
You are keen to develop in an environment that handles large volumes of data
You have a professional-level knowledge of French.
EQUAL OPPORTUNITIES STATEMENT
We are continuously striving to be an equal opportunity employer and we prohibit any discrimination based on sex, disability, origin, sexual orientation, gender identity, age, race, or religion. We believe that our diversity, breadth of experience, and multiple points of view are among the leading factors in our success.
CFM is a signatory of the Women Empowerment Principles
FOLLOW US
Follow us on Twitter and LinkedIn or visit our website to find out more about CFM."
606,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
607,Senior Data Engineer,Virginia Tech,"Blacksburg, VA•Remote",N,"Job no: 525109
Work type: Administrative & Professional
Senior management: Vice President-Info Technology
Department: Enterprise Systems
Location: Blacksburg, Virginia, Fully Remote, Hybrid
Categories: Information Systems / Technology
Job Description
Join a data team that works closely together and shares credit with our successes and responsibility with our failures. We debate architecture around best practices and new development ideas without tearing each other down. We provide great customer service to our business and technology partners by using relational collaboration and not transactional bureaucracy in order to help deliver data driven decisions to one of the top Universities in the country. We work with a wide range of technologies, and no we do not expect you to know all of them, but you will need to be a fast learner and be technically curious in order to be successful here.

There are many opportunities for growth, as VT has a wide range of technology experts who frequently give training in their area of expertise. As we move to a modern data-lake architecture, we need fast learners who are also passionate about supporting the existing processes that runs the University. The data management and analytics team is a part of central technology and runs Virginia Tech’s warehouse, data lake, data integrations and business intelligence solutions. We are looking for a problem solver who can help us deliver great service and technology solutions. Are you a good fit with the Hokies data management and analytics team?
Required Qualifications
Bachelor’s degree or equivalent relevant work experience in Information Technology related field
Architecture experience with ETL tools, like Talend, and data workflows where the candidate will setup framework and review code of other developers. This experience should include a cloud environment.
Extensive experience in relational databases, like Oracle, Postgres and Redshift including advanced SQL writing ability. Candidate will coach, guide and review work. Candidate will understand DBA role in order to communicate needs of the team
Experience with Business Intelligence tools such as MicroStrategy or Tableau
Demonstrated experience in working effectively in a team environment as a project leader. Strong communication and interpersonal skills with both technology team members and business partners. Ability to set competing priorities and meeting critical deadlines
Demonstrated experience in full system lifecycle development including requirements gathering, systems analysis, development of data services, testing and implementation of large enterprise systems as the subject matter expert
Preferred Qualifications
Extensive experience with process improvement that results in team buy-in and adoption
Demonstrated experience in managing priorities of others in an environment where individuals are working on multiple priorities with multiple skillsets
Extensive experience in data warehouse concepts and dynamic object architecture. Strong understanding in concepts and the features to maintain a well-structured and efficient database
Demonstrated experience with cloud technologies. Understanding architecture to where they can communicate needs to could engineers. Ideally AWS services such as EMR, Glue, Athena and Redshift
Experience in an object-oriented programming language like Python or Java.
Experience in an enterprise environment with DevOps best practices and technologies such as Docker, Version Control (GitLab), and Release Management (Jenkins)
Experience with data lakes in a Hadoop or AWS environment
Exposure to web services and APIs that return large data sets
Appointment Type
Regular
Salary Information
$90,000 - $110,000
Review Date
4/6/2023
Additional Information
The successful candidate will be required to have a criminal conviction check.
About Virginia Tech
Dedicated to its motto, Ut Prosim (That I May Serve), Virginia Tech pushes the boundaries of knowledge by taking a hands-on, transdisciplinary approach to preparing scholars to be leaders and problem-solvers. A comprehensive land-grant institution that enhances the quality of life in Virginia and throughout the world, Virginia Tech is an inclusive community dedicated to knowledge, discovery, and creativity. The university offers more than 280 majors to a diverse enrollment of more than 36,000 undergraduate, graduate, and professional students in eight undergraduate colleges, a school of medicine, a veterinary medicine college, Graduate School, and Honors College. The university has a significant presence across Virginia, including the Innovation Campus in Northern Virginia; the Health Sciences and Technology Campus in Roanoke; sites in Newport News and Richmond; and numerous Extension offices and research centers. A leading global research institution, Virginia Tech conducts more than $500 million in research annually.
Virginia Tech does not discriminate against employees, students, or applicants on the basis of age, color, disability, sex (including pregnancy), gender, gender identity, gender expression, genetic information, national origin, political affiliation, race, religion, sexual orientation, or military status, or otherwise discriminate against employees or applicants who inquire about, discuss, or disclose their compensation or the compensation of other employees or applicants, or on any other basis protected by law.
If you are an individual with a disability and desire an accommodation, please contact Brittany Bane at bmlester@vt.edu during regular business hours at least 10 business days prior to the event.
Advertised: March 23, 2023
Applications close:"
608,Senior Cloud Data Engineer,Mastercard,"2200 Mastercard Blvd, O'Fallon, MO 63368",N,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a
culture of inclusion
for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Job Title
Senior Cloud Data Engineer
Overview
Are you an experienced data engineer with a passion for cloud data innovation and a strong sense of ownership and innovative spirit? The Data and Analytics team is an excellent opportunity for you to design & develop the next wave of data innovation at MasterCard. Our program is leading the efforts to bring our data and analytics expertise into the cloud.

We are looking for a Senior Cloud Data Engineer to work on the development of data pipelines, cloud curated data sets and cloud data objects in our MasterCard approved cloud data frameworks. You will also work closely with cross-functional teams, including data scientists, data analysts, and software engineers, to drive data-driven decision making throughout the organization.

Role
Implement data engineering pipelines that extract data from various sources, perform transformation, aggregations and load data into centralized storage system in the cloud.
Work closely with senior data engineers and data architects using Agile methodology, take ownership of the delegated tasks and deliver the results in timely manner.
Support data scientists, data analysts in assessing, analyzing, and visualizing data, ensuring that data is accurate, reliable, and traceable.
Assist in troubleshooting and resolving issues with data pipelines, cloud data objects in Snowflake and integrations, ensuring they are running smoothly and efficiently.
Support the design and implementation of data models, object stores that serve BI reporting and curated data sharing and new analytics.
Stay up to date with emerging technologies and trends in the data engineering and cloud space and willingness to learn and use new tools and platforms that can improve efficiency.

All About You
Demonstrated ability to implement data pipelines in large-scale data systems and distributed computing framework.
Hands-on experience with Snowflake and SQL.
Knowledge of big data tools / technologies. (Hadoop, NIFI, Kafka, Impala, HIVE, OOZIE, Airflow etc.)
Knowledge of Snowflake cloud-based data warehousing and processing systems.
Knowledge of data modeling (SQL and NoSQL), data ETL and data warehousing concepts.
Good communication skills, with the ability to work in a team and collaborate with other data professionals and business stakeholders.
Strong attention to details and the ability to work under tight deadlines in a fast-paced environment.
Working experience of at least one programming language. (i.e. Java, Python, C#)
BS or higher degree in Computer Science, Information Technology or relevant fields.

Nice to have:
Cloud certifications and/or Azure/AWS Data Engineer certification
BI dashboard reporting experiences.
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines."
609,Data Engineer,Concurrency,United States•Remote,N,"Who We Are
We are change agents. We are inspired technologists. We are unlike any other technology consulting firm. Our team fearlessly challenges the status quo, relentlessly pursues what’s next and pushes the limits of what’s possible. A Microsoft Gold Partner and multiple time Partner of the Year award recipient, Concurrency is renowned for its ability to turn unmatched technology expertise into client outcomes. Have we inspired the technologist in you? Come be a change agent at Concurrency.
Who We’re Looking For
We’re excited to add a Data Engineer to our Data & AI team. In this role, you’ll work with a team of customer-focused professionals who are committed to defining technical strategy, architecting, designing, and delivering end-to-end digital transformation. you'll demonstrate strong technical competence and business acumen through engaging in senior-level technology decision-making discussions related to agility, business value, data warehousing, and cloud-oriented data solutions. You’ll empower other consultants by sharing subject matter expertise in large enterprise implementations, as well as overseeing the delivery of large, complex, and strategic projects for enterprise customers.
Position Responsibilities
Data Engineers for various and unanticipated worksites throughout the U.S. (HQ: Brookfield, WI).
Lead requirements and design sessions with customers and internal teams.
Author functional requirements and technical design documentation.
Build, automate, and modify ADF pipelines.
Create or modify ELT/ETL procedures and scripts in T-SQL.
Create or modify Python, Scala, and SQL programs.
Develop Power BI Tabular Models, Reports and Dashboards.
Work with the solution team to help set standard architectures, processes, and best practices.
Technical Environment: Data Analysis, Data Migration, Data Mining, Machine Learning, Data Modeling, ETL, Power BI, MS Azure ML, Azure SQL Database, SQL Server, R Studio, Python (NumPy, Pandas).
POSITION REQUIREMENTS:
Bachelor’s degree in Computer Science, Management Information Systems, or a related field plus 3 years of experience in the job offered or in data analytics required.
Required skills: Data Analysis, Data Migration, Data Mining, Machine Learning, Data Modeling, ETL, Power BI, MS Azure ML, Azure SQL Database, SQL Server, R Studio, Python (NumPy, Pandas). 100% telecommuting permitted.
Concurrency takes pride in bringing a different mindset to consulting—that takes a diversity of thought, collaboration and resilience. We are an innovation-obsessed yet a fun and progressive place to work. We offer flexible work schedules, competitive compensation, and great benefits for our people and their families.
In addition, all employees are eligible for several rewards and recognition programs, excellent training programs, and bonus opportunities to encourage our people to be the best versions of themselves in and out of work."
610,Staff Data Engineer,Machinify,California•Remote,"$150,000 - $193,000 a year","Machinify is a revolutionary healthcare software company with a mission to ensure that patients get the right medical treatment, at the right time, at the right price. Our cloud-based Machinify AI platform leverages the latest advances in machine learning, large language models, data analytics, and cloud processing to solve previously intractable problems, transforming healthcare administration and payment operations.
Machinify is seeking a Staff Data Engineer to own the models that power our ML pipelines running in production from start to finish. Machinify’s customers are at the center of everything we do, so we employ innovative thinkers to solve issues our customers don't even have yet.
Our people are at the heart of everything we do. Our teams are diverse and thrive on trust. Everyone has a voice. We are humans who understand our customer and work collaboratively to deliver value and make a difference.
What you’ll do:
Independently understand all aspects of a business problem including those unrelated to their area of expertise, weigh pros and cons of different approaches and suggest ones likely to succeed
Work with a cross-functional organization including engineering, delivering, subject-matter experts, product managers, as well as platform engineers to deliver a scalable framework.
Map the customer data into Machinify canonical form. Identify and ingest non canonical fields and generalize the process to a minimal level of customization.
Proactively design and adapt the canonical form to suit changing query patterns and needs.
Ultimately own data availability and quality for the Data Science organization.
Mentor and train other members of the team, setting frameworks that impact the performance of the Data function at Machinify
Set best practices and processes to optimize the way the team works towards business objectives
Proactively researches new tools and technologies, and makes savvy recommendations to drive business success for Machinify and customers
Enjoy working in an Agile/Scrum/Lean environment.
Be challenged when faced with solving complex problems
Bring a passion for improving the lives of others by making their jobs easier and more productive.
What You Bring:
Masters in Computer Science or STEM disciplines.
Experience applying modern NLP models in practical settings.
Plus if the candidate has worked with medical records and/or insurance claims.
Startup experience.
8+ years experience as hands-on Data Engineer in either as a lead developer role. SQL proficiency is a must.
7+ years of experience managing the delivery of complex data.
Experience understanding business problems and coming up with tech based solutions
Experience with cloud-based services and enterprise software environments.
Strong data technical background as evidenced by prior experience as a hands-on data/ETL engineer.
Familiarity with frameworks and protocols for data transfer, data consumption and data manipulation.
Analytical approaches to problem solving and data-driven decision making.
Ability to balance trade-offs among multiple competing priorities.
HealthCare Domain knowledge is a plus.
You are scrappy, fast, high bandwidth, adaptable and ambitious
Critical thinking and problem solving skills.
Comfortable with ambiguity and taking the initiative when required
Amazing communication skills
The base salary for this position will vary based on an array of factors unique to each candidate such as qualifications, years and depth of experience, skill set, certifications, etc. The base salary range for this role is $150k to $193k. We are hiring for different seniorities, and our Recruiting team will let you know if you qualify for a different role/range. Salary is one component of the total compensation package, which includes meaningful equity, excellent healthcare, flexible time off, and other benefits and perks.
What we offer:
Work from anywhere in the US! Machinify is digital-first.
Flexible and trusting environment where you’ll feel empowered to do your best work
Unlimited PTO, recharge days and one no-meetings day a week
Medical/Dental/Vision for employees & their families
Competitive salary, equity, 401(k) sponsorship
Generous Learning and Development Reimbursement policy
Equal Employment Opportunity at Machinify
Machinify is committed to hiring talented and qualified individuals with diverse backgrounds for all of its positions. Machinify believes that the gathering and celebration of unique backgrounds, qualities, and cultures enriches the workplace."
611,Sr. Data Engineer,BioCare,Remote,N,"Sr Data Engineer
Make 2023 your best year yet and join a growing company that included to 5000 America’s Fastest-Growing Private companies in 2022! BioCare is a distributor of Specialty Pharmaceuticals that treat rare diseases, such as hemophilia, immune system deficiency and ALS. We are looking for an Sr Data Engineer to join our growing IT team to design, build, and manage the BioCare data and analytics. If you want to make a difference in people’s lives and support our ability to get the right medication into the right hands, at the right time, apply today to be part of the BioCare family!
Why you should apply:
Low cost and flexible medical, dental & vision plans
20 days (160 hours) Paid Time Off accrued during your first year
Up to 40 hours Paid Sick Time accrued annually
401k with 5% employer match
Company paid life and disability insurance
Competitive pay
Typical duties:
Strong in data warehouse models and assist in building enhanced data warehouse from ground up with SAP S4/HANA
Develop and maintain scalable data pipelines in Azure cloud platform using data factory and through various API integrations from source applications
Architect, develop and maintain various data assets including data dimensions, TSQL objects, data scripts and data jobs to populate data models
Develop best practices for solution and tool frameworks, leveraging standard naming conventions, scripting, and coding practices to ensure consistency of data solution
Define and execute DW design and data movement standards, design reviews, pipeline CI/CD process, and data container policies to ensure high quality data management
Design and develop end-to-end data solutions by understanding business requirements by handling structured and unstructured data including ingestion, parsing, transform, auditing, logging, aggregation, modelling, and error handling
Maintain and optimize the performance of our data analytics infrastructure to ensure accurate, reliable, and timely execution of all data jobs
Understands structured and unstructured data from ERP such as SAP, CRM and other systems and develop data extract processes to meet end user reporting requirements
May designs and develops new reports and visualizations based upon defined request using power BI
Lead and manage delivery of daily, weekly, and monthly sales, finance, and transactional reports & data extracts both manual and automated.
Lead the development of data engineering components by following best practices or guidelines for repeatable tasks
Support and implement continuous improvement processes across the data operations and incorporate automation.
Demonstrate strong knowledge of entity relationship, dimensional modeling, enterprise data, and physical data models
Mentor data engineering team on various data projects, process improvement and technical innovations
May provide maintenance support, including enhancements and production support activities
Deliver quality customer service to all team members, customers, and stakeholders
Actively listen and collaborate with end users to design effective solutions that resolve issues
What we are looking for:
Bachelor’s degree in an IT or Computer Science discipline OR equivalent experience.
8+ years’ experience as a data engineer with the ability to extract data from both on premise and cloud sources using SQL components, Python, and data APIs
Extensive experience in creating ETL and pipelines using Azure Data framework, ADF, SSIS and maintaining enterprise data warehouse
Experience working with Azure data components such as Databricks, data services, ADLS, Synapse and Azure DBs
Experience in creating and managing ETL pipelines for data from enterprise ERP such as SAP S4
Experience with Power BI and SSRS/PBRS is a plus
Experience in working with source control tools such as SVN and GitHub
Experience with SAP or integrating to SAP is preferable
Certification in ADF and Synapse is preferable
Quality and Improvement focus to ensure data reliable, timely, and date accuracy
Ability to multitask and prioritize completing tasks
Possess an analytical mindset and problem-solving aptitude
Familiarity with working in Agile environment leveraging DevOps, change management process and tool like Jira
We are an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, gender, marital status, national origin, ancestry, disability, genetic information, medical condition, military/veteran status, sexual orientation, gender identity, gender expression, or any other characteristic protected by law.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Experience level:
8 years
Schedule:
Monday to Friday
Work Location: Remote

Health insurance"
612,Data Engineer,Quantum FBI,Michigan,N,"JOB OVERVIEW
We are a professional services firm that provides accounting and finance as a service, business advisory, and business intelligence services to early-stage and middle-market companies. We are searching for a Data Engineer responsible for all areas relating to business intelligence and data analysis. This position will be responsible for the design of the data infrastructure necessary for a product to be sold to clients. The Data Engineer will have contact with clients and client contacts, so native-like English proficiency is required.

SUCCESS FACTORS
Ability to manage and motivate a team
Superior ability to prioritize and focus in a high-paced, multi-tasking environment
Ability to quickly learn and use new technology software applications
Commitment to protecting client privacy, even within the firm, as appropriate
Ability to apply self to learning and applying technical skills
Co-operative team player
Interest in personal and professional development and advancement
Project management skills
Time management skills
Unrelenting commitment to client satisfaction

REQUIRED EXPERIENCE, SKILLS, AND QUALIFICATIONS
Bachelor’s Degree in Business Intelligence, Big Data, or similar areas.
2-3 years of experience creating processes
Strong technology skills using Power BI, Excel
Experience working in a paperless environment strongly preferred
Strong interpersonal and relationship-building skills
Team player with a positive ”can-do” approach
General and growing knowledge of firm products and services in the practice area, and general knowledge of products and services in other practice areas
Native-like English proficiency"
613,"Staff Software Engineer, Data Engineering",Sojern,"San Francisco, CA 94105•Remote",N,"About us:
Here at Sojern, we are on a mission to empower travel marketers to move travelers from dream to destination. Powered by machine learning and travel data, Sojern is quickly becoming the #1 travel marketing platform serving thousands of hotels, attractions, airlines and destinations worldwide.
We made Deloitte’s Technology Fast 500 list 6 years in a row, were recognized on the Top Company Cultures list by Entrepreneur Magazine, and were named a Best Place to Work by AdAge. As a globally distributed company, we are headquartered in San Francisco with employees based in 14 countries and counting.
Our team is passionate about travel and the core values that define our culture: Win as a Team, Embrace Inclusion, Be Genuine, Deliver Wow, and Center Around the Customer.
Need more convincing? Check out our Glassdoor reviews!
The Role:
As a Staff Software Engineer on our Attribution and Reporting team, some of the things you’d work on day-to-day include:
Shape the technical direction of this team, keeping in mind engineering best practices and larger technical trends
Mentor and help strengthen the skills of other team members
Working as a liaison with other teams to remove roadblocks and support the team as needed
The Expertise We’re Looking For:
Prior experience in AdTech, specifically with attribution, impression and click tracking
At least 6 years of software engineering experience
Ability to analyze and organize raw data; build data systems and pipelines
Experience with Golang and Python
Proficiency in SQL: constructing tables, normalization/denormalization techniques
Hands on experience with Docker, Kubernetes and Terraform
Excellent grasp of CS fundamentals - data structures and algorithms.
Experience with Distributed Systems.
Experience with Google Cloud Platform or Amazon Web Services infrastructure (EC2, S3).
The Value You Deliver:
Evangelizing new technologies and processes.
Actively participating in code/design reviews.
Working closely with the product, data science, and SRE teams to design, build, and deploy highly scalable and efficient core platform services that drive company-wide value and make an impact.
Being responsible for the delivery of data and information critical for Sojern’s business decisions and strategic planning.
Perks:
Flexi-Fridays: No internal meetings and shut down at Noon every Friday
Flexibility: Remote and hybrid work options, so our team can enjoy flexibility and do their best work regardless of location
Equity: Stock options are offered to every employee
Tech: Monthly cell phone & wifi stipend provided, in addition to laptop and tech equipment for your home office
Paid Parental Leave: 16 weeks pay for birthing parents and 12 weeks pay for non-birthing parents, beginning the day of joining until the child's first birthday. Where a local country has more liberal leave protocol, we happily comply! #embrace-inclusion
Learning Opportunities: All employees have access to an annual L&D stipend, as well as ongoing training and support to help you grow while leading creative and challenging projects
Sojern Gives Back: We regularly organize office volunteer programs with local charities and organizations, as well as provide 40 hours of paid time per year to volunteer
Culture: Strong core business values, focus on teamwork, vibrant, social and fun environment
Travel Perks: IATA membership and personal travel “hotel stay” benefit
Recognition: We use Bonusly to allow our team to recognize performance and teamwork #deliver-wow #win-as-a-team
Wellbeing: We take a whole-person approach to your wellness with Spring Health mental health benefits, a Wellness Coach, and Employee Assistance Programs
Paid Time Off: Unlimited PTO (we are a travel company, after all!)
Healthcare: Comprehensive medical, dental, and vision insurance for you and your dependents with healthy employer premium contributions
Life and Disability: Life, AD&D and Short- and Long-Term Disability insurance with premiums 100% paid by Sojern
Medical Travel: Travel expense coverage for when medical treatment is needed that is not available locally
Retirement: 401k available with company profit sharing contributions
Referred by Sojern?
If you are being referred for this position by a current employee of Sojern, please reach out to that person to submit you as a referral before applying!
Hiring Locations:
US Roles at Sojern based outside of our Omaha, NE or San Francisco, CA office can sit in any of the following states: AZ, CA, CO, CT, FL, GA, IL, IN, IA, KS, MA, MI, MN, MO, NE, NJ, NY, NC, PA, SD, TN, TX, UT, WA, and WI. Please only apply if you are able to live and work full-time in one of the states listed above. State locations and specifics are subject to change as our hiring requirements shift.
Our Commitment to Diversity Equity and Inclusion:
At Sojern, we work to create a brave space that seeks out, embraces, and promotes diverse thoughts, beliefs, and experiences of Sojernistas globally.

We are building a workforce that represents the customer base we are serving and the world we live in. A diversified workforce is an innovative workforce. Just as there is no one archetype traveler, there is no one archetype employee. We foster diversity and inclusion across the company, actively seeking to amplify underrepresented voices and apply diverse perspectives to ensure products, policies, and programs are relevant for our employees and clients alike.

Embracing our differences and celebrating them moves us towards our goal of making travel inclusive of all. Sojern has over 15 employee-led Affinity Groups, including Embrace Inclusion, So-Proud, Sojern Women's Group, Sojern Gives Back, and more, which are the core of our culture, fostering belonging, inspiring learning, and creating opportunities for all Sojernistas across the globe.
At Sojern, we value diversity and always treat all employees and job applicants based on merit, qualifications, competence, and talent. We do not discriminate on the basis of race, religion, color, national origin, gender, gender orientation, sexual orientation, age, marital status, veteran status, or disability status.

#LI-Remote
#LI-NB2

This is a remote position."
614,QA Engineer - Data (Remote),Everly Health,United States•Remote,N,"Everly Health's mission is to transform lives with modern, diagnostics-driven care, and we believe that the future of healthcare is meeting people where they are. Headquartered in Austin, Texas, Everly Health is the parent company to Everlywell, Everly Health Solutions, Natalist, and Everly Diagnostics. We've set a new standard of people-focused, diagnostic-driven care that puts patients at the center of their own health journey.

Our infrastructure guides the full testing experience with the support of a national clinician network that's composed of hundreds of physicians, nurses, genetic counselors, PharmDs, and member care specialists. Our solutions make world-class virtual care more attainable with rigorous clinical protocols and best-in-class science to tackle some of the healthcare industry's biggest problems.

Everlywell has seen unprecedented growth over the past couple of years. As we continue to grow rapidly, it is imperative to design an effective and scalable data infrastructure to support our current and future business needs. We are looking for a skilled and highly motivated individual to fill our QA Engineer (Data) role. You will be working on building our next generation of data stack to support the business. This job function will require putting on many different hats and working with multiple departments to create a reliable data pipeline. You will be working on creating effective data validation strategies and automation.
What You'll Do:
Build/Develop/Maintain scalable and configurable Data Quality frameworks in Big Data Environment
Work with Data Analytics team to help them refine their data needs and ensure quality of data for our visualizations
Build efficient ETL packages to move data through our data pipeline
Optimization of existing frameworks and Performance tuning
Help build a framework for anomaly detection and monitoring
Develop processes to help ensure data integrity across various data stores
Help create and maintain ERD’s for our relational and data warehouse databases
Help build a dataset for controlled application testing
Document our current flow and maintain documentation on an on-going basis
Build Data Analysis/Data Validation strategies to support development of Data Warehousing & Data Analysis solutions.
Writing complex SQL statements to validate data and perform extensive data analysis to identify defects.
Who you are:
Minimum 3 years of ETL experience.
Minimum 3 years experience with Data Analytics and SQL structured programming, ad hoc queries, and data analysis development
Minimum 3 years experience with at least one RDBMS
Experience with data visualization and dashboarding using tools like Tableau, Redash, etc.
Must have good written and verbal communication skills, and be able to communicate effectively with internal teams and external partners
Data Management and Architecture experience.
Quick learner in a rapidly changing technology environment
Must be able to work independently as well as in a team environment
Performance tuning experience
Preferred Skills/Experience
Hands on experience with PostgreSQL
Experience with ETL and orchestration tools like Airflow, Pentaho etc.
Experience with Big data solutions such as Snowflake or Redshift
Experience with PySpark
Experience with workflow tools to build and maintain an effective data strategy

You'll Love Working Here:

Venture backed by top-tier firms
The opportunity ahead knows no bounds
Open vacation policy for salaried team members
Front Loaded PTO for hourly team members
Employee discounts
Paid parental leave
Health benefits
401(k)

NO EXTERNAL RECRUITERS - INDIVIDUAL APPLICANTS ONLY

Everly Health is committed to providing equal employment opportunities in all employment practices. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, citizenship status, marital status, age, disability, protected veteran status, sexual orientation or any other characteristic protected by law.

HIPAA Disclaimer: This role will be in an environment that has access to protected health information (PHI) and all security standards to protect PHI must be followed."
615,Data Engineer,N,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
616,Software Engineer - Data Platform,Lacework,"Mountain View, CA•Remote","$119,000 - $300,000 a year","At Lacework, we strive to provide a supportive, collaborative environment where people are empowered to do the best work of their careers.
Our team members enjoy solving complex problems, big sky thinking, and obsess over getting the details right. We love what we do and are proud of our work to secure clouds and container environments for thousands of users worldwide.
At Lacework, we strive to provide a supportive, collaborative environment where people are empowered to do the best work of their careers.
Our team members enjoy solving complex problems, big sky thinking, and obsess over getting the details right—all while building bonds of teamwork and friendships that last a lifetime. We love what we do and are proud of our work to secure clouds and container environments for thousands of users worldwide.
Cloud computing is revolutionizing IT and forcing organizations to rethink their approach to cloud security. Lacework is at the forefront of this transformation. We enable security teams to effectively secure public and private clouds – AWS, Azure or collocations – by eliminating repetitive, manual and labor-intensive security tasks. Using Lacework, security teams operate security at the same pace as DevOps, which relies on automated tools to publish daily updates to the cloud.
We are looking for a Senior Software Developer, Data Platform

What You Will Accomplish
Architect, design, and implement highly scalable distributed systems that provide availability, scalability and latency guarantees.
Develop infrastructure as required to monitor and scale.
Own one or more services in its entirety within Data Platform from design, coding to delivery and monitoring post deployment.
Participate in functional scoping and work distribution (be a tech lead)
Peer Code reviews and technical design reviews
Mentor and guide junior team members.
What You Will Bring
Minimum 5 years of experience building highly available, scalable, distributed services.
Bachelors in CS or similar
Proven ability and an understanding of design for scalability, performance, and reliability
Deep understanding of relational databases such as MySQL, Postgres, Oracle, and SQL development are required. Experience with Snowflake is plus
Must have experience developing microservices with Java, using frameworks such as Spring, Drop wizard.
Experience developing Kubernetes based services on AWS Stack is highly desired.
Excellence in technical communication with peers and non-technical cohorts
Detail oriented, have a proven ability to be a technical lead for a small team / project.
Preferred Qualifications and Nice-To-Have
10 years of industry experience.
Advance degree (M.S. preferred)
Hands-on experience in developing Data Integration applications for large scale (petabyte scale) environments with experience in both batch and online systems.
Experience with Unix/Linux development/production environments
Powerful sense of ownership, customer obsession, and drive
Sharp analytical abilities, Quick learner; self-starter, detailed and thorough.
Strong skills on mentoring/growing junior people.
Demonstrated technical leadership abilities & a team player.
Demonstrated ability to achieve stretch goals in a highly innovative and fast paced environment.
Salary Range: $119k - $300k USD Annually + Benefits + Bonus + Equity
Actual compensation may vary based on factors such as geographic location, work experience, education/training and skill level.
Location: Mountain View, CA | Seattle, WA | Ireland | United Kingdom
Lacework is an Equal Opportunity Employer. It is the policy of Lacework to provide equal employment opportunity to all persons, regardless of age, race, religion, color, national origin, sex, political affiliations, marital status, non-disqualifying physical or mental disability, age, sexual orientation, membership, or non-membership in an employee organization, or on the basis of personal favoritism or other non-merit factors, except where otherwise provided by law"
617,Big Data Engineer,SoftTeco,Remote,N,"SoftTeco is looking for a Big Data Engineer who has solid experience working with Apache Hadoop and Flink, knows how to effectively scale databases in use, and is genuinely interested in secure data migration and integration.
You will be joining a challenging Fintech project and you will be required to relocate to Warsaw, Poland. SoftTeco will provide you with full support regarding the relocation process and you will get access to all our corporate benefits, including medical insurance, educational programs, and bonus programs.
Requirements:
A minimum of 4 years in server-side development.
Robust knowledge of the Apache ecosystem.
Experience in working with data streaming platforms such as Apache Kafka, Apache Flink, etc.
Good knowledge of spoken and written English to efficiently communicate with the client and the team."
618,Data Engineer,Nyla Technology Solutions,"Washington, DC•Remote",N,"Description
We are pursuing individual(s) for the position of Data Engineer. If this position sounds like a good fit and you have a track record of delivering solid results, please apply.
The Data Engineer designs strategies for enterprise database systems and set standards for operations, programming, and security. Design and construct large relational databases. Integrate new systems with existing warehouse structure and refine system performance and functionality. The candidate will possess and apply expertise on multiple complex work assignments. Assignments may be broad in nature, requiring originality and innovation in determining how to accomplish tasks. Operates with appreciable latitude in developing methodology and presenting solutions to problems. Contributes to deliverables and performance metrics where applicable.
This is a full time remote role that has an as needed onsite requirement in the DC Metro Area.
Required Skills
The candidate will have 3 to 10 years of experience and a BA/BS or MA/MS degree. Candidate will be able to perform all functional duties independently.
About Nyla Technology Solutions
Nyla Technology Solutions is a Women-Owned Small Business that is forward-thinking and bold at every step which has earned us a solid reputation of being technical trendsetters within the industry. Headquartered in the heart of Baltimore City, Maryland, Nyla delivers exceptional software systems engineering services for the U.S. Government. Our customers like how we tackle their toughest problems., and so Nyla is adding people who have a passion for doing fun, impactful work. If you are a person who welcomes opportunities to apply your skills in new ways, Nyla has challenges for you. We seek out people with agile, diverse mindsets who are looking for a place to grow—professionally and personally. We create opportunities for you to share your knowledge and experience with the team, and learn from others via training, mentoring, and movement across the many contracts Nyla supports. Nyla endeavors to give back to our community—lending our energy and talents to support local area organizations helping people in need. At Nyla, you will have a place to grow, get, and give where you are passionate.
How We Show Up Every Day
Team Nyla shows up every day with the intention to be awesome—proactively working to accurately interpret and fully understand our client’s challenges and add value to solving those challenges. Knowing the mission, context, and end-users is how we deliver optimal, creative, and innovative solutions. We design our systems with precision, logic, and adaptability, solving the technical and engineering needs of today with an eye on scaling for tomorrow. We endeavor to meet the highest standards of personal conduct and act as a trusted-provider for our clients-this is how we develop and maintain our enduring relationships with customers and business partners.
Progress Through Our People
Nyla’s culture is built on the understanding that together, with our collective energy, talent, and fierce determination, we never stop believing and building the impossible. We are life-long learners, ever-curious and not afraid to dream.
If you have the unique experience and expertise we are seeking, along with the desire and determination to invest your time and energy as a part of Nyla’s Team, we will provide you with a first-class compensation package along with our uniquely Nyla benefits.
We Have the Total Package | Healthy Lifestyle, Learning, Health and Wellness, Financial Wellness, Financial Protection. From the start, Nyla is different. We never embraced the idea of a workplace and policies stuck in the industrial age-we treat our employees as valued contributors, not commodities. At Nyla, we strive to understand the needs of our workforce and are committed to continuously evolving practices and policies, making adjustments that meaningfully address the changing needs and desires of our workforce.
At Nyla we talk about employee investment, not employee costs—a small way we outwardly provide our great people with benefits that cover your overall well-being. Nyla offers group benefits, 100% paid by Nyla, that include CareFirst medical, dental, and vision coverage. To support your personal goals for overall well-being and health, we provide a healthy living and fitness benefit of up to $500.00 per anniversary year. We believe that an ongoing investment in your professional learning and development is a cornerstone to our success, so we go all-in and provide up to $5,000 per anniversary year to support your growth or tuition assistance of up to $5,250 per year. We want to protect you and your future—Nyla provides, at no cost to you, short-term and long-term disability, and life insurance. And, to augment your financial beyond today’s salary, Nyla offers up to 10% employer contribution/match in our retirement plan. Lastly, we understand the importance of recharging and replenishing yourself, so we provide new employees with 4 weeks of Paid Annual Leave and 11 holidays, plus each year you are with the company, you get an extra day off.
We are an equal opportunity employer-but we are more than that – we are open-minded, and care only about your capability, your drive, and your desire and determination to contribute your gifts and talents fully."
619,Data Warehouse Engineer,PRIMUS GLOBAL SERVICES,Remote,"$135,000 - $138,000 a year","All consultants must work on PRIMUS payroll.
Role: Sr. Data Warehouse Engineer
Location: Miami, FL REMOTE
10+ years in Data Warehousing, Informatica products, and Oracle databases.
Expert understanding of PL/SQL developer, functions, procedures, packages, and triggers.
Expert understanding of SQL Navigator and/or Toad. Recent experience developing on Oracle 19c or higher.
For immediate consideration, please contact:
Kumar
972-798-2661
Primus Global Services
Job Type: Full-time
Salary: $135,000.00 - $138,000.00 per year
Schedule:
8 hour shift
Monday to Friday
Experience:
Informatica Power Centre: 4 years (Required)
Oracle Databases: 4 years (Required)
Work Location: Remote"
620,Sr. Data Engineer,Apolis,Remote,Up to $85 an hour,"Data Engineer
Corp-Corp
Remote
Description:
We need Data Engineer with hands-on experience in PySpark to build data pipes in AWS environments.
Should be able to write design documents and independently build the Data Pipes based on the defined Source to Target mappings.
The candidate should have good exposure to RDBMS and able to convert complex Stored Procedures, SQL Triggers, etc. logic using PySpark in the Cloud platform.
The candidate should be open to learn new technologies and implement solutions quickly in the cloud platform
Data Modeling ,PySpark, SQL, AWS, Python
Any knowledge of the Palantir Foundry Platform will be a big plus.
Communicate with program key Stakeholders to keep the project aligned with their goals.
Effectively Interact with QA and UAT team for code testing and migrate to different regions.
4-5 years of hands-on experience in PySpark to build data pipes in AWS environments.
Expert in writing shell scripts to execute various job scheduler
Basic understanding of Informatica Mapping and Workflows.
The candidate should able to convert complex Stored Procedures, SQL Triggers, etc. logic using PySpark in the Cloud platform.
Job Types: Full-time, Contract
Salary: Up to $85.00 per hour
Experience level:
10 years
11+ years
9 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
AWS: 5 years (Required)
Pyspark: 4 years (Required)
Work Location: Remote
Speak with the employer
+91 7322856236"
621,Data Engineer,Aroma360,"Miami, FL","$90,000 - $100,000 a year","Aroma360 is a boutique Scent Marketing and Branding company that specializes in providing the highest-quality essential oil-based scenting solutions to businesses and individuals all around the world. As the only full-service scent company, Aroma360 guides clients from concept and development to strategic implementation while priding themselves on exceptional customer service every step of the way. Aroma360 is continuously raising the bar for healthy scenting solutions in the industry!
We are looking for a talented and passionate IT professional with a broad range of experience and specialized skills in database management and data analytics to join our IT and software development team. Here's what you can expect from this role:
Responsibilities:
Managing databases and data warehouses
Extracting, standardizing, and joining data from a variety of software platforms, including Salesforce CRM and SQL databases
Designing and maintaining ETL scripts
Working with stakeholders throughout the company to analyze requirements for reporting and business analytics
Constructing reports and dashboards to convey relevant business metrics
Contributing to process automation projects
Assisting IT team with overflow support requests
Desired Skills:
Experience with any or all of the following languages: Python, JavaScript (inc. React, Angular, and Node.js), PHP, Apex
Strong ability to model business logic and rapidly develop feasible technical solutions in a changing business environment
Knowledge of Unix/Linux shell scripting
Familiarity with Grafana or equivalent data visualization tools
Strong knowledge of SQL; hands-on experience with PostgreSQL is a plus
ETL processes and reporting automation solutions; Talend experience is a plus
Jira or other Kanban-style project management tools
Familiarity with MS Excel or comparable spreadsheets
Experience with ViciDial-based contact center platforms is a plus
Experience with Salesforce and/or NetSuite is a plus
Experience with Google Cloud Platform is a plus
Desired Qualities:
Trustworthiness and dependability
Orientation toward quality and results
Willingness to take initiative
Strong expertise in data, project, and time management
Strategic and creative problem solving
Clear and effective communication skills
Consistent accuracy and attention to detail
Comprehensive technical and business knowledge
Desired Education:
A bachelor's degree in Computer Science, Data Science, Information Systems, or an equivalent field of study is preferred.
Joining our team comes with a range of exciting benefits to support your health, well-being, and professional growth, including:
Comprehensive health coverage, including dental and vision insurance, to ensure you and your family are taken care of.
Life insurance provides peace of mind for you and your loved ones.
Paid time off, allowing you to recharge and enjoy life outside of work.
Access to a 401(k) plan to help you plan for a secure financial future.
Employee discount to take advantage of great deals on our products and services.
Opportunities for paid training to develop your skills and advance your career.
Fun and exciting company events.

Our organization is an equal opportunity employer and does not discriminate against any candidate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status, or any other legally protected characteristics. We are committed to promoting diversity, equity, and inclusion in our workplace and welcome candidates from all backgrounds to apply for any open positions."
622,Senior Data Engineer,N,"Chicago, IL•Remote",N,"At Janus, we believe in a world where healthcare functions efficiently. Join us on our mission to improve the lives of administrative workers and fundamentally change the way work is done. Our team is building a world-class process improvement platform to help healthcare providers generate more cash with fewer resources.
Summary
The Senior Data Engineer will be responsible for working closely with the Product and Customer Success teams as well as clients to design and build highly reliable and scalable data pipelines supporting our automation systems. As a Senior Data Engineer, you will use various methods to transform raw data into useful data systems, creates algorithms, and conducts statistical analysis. Our ideal candidate has strong analytical skills, is detail-oriented, and has excellent organizational skills.
Responsibilities
Build data systems and pipelines.
Evaluate business needs and objectives.
Interpret trends and patterns.
Conduct complex data analysis and report on results.
Prepare data for prescriptive and predictive modeling.
Build algorithms and prototypes.
Combine raw information from different sources.
Explore ways to enhance data quality and reliability.
Identify opportunities for data acquisition.
Develop analytical tools and programs.
Collaborate with data scientists and architects on several projects.
Qualifications
5+ years of relevant work experience.
Bachelor's degree in computer science or related field and/or equivalent work experience.
Strong knowledge of ETL, and data pipelines.
Strong knowledge of Python and PySpark.
Experience with SQL and manipulating databases.
Experience with AWS.
Experience with Quicksight and Databricks.
Experience with relational databases and models.
Passion for growing your skills, tackling interesting work and complex problems.
Excellent prioritization, logical deduction and organizational skills.
Ability to innovate in a fast-growing work environment and comfortable dealing with ambiguity.
Highly motivated self-starter who is an excellent team player.
Outstanding communication (both verbal and written) skills.
Flexibility and comfort working in a dynamic and constantly changing environment of a startup.
Has fun, celebrates success, and contributes to a positive culture!
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Care for the Whole Person
At Janus, our commitment is to provide each employee with what they need to be successful. Our benefits package has been designed in a thoughtful way that allows our employees to be happy, healthy and whole. Here are a few things we offer:
We contribute 100% of base plan medical premiums for employees and 50% of premiums for family members.
We contribute 75% of premiums for dental and vision insurance for employee-only plans.
We have an employee assistance program that allows you the chance to work through any issues that may arise with the appropriate professional.
We have a 401k plan with 0% portfolio fees (traditional and 401k options, as well as rollovers and loan capabilities).
We offer stock options to share in the value we create and in the ownership of Janus, so let's make it something that we are proud of.
We offer unlimited PTO because we want our employees to take the time they need to rejuvenate and relax. At minimum, encourage all employees to take at least 15 fully unplugged days off each year.
We encourage sacred moments that are free from distractions and allow you to create a connection with someone or something that is meaningful to you.
We provide a monthly allowance to cover the cost related to working in a remote environment like upgraded internet or to offset your cell phone bill.
We offer parental leave because bonding with your newest addition is so important!
We have caregiving leave for our employees that are the primary caregiver for a loved one and needs time to care for that person.
We want you to look for personal enrichment opportunities and will give you up to $500 per year to invest in yourself. We want you to be the best version of you and take time to do things you enjoy!
We encourage on-going training, additional certifications and professional development related to your role and will review all requests for additional growth (including travel).
We have committees focused on organizational initiatives to increase employee happiness and the recruitment and retention of a broad, inclusive workforce that represents a diverse range of interests, abilities, talents, and cultures. You are welcome to join!
We have a benefits summary for you to review and will send more comprehensive information with your offer letter. If you want to review it sooner, just let us know!
Equal Opportunity Statement
Janus is an equal opportunity employer. We hire great people from a wide variety of backgrounds and appreciate our differences. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Janus provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Janus' policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require reasonable accommodation, please contact the People team.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
#LI-REMOTE"
623,Senior Data Security Engineer - Data Loss Prevention and Encryption (REMOTE),GEICO,"Chevy Chase, MD 20815•Remote","$82,000 - $185,000 a year","We are seeking a skilled and experienced Senior Data Security Engineer to join our team. In this role, you will be responsible for developing, implementing, and maintaining our data loss prevention (DLP) and encryption strategies. You will be tasked with ensuring that our organization's sensitive data is protected against unauthorized access, modification, and theft. You will also work closely with our internal stakeholders to assess and mitigate any potential risks to our data. If you have a passion for data security and a desire to work in a fast-paced and dynamic environment, then we encourage you to apply for this exciting opportunity.

Key Responsibilities:
Develop and implement data loss prevention (DLP) policies and procedures, including data classification, access controls, and monitoring.
Deploy and manage encryption technologies to protect data in transit and at rest.
Work closely with cross-functional teams to ensure security requirements are integrated into new projects and initiatives.
Develop and maintain security documentation, including policies, standards, and procedures.
Stay up-to-date on emerging security threats, technologies, and industry best practices.
Train and mentor junior team members on security best practices and procedures.

Experience:
5+ years of experience in data security engineering, with a focus on data loss prevention and encryption.
Experience with DLP technologies, such as Symantec DLP, Proofpoint DLP, or Forcepoint.
Knowledge of encryption technologies, such as TLS, SSL, and PGP.
Experience with cloud security technologies, such as AWS, Azure, or Google Cloud.
Strong understanding of networking and operating system security.
Excellent communication skills and ability to work collaboratively with cross-functional teams.

Certifications:
Relevant industry certifications, such as CISSP and CISM are a plus.
Security+ is required within 6 months of hire.
Benefits:
At GEICO, we make sure you have the support and resources to leverage and develop your skills, secure your financial future, and take care of your health and well-being. GEICO continually seeks to provide a workplace where everyone can be their authentic self. To help achieve this goal, we support associate-led Employee Resource Groups that foster a true sense of community. Through GEICO’s competitive benefits offerings and various training and development opportunities, we have you covered with our Total Rewards Program* that includes:
Premier Medical, Dental and Vision Insurance with no waiting period**
Paid Vacation, Sick and Parental Leave
401(k) Plan
Tuition Assistance including Direct Billing and Reimbursement payment plan options
Paid Training, Licensures and Certificates
Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
**Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire coverage to take effect.
GEICO is proud to be an equal opportunity employer. We are committed to cultivating an environment where equal employment opportunities are available to all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO celebrates diversity and believes it is critical to our success. As such, we are committed to recruit, develop and retain the most talented individuals to join our team.
#LI-AW1
Annual Salary
$82,000.00 - $185,000.00
The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations."
624,Senior Data Protection Engineer,Publix,"331 S Florida Ave, Lakeland, FL 33801","$106,600 - $161,395 a year","The job responsibilities for the IS Senior Engineer 2 position are:
provide technical support and system administration for enterprise security tools such as hardware security modules (HSMs), certificate management, file encryption, tokenization.
Evaluate, Implement, and administer solutions to reduce risk and enhance the security around sensitive and private data.
Evaluate and update current policies to provide risk analysis and implementation guidance to advance data protection efforts
Stay up to date with the latest trends in Security and Software Engineering practices
provide project management for security projects and new security tools to include requirements development, evaluation, vendor selection, installation, configuration, tuning, and creation of program and process documentation,
fulfill the duties of the subject matter expert for cryptographic key encryption within Publix, including symmetric and asymmetric encryption methodologies, and
provide support for production, development, and disaster recovery (DR) systems with implementation involvement.


Required Qualifications: The required qualifications for the IS Senior Engineer 2 position are:
must have a bachelor’s degree in Management Information Systems, Computer Science, Business, or other technical/analytical disciplines, or equivalent experience,
must have at least four years of experience in IT security providing technical support/system administration for security tools such as hardware security modules (HSM), certificate management solutions, file encryption solutions for structured and unstructured data, or email security and data loss prevention solutions for large corporate enterprise,
must have basic knowledge of the following: PCI-DSS, SOX, HIPAA requirements
must have excellent written and verbal communication skills with the ability to relate to all levels of Publix associates,
must have the ability to resolve complex business and technical issues by making decisions using sound business judgment,
must have excellent customer service skills and commitment to teamwork,
must be flexible and able to handle stressful situations in a professional manner,
must demonstrate ability to work under minimal supervision,
must have strong analytical, problem-solving, and conceptual skills,
must have a high degree of confidentiality, maturity, self-motivation, commitment, and integrity and,
must show enthusiasm, initiative, attention to detail, punctuality, pride in work, and a commitment to Publix and our Mission.
Address: 331 S. Florida Ave"
625,Azure Data Engineer,Intuceo,Remote,"Up to $180,000 a year","Job Description: Looking for US Citizens only for Federal Client
Bachelor’s degree in science, engineering or technology.
Minimum 8+ years’ experience in designing, delivering and supporting IT solutions.
Experience in Big data analytics projects and products, business processes and architecture.
Experience in Azure Services like ADF, ADLS, Synapse, Databricks, Azure Monitoring.
Experience in Creating Data Pipelines using Apache Hive, Apache Spark, Apache Kafka.
Experience in Programming languages like Python/Java, ETL is required
Experience working with huge datasets, SQL/NO SQL exposure
Azure Data Engineering Certification is plus
Experience in the Insurance industry is preferred.
Please share the resume and reach out to me on 9042041368
Job Type: Full-time
Salary: Up to $180,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Monday to Friday
Experience:
Azure Data Engineer: 10 years (Preferred)
SQL: 5 years (Preferred)
Azure: 3 years (Preferred)
License/Certification:
Azure Data Engineer (Preferred)
Work Location: Remote"
626,"Software Engineer, Data",FATHOM,"New York, NY 10002•Remote",$100 an hour,"Fathom is on a mission to use AI to understand and structure the world's medical data, starting by making sense of the terabytes of clinician notes contained within the electronic health records of the world's largest health systems. Our deep learning engine automates the translation of patient records into the billing codes used for healthcare provider reimbursement, a process today that costs hospitals in the US $15B+ annually and tens of billions more in errors and denied claims. We are a venture-backed company that completed a Series B round of financing for $46M in late 2022.
We are looking for a Software Engineer, Data to work on data products that drive the core of our business. We want to work with teammates in New York, who are excited about learning how to build and support machine learning pipelines that scale not just computationally, but in ways that are flexible, iterative, and geared for collaboration. If you are a data expert able to unify data, and build systems that scale from both an operational and an organizational perspective, Fathom might be the right fit for you.
Your role and responsibilities will include:
Developing data infrastructure to ingest, sanitize and normalize a broad range of medical data, such as electronics health records, journals, established medical ontologies, crowd-sourced labelling and other human inputs
Building performant and expressive interfaces to the data
Creating infrastructure to help us not only scale up data ingest, but large-scale cloud-based machine learning
We are looking for a teammate with:
2+ years of development experience in a company/production setting
Experience building data pipelines from disparate sources
Hands-on experience building and scaling up compute clusters
A solid understanding of databases and large-scale data processing frameworks like Hadoop or Spark and the ability to evaluate which tools to use on the job
A unique combination of creative and analytic skills apt of designing a system capable of pulling together, training, and testing dozens of data sources under a unified ontology
Bonus points if you have:
Know-how of developing systems to do or support machine learning, including experience working with NLP toolkits like Stanford CoreNLP, OpenNLP, and/or Python's NLTK
Expertise with wrangling healthcare data and/or HIPAA
Experience with managing large-scale data labelling and acquisition, through tools such as through Amazon Turk or DeepDive

Salary range:
$100 000 - $160 000 USD"
627,Senior Data Engineer,Ensemble Health Partners,Remote,N,"Thank you for considering a career at Ensemble Health Partners!
Ensemble Health Partners is a leading provider of technology-enabled revenue cycle management solutions for health systems, including hospitals and affiliated physician groups. They offer end-to-end revenue cycle solutions as well as a comprehensive suite of point solutions to clients across the country.
Ensemble keeps communities healthy by keeping hospitals healthy. We recognize that healthcare requires a human touch, and we believe that every touch should be meaningful. This is why our people are the most important part of who we are. By empowering them to challenge the status quo, we know they will be the difference
The Opportunity:
Job Competencies
Valuing Differences - Works effectively with individuals of diverse cultures, interpersonal styles, abilities, motivations, or backgrounds; seeks out and uses unique abilities, insights, and ideas. Considers the collective.
Collaboration - Works cooperatively within teams and partners with others, both internally and externally as needed, to achieve success; focuses on the results of the team, not the achievements of one person. It’s “All for One and One for All”
Accountability - Accepts personal responsibility and/or consequences of failure and successes, delivering on commitments and refocusing effort when needed. Someone who is willing to step up and own it.
Time Management - Effectively manages personal time and resources to ensure that work is completed efficiently.
Developing Trust - Gains others’ confidence by acting with integrity and following through on commitments; treats others and their ideas with respect and supports them in the face of challenges.
Takes Initiative - Takes prompt action to accomplish goals and achieve results beyond what is required; is proactive and pursues relentlessly.
Essential Job Functions
Develop, test, deploy, monitor, maintain, and continuously improve scalable data pipelines and builds our new API integrations to support continuing increases in data volume and complexity.
Translate product concepts into project commitments that deliver incremental value to our customers frequently and with high quality.
Review and recommend architectural patterns for data access and performance that align with the company’s best practices and platforms.
Validate, maintain, and enhance current data pipelines. Partner with internal and external clients to ensure company data needs are met.
This document is not an exhaustive list of all responsibilities, skills, duties, requirements, or working conditions associated with the job. Associates may be required to perform other job related duties as required by their supervisor, subject to reasonable accommodation.
Join an award-winning company
Three-time winner of “Best in KLAS” 2020-2022
2022 Top Workplaces Healthcare Industry Award
2022 Top Workplaces USA Award
2022 Top Workplaces Culture Excellence Awards
Innovation
Work-Life Flexibility
Leadership
Purpose + Values
Bottom line, we believe in empowering people and giving them the tools and resources needed to thrive. A few of those include:
Associate Benefits – We offer a comprehensive benefits package designed to support the physical, emotional, and financial health of you and your family, including healthcare, time off, retirement, and well-being programs.
Our Culture – Ensemble is a place where associates can do their best work and be their best selves. We put people first, last and always. Our culture is rooted in collaboration, growth, and innovation.
Growth – We invest in your professional development. Each associate will earn a professional certification relevant to their field and can obtain tuition reimbursement.
Recognition – We offer quarterly and annual incentive programs for all employees who go beyond and keep raising the bar for themselves and the company.
Ensemble Health Partners is an equal employment opportunity employer. It is our policy not to discriminate against any applicant or employee based on race, color, sex, sexual orientation, gender, gender identity, religion, national origin, age, disability, military or veteran status, genetic information or any other basis protected by applicable federal, state, or local laws. Ensemble Health Partners also prohibits harassment of applicants or employees based on any of these protected categories.
Ensemble Health Partners provides reasonable accommodations to qualified individuals with disabilities in accordance with the Americans with Disabilities Act and applicable state and local law. If you require accommodation in the application process, please contact
TA@ensemblehp.com
.
EEOC – Know Your Rights

FMLA Rights - English
La FMLA Español"
628,Senior Data Engineer (Data Systems),Demandbase,"San Francisco, CA 94107•Remote","$146,000 - $259,000 a year","Introduction to Demandbase:
Demandbase is the Smarter GTM™ company for B2B brands. We help marketing and sales teams overcome the disruptive data and technology fragmentation that inhibits insight and forces them to spam their prospects. We do this by injecting Account Intelligence into every step of the buyer journey, wherever our clients interact with customers, and by helping them orchestrate every action across systems and channels - through advertising, account-based experience, and sales motions. The result? You spot opportunities earlier, engage with them more intelligently, and close deals faster.
As a company, we're as committed to growing careers as we are to building world-class technology. We invest heavily in people, our culture, and the community around us. We have offices in the San Francisco Bay Area, New York, Seattle, and teams in the UK and India, and allow employees to work remotely. We have also been continuously recognized as one of the best places to work in the San Francisco Bay Area.
We're committed to attracting, developing, retaining, and promoting a diverse workforce. By ensuring that every Demandbase employee is able to bring a diversity of talents to work, we're increasingly capable of living out our mission to transform how B2B goes to market. We encourage people from historically underrepresented backgrounds and all walks of life to apply. Come grow with us at Demandbase!
About the Role:
The compensation range for this role is: $146,000 - $259,000
In this role, you will help build the next generation unified data platform that will combine datasets from across the Demandbase ecosystem. Using the latest open source tools, you'll solve complex data warehousing problems to ensure quality, discoverability and accessibility of data. You'll build batch and streaming data pipelines for ingestion, normalization and analysis; in addition, you'll develop standard design and access patterns that will allow these pipelines to stay flexible and grow over time as the needs of the business change. You'll be a leader in the unification of data from our multiple products as we come together as one Demandbase platform.
What you'll be doing:
Build out all aspects of the Demandbase Data ecosystem and move products from R&D into production scale
Design and build data pipelines to create the next generation of Demandbase's Unified Data Platform
Work across the data stack to build and productionalize data pipelines for massive amounts of data
Build DAGs in Airflow for orchestration and monitoring of data pipelines
What we're looking for:
Four-year degree in Computer Science, or related field OR equivalent experience
Progressive experience in all of the following areas:
Designing frameworks and writing efficient data pipelines, including batches and real time streams
Understanding of data strategies, articulate data analysis & data model design and evolve data products according to business requirements.
Experience with the Spark Ecosystem (YARN, Executors, Livy, etc)
Experience in large scale data streaming, particularly Kafka or similar technologies (Pulsar, Kinesis, etc)
Experience with data orchestration frameworks, particularly Airflow or similar
Experience with columnar data stores, particularly Parquet and Clickhouse
Strong SDLC principles (CI/CD, Unit Testing, git, etc)
General understanding of AWS EMR, EC2, S3
Even better if you have….
Experience with
""Lakehouse"" technologies, particularly Iceberg or similar (DeltaLake, Hudi, etc)
Terraform for AWS
Astronomer
Open Source contribution experience
Benefits:
Our benefits include options for up to 100% paid Medical and Vision premiums for employees, flexible PTO policy, no internal meeting Fridays, Modern Health mental wellness platform, and 11 paid holidays and 2 additional weeks where all Demandbase employees take off (the week of July 4th and the week of Thanksgiving). Plus 401(k), short-term/long-term disability, life insurance, and all those good things.
Our Commitment to Diversity, Equity, and Inclusion at Demandbase
At Demandbase, we believe in creating a workplace culture that values and celebrates diversity in all its forms. We recognize that everyone brings unique experiences, perspectives, and identities to the table, and we are committed to building a community where everyone feels valued, respected, and supported. Discrimination of any kind is not tolerated, and we strive to ensure that every individual has an equal opportunity to succeed and grow, regardless of their gender identity, sexual orientation, disability, race, ethnicity, background, marital status, genetic information, education level, veteran status, national origin, or any other protected status. We do not automatically disqualify applicants with criminal records and will consider each applicant on a case-by-case basis.
We also understand that women often face unique challenges pursuing careers in tech, and we believe that diversity drives innovation and growth. That's why we encourage talented women to apply for roles at Demandbase, even if they don't have all of the required skills listed in the job description.
We acknowledge that true diversity and inclusion require ongoing effort, and we are committed to doing the work required to make our workplace a safe and equitable space for all. Join us in building a community where we can learn from each other, celebrate our differences, and work together.

Personal information that you submit will be used by Demandbase for recruiting and other business purposes. Our Privacy Policy explains how we collect and use personal information."
629,Senior Data Engineer,Glue,"Seattle, WA•Remote","$165,000 - $200,000 a year","About Glue
Glue is on a mission to make people as happy as they can possibly be at work. To help us get there, we're adding even more of the brightest minds to our team; could that be you?
Glue builds technology empowering distributed teams to stick together. Through an AI-powered connection algorithm HR and People leaders understand and act on the relationships within their organizations. Glue's platform also utilizes Meetups, Events, and Pulse surveys to build connections that further drive retention. Glue is backed by leading investors including Greylock, and Founder's Coop.
Culture is at the heart of what we do—come join a dynamic and innovative team that's bringing wonder to work!
Job Overview: We are seeking a highly motivated and experienced Senior Data Engineer to join our team. As a Senior Data Engineer, you will be responsible for building and maintaining our data infrastructure, designing and implementing scalable data pipelines, and providing guidance to junior engineers. You will work closely with our data scientists and business analysts to ensure that our data is accurate, reliable, and easily accessible.
Responsibilities:
Design and implement scalable, reliable, and efficient data pipelines to support data ingestion, processing, and delivery
Build and maintain data infrastructure, including data warehouses, databases, and ETL processes
Collaborate with cross-functional teams to ensure that data solutions are aligned with business requirements
Ensure data accuracy, completeness, and consistency by implementing appropriate data quality checks and data governance processes
Provide technical guidance and mentorship to engineers, data engineers, machine learning engineers, and data scientists
Continuously research and evaluate new technologies, tools, and methodologies to improve our data infrastructure and processes
Develop and maintain documentation for data infrastructure, processes, and solutions
Qualifications:
Minimum of 5 years of experience in data engineering, with a proven track record of designing and implementing large-scale data pipelines and data solutions
Experience building and maintaining offline and online machine learning, analytics, and production infrastructure
Strong expertise in data warehousing, ETL, SQL, and technologies (such as Snowflake, PostgreSQL, Airflow, Fivetran, dbt)
Experience with cloud computing platforms and infrastructure-as-code (such as AWS, Docker, Terraform, Kafka, CI/CD)
Experience with event streaming platforms (such as Kafka, Kinesis)
Strong programming and software engineering skills
Excellent communication, collaboration, and problem-solving skills
Strong attention to detail and ability to work as a tech lead and independently in a fast-paced environment
Compensation:
The salary for this role will be between $165,000 and $200,000. In addition to salary this role will include equity in the form of stock options.
Benefits
Glue is proud to provide the following benefits to our employees:
Health, Vision, Dental coverage included
Unlimited PTO with a minimum of 2 weeks a year
Company sponsored 401k
Learning and development Stipend
Home office stipend
Bi-annual all company offsite
Monthly team events
12 weeks paid parental leave"
630,Data Engineer or Staff Data Engineer,N,United States•Remote,N,"ABOUT US

At Vida, we help people get better — and we’re helping the healthcare system get better, too.

Vida provides expert, personalized, on-demand health coaching and programs through a network of experienced health care providers — like dietitians, therapists, and health coaches and leading medical institutions — coupled with an easy-to-use app with award-winning content.

We focus on chronic conditions — like diabetes, depression, and hypertension — which account for 80% of the $3 trillion spent on healthcare in the US.

By combining advanced technology with the top-notch healthcare providers, Vida is breaking down the barriers that have historically kept people from getting the best care. Vida’s cloud-based platform captures real-time data from 100+ devices and apps and delivers AI-driven insights back to employers, health plans, and providers to improve care. We are trusted by Fortune 1000 companies, major national payers, and large providers to enable their employees to live their healthiest lives.

**Vida is authorized to do business in many, but not all, states. If you are not located in or able to work from a state where Vida is registered, you will not be eligible for employment. Please speak with your recruiter to learn more about where Vida is registered.

ABOUT THE ROLE

We are considering candidates ranging from mid to senior level.

We are looking for a Data Engineer who can help us build, manage, and optimize Vida Health’s data pipelines within our Google Cloud Platform (GCP) environment. From measuring Vida’s impact on healthcare costs to correlating member activities with successful health outcomes, you’ll have a substantial impact on projects whose results can lead to changes in product design and member experience. Our data strategy is Polychronic by Design, meaning we leverage population data insights to help members with multiple interrelated chronic conditions throughout their lifetime.

Because Vida is a startup, you’ll have the opportunity to work on a diverse set of projects that involve multiple parts of our data infrastructure. One of your responsibilities will be to connect all our sources of data such as claims data and app data while making the data structured and accessible for analysis and other ELT pipelines. You’ll also work on optimizing our ELT pipelines, including our ML pipelines which deliver recommendations and predictions that influence the member and health provider’s experience on the platform.

You’ll have the pleasure of working with a team who is excited about providing care to people living with chronic conditions. We also have experts in health care who are an invaluable resource for learning more about the health domain. We hope that you’ll consider embarking on this journey into polychronic health care with us.
RESPONSIBILITIES
Design the foundational layer of Vida Health’s data environment to make data standardized and reusable
Manage the infrastructure of our Data Platform alongside your peers.
Manage data pipelines that collect and transform data from multiple sources to support ML models, analysis, and reportingBuild out the infrastructure to serve Machine Learning models and recommendations for our Application and business users.
Work with Data Analysts and Data Scientists to design data architectures that will improve the productivity and velocity of both teams’ workflows
Optimize the costs of data pipelines and data products
Define and meet service-level agreements (SLAs) for data pipeline processes and ML-powered APIs
Work with serving Large Language Models (LLMs) in a healthcare environment.
QUALIFICATIONS
Experience with cloud platforms such as AWS or Google Cloud
Advanced knowledge of: Data Warehouses, SQL, Python, REST APIs
Track record of standardizing data for analysis on BI tools or delivering ML applications
Experience managing enterprise data exchanges, Analytics ETL, or ML Ops.
Has a relentless focus on delivering maximum value to the end user
Has an ownership mindset and is excited about monitoring and alerting on their systems
Has at least 5+ years of relevant work experience in Data Engineering or similar roles
BONUS SKILLS
Database tools such as BigQuery, Datastore, and Firestore
Asynchronous systems such as Kafka, RabbitMQ, PubSub, and Kinesis
Workflow orchestration services such as Apache Airflow
ETL Tools such as DBT, Matillion, Snowplow, Fivetran, Stitch, and Talend
Self-service BI tools such as Datastudio, Looker, Amplitude, and Tableau
HIPAA and healthcare data such as Medical and RX claims
Management and/or team lead experience preferred
Manage infrastructure with tools like Terraform, puppet, or chef
BENEFITS
Competitive compensation with meaningful stock options
Medical, Dental, Vision, Disability and Life Insurance (We cover 100% of your premium and 80% for your dependents)
Healthcare FSA Plan
Dependent Care FSA Plans
Commuter and Parking Benefits
401K Program
Flexible PTO Policy
Paid Parental Leave
10 Paid Company Holidays
PERKS
We’re a distributed company, so you can work from most US states (We still have a HQ office in San Francisco)
Access to a Vida Health Coach and the full Vida App
New hire home office stipend
Monthly wellness benefit
Training and leadership development programs
Weekly meetups with team members across the country through our #connectandcommit program
Quarterly All Company Events
Quarterly Team Based Connection Opportunities
Significant opportunities for growth and development as the business grows

Vida is proud to be an Equal Employment Opportunity and Affirmative Action employer.

Diversity is more than a commitment at Vida—it is the foundation of what we do. All qualified applicants will receive consideration for employment without regard to race, color, ancestry, religion, gender, gender identity or expression, sexual orientation, marital status, national origin, genetics, disability, age, or Veteran status. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.

We seek to recruit, develop and retain the most talented people from a diverse candidate pool. We don’t just accept differences — we celebrate them, we support them, and we thrive on them for the benefit of our employees, our platform and those we serve. Vida is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures.

We do not accept unsolicited assistance from any headhunters or recruitment firms for any of our job openings. All resumes or profiles submitted by search firms to any employee at Vida in any form without a valid, signed search agreement in place for the specific position will be deemed the sole property of Vida. No fee will be paid in the event the candidate is hired by Vida as a result of the unsolicited referral.

#LI-remote"
631,Sr Data Engineer,BioCare,Remote,N,"BioCare

Analytics

Remote

Make 2023 your best year yet and join a growing company that included to 5000 America’s Fastest-Growing Private companies in 2022! BioCare is a distributor of Specialty Pharmaceuticals that treat rare diseases, such as hemophilia, immune system deficiency and ALS. We are looking for an Sr Data Engineer to join our growing IT team to design, build, and manage the BioCare data and analytics. If you want to make a difference in people’s lives and support our ability to get the right medication into the right hands, at the right time, apply today to be part of the BioCare family!

Why you should apply:
Low cost and flexible medical, dental & vision plans

20 days (160 hours) Paid Time Off accrued during your first year

Up to 40 hours Paid Sick Time accrued annually

401k with 5% employer match

Company paid life and disability insurance

Competitive pay

Typical duties:
Strong in data warehouse models and assist in building enhanced data warehouse from ground up with SAP S4/HANA

Develop and maintain scalable data pipelines in Azure cloud platform using data factory and through various API integrations from source applications

Architect, develop and maintain various data assets including data dimensions, TSQL objects, data scripts and data jobs to populate data models

Develop best practices for solution and tool frameworks, leveraging standard naming conventions, scripting, and coding practices to ensure consistency of data solution

Define and execute DW design and data movement standards, design reviews, pipeline CI/CD process, and data container policies to ensure high quality data management

Design and develop end-to-end data solutions by understanding business requirements by handling structured and unstructured data including ingestion, parsing, transform, auditing, logging, aggregation, modelling, and error handling

Maintain and optimize the performance of our data analytics infrastructure to ensure accurate, reliable, and timely execution of all data jobs

Understands structured and unstructured data from ERP such as SAP, CRM and other systems and develop data extract processes to meet end user reporting requirements

May designs and develops new reports and visualizations based upon defined request using power BI

Lead and manage delivery of daily, weekly, and monthly sales, finance, and transactional reports & data extracts both manual and automated.

Lead the development of data engineering components by following best practices or guidelines for repeatable tasks

Support and implement continuous improvement processes across the data operations and incorporate automation.

Demonstrate strong knowledge of entity relationship, dimensional modeling, enterprise data, and physical data models

Mentor data engineering team on various data projects, process improvement and technical innovations

May provide maintenance support, including enhancements and production support activities

Deliver quality customer service to all team members, customers, and stakeholders

Actively listen and collaborate with end users to design effective solutions that resolve issues

What we are looking for:
Bachelor’s degree in an IT or Computer Science discipline OR equivalent experience.

8+ years’ experience as a data engineer with the ability to extract data from both on premise and cloud sources using SQL components, Python, and data APIs

Extensive experience in creating ETL and pipelines using Azure Data framework, ADF, SSIS and maintaining enterprise data warehouse

Experience working with Azure data components such as Databricks, data services, ADLS, Synapse and Azure DBs

Experience in creating and managing ETL pipelines for data from enterprise ERP such as SAP S4

Experience with Power BI and SSRS/PBRS is a plus

Experience in working with source control tools such as SVN and GitHub

Experience with SAP or integrating to SAP is preferable

Certification in ADF and Synapse is preferable

Quality and Improvement focus to ensure data reliable, timely, and date accuracy

Ability to multitask and prioritize completing tasks

Possess an analytical mindset and problem-solving aptitude

Familiarity with working in Agile environment leveraging DevOps, change management process and tool like Jira

We are an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, gender, marital status, national origin, ancestry, disability, genetic information, medical condition, military/veteran status, sexual orientation, gender identity, gender expression, or any other characteristic protected by law."
632,"Data Engineer, Sr.",Health Catalyst,Remote,N,"Join one of the nation’s leading and most impactful health care performance improvement companies. Over the years, Health Catalyst has achieved and documented clinical, operational, and financial improvements for many of the nation’s leading healthcare organizations. We are also increasingly serving international markets. Our mission is to be the catalyst for massive, measurable, data-informed healthcare improvement through:
Data: integrate data in a flexible, open & scalable platform to power healthcare’s digital transformation
Analytics: deliver analytic applications & services that generate insight on how to measurably improve
Expertise: provide clinical, financial & operational experts who enable & accelerate improvement
Engagement: attract, develop and retain world-class team members by being a best place to work
Role: Data Engineer, Sr
Team: Data Quality / Data Infrastructure
Location: US Remote
Travel: <5%, US
The Data Engineer supports the Data Quality Expert Data Collections(EDC) Operations department and is responsible for working with a team of data engineers, implementation, and clients to operationalize and support to improve the EDC experience and value to clients. The position requires cross-team communication, attention to detail and the ability to develop innovative processes and approaches for EDC success.
What you'll own:
Define, drive, and help implement data quality checks based upon EDC experience to provide valid and relevant data to the clients.
Investigate and troubleshoot client support tickets, internal bugs, with the intent to improve and operationalize the implementation, data quality and success of EDCs.
Participate in code reviews that include SQL queries, IDEA and effectively communicate issues and risks.
Collaborate with other teams to provide guidance on extending the EDC standard data model.
Optimize code for maximum scalability and maintainability.
Incorporate unit testing and regression testing to ensure defect-free builds and releases.
Work with data architects and product managers to provide additional development assistance on demand.
What you bring:
BS or MS in Computer Science or equivalent professional experiencepreferred.
5+ years SQL Server and/or RDBMS experience with current technology required.
A solid understanding of data structures (e.g., SQL/XML/SGML/DTD/JSON).
Experience writing complex and efficient SQL queries and stored procedures.
Deep SQL Server working knowledge including order of operations, transactions and concurrency, file tables and security, brokering technologies, transactional replication, indexing strategies and maintenance.
Familiar with Git and branching strategies.
Azure knowledge highly desired.
Databricks knowledge preferred.
Demonstrable experience implementing enterprise-scale, high volume, high availability systems.
Demonstrated ability to deliver major critical projects.
Experience with Agile and Scrum team development environments.
Who you are:
Comfort with some ambiguity and some self-direction.
Must be well organized, accurate and detail oriented.
Excellent written and verbal communication with technical and non-technical staff.
Ability to work in complex code bases written by others.
Strong organizational, presentation, interpersonal and consultative skills a must.
Ability to manage multiple projects/tasks simultaneously.
Good judgment and decision-making skills.
Enthusiastic about sharing knowledge and experience.
Maintains a positive and results-oriented attitude.
The above statements describe the general nature and level of work being performed in this job function. They are not intended to be an exhaustive list of all duties, and indeed additional responsibilities may be assigned by Health Catalyst .
Studies show that candidates from underrepresented groups are less likely to apply for roles if they don’t have 100% of the qualifications shown in the job posting. While each of our roles have core requirements, please thoughtfully consider your skills and experience and decide if you are interested in the position. If you feel you may be a good fit for the role, even if you don’t meet all of the qualifications, we hope you will apply. If you feel you are lacking the core requirements for this position, we encourage you to continue exploring our careers page for other roles for which you may be a better fit.
At Health Catalyst, we appreciate the opportunity to benefit from the diverse backgrounds and experiences of others. Because of our deep commitment to respect every individual, Health Catalyst is an equal opportunity employer."
633,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
634,Senior Data Analytics/Engineer (Government),AT&T,"Great Falls, VA","$210,000 a year","AT&T Global Public Sector is a trusted provider of secure, IP enabled, cloud-based, network solutions and professional services to the Federal Government. We are dedicated to recruiting, developing and empowering a diverse, high-performing workforce that is passionate about what they do, committed to our shared values and dedicated to our customers’ mission.
Our National Security teams design, manage, and operate complex Enterprise Information Technology (EIT) for our Intelligence Community (IC) clients and provide solutions that support the IC’s mission. We build secure, reliable, scalable solutions that are adaptable to change and incorporate innovation. We further provide Subject Matter Expertise to our customers on the technologies, design, implementation, and operation of large commercial networks.
Position is in support of a program providing Subject Matter Expertise to National Security customers on the operations and functionality of commercial mobile networks. Specific focus is on how usage patterns might be determined for devices on such networks.
The job duties are as follows:
Inspection, analysis, and reporting on very large data sets within commercial networks focusing on determining usage patterns of devices using these networks.
Coordination and reporting on process and results with SME teammates as well as government customers.
Sources of data include Communications Detail Records, IP data flows, and device application reports.
Required Clearance:
TS/SCI with poly (#ts/sci) (#polygraph)
Required Qualifications:
This position seeks an experienced data analyst/engineer with 5+ years of experience working with very large datasets and with knowledge of telephony and or wireless/mobile communications technology.
Candidates must have experience analyzing very large datasets in cloud databases (in Snowflake using SQL) and/or structured flat-files using procedural and/or scripting languages and must be capable of reading and applying telecommunications standards (3GPP, ETSI, ITU-T, etc).
Candidates must also have strong requirements analysis skills, good technical writing and verbal communication skills, be able to work effectively with a geographically distributed team and must possess a TS/SCI clearance with poly.
Ready to join our team? Apply Today!

Our Senior Data Analytics/Engineer’s earn between $98,100 & $210,000 yearly. Not to mention all the other amazing rewards that working at AT&T offers. Individual starting salary within this range may depend on geography, experience, expertise, and education/training.
Joining our team comes with amazing perks and benefits:
Medical/Dental/Vision coverage
401(k) plan
Tuition reimbursement program
Paid Time Off and Holidays (based on date of hire, at least 23 days of vacation each year and 9 company-designated holidays)
Paid Parental Leave
Paid Caregiver Leave
Additional sick leave beyond what state and local law require may be available but is unprotected
Adoption Reimbursement
Disability Benefits (short term and long term)
Life and Accidental Death Insurance
Supplemental benefit programs: critical illness/accident hospital indemnity/group legal
Employee Assistance Programs (EAP)
Extensive employee wellness programs
Employee discounts up to 50% off on eligible AT&T mobility plans and accessories, AT&T internet (and fiber where available) and AT&T phone"
635,Principal Data Engineer (294),Amplify Consulting Partners,Hybrid remote,"$120,000 - $180,000 a year","ABOUT THE COMPANY
Amplify Consulting Partners is a data-first consulting company trusted by Fortune 500 businesses to deliver high-impact professional services across the technology ecosystem—from data engineering and visual analytics to data-driven marketing and program management.
We build and empower high-performing people by promoting growth and connection across our company culture. We don't just hang core values on the wall, we make every decision with them in mind—developing trusted, long-term relationships on a foundation of transparency and accountability.
DEI STATEMENT
We hold ourselves accountable for creating an authentic workplace where every person feels heard and experiences a sense of belonging.
We believe that organizations can be an instrument for positive human impact when they champion a diverse, inclusive, and equitable environment. We do this at Amplify by enacting programs and policies that promote DEI—and with humility, if we miss the mark, we rigorously amend our practices to better achieve our targeted outcomes.

Simply put, we turn our words into action.

ABOUT THE POSITION
Amplify is looking for a Sr. Data Engineer to join our Analytics team. In this role, you will be responsible for building API scrips between multiple data-sources and using Alteryx to curate and transfer to SQL server. Knowledge of Snowflake or Azure is a bonus. Any given project will involve a mix of technical and non-technical stakeholders, so you’ll also communicate and document your work for a range of audiences. You’ll allocate most of your time and energy on hands-on client work. However, if your ambitions include scoping projects, honing technical best practices, or developing new business, then we’ll ensure you have opportunities to test the waters. The successful candidate will bring an attention to detail along with modern engineering practices and help us build a high quality, modern data architecture at scale.

What makes you a great fit?
You MUST be located in the Seattle or Kansas City area and open to having a hybrid work schedule
You’re an all-around data geek who finds satisfaction in solving data problems and creating orderly systems.
You’ve spent years writing complex SQL queries on a daily basis and have mastered the art of turning plain-English business requirements into SQL.
You’ve built production data models from scratch following (or inspired by) a star schema.
You have at least a working knowledge of Alteryx, Rest APIs, SQL Server, and are eager to dive deeper into additional technologies like Snowflake.
You aren’t necessarily a visualization expert, but you’ve spent enough time in reporting tools (like Power BI, Tableau, or Looker) to understand how they interact with data models.
Your communication is clear and concise through every medium.

Please note that we unfortunately cannot provide sponsorship at this time.
SALARY AND BENEFIT HIGHLIGHTS
At Amplify we take a holistic approach to total rewards in order to invest in the satisfaction and success of our employees both now and in the future. We consider the whole person and want to support our employees in living full lives both personally and professionally. The following is an overview of what you’ll get as a member of our team.

Salary Range: We are committed to equitable and fair pay practices. For this role, the base salary pay range is $120,000 - $180,000 a year. We consider a variety of factors when making compensation decisions and generally do not hire employees at the highest point of the pay range. We share this to ensure transparency, expectation-setting and education for candidates considering opportunities with us.
50% remote work option
Flexible time off (time to recharge when you need it!)
11 observed holidays
Medical/dental/vision – employee is covered at 100%, dependents are subsidized
Parental leave, short-term disability, long-term disability, and life insurance options
401k program and Amplify matching up to 3%
‘Amplify You’ program – $1,000 annually for your own development or investment in well-being after one year
Student loan payback program
Mentorship and training opportunities
Business and employee referral bonus opportunities
OUR HIRING PRACTICES
At Amplify, all qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We are committed to creating a diverse and welcoming workplace that includes all employees with diverse backgrounds and experiences. We believe it enables us to better meet our mission and values while serving clients throughout our communities. People of color, women, LGBTQIA+, veterans, and persons with disabilities are encouraged to apply. Qualified applicants with criminal histories will be considered for employment in a manner consistent with all federal state and local ordinances. Amplify is committed to offering reasonable accommodation to job applicants with disabilities. If you need assistance or accommodation due to a disability, please contact us at HR@amplifycp.com."
636,Data Engineer,Aroma360,"Miami, FL","$90,000 - $100,000 a year","Aroma360 is a boutique Scent Marketing and Branding company that specializes in providing the highest-quality essential oil-based scenting solutions to businesses and individuals all around the world. As the only full-service scent company, Aroma360 guides clients from concept and development to strategic implementation while priding themselves on exceptional customer service every step of the way. Aroma360 is continuously raising the bar for healthy scenting solutions in the industry!
We are looking for a talented and passionate IT professional with a broad range of experience and specialized skills in database management and data analytics to join our IT and software development team. Here's what you can expect from this role:
Responsibilities:
Managing databases and data warehouses
Extracting, standardizing, and joining data from a variety of software platforms, including Salesforce CRM and SQL databases
Designing and maintaining ETL scripts
Working with stakeholders throughout the company to analyze requirements for reporting and business analytics
Constructing reports and dashboards to convey relevant business metrics
Contributing to process automation projects
Assisting IT team with overflow support requests
Desired Skills:
Experience with any or all of the following languages: Python, JavaScript (inc. React, Angular, and Node.js), PHP, Apex
Strong ability to model business logic and rapidly develop feasible technical solutions in a changing business environment
Knowledge of Unix/Linux shell scripting
Familiarity with Grafana or equivalent data visualization tools
Strong knowledge of SQL; hands-on experience with PostgreSQL is a plus
ETL processes and reporting automation solutions; Talend experience is a plus
Jira or other Kanban-style project management tools
Familiarity with MS Excel or comparable spreadsheets
Experience with ViciDial-based contact center platforms is a plus
Experience with Salesforce and/or NetSuite is a plus
Experience with Google Cloud Platform is a plus
Desired Qualities:
Trustworthiness and dependability
Orientation toward quality and results
Willingness to take initiative
Strong expertise in data, project, and time management
Strategic and creative problem solving
Clear and effective communication skills
Consistent accuracy and attention to detail
Comprehensive technical and business knowledge
Desired Education:
A bachelor's degree in Computer Science, Data Science, Information Systems, or an equivalent field of study is preferred.
Joining our team comes with a range of exciting benefits to support your health, well-being, and professional growth, including:
Comprehensive health coverage, including dental and vision insurance, to ensure you and your family are taken care of.
Life insurance provides peace of mind for you and your loved ones.
Paid time off, allowing you to recharge and enjoy life outside of work.
Access to a 401(k) plan to help you plan for a secure financial future.
Employee discount to take advantage of great deals on our products and services.
Opportunities for paid training to develop your skills and advance your career.
Fun and exciting company events.

Our organization is an equal opportunity employer and does not discriminate against any candidate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status, or any other legally protected characteristics. We are committed to promoting diversity, equity, and inclusion in our workplace and welcome candidates from all backgrounds to apply for any open positions."
637,Data Engineer,Glow Networks,"Dallas, TX 75252",$73 an hour,"Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice."
638,Data Engineer,Airbus Americas,"Atlanta, GA 30308",N,"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.

Position Summary:
We are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.

To ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.

Primary Responsibilities:
Write code to transform real-world data into high-signal models.
Loading disparate data sets and conducting pre-processing transformation services.
Extract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.
Collaborate with the software development teams.
Maintaining production systems.
Training staff on data resource management.

Qualified Experience / Skills / Training:
Bachelor's degree in computer engineering or computer science.
Previous experience as a big data engineer.
2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.
Prior experience working with Apache Spark is also desirable.
Good project management skills.
Good communication skills.
Ability to adapt to a dynamic work environment by consistently revising your approach in response to new information.

Education:
Bachelor's Degree or equivalent work experience

Experience:
Experience with Palantir big data platform.
Data Science expertise.

Eligibility:
Authorized to Work in the US
Clearance:
None

Equal Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
As a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice."
639,Data Engineer,Impact Advisors LLC,United States,N,"Healthcare Data Engineer

Work You’ll Do:

As a Healthcare Data Engineer, you will work closely with a multidisciplinary Agile team to build high-quality data pipelines driving analytic solutions. Utilizing your deep understanding of data architecture, data engineering, data analysis, reporting, and basic understanding of data science, the solutions you create will generate insights from the organization’s connected data which will enable the advancement of data-driven decision-making capabilities within the enterprise. You will utilize your strong problem-solving skills, ability to work as part of a technical, cross-functional analytics team, and desire to solve complex data problems to deliver the insights which enable analytics strategies.

About Impact Advisors:

We deliver Best in KLAS advisory, implementation and optimization services to healthcare organizations. At Impact Advisors, we are committed to exceeding our clients’ expectations. We are a nationally recognized partner to many of the nation’s top healthcare organizations. Our commitment to patient-centered, value-driven outcomes has earned us some of the industry’s most prestigious awards. Please visit our website at www.impact-advisors.com for additional information.

Your Responsibilities:
Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals
Solve complex data problems to deliver insights that help business to achieve goals
Create data products for analytics and data scientist team members to improve their productivity
Advise, consult, mentor and coach other data & analytics professionals on data standards and practices
Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions
Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering
Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes
Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Learn about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics

Your Expertise:
Bachelor’s degree preferred; Computer Science, MIS, or Engineering preferred
Certification in one or more of the following Epic Systems modules: Cogito, Caboodle, Clarity, Reporting Workbench
5 years of experience working in data engineering or architecture role, 7+ preferred (3 years preferred for Jr. role)
Expertise in SQL and data analysis and experience with at least one programming language (Python preferred)
Significant experience developing and maintaining data warehouses in big data solutions (e.g., Snowflake, SAP Hana, Oracle, SQL Server, Teradata, etc.)
Experience with developing solutions on cloud computing services and infrastructure in the data and analytics space (preferred)
Database development experience using Hadoop or BigQuery and experience with a variety of relational, NoSQL, and cloud database technologies
Worked with BI tools such as Tableau, Power BI, Looker
Deep knowledge of data and analytics, such as dimensional modeling, ETL, reporting tools, data governance, data warehousing, structured and unstructured data.
Big Data Development experience using Hive, Impala, Spark and familiarity with Kafka
Exposure to machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics
Experience in using data base connections, SSIS, API, ODBC, etc.
Healthcare experience preferred but not required.

Our People and Culture:

We believe in a caring, fun, honest and autonomous work environment and we recognize that our dedication to our associates drives our success. Our mission to create a Positive Impact fuels our associates to innovate and deliver high value services to our clients.

In healthcare, many of the greatest ideas and discoveries come from a diverse mix of minds, backgrounds and experiences, and we are committed to cultivating an inclusive work environment. Impact Advisors provides equal opportunities to all employees and applicants for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, genetic disposition, neurodiversity, disability, veteran status, or any other protected category under federal, state and local law"
640,Data Engineer,Massachusetts General Hospital(MGH),"149 13th St, Charlestown, MA 02129",N,"Data Engineer
- (3227183)

GENERAL SUMMARY/ OVERVIEW STATEMENT:
The Data Engineer will be part of a diverse interdisciplinary team of computer and neuroscientists with broad expertise spanning computer science, neuroscience, psychology, psychiatry, neuropsychology, cognitive neuroscience, neuroimaging, bioinformatics, biostatistics, epidemiology, and neurophysiology. The project will involve data processing and coordination for new multisite data collection networks, as well as develop and apply stratification tools to identify and validate biomarkers to predict outcome trajectories in individuals at high risk to develop psychosis.
This position joins the data management, processing and archiving team. The successful candidate will help develop, deploy and maintain the bioinformatics infrastructure of several large projects. Our group (spanning MGH and BWH) is also actively developing new technologies to characterize brain structure and function, which has led to the design of state-of-the-art image analysis pipelines capable of robustly processing hundreds of neuroimaging datasets. The role of the engineer will be to maintain and enhance existing image processing pipelines and to develop new pipelines based on the latest research with an emphasis on version tracking, data provenance, and high performance computing.
PRINCIPAL DUTIES AND RESPONSIBILITIES:
Relevant activities include, but are not limited to the following:
Maintain and enhance existing image processing pipelines.
Design new image processing pipelines, with an emphasis on version tracking, data provenance, and high performance computing.
Develop neuroinformatics tools to track data provenance and project management.
Test and evaluate a range of neuroimaging packages to determine their suitability for research goals.
Regular, direct interaction with neuroscientists from within and outside the DPACC to assist them with neuroimaging data analysis using a range of methods including FSL, SPM, Slicer and other specialized tools.
Design, implement, test, maintain and support applications to capture, manage, archive and monitor multi-site, multi-modal study data. Applications may include but are not limited to study monitoring systems, data management systems, workflow execution and monitoring systems, interactive viewers, and reporting tools.
Support web and application server configuration and deployment.
Support data engineering efforts, including database and API design, data extraction/transformation/load, and data aggregation/integration.
Containerize and deploy software and workflows on local high performance computing platforms and cloud computing infrastructure (AWS).

Required:
Bachelor’s Degree in Computer Science, Mathematics, Physical Sciences, Engineering, or related field
Excellent programming skills in Python, Bash, MATLAB
Superior Linux/Unix skills and comfort with command line programs – the ability to get new programs and packages running, overcoming hurdles as they arise, is particularly helpful.
Familiarity with standard software evolution method—version controlling (Git), pull requests, code reviews, issue and release management
Ability to work in an interdisciplinary, diverse, and international team in a highly collaborative and intellectually challenging environment.
Excellent oral and written communication skills
Basic knowledge of neuroscience and neuroanatomy.
Understanding of structural, diffusion, and functional Magnetic Resonance Imaging.
Preferred:
Master’s Degree in Computer Science, Mathematics, Physical Sciences, Engineering, or related field
Familiarity with C/C++ programming
Experience in neuroimaging software FSL, FreeSurfer, Slicer, DIPY, Nipype, Neurodocker, REDCap, XNAT
Experience with database management systems (e.g., SQL, PostgreSQL, MongoDB, CouchDB)
Experience with Linux container engines (e.g., Docker, rkt) and container orchestration systems (e.g., Kubernetes)
Experience with JavaScript libraries for interactive data visualization (e.g. d3, Recharts, Charts.js).
Experience with at least one web framework for building single-page web applications (e.g., React, Angular, Vue)
EEO Statement

Massachusetts General Hospital is an Equal Opportunity Employer. By embracing diverse skills, perspectives and ideas, we choose to lead. Applications from protected veterans and individuals with disabilities are strongly encouraged.
Primary Location MA-Charlestown-MGH 13th Street
Other Locations MA-Charlestown
Work Locations MGH 13th Street 149 13th Street Charlestown 02129
Job IT/Health IT/Informatics-Engineer
Organization Massachusetts General Hospital(MGH)
Schedule Full-time
Standard Hours 40
Shift Day Job
Employee Status Regular
Recruiting Department MGH Psychiatry
Job Posting Jan 3, 2023"
641,Principal Data Platform Engineer (REMOTE),GEICO,"Chevy Chase, MD 20815•Remote","$100,000 - $236,500 a year","Under general supervision, leads engineering staff with development of data software applications. As a subject matter expert, mentors and leads engineering teams and team members in software delivery within data in an agile environment.
Essential Functions:
1. Provides leadership to engineering teams utilizing data and related technologies.
2. Owns complete solution across entire life cycle while utilizing strong problem-solving ability.
3. Influences and builds vision with product managers, team members, customers, and other engineering teams to solve complex problems for building enterprise class business applications.
4. Holds accountability for the quality, usability, and performance of the solutions.
5. Leads design sessions and code reviews to elevate the quality of engineering across the organization.
6. Utilizes programming languages / databases (.net, sql, nosql), container orchestration services (docker and kubernetes), and a variety of azure tools and services across the software development lifecycle.
7. Executes software delivery utilizing an agile environment (scrum/kanban/safe) for continuous delivery, infrastructure as code, powershell scripting, operation portals (e.g., azure portal) and monitoring portals (e.g., splunk and application insights).
8. Mentors junior team members in data and related technology.
9. Shares best practices and improves processes within and across teams.
10. Adheres to GEICO code of conduct, company policies, and operating principles.
11. Meets attendance standard at business location to perform necessary job functions and to facilitate interaction with management and co-workers.
MEETS the requirements specified below.
Must be able to, with or without accommodation, perform the essential functions which include, but are not limited to seeing, hearing, typing and speaking.
Must be able to concentrate and demonstrate a capacity for learning technical concepts and adapting to new technologies quickly.
Must be able to use a PC.
Must be able to follow complex instructions, resolve conflicts or facilitate conflict resolution, and have strong organization/priority setting skills.
Must be able to multi-task.
Must be able to learn and apply large amounts of technical and procedural information and follow processes that have been published.
Must be knowledgeable of software coding and following standards and processes that have been published and the guidelines for the design.
Must be able to provide system training to team members as needed.
Must be able to communicate in a clear, concise, professional oral or written manner, to be understood by customers, clients, co-workers and other employees of the organization.
Must be able to perform under pressure and stressful situations.
May be required to be on call for production support 24x7.
Must be able to concentrate and demonstrate a capacity for learning technical concepts and adapting to new technologies quickly.
Must have the following experience:
o Bachelor’s Degree in Computer Science (or related field)
o 5+ years of experience in data software development, using data technologies such as Relational & NoSQL databases, open data formats, and programming languages such as Python, Scala, and/or other frameworks, building data pipelines (ETL and ELT) with batch or streaming ingestion, loading and transforming data, and developing with big data technologies such as Spark, Hadoop, and MapReduce
o 3+ years of experience designing and building for data quality assurance, reliability, availability and scalability, on existing and new data applications
o 3+ years of experience Cloud DevOps Concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework
o 2+ years of experience in designing and building solutions for data security, data quality and observability, metadata management, data lineage, and data discovery
o 2+ years of experience building data software in microservices-oriented architecture and extensible REST APIs
o 2+ years of experience in open source frameworks
Desired Skills
Demonstrated ability to independently architect, design & develop data ingestion pipelines using traditional, big data or and cloud technologies
Highly Proficient in coding using Java, Python, Scala, R with 5 + years of experience in any combination
Demonstrated ability in developing tools/functionality for data pipelines using Python, Java etc.
Solid experience in Kafka, Hadoop, Spark etc.
2 + years’ of Fivetran, HVR or related administration experience
Designed solutions using Azure technologies like Azure Synapse, Azure stream analytics, ADF, ADLS 2, Azure Databricks, Snowflake
Benefits:
As a full time associate, you’ll enjoy our Total Rewards Program* to help secure your financial future and preserve your health and well-being, including:
Premier Medical, Dental and Vision Insurance with no waiting period**
Paid Vacation, Sick and Parental Leave
401(k) Plan with Profit Sharing
Tuition Assistance including Direct Billing and Reimbursement payment plan options
Paid Training, Licensures, and Certificates
Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
**Coverage begins with the pay period after hire date. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
GEICO is proud to be an equal opportunity employer. We are committed to cultivating an environment where equal employment opportunities are available to all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO celebrates diversity and believes it is critical to our success. As such, we are committed to recruit, develop and retain the most talented individuals to join our team
#LI-AP1
Annual Salary
$100,000.00 - $236,500.00
The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations."
642,Software Engineer (Ctrl Laws &Air Data)– Early Career,Sikorsky Aircraft Corporation,"4800 Overton Plz, Fort Worth, TX 76109",N,"At Lockheed Martin, we apply our passion for purposeful innovation to keep people safe and solve the world's most complex challenges.

Mission-Focused Innovation: From aerospace to outer space to cyber space, you can solve the world's most complex challenges for our customers.

Foundational Values: Our culture of performance excellence, ethics, teamwork and inclusion is embedded in everything we do.

Diverse Career Opportunities with Meaningful Work: Grow your career and skills for life. Our technology-driven learning platforms and programs enable your development and agility.

Your Health, Your Wealth, Your Life: Our flexible schedules, competitive pay and comprehensive benefits enable you to live a healthy, fulfilling life at and outside of work.

Empowered to Be Your Best: Use your strengths to make a difference in the lives of one another, our customers, our communities and our planet.

Commitment to Sustainability: We foster innovation, integrity and security to preserve the environment, strengthen diverse communities and propel growth.

Here, the possibilities are endless because we offer:
Flexible Schedules, dependent on role
Levels: Student, Entry, Mid, Senior, Management
Locations: Nationwide & OCONUS Positions

Qualifications

See what it takes to be a successful Lockheed Martin employee.

Collaborative

There is no innovation without imagination. You bring new, different and exciting ideas to the table every day. You are committed to the success of your team.

Curious

The ability to explore and drive new ideas. You think innovatively to come up with creative solutions to complex challenges.

Goal-oriented

From planning and organizing to decision making, goal orientation produces impactful results. You set goals and strive to make things happen quickly and efficiently.

Proactive

The ability to anticipate, plan and prepare for what lies ahead. You act rather than react to potential situations.

Problem Solver

You work to tackle tough problems with complex solutions.

Strategic

You observe, reflect and analyze processes to make more informed decisions.

Quote

“Before I came to Lockheed Martin, I never thought I’d get these opportunities. The benefits available and the company’s value culture make it an incredibly supportive place to work, and I’m grateful to work somewhere that wants me to succeed. I hope to have a long career here making an impact, and I know I’ll be supported in doing that every step of the way.”

Ryan J., Enterprise Operations
Video length: 91 seconds
Benefits
HEALTHCARE

Medical, Dental and Vision coverage is available for employees who opt-in.

401(K)/RETIREMENT PLANS

Our 401(k) plan features generous matching and company contributions.

PROFESSIONAL DEVELOPMENT

We support our employees through mentoring, internal & external educational programs, networking, skills enhancement and career-building programs.

MATERNITY AND PATERNITY LEAVE

Our generous parental leave policies support your journey into parenthood. When you return to work, our facilities offer mother's rooms to support your transition and work/life balance.

PAID TIME OFF

We offer PTO, paid holidays and paid time off for jury duty and military obligations.

FLEX-TIME

Depending on the position, we offer flexible work schedules.

Job Responsibilities

Software Engineer (Ctrl Laws &Air Data)– Early Career

Fort Worth, Texas

JOB ID: 624590BR
Date posted: Apr. 10, 2023
Locations: Fort Worth, Texas

Description: By bringing together people that use their passion for purposeful innovation, at Lockheed Martin we keep people safe and solve the world's most complex challenges. Our people are some of the greatest minds in the industry and truly make Lockheed Martin a great place to work. With our employees as our priority, we provide diverse career opportunities designed to propel development and boost agility. Our flexible schedules, competitive pay, and comprehensive benefits enable our employees to live a healthy, fulfilling life at and outside of work. At Lockheed Martin, we place an emphasis on empowering our employees by fostering innovation, integrity, and exemplifying the epitome of corporate responsibility. Your Mission is Ours.

Lockheed Martin Aeronautics in Fort Worth, Texas is seeking a full-time Early Career Software Engineer for Control Laws and Air Data. In this role, you will Develop and maintain safety critical control law software in accordance with applicable software development processes. Ensure control law software meets dynamic performance and stability requirements. Perform flying qualities analysis and ensure that control laws satisfy flying qualities requirements for required flight conditions, aircraft loadings and environmental conditions. The successful candidate will have experience and/or knowledge of aeronautical engineering. Must be a US Citizen; this position will require a government security clearance. This position is located at a facility that requires special access.

Basic Qualifications:
Bachelor's degree in Aeronautical/Aerospace Engineering.
Must be a US Citizen; this position will require a government security clearance. This position is located at a facility that requires special access.

Desired Skills:
Highly motivated individual with the ability to learn continuously and the ability to identify multiple avenues to successfully reach team goals.
Candidate must work well individually and with team members across multiple organizations.
Strong written and oral communication skills.
ability to develop clear technical presentations for customers and suppliers.
Microsoft Office Skills - of particular importance: Outlook, PowerPoint, Excel, Word, and Project.
Security Clearance Statement: This position requires a government security clearance, you must be a US Citizen for consideration.
Clearance Level: Secret
Other Important Information You Should Know
Expression of Interest: By applying to this job, you are expressing interest in this position and could be considered for other career opportunities where similar skills and requirements have been identified as a match. Should this match be identified you may be contacted for this and future openings.
Ability to Work Remotely: Onsite Full-time: The work associated with this position will be performed onsite at a designated Lockheed Martin facility.
Work Schedules: Lockheed Martin supports a variety of alternate work schedules that provide additional flexibility to our employees. Schedules range from standard 40 hours over a five day work week while others may be condensed. These condensed schedules provide employees with additional time away from the office and are in addition to our Paid Time off benefits.
Schedule for this Position: 4x10 hour day, 3 days off per week
Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Join us at Lockheed Martin, where your mission is ours. Our customers tackle the hardest missions. Those that demand extraordinary amounts of courage, resilience and precision. They’re dangerous. Critical. Sometimes they even provide an opportunity to change the world and save lives. Those are the missions we care about.

As a leading technology innovation company, Lockheed Martin’s vast team works with partners around the world to bring proven performance to our customers’ toughest challenges. Lockheed Martin has employees based in many states throughout the U.S., and Internationally, with business locations in many nations and territories.
Experience Level: 4 yr and up College
Business Unit: AERONAUTICS COMPANY
Relocation Available: Yes
Career Area: Software Engineering
Type: Full-Time
Shift: First"
643,Project Engineer/Data Science,"Hottinger, Bruel & Kjaer","Southfield, MI",N,"This job will provide you with an opportunity to further your career alongside some of the best and most passionate technology experts from around the world in a leading company within the test, measurement and data analytics industry. You will be a strong contributor collaborating closely with colleagues from various business functions.
At HBK, we live up to our three values: Be True, Own It and Aim High. We believe in absolute integrity – it’s how we win for stakeholders, the environment and each other. We believe in teamwork and keeping our promises – to ourselves and others. Finally, we believe in being bold and positive. This is how we perform at our best and achieve greater success.
The position
Hottinger Bruel & Kjaer Solutions LLC is looking for a high caliber individual with a passion for solving engineering problems using engineering principles, data analytics and computer programming. This individual should possess solid engineering and data analytics skill sets. In this position, you will be working with a highly energetic team of engineers, researchers, developers, and sales teams to develop and deploy solutions focused on improving reliability, durability and availability of our customer assets (vehicles, aircrafts, machines, structures, etc.), delivering actionable information and providing solutions to customer problems. If you are eager to demonstrate your potential, solve problems for world-class customers and work with a team of subject matter experts, software developers, engineers and data scientists we would like to hear from you.
Primary responsibilities
You will be responsible for:
Analyze asset data (usage, failures, maintenance, test, cost, etc.) using engineering, mathematics, statistics, machine learning (ML), etc. for design, operations and maintenance
Apply methodologies associated with reliability and/or durability to solve customer problems
Collect and prepare data sets, including data processing, cleansing, verifying data integrity, and combining data from multiple sources
Research, develop and apply advanced analytics/algorithms using common open source tools
Interpret results to draw conclusions and provide recommendations
Prepare reports/presentations for project deliverables and internal documentation
Other duties as needed
Professional qualifications
US Citizen or Green card holder (Requirement - no exceptions)
Advanced Degree in Engineering
Experience with engineering concepts and data preparation
Excellent analytical, problem solving, and communication and collaboration skills
Experience with computer programming, including python, in developing analytical solutions
Working knowledge of ML concepts, including data preparation and visualization, with experience in ML model development (including feature engineering, training and prediction), deployment and management
Desire to continuously learn ML core concepts, application, and deployment skills as this technology evolves
We offer
The job will provide you with an opportunity to further your career alongside some of the best and most passionate technology experts from around the world in a leading company within the test, measurement and data analytics industry. You will be a strong contributor collaborating closely with colleagues from various business functions.
At HBK, we live up to our three values: Be True, Own It and Aim High. We believe in absolute integrity – it’s how we win for stakeholders, the environment and each other. We believe in teamwork and keeping our promises – to ourselves and others. Finally, we believe in being bold and positive. This is how we perform at our best and achieve greater success.
One company – HBK
Hottinger Brüel & Kjaer (HBK) is a global leader in the fields of sensors, data acquisition, analytics and collaboration for various R&D, production and in-operation applications.
Until the end of July 2020, the companies were known as Hottinger Baldwin Messtechnik GmbH (HBM) and Brüel & Kjær Sound & Vibration Measurement A/S respectively.
HBK is a subsidiary of Spectris plc and employs around 3000 people worldwide. Our product eco system covers all layers from sensors, electronics, to software and collaboration. Our customers range from end users of the entire tool chain focusing on analytics and results in virtual testing, physical testing, and monitoring, to our OEM and system integrator partners and customers integrating our products into their own offering and solution. The product portfolio is as versatile and varied as our customer base covering many industries.
We have engineering and production facilities in Germany, Denmark, UK, Portugal, USA and China and are represented in over 80 countries worldwide. We are proud to be one of the top three suppliers worldwide in our market segments served, thanks to our high-quality products and the commitment of our employees.
Application deadline
Please submit your application and CV by using the direct application link.
Please note that we will be conducting interviews on an ongoing basis."
644,Data Engineer I - (Remote),Help At Home,Remote,N,"Help at Home is the leading national provider of in-home personal care services, where our mission is to enable individuals to live with independence and dignity at home. Our team supports 66,000 clients monthly with the help of 49,000 compassionate caregivers across 12 states. We're looking for people who care about others, who are willing to listen, lean in and make impactful change. Each role at Help at Home can have a positive impact in supporting our caregivers and clients. If you are someone who leads with passion and integrity and are looking to join a rapidly growing, industry leading team, Help at Home may be a good fit for you.
Job Summary:

The Data Engineer is responsible for delivering data warehouse solutions by building enterprise data models and writing ETL/ELT processes to map, cleanse and standardize multiple source systems of data to populate the enterprise data models for business consumption. They will be working throughout a multi-layered data warehouse environment in order to support a wide variety of business needs. This role is responsible for delivering solutions that meet our growing business needs as they relate to our enterprise data and analytics strategy for Care Coordination and Help at Home. The ideal candidate should be comfortable with driving creation of a platform with focus on the Data Mesh architecture.

As a key member of the team:
You are flexible and can embrace change
You value progress over perfection
You care about your work, the team you're on, and the people we are helping
You make it a priority to get to know the people around you - build relationships with your colleagues and business partners
You say what needs to be said, while considering how it'll affect culture and output
Hold others to a high standard

Duties/Responsibilities:
Leverages CDC to optimize the ETL/ELT processes they develop including being able to develop routines to determine changed records when they are not provided by the source system
Creates GitHub actions and builds pipelines for dev, stage, and prod
Effectively communicates with stakeholders to understand business requirements with the ability to translate requirements into technical designs and solutions and convey the requirements and designs to team members
Profiles the source system data and assess its data quality to design and develop solutions to improve the data quality such that it maps properly into the data warehouse structures and meets the data warehouse standards
Works with the business to determine survivorship rules, builds the golden record based on the rules and then builds and maintains structures for the integrated dimensions and facts; understands master data guiding principles and best practices in terms of the technical de-duplication process, which includes enhancing data quality to support matching and grouping
In alignment with Data Mesh, builds ingestion, integration and sharing patterns and frameworks for better data access
Maps source system data structures into the data warehouse data model (source-target mapping) and enhance the data warehouse data model as needed to meet the business needs
Improves our overall data security posture; strengthens our SDLC and Devops strategy in support of sustained business growth
Maintains knowledge of current trends and developments in the field and actively explore emerging technologies
Required Skills/Abilities:
Cloud-first mindset
Ability to work in a fast-paced dynamic, environment delivering solutions that significantly impact the business
Knowledge of testing frameworks and TDD or BDD
Self-starter, self-managed, quick learner, problem-solver with a positive, collaborative, and team-based attitude who is willing to support and teach fellow team members
Strong data analysis skills
Strong relational database skills including advanced SQL knowledge and the ability to create complex queries and stored procedures.
Strong understanding of data warehouse and business intelligence design principles and industry best practices, including relational and dimensional modeling and ETL/ELT methods
Understanding of trunk-based development.
Working knowledge of Snowflake
Working knowledge of Snowflake DBMS and JSON
Education and Experience:
AWS architecture, developer, security, and networking experience.
3+ Years of experience in data engineering required.
Bachelor's Degree in Computer Science, Data & Analytics, Information Management, Healthcare Informatics, Business Administration, Statistics, or related field required.
Demonstrated experience with automation.
Demonstrated experience with one or more of the following languages: Go, Python, Typescript.
Strong experience with various AWS services like (S3, Lambda, Glue, EMR, CloudFormation, MWAA, Kinesis, MSK).
Cloud (AWS), Warehousing, and Snowflake experience preferred

The above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not intended to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions upon request.

Help At Home is an Equal Employment Opportunity (EEO) employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or religion or other legally protected status.

Mid Level"
645,Data Engineer,University of Utah,"201 Presidents Circle, ROOM 201, Salt Lake City, UT 84112","$88,000 - $101,649 a year","We are looking for a self-motivated, innovative, naturally curious person interested in solving complex health care issues in new and innovative ways. This individual must utilize expertise to query, design and translate complex data analysis into simple, user-friendly mediums that are easily digestible by health system executives and front-line clinicians. The ideal person for this role will have healthcare knowledge or be willing to learn healthcare terminology. Must be comfortable working on a team and have strong communication and presentation skills. The Data Engineer will have direct reporting responsibility to the Data Science Team Lead within the University of Utah Health Medical Group Analytics Team.
As a Data Engineer, you will be responsible for using APIs to integrate with the EPIC
Electronic Medical Record (EMR) system to build applications. In addition to
this, you will also be extracting data from third-party systems for use in
analytics. In this role, you will be expected to have strong programming
skills, particularly in languages such as Python or Java, and have experience
working with APIs. The ability to work with large datasets and experience with
data modeling and SQL is also highly desired. Overall, this role will involve
using your technical skills to bridge the gap between data sources and
analytics, and will be critical to the success of the data analytics efforts
within the organization.

Responsibilities
Essential Job Functions
Queries relational databases to produce data products used by Data Scientists and Business Analysts
Serves as a SME on ETL best practices
Uses APIs extensively to integrate within EPIC (EMR system) to build apps as well as extract data from third-party systems for use in analytics
Transforms data using SQL/Python/Java into a consumable structure
Aids in the development/implementation of new tools to improve data processes
Technical Skills
Proficient with standard office productivity tools, such as MS Office
Proficient with SQL
Proficient with Python and Java
Familiarity with DBT, Databricks, SSIS, Informatica etc. are preferred but not required

Work Environment and Level of Frequency typically required
Nearly Continuously: Office environment.
Physical Requirements and Level of Frequency that may be required
Nearly Continuously: Sitting, hearing, listening, talking.
Often: Repetitive hand motion (such as typing), walking.
Seldom: Bending, reaching overhead.

Minimum Qualifications
Requires a bachelor’s degree or equivalency in a related area and 2-4 years of experience in the field or in a related area.

Applicants must demonstrate the potential ability to perform the essential functions of the job as outlined in the position description.

Preferences
We enjoy a flexible/hybrid work environment.
This position has no responsibility for providing care to patients.

Type
Benefited Staff

Special Instructions Summary

Additional Information
The University of Utah values candidates who have experience working in settings with students from diverse backgrounds and possess a strong commitment to improving access to higher education for historically underrepresented students.

Individuals from historically underrepresented groups, such as minorities, women, qualified persons with disabilities and protected veterans are encouraged to apply. Veterans’ preference is extended to qualified applicants, upon request and consistent with University policy and Utah state law. Upon request, reasonable accommodations in the application process will be provided to individuals with disabilities.

The University of Utah is an Affirmative Action/Equal Opportunity employer and does not discriminate based upon race, ethnicity, color, religion, national origin, age, disability, sex, sexual orientation, gender, gender identity, gender expression, pregnancy, pregnancy-related conditions, genetic information, or protected veteran’s status. The University does not discriminate on the basis of sex in the education program or activity that it operates, as required by Title IX and 34 CFR part 106. The requirement not to discriminate in education programs or activities extends to admission and employment. Inquiries about the application of Title IX and its regulations may be referred to the Title IX Coordinator, to the Department of Education, Office for Civil Rights, or both.

To request a reasonable accommodation for a disability or if you or someone you know has experienced discrimination or sexual misconduct including sexual harassment, you may contact the Director/Title IX Coordinator in the Office of Equal Opportunity and Affirmative Action:

Director/ Title IX Coordinator

Office of Equal Opportunity and Affirmative Action ( OEO /AA)

135 Park Building

Salt Lake City, UT 84112

801-581-8365

oeo@utah.edu

Online reports may be submitted at oeo.utah.edu

For more information: https://www.utah.edu/nondiscrimination/

To inquire about this posting, email: employment@utah.edu or call 801-581-2300.


The University is a participating employer with Utah Retirement Systems (“URS”). Eligible new hires with prior URS service, may elect to enroll in URS if they make the election before they become eligible for retirement (usually the first day of work). Contact Human Resources at (801) 581-7447 for information. Individuals who previously retired and are receiving monthly retirement benefits from URS are subject to URS’ post-retirement rules and restrictions. Please contact Utah Retirement Systems at (801) 366-7770 or (800) 695-4877 or University Human Resource Management at (801) 581-7447 if you have questions regarding the post-retirement rules.

This position may require the successful completion of a criminal background check and/or drug screen.

Posting Specific Questions
Required fields are indicated with an asterisk (*).
* Do you have a related Bachelor's degree or equivalency? (2 years related work experience may be substituted for 1 year of education)
Yes
No
Will you now or in the future require sponsorship for employment visa status (e.g., H-1B status)?
Yes
No
Applicant Documents
Required Documents
Resume
Optional Documents
Cover Letter
Appropriate discharge document (such as a DD-214 – Member Copy 4) – Veteran Only – Call 801.581.2169
Addendum to the University of Utah - Veteran Only - Call 801.581.2169 after submission

Open Date
12/21/2022

Requisition Number
PRN33318B

Job Title
Data Architect

Working Title
Data Engineer

Job Grade
G

FLSA Code
Computer Employee

Patient Sensitive Job Code?
No

Standard Hours per Week
40

Full Time or Part Time?
Full Time

Shift
Day

Work Schedule Summary

VP Area
U of U Health - Academics

Department
00728 - U of U Medical Group

Location
Anywhere Utah

City
Salt Lake City, UT

Type of Recruitment
External Posting

Pay Rate Range
$88,000 - $101,649

Close Date

Open Until Filled
No"
646,Data Engineer,Alliant Credit Union,"Chicago, IL 60666•Hybrid remote","$90,000 - $115,000 a year","Data Engineer
Location: Chicago
Hybrid model: requires a few days onsite
What will your day look like?
You will assist the organization through the continued build-up and operationalization of an enterprise class Modern Data environment, including various components within the Azure data system. Resources to do the job require substantial hands-on experience working with the technologies encompassed within the Azure data warehouse technology stack.
The Data Engineer coordinates, designs, builds, and integrates complex application technology solutions, aligned to architectural standards and definitions, and will help ensure IT services are delivered effectively and efficiently. General direction is received from the Director, Data Engineering.
Do you see yourself doing this?
Data Engineer Core Responsibilities
Responsible for day-to-day operation and support of Azure datalake and Modern Data environments
Collaborate with data center and systems engineering teams on all azure setup, software installation, testing, monitoring, tuning/optimizing, troubleshooting, maintenance
Influence adoption of Modern Data new concepts, practices, and approaches
Build the infrastructure as code required for data platform
Responsible for extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure technologies
Develops databases, objects, functions and structures for data storage, retrieval, and reporting
Designs and develops Extract, Transform and Load (ETL) solutions to populate the data warehouse
Develops automated and reusable routines for extracting requested information from database systems and/or enterprise management system
Defines, develops, tests, analyzes, and maintains software applications and back-end interfaces (if applicable) in support of business objectives
Builds report and analytic prototypes based on initial business requirements.
Build analytics tools that utilize the data pipeline to provide actionable insights, operational efficiency, and other key business performance metric
Collaborate with development and strategy teams on component and 3rd party tool identification, recommendation, installation and management of azure services
Collaborate with the data architecture and Infrastructure teams in technical investigations, development, and prototypes
Promote strong quality practices by performing unit testing and providing appropriate level of support for user acceptance testing, as needed
Act as a support resource for the technical support that platform-reported issues are being addressed in an efficient and accurate manner. Serve as a liaison with database administrators, developers, and vendors (if needed) to assist in resolving problems with systems
Maintain and test the data warehouse in compliance with the company's standard guidelines
Participate in the design and implementation of a Disaster Recovery strategy for all Modern Data components
Participate in design, implementation and management of alignment activities with all pertinent audit and compliance activities
Provide input/develop new processes/standards in support of the organization's business/functional short-term strategies, with limited impact on the business/function overall results
Work with internal and external stakeholders to assist with data-related technical issues and support data infrastructure needs
Additional Responsibilities:
Manage current data warehouse backlog and support data warehouse enhancements
Manage Tableau and BO development needs from data engineering requirements
Drive projects and general progress towards the completion of requirements and implementation
Maintain a close partnership with Core Banking software engineering managers and Core Banking principal engineers to collaborate on data integration
What makes you a great fit?
You’ll be a great fit if in addition to the completion of a Bachelor’s degree in Computer Science or a related field, required, and you have:
3+ years’ experience in Azure Data Development
Proven development and operational experience within data warehousing system
High proficiency in SQL, data integration toolsets
Expert knowledge of key data structures and algorithms
Experience with the entire Software Development Lifecycle (SDLC) process such as change management, defect and issue tracking, to resolve data issues or to implement development enhancements
Hands on experience with azure monitoring tools
Knowledge of metadata management and governance capabilities
Familiarity with Data Science notebooks
Automation experience with CI/CD pipelines for Azure DevOps
Prior experience working in Financial Services industry preferred
Project Management experience with agile and project management methodologies (Scrum and/or Kanban)
Excellent written and verbal communication skills
Has an analytical and problem-solving mindset
Highly organized and efficient
Ability to leverage strategic and tactical thinking
Works calmly under pressure and with tight deadlines
Demonstrates effective decision-making skills
Job Type: Full-time
Pay: $90,000.00 - $115,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Chicago, IL 60666: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Development: 3 years (Required)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Chicago, IL 60666

Health insurance"
647,Senior Quality & Data Engineer (Remote) Contract,Asics Digital,Remote,$75 - $90 an hour,"We are ASICS. And our five letters have meaning. ‘Anima Sana in Corpore Sano’ or a Sound Mind in a Sound Body.
We’ve always believed in the positive benefits of movement. And this year, we’ve recommitted to this founding purpose, supporting more people to experience the transformative power of movement, on the body and the mind.
So, this is your chance to join a truly purpose-driven brand.

Senior Quality & Data Engineer (Remote)
We are ASICS. And our five letters have meaning. ‘Anima Sana in Corpore Sano’ or a Sound Mind in a Sound Body. We’ve always believed in the positive benefits of movement. And this year, we’ve recommitted to this founding purpose, supporting more people to experience the transformative power of movement, on the body and the mind. So, this is your chance to join a truly purpose-driven brand.
ASICS Digital manages the e-Commerce and app development departments of ASICS. We are dedicated to expanding our employees’ skillset and creating a positive and freeing work environment such that our employees have a favorable work-life balance.
We are actively seeking an inquisitive, learning-minded Senior QA Engineer with experience working on data-centric projects and test automation. ASICS Digital is embarking on numerous projects this year related to consolidation and maintenance of customer data. For example, establishment of Single Sign-On feature across our online presences.
The ideal candidate will have sharp QA skills, established knowledge of coding principles, deep understanding of data structures and analytics, and ready to write well-packaged code for web-based testing.


Responsibilites:
Apply your expertise to help drive testing strategy and direction of major data-driven initiatives.
Work in partnership with software engineers, testers, and product managers to create new tests to the specifications of new features before release to the production website.
Write clear and concise test plans, track test coverage and key performance metrics, log and prioritize bugs as they are found. Coordinate testing efforts across multiple teams and time zones.
Increase regression test coverage by adding new automated tests and maintaining existing tests using Python and Selenium.
Apply abstraction techniques, design principles, and architectural code design daily to further a well-established testing repo.
Maintain CI/CD integrations between automated tests, Testrail, and AWS.
Contribute to the continuous improvement of the QA processes, ensure repeatability, be an advocate for quality.
Multitask with ease between projects, shifting focus as needed.
Create complex SQL statements, data audits, and automation of data processes.

Requirements:
Bachelor’s Degree in Computer Science, or related field.
5-8 years total experience in software engineering or quality assurance, with minimum 3 years of experience focused on large-scale data initiatives.
Experience with Sales Force Commerce Cloud, Tealium, Snowflake data storage.
Experience writing automated tests in Python, Selenium, Pytest, Pycharm, JavaScript.
Knowledge and application of both object oriented and functional design principles.
Intimate knowledge of advanced coding standards.
Ability to postulate positive and creative solutions to issues and present these ideas to the team.
Knowledge of SQL, HTML, CSS, Bash, React and API Testing.
Experience working in a fast-paced environment, using agile methodologies.

Nice to Haves:
Knowledge of Salesforce/Lightning, Allure, ContentStack, Launch Darkly, Yaml.
Familiarity with Jira, TestRail, GitHub, AWS.
Experience in performance testing.

Location:
Boston, MA is the preferred location for this position (Hybrid). The Company will consider a remote work arrangement in the following locations only: AZ, CO, CT, FL, GA, IL, KS, MA, MD, MI, MO, NH, NJ (except for Jersey City), NV, OH, OR, PA, NC, SC, TN, TX, VT, VA, or WI.

Additional Information:
The expected Hourly Range for this position is $ 75 to $ 90 per hour.
ASICS Applicant Privacy Notice:
https://bit.ly/3l35Une
$75 - $90 an hour
Pay may be adjusted based on applicant's work location.
Become a part of the ADI community:
ADI is taking active steps towards becoming a diverse, equitable, and inclusive workplace. We aim to engage in D&I work that permeates our organization and all employees are expected to be actively involved.
ADI is a strong, global community where we collaborate and care for each other.
We value a diversity of opinion, everyone’s input, and increasing the number of voices at the table.
You’ll have the opportunity to join the D&I task force, participate in affinity spaces, learn and grow on your anti-racist journey. We all need to know what anti-racist is so that everyone can talk about what it actually means.
We center our employees as full people. We don’t just accept difference, we celebrate it, support it, and thrive on it for the benefit of our employees, our products, and our community.

Equal Opportunity Employer Description:
At ADI, we don’t just accept diversity— we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products, and our community. ASICS Digital is proud to be an equal opportunity workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, veteran status, or fitness level."
648,Data Engineer,Cabinetworks Group,"Ann Arbor, MI 48105•Hybrid remote",N,"The Data Engineer will be responsible for building and maintaining a development team to support the company initiatives and will also be responsible for taking requirements from Business Analysts and develop Business Intelligence solutions.
PRINCIPAL FUNCTIONAL RESPONSIBILITIES:
Work closely with Business Analysts to gather requirements and design solutions
Analyze raw data sets, data flows, and business logic.
Design, build and implement BI reports and dashboards using Power BI.
Develop a thorough understanding of the business functions.
Build and maintain an effective team which may include consultants and an offshore team.
Coordinate priority of developer’s tasks.
Design, build, validate and monitor Azure resources.
Create, schedule, and optimize SQL Stored Procedures.
Solution architecture documentation.
ESSENTIAL QUALIFICATIONS AND SKILLS:
Bachelor’s Degree in Computer Science, Engineering, Mathematics, Statistics or equivalent work experience in Data Analytics or relevant experience equivalent to 4 years.
4+ years work experience with data and Analytics on complex data sets
2+ years work experience in the manufacturing industry preferred.
Experience with SQL and Power BI
Experience with Azure Platform
Experience with Time-Series data, R or Python is a plus
Experience working with offshore resources
Strong written and oral communications skills
Strong work ethic, and ability to work with minimal supervision
Good organizational and analytical skills utilizing structured development and problem-solving techniques
Ability to collaborate on cross-functional teams
Ability to learn and adapt quickly to a dynamic environment.
Ability to multi-task and efficiently prioritize tasks assigned.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Employee discount
Health insurance
Life insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
4 years
Schedule:
8 hour shift
Experience:
complex data sets: 4 years (Required)
Manufacturing: 2 years (Required)
Work Location: Hybrid remote in Ann Arbor, MI 48105

Health insurance"
649,"Data Engineer, Sr.",Health Catalyst,Remote,N,"Join one of the nation’s leading and most impactful health care performance improvement companies. Over the years, Health Catalyst has achieved and documented clinical, operational, and financial improvements for many of the nation’s leading healthcare organizations. We are also increasingly serving international markets. Our mission is to be the catalyst for massive, measurable, data-informed healthcare improvement through:
Data: integrate data in a flexible, open & scalable platform to power healthcare’s digital transformation
Analytics: deliver analytic applications & services that generate insight on how to measurably improve
Expertise: provide clinical, financial & operational experts who enable & accelerate improvement
Engagement: attract, develop and retain world-class team members by being a best place to work
Role: Data Engineer, Sr
Team: Data Quality / Data Infrastructure
Location: US Remote
Travel: <5%, US
The Data Engineer supports the Data Quality Expert Data Collections(EDC) Operations department and is responsible for working with a team of data engineers, implementation, and clients to operationalize and support to improve the EDC experience and value to clients. The position requires cross-team communication, attention to detail and the ability to develop innovative processes and approaches for EDC success.
What you'll own:
Define, drive, and help implement data quality checks based upon EDC experience to provide valid and relevant data to the clients.
Investigate and troubleshoot client support tickets, internal bugs, with the intent to improve and operationalize the implementation, data quality and success of EDCs.
Participate in code reviews that include SQL queries, IDEA and effectively communicate issues and risks.
Collaborate with other teams to provide guidance on extending the EDC standard data model.
Optimize code for maximum scalability and maintainability.
Incorporate unit testing and regression testing to ensure defect-free builds and releases.
Work with data architects and product managers to provide additional development assistance on demand.
What you bring:
BS or MS in Computer Science or equivalent professional experiencepreferred.
5+ years SQL Server and/or RDBMS experience with current technology required.
A solid understanding of data structures (e.g., SQL/XML/SGML/DTD/JSON).
Experience writing complex and efficient SQL queries and stored procedures.
Deep SQL Server working knowledge including order of operations, transactions and concurrency, file tables and security, brokering technologies, transactional replication, indexing strategies and maintenance.
Familiar with Git and branching strategies.
Azure knowledge highly desired.
Databricks knowledge preferred.
Demonstrable experience implementing enterprise-scale, high volume, high availability systems.
Demonstrated ability to deliver major critical projects.
Experience with Agile and Scrum team development environments.
Who you are:
Comfort with some ambiguity and some self-direction.
Must be well organized, accurate and detail oriented.
Excellent written and verbal communication with technical and non-technical staff.
Ability to work in complex code bases written by others.
Strong organizational, presentation, interpersonal and consultative skills a must.
Ability to manage multiple projects/tasks simultaneously.
Good judgment and decision-making skills.
Enthusiastic about sharing knowledge and experience.
Maintains a positive and results-oriented attitude.
The above statements describe the general nature and level of work being performed in this job function. They are not intended to be an exhaustive list of all duties, and indeed additional responsibilities may be assigned by Health Catalyst .
Studies show that candidates from underrepresented groups are less likely to apply for roles if they don’t have 100% of the qualifications shown in the job posting. While each of our roles have core requirements, please thoughtfully consider your skills and experience and decide if you are interested in the position. If you feel you may be a good fit for the role, even if you don’t meet all of the qualifications, we hope you will apply. If you feel you are lacking the core requirements for this position, we encourage you to continue exploring our careers page for other roles for which you may be a better fit.
At Health Catalyst, we appreciate the opportunity to benefit from the diverse backgrounds and experiences of others. Because of our deep commitment to respect every individual, Health Catalyst is an equal opportunity employer."
650,Data Engineer,Glow Networks,"Dallas, TX 75252",$73 an hour,"Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice."
651,Data Engineer,Comcast,"Philadelphia, PA 19103",N,"Comcast’s Technology, Product & Experience organization works at the intersection of media and technology. Our innovative teams are continually developing and delivering products that transform the customer experience. From creating apps like TVGo to new features such as the Talking Guide on the X1 platform, we work every day to make a positive impact through innovation in the pursuit of building amazing products that are enjoyable, easy to use and accessible across all platforms. The team also develops and supports our evolving network architecture, including next-generation consumer systems and technologies, infrastructure and engineering, network integration and management tools, and technical standards. In most cases, Comcast prefers to have employees on-site collaborating unless the team has been designated as virtual due to the nature of their work. If a position is listed with both office locations and virtual offerings, Comcast may be willing to consider candidates who live greater than 100 miles from the office for the remote option.

Job Summary
Comcast is seeking a Data Engineer in the CONNECT organization. The role provides candidate with broad exposure and opportunity to up level the Cybersecurity maturity of multiple lines of business and next generation platforms utilized for providing high-speed Internet services. First and foremost an ideal candidate must demonstrate high degree of self-initiative and adaptability to work on multiple security work streams in a fast-paced environment. The candidate should ideally possess a technical background in security threat/risk management, background in database administration, and python. Lastly, the candidate is required to collaborate with stakeholders in peer security and technology groups to strengthen overall security posture.
Job Description
Core Responsibilities
Monitors reporting platform to support security remediation of security vulnerabilities across all platforms and services
Maintains SQL database for a reporting platform for security vulnerabilities to owners
Identifies, documents, and recommends reports and dashboards for trending data issues for teams
Stays current with security technologies, trends, vulnerabilities and threats
Follow our agile process, working in sprints together with the rest of the team to deliver on time with technical quality
Continuously improve yourself and the products you work on
Supports building a culture of security by educating others and advocating an open security
posture
Consistently exercises best judgment and discretion in matters of significance
Other duties and responsibilities as assigned
Skills & Experience
Experience in the area of Data Analysis
SQL databases
Frontend dashboard tools
Python
Hands on experience with Linux/Unix
Familiarity AWS, Openstack
Familiarity with Information Security (vulnerability testing, risk analysis, and security assessments)
Excellent written and verbal communication skills, interpersonal and collaborative skills
Must have strong problem-solving and analytical skills
High degree of initiative and be well organized
Ability to manage multiple projects with strict timelines
High level of personal integrity
Enjoys working in a demamding and dynamic enviroment
Disclaimer:
This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications.
About Our Perks & Benefits
We are determined to build an environment where our employees feel valued, understand our business goals, and are motivated.
Here's a look at just some of the perks and benefits we make available to our US-based employees:
Medical & Dental
401(k) Savings Plan
Generous paid time off
Life Milestones - from adoption assistance, childcare resources, pet insurance, and more, Comcast supports you at all life stages.
Courtesy Services - We offer all of our full-time employees in serviceable areas free digital TV and internet.
Discount tickets for Universal Resorts, including theme park tickets and onsite hotel rooms.
Learn more at https://jobs.comcast.com/life-at-comcast/benefits (https://jobs.comcast.com/life-at-comcast/benefits)
Reasonable Accommodation:
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment. Please contact us to request accommodation.
Comcast is an EOE/Veterans/Disabled/LGBT employer.
=== THIS POSITION IS INELIGIBLE FOR VISA SPONSORSHIP. TO BE CONSIDERED FOR THIS ROLE, YOU MUST BE LEGALLY AUTHORIZED TO WORK IN THE UNITED STATES AND NOT REQUIRE SPONSORSHIP FOR EMPLOYMENT NOW OR IN THE FUTURE. ===
Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other basis protected by applicable law.

Education
Bachelor's Degree
While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.
Relevant Work Experience
2-5 Years

Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That’s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality – to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details."
652,"Sr. Data Engineer & Developer, Music Analytics",Sony Music Entertainment US,"New York, NY","$130,000 - $175,000 a year","Columbia Records is seeking a Senior Data Engineer, Music Analytics to work under the label's Insights & Analytics team. You will be responsible for establishing, building, and maintaining all data pipelines and internal data interfaces that power Columbia's analytics and A&R Research processes. This role is vital to Columbia's ability to find, analyze, and make recommendations on new talent signings and music marketing opportunities. This is a unique opportunity to apply web development and database management skills to support A&R and marketing decisions that have a significant impact throughout the label.
What you'll do:
Build and maintain internal web interfaces and databases that help the team collect and evaluate data related to new music releases and unsigned artists.
Discover and implement new data pipelines across the digital music-related landscape. Manage existing data pipelines and add additional sources of music data as needed.
Create new company-wide and department-wide reports, and enhance existing reports, to present music data in an understandable and thoughtful way.
Collaborate with multiple departments, including A&R, to efficiently sort through millions of data points to identify interesting trends and patterns.
Who you are:
You have excellent Python skills and relational database skills (3-5 years experience).
You are proficient with version control tools and Python web frameworks such as Django.
A detail-oriented professional that can approach data problems thoughtfully to consider and solve for issues of scaling and accuracy across many disparate data sources.
An excellent communicator who can both collaborate with a team and work independently.
A self-starter who can identify and approach technical needs and issues, and can quickly learn and apply new concepts, principles, and solutions.
What we give you:
You join an inclusive, collaborative and global community where you have the opportunity to fuel the creative journey.
A modern office environment designed to foster productivity, creativity, and teamwork.
Our Hybrid with Flexibility approach combines the flexibility of remote working with the benefits of in-person collaboration whenever we need to come together to do our best work. Managers will partner with their teams and employees to establish work arrangements that meet the business, team, and individual needs.
An attractive and comprehensive benefits package including medical, dental, vision, life & disability coverage, and 401K + employer matching.
Voluntary benefits like company-paid identity theft protection and resources for pets, mental health and meditation resources, industry-leading fertility coverage, fully paid leave for childbirth or bonding, fully paid leave for caregivers, programs for loved ones with developmental disabilities and neurodiversity, subsidized back-up child and elder care, and reimbursement for adoption, surrogacy, tuition and student loans.
We invest in your professional growth & development.
Time off for a winter recess.
The anticipated annual base salary for this position is $130,000 to $175,000. This range does not include any other compensation components or other benefits that an individual may be eligible for. The actual base salary offered depends on a variety of factors, which may include as applicable, the qualifications of the individual applicant for the position, years of relevant experience, specific and unique skills, level of education attained, certifications or other professional licenses held, and the location in which the applicant lives and/or from which they will be performing the job.
Sony Music is committed to providing equal employment opportunity for all persons regardless of age, disability, national origin, race, color, religion, sex, sexual orientation, gender, gender identity or expression, pregnancy, veteran or military status, marital and civil partnership/union status, alienage or citizenship status, creed, genetic information or any other status protected by applicable federal, state, or local law.
The anticipated annual base salary does not include any other compensation components or other benefits that an individual may be eligible for. The actual base salary offered depends on a variety of factors, which may include as applicable, the qualifications of the individual applicant for the position, years of relevant experience, specific and unique skills, level of education attained, certifications or other professional licenses held, and the location in which the applicant lives and/or from which they will be performing the job.
New York Pay Range
$130,000—$165,000 USD"
653,Data Migration Engineer,Mark43,"Chicago, IL•Remote","$80,000 - $100,000 a year","Mark43 is approved to hire in 27 states, including AL, AZ, CA, CO, CT, DC, DE, FL, GA, ID, IL, IN, IA, KS, KY, LA, MA, MD, ME, MI, MN, NE, NH, NJ, NM, NV, NY, NC, OH, OR, PA, SC, SD, TN, TX, UT, VA, VT, WA, WV, and WI. Mark43 is also approved to hire in Canada and the United Kingdom. Before applying to a remote role, please ensure that you are able to perform the position in one of the states or countries listed above. State locations and specifics are subject to change as our hiring requirements shift.
Mark43's mission is to empower communities and their governments with new technologies that improve the safety and quality of life for all. We build powerful, scalable, and elegant software that sets a new standard for the tools upon which our first responders rely. Our users are diverse, and we are therefore committed to embracing diversity of thought and experience within our team.
We are looking for a Data Migration Engineer (ETL Developer) to join our team. You will be responsible for designing, developing, and executing data migrations that allow our customers to seamlessly transition critical operations to Mark43. As a Data Migration Engineer you will not only manage your own small portfolio of migration projects but will also play a critical role in helping our migration partners perform successful projects for our customers. You will partner with our migration contractors to provide them with project management, decision making and executable feedback. You will also partner with our customers to understand and find new value in their legacy data. You will collaborate with Mark43 migration, engineering, and deployments teams to continuously improve the data migration process to help us scale over time.
This role requires a sharp attention to detail, strong organizational skills, an interest in working directly with public safety customers, and a passion for data. Members of our current team have backgrounds in data migration engineering, ETL development and SQL development at large SaaS companies.

What you can expect to work on:
Perform mid to large-scale data migrations from customers' legacy systems into a multi-cloud environment
Manage our migration contractor partners through our end to end migration lifecycle
Analyze legacy databases to understand their unique data models and how that can be transformed to live in Mark43
Write and execute complex SQL-based ETLs to Extract, Transform and Load legacy data into Mark43
Become an expert on the Mark43 database and the implications of data decisions to our customers
Work towards a worthwhile mission with a team of friendly and intelligent coworkers

What we expect from you:
2+ years of experience working in a data migration engineering, ETL development and/or SQL Development role
Comprehensive understanding of the ETL process
Experience writing complex SQL scripts
Deep knowledge & understanding of database design, setup, and maintenance
Experience managing contractors is a plus
Interest in becoming an expert in public safety data as well as Mark43's products
Strong written and oral communication skills
Ownership over your own work and a commitment to every part of a task, from big picture to small details
An attitude that is humble, detail oriented, and committed to quality
Experience working with public safety data systems a plus

What you can expect from us:
Constant collaboration with numerous Mark43 teams, including Global Services, Engineering, and Product
Building mission critical and socially responsible software to enable first responders to better serve their communities
A team that respects and embraces your ideas and expertise
Coworkers that are motivated by pursuing excellence, rather than the prospect of personal gain
A workplace dedicated to supporting and bettering public safety and government agencies
We feel passionately about equal pay for equal work, and transparency in compensation is one vehicle to achieve that. Total compensation for this role is market competitive, including a target base annual salary range of $80,000 - $100,000, plus bonus opportunity, company stock options, and a full benefits package, including health insurance, paid time off, and a 401k with a company match. Please note that the higher end of this range will be reserved for candidates with appropriate experience who reside in high cost of living areas."
654,Data Engineer,N,"Houston, TX",N,"Quidnet Energy is pioneering a breakthrough technology in one of the most important industries for the 21st century. Quidnet utilizes proven drilling and well development techniques to provide ultra-low cost electricity storage at grid scale, enabling renewables to compete without compromise or subsidy. We are looking for a Data Engineer to help establish a new system architecture, establish productive analytical workflows and help to shape the future structure of our company.
AS DATA ENGINEER, YOU WILL:
Shape and execute the digital strategy for Quidnet’s long-duration energy storage, and:

Own and automate the data pipeline at all our sites, from collection, to cleaning, to cloud storage,
to dashboarding
Establish data storage architectures that combine several primary sources into a functional
integrated database
Develop new visualization techniques leveraging that architecture to uncover previously
undiscovered insights
Partner with teams across the organization to understand their analytic needs, build solutions
into our workflow, and serve as a data expert and process owner
Build and maintain domain expertise in relevant data sets and be able to think strategically about
what analytical techniques are appropriate for different use cases
Harness existing proprietary analyses by deepening and extending current interpretations
Streamline operational workflows to measurably reduce decision-making time while improving
decision quality
Engage and manage contractors to support the development and implementation of Quidnet’s
data architecture
WHAT WE LOOK FOR IN OUR DATA ENGINEER:
Demonstrated ability to:
Understand solving subsurface engineering problems
Be a self starter who can identify second and third order conclusions in the absence of task
specific instruction
Leverage data to solve messy real-world challenges, and draw the best possible conclusions in
the face of uncertainty
Communicate complex analytical concepts to a non-expert audience
Have an opinion, and be willing to advocate for evidence based conclusions
Operate in a flat and fast paced organizational structure
Develop new data handling skillsets, through novel coding (Python, R)
Power user of and passionate about:
Cloud architecture; databases, data warehouses, and data lakes; ETL/ELT processeses
Visualization coding (Python) and BI tools such as Spotfire, Power BI, or Tableau
Data mining, Exploratory data analysis and Algorithm Development
Microsoft Office Suite
WORK ENVIRONMENT
In-person assignment at Quidnet’s Houston headquarters with limited travel required
WHAT’S IN IT FOR YOU?
Impact. Your talent, time, and energy will critically impact the company’s success in providing low- cost long duration storage for the energy transition and solving a key challenge of our time
Growth. You will encounter, be challenged by, and work through an incredibly diverse range of obstacles
Distinction. You introduce a fundamentally differentiated solution into a predominantly commoditized marketplace that has the potential to dominate and transform utility-scale power project development
Ownership. You hold broad responsibilities with high autonomy in a communicative, collaborative, and fast-paced environment
Compensation. You are vested in a high-potential success story, with competitive compensation, benefits, and comprehensive PTO and leave policies, complemented by stock options in a high- growth start-up company with world-class venture capital backing.
WHO WE ARE
The energy transition needs a new way to store electricity over long durations. Batteries are too expensive at long durations and the incumbent solution (pumped hydro storage) is too limited by topography
Quidnet Energy unlocks pumped hydro storage’s terrain-constraint to fundamentally change the economics of long-duration electricity storage
Our patented approach stores energy as high-pressure water underground with an extremely lean construction profile. In doing so, we seek to secure the lowest-cost position in long duration storage market and enable renewables to compete without compromise or subsidy
Our solution is built on mature drilling and well-construction supply chains for rapid scaling and is backed by world-class investors including Breakthrough Energy Ventures. The core team has deep expertise in power and energy storage project development, finance, geology, reservoir and facilities development"
655,Principal Data Engineer (294),Amplify Consulting Partners,Hybrid remote,"$120,000 - $180,000 a year","ABOUT THE COMPANY
Amplify Consulting Partners is a data-first consulting company trusted by Fortune 500 businesses to deliver high-impact professional services across the technology ecosystem—from data engineering and visual analytics to data-driven marketing and program management.
We build and empower high-performing people by promoting growth and connection across our company culture. We don't just hang core values on the wall, we make every decision with them in mind—developing trusted, long-term relationships on a foundation of transparency and accountability.
DEI STATEMENT
We hold ourselves accountable for creating an authentic workplace where every person feels heard and experiences a sense of belonging.
We believe that organizations can be an instrument for positive human impact when they champion a diverse, inclusive, and equitable environment. We do this at Amplify by enacting programs and policies that promote DEI—and with humility, if we miss the mark, we rigorously amend our practices to better achieve our targeted outcomes.

Simply put, we turn our words into action.

ABOUT THE POSITION
Amplify is looking for a Sr. Data Engineer to join our Analytics team. In this role, you will be responsible for building API scrips between multiple data-sources and using Alteryx to curate and transfer to SQL server. Knowledge of Snowflake or Azure is a bonus. Any given project will involve a mix of technical and non-technical stakeholders, so you’ll also communicate and document your work for a range of audiences. You’ll allocate most of your time and energy on hands-on client work. However, if your ambitions include scoping projects, honing technical best practices, or developing new business, then we’ll ensure you have opportunities to test the waters. The successful candidate will bring an attention to detail along with modern engineering practices and help us build a high quality, modern data architecture at scale.

What makes you a great fit?
You MUST be located in the Seattle or Kansas City area and open to having a hybrid work schedule
You’re an all-around data geek who finds satisfaction in solving data problems and creating orderly systems.
You’ve spent years writing complex SQL queries on a daily basis and have mastered the art of turning plain-English business requirements into SQL.
You’ve built production data models from scratch following (or inspired by) a star schema.
You have at least a working knowledge of Alteryx, Rest APIs, SQL Server, and are eager to dive deeper into additional technologies like Snowflake.
You aren’t necessarily a visualization expert, but you’ve spent enough time in reporting tools (like Power BI, Tableau, or Looker) to understand how they interact with data models.
Your communication is clear and concise through every medium.

Please note that we unfortunately cannot provide sponsorship at this time.
SALARY AND BENEFIT HIGHLIGHTS
At Amplify we take a holistic approach to total rewards in order to invest in the satisfaction and success of our employees both now and in the future. We consider the whole person and want to support our employees in living full lives both personally and professionally. The following is an overview of what you’ll get as a member of our team.

Salary Range: We are committed to equitable and fair pay practices. For this role, the base salary pay range is $120,000 - $180,000 a year. We consider a variety of factors when making compensation decisions and generally do not hire employees at the highest point of the pay range. We share this to ensure transparency, expectation-setting and education for candidates considering opportunities with us.
50% remote work option
Flexible time off (time to recharge when you need it!)
11 observed holidays
Medical/dental/vision – employee is covered at 100%, dependents are subsidized
Parental leave, short-term disability, long-term disability, and life insurance options
401k program and Amplify matching up to 3%
‘Amplify You’ program – $1,000 annually for your own development or investment in well-being after one year
Student loan payback program
Mentorship and training opportunities
Business and employee referral bonus opportunities
OUR HIRING PRACTICES
At Amplify, all qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We are committed to creating a diverse and welcoming workplace that includes all employees with diverse backgrounds and experiences. We believe it enables us to better meet our mission and values while serving clients throughout our communities. People of color, women, LGBTQIA+, veterans, and persons with disabilities are encouraged to apply. Qualified applicants with criminal histories will be considered for employment in a manner consistent with all federal state and local ordinances. Amplify is committed to offering reasonable accommodation to job applicants with disabilities. If you need assistance or accommodation due to a disability, please contact us at HR@amplifycp.com."
656,Data Engineer,Impact Advisors LLC,United States,N,"Healthcare Data Engineer

Work You’ll Do:

As a Healthcare Data Engineer, you will work closely with a multidisciplinary Agile team to build high-quality data pipelines driving analytic solutions. Utilizing your deep understanding of data architecture, data engineering, data analysis, reporting, and basic understanding of data science, the solutions you create will generate insights from the organization’s connected data which will enable the advancement of data-driven decision-making capabilities within the enterprise. You will utilize your strong problem-solving skills, ability to work as part of a technical, cross-functional analytics team, and desire to solve complex data problems to deliver the insights which enable analytics strategies.

About Impact Advisors:

We deliver Best in KLAS advisory, implementation and optimization services to healthcare organizations. At Impact Advisors, we are committed to exceeding our clients’ expectations. We are a nationally recognized partner to many of the nation’s top healthcare organizations. Our commitment to patient-centered, value-driven outcomes has earned us some of the industry’s most prestigious awards. Please visit our website at www.impact-advisors.com for additional information.

Your Responsibilities:
Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals
Solve complex data problems to deliver insights that help business to achieve goals
Create data products for analytics and data scientist team members to improve their productivity
Advise, consult, mentor and coach other data & analytics professionals on data standards and practices
Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions
Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering
Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes
Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Learn about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics

Your Expertise:
Bachelor’s degree preferred; Computer Science, MIS, or Engineering preferred
Certification in one or more of the following Epic Systems modules: Cogito, Caboodle, Clarity, Reporting Workbench
5 years of experience working in data engineering or architecture role, 7+ preferred (3 years preferred for Jr. role)
Expertise in SQL and data analysis and experience with at least one programming language (Python preferred)
Significant experience developing and maintaining data warehouses in big data solutions (e.g., Snowflake, SAP Hana, Oracle, SQL Server, Teradata, etc.)
Experience with developing solutions on cloud computing services and infrastructure in the data and analytics space (preferred)
Database development experience using Hadoop or BigQuery and experience with a variety of relational, NoSQL, and cloud database technologies
Worked with BI tools such as Tableau, Power BI, Looker
Deep knowledge of data and analytics, such as dimensional modeling, ETL, reporting tools, data governance, data warehousing, structured and unstructured data.
Big Data Development experience using Hive, Impala, Spark and familiarity with Kafka
Exposure to machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics
Experience in using data base connections, SSIS, API, ODBC, etc.
Healthcare experience preferred but not required.

Our People and Culture:

We believe in a caring, fun, honest and autonomous work environment and we recognize that our dedication to our associates drives our success. Our mission to create a Positive Impact fuels our associates to innovate and deliver high value services to our clients.

In healthcare, many of the greatest ideas and discoveries come from a diverse mix of minds, backgrounds and experiences, and we are committed to cultivating an inclusive work environment. Impact Advisors provides equal opportunities to all employees and applicants for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, genetic disposition, neurodiversity, disability, veteran status, or any other protected category under federal, state and local law"
657,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
658,Data Engineer or Staff Data Engineer,Vida Health,United States•Remote,N,"ABOUT US

At Vida, we help people get better — and we’re helping the healthcare system get better, too.

Vida provides expert, personalized, on-demand health coaching and programs through a network of experienced health care providers — like dietitians, therapists, and health coaches and leading medical institutions — coupled with an easy-to-use app with award-winning content.

We focus on chronic conditions — like diabetes, depression, and hypertension — which account for 80% of the $3 trillion spent on healthcare in the US.

By combining advanced technology with the top-notch healthcare providers, Vida is breaking down the barriers that have historically kept people from getting the best care. Vida’s cloud-based platform captures real-time data from 100+ devices and apps and delivers AI-driven insights back to employers, health plans, and providers to improve care. We are trusted by Fortune 1000 companies, major national payers, and large providers to enable their employees to live their healthiest lives.

**Vida is authorized to do business in many, but not all, states. If you are not located in or able to work from a state where Vida is registered, you will not be eligible for employment. Please speak with your recruiter to learn more about where Vida is registered.

ABOUT THE ROLE

We are considering candidates ranging from mid to senior level.

We are looking for a Data Engineer who can help us build, manage, and optimize Vida Health’s data pipelines within our Google Cloud Platform (GCP) environment. From measuring Vida’s impact on healthcare costs to correlating member activities with successful health outcomes, you’ll have a substantial impact on projects whose results can lead to changes in product design and member experience. Our data strategy is Polychronic by Design, meaning we leverage population data insights to help members with multiple interrelated chronic conditions throughout their lifetime.

Because Vida is a startup, you’ll have the opportunity to work on a diverse set of projects that involve multiple parts of our data infrastructure. One of your responsibilities will be to connect all our sources of data such as claims data and app data while making the data structured and accessible for analysis and other ELT pipelines. You’ll also work on optimizing our ELT pipelines, including our ML pipelines which deliver recommendations and predictions that influence the member and health provider’s experience on the platform.

You’ll have the pleasure of working with a team who is excited about providing care to people living with chronic conditions. We also have experts in health care who are an invaluable resource for learning more about the health domain. We hope that you’ll consider embarking on this journey into polychronic health care with us.
RESPONSIBILITIES
Design the foundational layer of Vida Health’s data environment to make data standardized and reusable
Manage the infrastructure of our Data Platform alongside your peers.
Manage data pipelines that collect and transform data from multiple sources to support ML models, analysis, and reportingBuild out the infrastructure to serve Machine Learning models and recommendations for our Application and business users.
Work with Data Analysts and Data Scientists to design data architectures that will improve the productivity and velocity of both teams’ workflows
Optimize the costs of data pipelines and data products
Define and meet service-level agreements (SLAs) for data pipeline processes and ML-powered APIs
Work with serving Large Language Models (LLMs) in a healthcare environment.
QUALIFICATIONS
Experience with cloud platforms such as AWS or Google Cloud
Advanced knowledge of: Data Warehouses, SQL, Python, REST APIs
Track record of standardizing data for analysis on BI tools or delivering ML applications
Experience managing enterprise data exchanges, Analytics ETL, or ML Ops.
Has a relentless focus on delivering maximum value to the end user
Has an ownership mindset and is excited about monitoring and alerting on their systems
Has at least 5+ years of relevant work experience in Data Engineering or similar roles
BONUS SKILLS
Database tools such as BigQuery, Datastore, and Firestore
Asynchronous systems such as Kafka, RabbitMQ, PubSub, and Kinesis
Workflow orchestration services such as Apache Airflow
ETL Tools such as DBT, Matillion, Snowplow, Fivetran, Stitch, and Talend
Self-service BI tools such as Datastudio, Looker, Amplitude, and Tableau
HIPAA and healthcare data such as Medical and RX claims
Management and/or team lead experience preferred
Manage infrastructure with tools like Terraform, puppet, or chef
BENEFITS
Competitive compensation with meaningful stock options
Medical, Dental, Vision, Disability and Life Insurance (We cover 100% of your premium and 80% for your dependents)
Healthcare FSA Plan
Dependent Care FSA Plans
Commuter and Parking Benefits
401K Program
Flexible PTO Policy
Paid Parental Leave
10 Paid Company Holidays
PERKS
We’re a distributed company, so you can work from most US states (We still have a HQ office in San Francisco)
Access to a Vida Health Coach and the full Vida App
New hire home office stipend
Monthly wellness benefit
Training and leadership development programs
Weekly meetups with team members across the country through our #connectandcommit program
Quarterly All Company Events
Quarterly Team Based Connection Opportunities
Significant opportunities for growth and development as the business grows

Vida is proud to be an Equal Employment Opportunity and Affirmative Action employer.

Diversity is more than a commitment at Vida—it is the foundation of what we do. All qualified applicants will receive consideration for employment without regard to race, color, ancestry, religion, gender, gender identity or expression, sexual orientation, marital status, national origin, genetics, disability, age, or Veteran status. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.

We seek to recruit, develop and retain the most talented people from a diverse candidate pool. We don’t just accept differences — we celebrate them, we support them, and we thrive on them for the benefit of our employees, our platform and those we serve. Vida is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures.

We do not accept unsolicited assistance from any headhunters or recruitment firms for any of our job openings. All resumes or profiles submitted by search firms to any employee at Vida in any form without a valid, signed search agreement in place for the specific position will be deemed the sole property of Vida. No fee will be paid in the event the candidate is hired by Vida as a result of the unsolicited referral.

#LI-remote"
659,Data and Knowledge Engineer,Fiable Consulting,Remote,"From $56,154 a year","REQUIRED
Python, Data Pipelines/ETL, Apache NiFi, Databases, SQL, Big Data Analytics ( Snowflake, Databricks, Spark)
Desired:
Enterprise Knowledge Graph (Stardog, RDF, OWL, SPARQL)
Job Types: Full-time, Contract
Salary: From $56,153.57 per year
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote

Health insurance"
660,Senior Data Engineer,Glue,"Seattle, WA•Remote","$165,000 - $200,000 a year","About Glue
Glue is on a mission to make people as happy as they can possibly be at work. To help us get there, we're adding even more of the brightest minds to our team; could that be you?
Glue builds technology empowering distributed teams to stick together. Through an AI-powered connection algorithm HR and People leaders understand and act on the relationships within their organizations. Glue's platform also utilizes Meetups, Events, and Pulse surveys to build connections that further drive retention. Glue is backed by leading investors including Greylock, and Founder's Coop.
Culture is at the heart of what we do—come join a dynamic and innovative team that's bringing wonder to work!
Job Overview: We are seeking a highly motivated and experienced Senior Data Engineer to join our team. As a Senior Data Engineer, you will be responsible for building and maintaining our data infrastructure, designing and implementing scalable data pipelines, and providing guidance to junior engineers. You will work closely with our data scientists and business analysts to ensure that our data is accurate, reliable, and easily accessible.
Responsibilities:
Design and implement scalable, reliable, and efficient data pipelines to support data ingestion, processing, and delivery
Build and maintain data infrastructure, including data warehouses, databases, and ETL processes
Collaborate with cross-functional teams to ensure that data solutions are aligned with business requirements
Ensure data accuracy, completeness, and consistency by implementing appropriate data quality checks and data governance processes
Provide technical guidance and mentorship to engineers, data engineers, machine learning engineers, and data scientists
Continuously research and evaluate new technologies, tools, and methodologies to improve our data infrastructure and processes
Develop and maintain documentation for data infrastructure, processes, and solutions
Qualifications:
Minimum of 5 years of experience in data engineering, with a proven track record of designing and implementing large-scale data pipelines and data solutions
Experience building and maintaining offline and online machine learning, analytics, and production infrastructure
Strong expertise in data warehousing, ETL, SQL, and technologies (such as Snowflake, PostgreSQL, Airflow, Fivetran, dbt)
Experience with cloud computing platforms and infrastructure-as-code (such as AWS, Docker, Terraform, Kafka, CI/CD)
Experience with event streaming platforms (such as Kafka, Kinesis)
Strong programming and software engineering skills
Excellent communication, collaboration, and problem-solving skills
Strong attention to detail and ability to work as a tech lead and independently in a fast-paced environment
Compensation:
The salary for this role will be between $165,000 and $200,000. In addition to salary this role will include equity in the form of stock options.
Benefits
Glue is proud to provide the following benefits to our employees:
Health, Vision, Dental coverage included
Unlimited PTO with a minimum of 2 weeks a year
Company sponsored 401k
Learning and development Stipend
Home office stipend
Bi-annual all company offsite
Monthly team events
12 weeks paid parental leave"
661,Big Data Engineer,Narvee Tech,Remote,$40 - $50 an hour,"Its an W2 Position (NO C2C)
only US Citizen and Green Card
Title:Big Data Engineer - Remote!
Job Discription:
1)Extensive Java experience.
2) Knowledge of Python and R programming.Familiarity with the Apache Spark streaming and batch frameworks, Kafka, Storm, and Zookeeper.
3) Understanding of Redis and Hadoop Equities Analytics and/or electronic execution.
4) A strong understanding of KDB+/Q.5) Strong skills in Linux, API (REST & SOAP), Version control, SQL, Cluster monitoring tools,Java.
Interested share resume at selena(at)narveetech.com
contact: 4698606714
Job Type: Contract
Pay: $40.00 - $50.00 per hour
Experience level:
10 years
9 years
Schedule:
8 hour shift
Work Location: Remote"
662,Data Engineer,Quantum FBI,Michigan,N,"JOB OVERVIEW
We are a professional services firm that provides accounting and finance as a service, business advisory, and business intelligence services to early-stage and middle-market companies. We are searching for a Data Engineer responsible for all areas relating to business intelligence and data analysis. This position will be responsible for the design of the data infrastructure necessary for a product to be sold to clients. The Data Engineer will have contact with clients and client contacts, so native-like English proficiency is required.

SUCCESS FACTORS
Ability to manage and motivate a team
Superior ability to prioritize and focus in a high-paced, multi-tasking environment
Ability to quickly learn and use new technology software applications
Commitment to protecting client privacy, even within the firm, as appropriate
Ability to apply self to learning and applying technical skills
Co-operative team player
Interest in personal and professional development and advancement
Project management skills
Time management skills
Unrelenting commitment to client satisfaction

REQUIRED EXPERIENCE, SKILLS, AND QUALIFICATIONS
Bachelor’s Degree in Business Intelligence, Big Data, or similar areas.
2-3 years of experience creating processes
Strong technology skills using Power BI, Excel
Experience working in a paperless environment strongly preferred
Strong interpersonal and relationship-building skills
Team player with a positive ”can-do” approach
General and growing knowledge of firm products and services in the practice area, and general knowledge of products and services in other practice areas
Native-like English proficiency"
663,Data Engineer,GameOn Technology,"San Francisco, CA•Remote",N,"At GameOn, we believe in the potential of AI to better the world and our mission is to make AI accessible through conversational user experiences.

To further our mission, we are looking for our first Data Engineer to join our growing data team. As GameOn’s first data engineer, you’ll play a central role in scaling our data and capabilities as we grow, and you’ll be a primary influence on our company’s data practices. You’ll build and maintain ETL pipelines, to provide new data sources and insights to our company and customers. You’ll help to expand our data infrastructure to meet the demands of our products as they evolve. And you’ll collaborate on teams to produce data products and applications to complement our ChatOS.

Come join us! GameOn is an open and ever-evolving work environment where you’ll have ample opportunity to contribute and grow in areas of the company that interest you. We are a people-first organization that embraces a flexible work environment in order to meet the needs of our team members. Whether you prefer working-from-home, coming into our energy-rich office in the Financial District of San Francisco, or maybe a combination of the two - we support you!
Responsibilities
Integrate and surface new data sources into our warehouse, from both internal and external origins.
Initiate and execute testing and monitoring on our data, to ensure data quality and pipeline health.
Monitor and advise on upcoming data infrastructure needs as we scale
Promote data best practices within the company for data storage, access, and pipeline architecture.
Collaborate on the design of data-related apps and services.
Requirements
Strong Proficiency in SQL and Python.
2+ years experience maintaining and building ETL/ELT pipelines.
Understanding of system design for data infrastructure, toward evolving our current data infrastructure to meet near-future needs.
Experience with the GCP platform and data stack is preferred.
Understanding of best practices in maintaining data quality at volume.
Ability to work both independently and collaboratively, as our first data engineer.
Experience on teams/projects supporting data product deployment through API is preferred.
Benefits & Perks
Competitive salary and equity packages
Medical
Dental
Vision
Life
Short-term Disability
Long-term Disability
401k Program
Professional Development Budget
Home Office Stipend
Parental Leave (12 weeks fully paid)
Time Off (15 company holidays + Unlimited PTO)
Virtual and in-person team events
Company Background
GameOn is the industry-leading intelligent chat platform that powers authentic conversational experiences for some of the world's largest and most popular brands, teams and content properties. Established in 2014 and based in San Francisco, GameOn's omnichannel technology engages fans, drives profitability and saves time and money for partners like the NBA, NFL, NHL, PGA Tour, FIFA, TIME Inc., among others. Founded by proven entrepreneurs Alex Beckman, Kalin Stanojev and Nate Simmons, GameOn has raised $54 million from leading VC firms like Quest Ventures, Mirae Asset Venture Investment, Mighty Capital and celebrity investors like Snoop Dogg, Joe Montana and Gary Payton.

About GameOn
GameOn Technology is proud to be an equal opportunity workplace and we are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Our company is based on a culture of support, collaboration and passion driven by the success of our customers in the spirit of developing fans out of users."
664,Data Engineer,Cornerstone Defense,"Reston, VA",N,"Title: Data Engineer
Location: Reston, VA
*Clearance: *Active TS/SCI w/ Polygraph needed to apply *

Company Overview:
Cornerstone Defense, in partnership with our military, intelligence, and civil government customers, supports U.S. operations worldwide through the use of many different types of intelligence, satellite, and cyber technologies. Cornerstone’s Intelligence Sector provides solutions to the United States Government for information collection, operations, exploitation and dissemination, and research activities. Our Team specializes in software development, cloud architecture, systems and network engineering, systems integration, agile management, as well as targeting operations and intelligence analysis. Our support to our mission customers includes cyber network operations, exploitation and defense, signals intelligence, human intelligence, and critical missions and networks.

Job Description: Develop complex data integration and manipulation issues, to include multi-INT sources and cross-domain solutions, as well as mentor more junior staff.
These back-end focused developers will also independently handle ETL Engineering challenges that come with building applications and tools of this caliber.

???????"
665,Field Data Engineer (US-Remote Optional),Ascend.io,Remote,"$88,500 - $150,000 a year","Ascend's mission is to enable every organization to innovate faster by harnessing the power of data automation. We help customers spend more time building pipeline logic and analyzing data and allow the Ascend platform to simplify and accelerate data from ingest, transformation, orchestration, observability and delivery. We have been named as one of the ""Best Places to Work"" by Built In for 2023!

You will help our customers accelerate their data projects, focused primarily on the initial technical implementation of Ascend. While you will have direct responsibility for building parts of the overall solution, you will also focus intently on up-leveling the customer in their ability to design and build data systems themselves. When you are not working directly with a customer, you will be part of the broader field engineering team working on customer POC's, staying current on leading edge technologies, helping other customers, and creating content. If you love a challenge, working with insanely bright and motivated people, and learning more faster than you thought possible, join Ascend, recently named a Gartner Cool Vendor!
What you'll do:
With the help of a Solution Architect, you will design cutting-edge data solutions specific to customer requirements.
Implement aspects of the solutions while providing support and guidance to customer developers on the team.
Help our customer developers become raving fans of Ascend.
Provide feedback to the product team on new features that will drive adoption and create raving fans.
Assist/drive POC's with new potential customers.
Research and implement technologies that are adjacent to and complementary to Ascend.
Research and implement competitive technologies.
Generate content that will accelerate customer learnings and promote self-sufficiency.
Assist other customers as time permits.
Build additional features / extensions for the Ascend platform.
What we look for:
1-3 years of Software Engineering experience.
Some Experience with public cloud infrastructure and data services such as Athena, EMR, Redshift, S3, BigQuery, GCP, etc.
Hands-on with at least 1 scripting language (such as Python, Ruby, Javascript).
Deep desire to learn/improve and be part of a winning team.
What we offer:
Medical/Dental/Vision
Long-term & Short-term disability
Life Insurance
Stock options
401(k)
Flexible PTO
Remote Friendly
The reasonably estimated base salary for this role ranges from $88,500 - $150,000 USD, plus a competitive equity package, and may include variable compensation. Actual compensation is based on factors such as the candidate's skills, qualifications, experience, and location.

Don’t meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Ascend we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.

Ascend.io is an equal opportunity employer. At Ascend.io, we are committed to treating all applicants fairly based on their abilities, achievements, and experience without regard to race, national origin, sex, age, disability, veteran status, sexual orientation, gender identity, or any other classification protected by law."
666,(Level Up) Data Engineer 1,H-E-B,"San Antonio, TX 78204",N,"Overview:
H-E-B is one of the largest, independently owned food retailers in the nation operating over 420+ stores throughout Texas and Mexico, with annual sales generating over $34 billion. Described by industry experts as a daring innovator and smart competitor, H-E-B has led the way with creative new concepts, outstanding service and a commitment to diversity in our workforce, workplace and marketplace. H-E-B offers a wealth of career opportunities to our 145,000+ Partners (employees), competitive compensation and benefits program and comprehensive training that lead to successful careers.
Responsibilities:
Do you have a:
HEART FOR PEOPLE… willingness to collaborate closely with
various business units and work teams?
HEAD FOR BUSINESS… an ability to ask clarifying questions
to ensure solutions are thoroughly designed to meet
business objectives and timelines
PASSION FOR RESULTS… an ability to communicate in order
to draw out requirements, focusing on both big picture and
short-term results?

We are looking for:
a related degree or comparable formal training,
certification, or work experience
expertise in SQL development

What is the work?
Design & Development:
Responsible for gathering and analyzing data
requirements; translates them into a usable data
architecture, which may include an enterprise data model,
associated metadata model, common business
vocabulary, ontologies, and taxonomies to be used to
guide enterprise solution development and achieve
consistency of information assets across the application
portfolio
Collaborates closely with developers and business units to
extract and define end product and business rules,
factoring non-functional requirements into designs
Develops and influences non-functional requirements
based on business processes and needs; identifies
potential technical and non-technical obstacles to meeting
requirements and develops methods to efficiently reuse
existing components
Continually follows up with developers to verify specs to
original requirements
Responsible for the overall design of enterprise-wide data
architecture, taking into account enterprise architectural
patterns, security requirements, and performance
requirements
Develops and maintains key architectural deliverables,
including architectural standards, best practices, and nonfunctional requirements questions
Identifies sources of data, where they should be drawn
from, how they should be incorporated and modeled into
business applications; develops logical and / or physical
data models
Designs and defines data profile techniques to be used
when analyzing data stores data quality, including
evaluating for data duplication, data validation, and
thoroughness and consistency of source and target data
Defines integration patterns to be leveraged within the
solution
Works with data teams to ensure integration capabilities
are effectively designed, database deployment is aligned,
and analytical data store and integrations are designed
and implemented
Works with application architecture, applications, and
systems teams to ensure designs and data models support
the integration of data / information flow across systems
and platforms
Leverages enterprise data design patterns to determine
best method for application integration into enterprise
analytical systems
Develops / maintains positive, collaborative relationships
with IS teams, business partners, and many levels of
leadership
Asks clarifying questions to ensure solutions are
thoroughly designed to meet business objectives and
timelines
Assists in developing / enforcing methodologies,
standards, policies, procedures for the organization,
structure, attributes, and nomenclature of data elements;
applies data content standards across projects
Ensures existing data / information assets are identified,
stewarded, and leveraged across the enterprise
Strategy:
Conducts business reviews; assesses current applications
Facilitates consistent business analysis, data acquisition
and access analysis / design, database management
systems optimization, archiving and recovery strategy,
load strategy design and implementation
What is your background?
A related degree or comparable formal training,
certification, or work experience
Experience in developing applications, data
integrations, or database development
Experience in SQL development

Do you have what it takes to be a fit as an H-E-B Data
Engineer 1?
Comprehensive knowledge of the field
Leadership skills
Analytical and logical thinking skills
Judgement and problem-solving skills
Business process analysis skills
Ability to communicate with business partners to draw
out requirements / rules, focusing on the big picture, and
taking the short-term into consideration as well, in order
to design the best product

Can you…
Function in a fast-paced, retail, office environment
Travel by car or plane with overnight stays
Work extended hours; sit for extended periods

Level3232"
667,Data Engineer Job Ref #: 478039,Concentrix Catalyst,"Omaha, NE 68102",N,"Overview
Concentrix CVG Customer Management Group Inc. has multiple openings for the position of Data Engineer based out of its U.S. offices in Omaha, NE. The employee may also work at various unanticipated locations throughout the U.S. Travel and/or relocation to various unanticipated locations throughout the U.S. is required. Telecommuting may be permitted.
Responsibilities
The position of Data Engineer will write, update, and maintain software applications; perform production maintenance of code; gather solutions requirements. Own technical commitments to clients and work with the team to successful delivery of solutions. Analyze, design, and code for complex requirements as well as write programs of complexity. Responsible for defining problems, collecting data, establishing facts, drawing valid conclusions, and preparing appropriate reports.
Qualifications
The position of Data Engineer requires a Master’s degree in Computer Science, Engineering (any), or any technical/analytical field that is closely related to the specialty, plus knowledge of the following: Spark, Hadoop, HBase and Hive.
To apply, send Resumes to ctlyst_postings@concentrix.com with Job Ref# 478039 in the subject line of the email."
668,Software Data Engineer,N,"Chicago, IL",N,"Role: Software Data Engineer
Location: Chicago or New York
At Aquatic, we are actively recruiting for a Software Data Engineer in our Chicago or New York office. In this role, you will work with quantitative researchers and software engineers to onboard new data sources and turn them into signals suitable for machine learning.
Responsibilities:
Support signals research by onboarding new data sources
Interact with vendors to understand data schemas and plan for changes
Respond to data outages during normal working hours
Work with signals researchers to optimize queries and refactor schemas as needed
Technical Skills:
2+ years full-time professional experience
Experience with SQL, relational databases, and structured data formats
Experience building batch data processing jobs in Python
Familiarity with cloud-based data solutions
Experience in finance is beneficial, but not required
Candidate Qualities:
Exceptional communication and coordination skills
Strong bias for action
Driven by accountability and internal urgency
Comfortable providing and receiving actionable feedback in a collaborative team setting
Motivated by an ambitious environment and driven colleagues
The base salary for this role is anticipated to be between $150,000 and $300,000, which is based on information at the time of posting. This position may also be eligible for additional forms of compensation, such as a discretionary bonus, and benefits. Discretionary bonus can be a significant portion of total compensation. Actual compensation for successful candidates will be carefully determined based on a number of factors, including their unique skills, qualifications and relevant experience.
Benefits:
Benefits: Fully paid medical, dental, and vision for employees and dependents, competitive 401k plan, employer-paid life & disability insurance
Perks: Wellness programs, casual dress, snacks, team and company events
Development: Open environment to maximize learning and knowledge sharing
Time: Generous PTO, paid holidays, competitive paid caregiver leaves
Aquatic Capital
Aquatic is a quantitative trading and investment company recently launched by Jon Graham. Prior to founding Aquatic, Jon was a Partner and Senior Managing Director at Citadel, where he worked for more than 13 years. At Citadel, Jon held numerous senior positions over the years, including head of Statistical Arbitrage and Equity High Frequency, culminating in leading the highly successful Global Quantitative Strategies business.
Aquatic develops systematic investment strategies, enabled by a leading-edge research and development platform. The vision is simple: to build a world class, quantitative trading company with a collaborative team of highly capable researchers and engineers.
This role represents a unique opportunity to join a quantitative investment manager at the foundational level of building a world class operation from scratch. The firm's culture will be shaped by collaboration, meritocracy, ambition, and calm determination.
Aquatic is a proud equal opportunity workplace. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity/expression, age, status as a protected veteran, status as an individual with a disability, or any other applicable legally protected characteristics."
669,Junior Data Engineer,vagus,"Southlake, TX 76092","From $80,000 a year","The right candidate will be equipped with skills that allow them to be self-sufficient in the process of finding data sources, performing ETL, t-SQL coding, analyzing and then presenting data results to their team and relevant stakeholders through BI tools such as Tableau. The role requires a high degree of autonomy during the research, planning and execution phases, therefore the candidate must be a self-starter with an entrepreneurial mind-set and have exceptional problem solving skills.
· Work alongside all departments of the organization to define business requirements for data solutions
· Architect complex data-centric and business intelligence solutions
· Formulate recommendations that enable leadership teams to make informed business decisions.
· Build scripts that will improve our proprietary data evaluation process by making it more flexible or scalable across data sets
· Monitor automated data collection, analytics, and cleansing infrastructure
· Visualize data using tableau, create several dashboards and visualizations
· Conduct research autonomously to learn where data exists and how it can be retrieved and analyzed in order to help build useful reports, analysis, dashboards etc.
· Bachelor’s degree in Business Intelligence, Data Analytics, Data Engineering, Data Science, Management of Information Systems or a related field (Master’s degree is a plus)
· 2-3 years of data analytics experience in enterprise reporting, preferably in a consulting/advisory, business analyst, data architect or data developer role
· Prior experience developing business requirements documents and managing data projects
· Experience designing enterprise reporting solutions driven by a data lake, data warehouse or operational data store
· Experience with one or more of the following or comparable technology: SQL, Tableau, Power BI, Alteryx,
· Mastery of Microsoft Office Products, advanced Excel and PowerPoint skills
· Working knowledge of data architecture within a healthcare organization
· Strong aptitude and curiosity to learn new technologies and analysis techniques
· Entrepreneurial spirit and a solution-oriented mindset
Ability to craft and deliver verbal, written, and oral messages and make recommendations to a variety of audiences, including client leadership and internal teams
· Ability to work in ever-changing, dynamic environments
Job Type: Full-time
Salary: From $80,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Experience level:
2 years
Schedule:
8 hour shift
Ability to commute/relocate:
Southlake, TX 76092: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Are you a US Citizen or GC Holder?
Are willing to work this fulltime/Onsite role at ""Southlake, TX""?
May I know your expected salary for fulltime?
Experience:
Data engineer: 3 years (Required)
T-SQL: 3 years (Required)
Tableau or PoweBI: 2 years (Required)
ETL Tool: 2 years (Required)
Work Location: One location

Health insurance"
670,Senior Quality Engineer (Data Platform),Atlas Labs,Remote,"$120,000 - $165,000 a year","No Patient Left Behind
Atlas Health automates philanthropic aid to improve access, affordability, outcomes and health equity for vulnerable populations.
Through intelligent matching and enrollment to 20,000 philanthropic aid programs, healthcare organizations can improve patient outcomes and reputation, increase cash and reduce staff administrative burden.
At Atlas Health, we are dedicated to eliminating the unfortunate reality of patients not being able to afford to survive. We may not be providers, but we do save lives by providing patients with the ability to receive treatment, when they otherwise may have never started or stopped for fear of negative financial outcomes. We love that we are saving lives by improving access and affordability. We view every dollar we find for a patient as hope.
Ultimately, we want to ensure no patient is left behind.
Quality Engineers on our data platform team develop testing software and automation infrastructure that validate the quality and performance of Atlas data infrastructure. These systems are responsible for handling sensitive patient data, and members of the team are technical champions of ensuring HIPAA standards for confidentiality and compliance. This role is heavily involved in the data team’s SDLC and is responsible for all quality concerns around infrastructure, data quality, and functional correctness. This includes automated end to end and system integration testing of infrastructure and component level white-box testing, and the incorporation of quality frameworks and best practices in support of continuous integration.

Responsibilities:
Leverage cloud infrastructure to engineer automated testing solutions
Develop processes and acceptance plans with engineering and product partners for incorporation into release cycles
Participate in improvement initiatives to enhance our team throughput and system performance
Identify and provide quality metrics for product and engineering to help improve delivery
Establish and evangelize best practices alongside data and infrastructure team members around software and data quality
Collaborate closely with team members and product stakeholders, as well as other engineers across the organization
Develop readable, maintainable test applications and suites
Provide guidance to other members of the team on best practices on quality engineering practices

Requirements:
Ability to break down complex problems into simple, organized, and actionable solutions
Demonstrated experience and understanding of both software and quality engineering concepts
Working knowledge and experience in developing and delivering quality metrics
Strong collaboration and communication skills
Professional experience with cloud-based systems
Exposure to data architectures and tools in support of data processing
Solid understanding of distributed task orchestration
Computer Science or related technical degree in a related field or equivalent technical experience

Bonus points:
Experience working in an ePHI environment
Prior experience having worked in a cross-functional, “DevOps” environment
Familiarity with domain driven design concepts

Preferred Qualifications:
Fluency in Python
Comfort in working with containerization tools
Salary: The salary range for this position is $120,000 - $165,000
Benefits:
We offer a comprehensive benefit plan for our U.S. based employees which includes:
Health, dental and vision insurance
401K
Flexible time off
Paid holidays
Why Join Our Team:
Because you’re motivated by a combination of success, working alongside incredible people, and have a passion for helping clients and patients. Atlas helps people access essential medical treatment, and avoid financial ruin from medical debt. You care about being a part of the journey and wish to play a key role in our organization’s success.
Atlas values diversity of all kinds, and we’re committed to building a diverse and inclusive workplace where we learn from each other. We are an equal opportunity employer and welcome people of all different backgrounds, experiences, abilities, and perspectives."
671,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
672,Product Solutions Engineer- Data Integrations (Kargo Commerce),KARGO,"New York, NY 10003","$100,000 - $120,000 a year","Kargo unites the world's leading brands, retailers and premium publishers across screens using innovative technology and advanced creative ad formats. At Kargo, we're all about bringing together the best of the best with a spark of creativity to stand out from the crowd. The same is true for our employees. What makes Kargo and each Kargo team member exceptional makes our company special. Kargo believes differences should be celebrated and is committed to diversity in the workplace. As an Equal Opportunity employer, we do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, marital status, age, national origin, protected veteran status, disability or other legally protected status. Individuals with disabilities are provided reasonable accommodation to participate in the job application process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Kargo is 500 employees strong across offices in New York City, Chicago, Austin, Dallas, Los Angeles, Sydney, Auckland, Ireland, and London. But we're not stopping there, so stay tuned as we expand our map and our team.
For more information, visit Kargo at www.kargo.com or follow Kargo on Instagram (@kargomobile), Twitter (@kargo) and LinkedIn (Kargo).
Title: Product Solutions Engineer - Data Solutions
Job Type: Full time, Hybrid
Job Location: Los Angeles
Salary Range: $100,000-$120,000 OTE
Summary of Job's Purpose:
The Product Solutions Engineer will sit within the Product team and have close relationships with the Engineering & Customer Success teams. In this role, you will focus on support processes, technical solutions & diagnosing the origin of the issue between Kargo Commerce, social publishers, or user error. You will be the bridge between our technical & customer facing orgs, breaking silos as we support our customers in their innovative advertising programs. You will be responsible for bringing our technology to life each day by pushing existing features to their limits, ideating new solutions & testing prototypes that can one day become official products.
The Position:
In this role, you will
Solve complex & unique technical problems for the world's largest advertisers through ""outside the box"" methods including scripts, automation & more
Act as an escalation point for Customer Success teams to diffuse operational roadblocks that the team may be facing by managing the process and tools used to support our tech
Help our internal & external users understand, use, and troubleshoot Kargo Commerce's paid social ad management platform
Inform trainings (education) that our Learning & Development team create for users
Create and share best practices across Kargo Commerce users as the technical frontline.
Help to define product improvements based on user feedback & troubleshooting
The Requirements:
Minimum 3 years experience within an ad technology company or social media publisher, preferably in Customer Success, Engineering or QA
Computer Science or Engineering related education & background
Strong troubleshooting knowledge with HTML, JS, CSS, familiarity with JSON
Familiarity with Adobe Creative Suite (Photoshop, After Effects) a plus
Knowledge of Meta APIs - Snapchat, Pinterest and/or TikTok APIs a plus
Experience working with JIRA, Confluence and Asana
Ability to articulate technical issues to non-technical users & stakeholders
A genuine interest in helping our teams & customers solve problems in innovative ways
Comfortable with customer-facing communication
Experience working in a fast paced, ever-changing environment with the ability to work across multiple time zones
The Perks:
Flexible PTO + 10 sick days a year
Tuition reimbursement up to $1,000 a year
Monthly wellness benefit
Daily complimentary lunch when working in a Kargo office
The Recognition:
Kargo and Kargo leadership are regularly recognized for the company's growth and achievements, including:
B&T's Best Media Platform (2021)
Digiday Awards' Media Award for Best Digital Product Innovation (2021)
Martech Breakthrough Awards' Content Management Innovation (2021) (2017)
Three MMA Smarties (2018)
Adweek Innovative Mobile Ad Player (2017)
Best Places to Work (2017)"
673,Data Engineer,DiamondPick,"New York, NY 10001","Up to $110,000 a year","Hi, Greetings from Diamond Pick !!!!! Here is the Job Description for the below Job Opening Role:Data Engineer Location :NYC,NY Duration-Full Time
""Familiar with at least one language (Python, Scala, Spark)
Have good working experience in Databricks.
Proficient in Snowflake. Good understanding of SQL Engine and able to conduct advanced performance tuning.
Experience with Git and the pull request workflow
Experience working closely with Analytics/Data Science/ML and product teams.
Job Type: Full-time
Salary: Up to $110,000.00 per year
Benefits:
401(k)
Health insurance
Compensation package:
Bonus pay
Yearly pay
Experience level:
7 years
Schedule:
8 hour shift
Ability to commute/relocate:
New York, NY 10001: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location

Health insurance"
674,Data Engineer,Jahnel Group,"Iowa City, IA 52240•Remote",N,"Jahnel Group’s mission is to provide the absolute best environment for software creators to pursue their passion by connecting them with great clients doing meaningful work. This is a full time position with one of our closest clients.
JOB OVERVIEW
Revology is a newly formed organization that provides technology-enabled revenue cycle management (RCM) services to hospitals, health systems, and physician groups.
We are currently searching for a Data Engineer to manage vital data infrastructure for large-scale data processing to support the functionality of Revology’s revenue cycle applications. This role will work with enterprise and client leaders to translate business and functional requirements into technical specifications.
The ideal candidate for Revology is analytical, strategic and a team player that is comfortable in a fast-paced, dynamic environment
RESPONSIBILITIES:
Develop and maintain efficient and scalable data pipelines that integrate data from multiple sources into common data models.
Transform raw data into usable information that can be consumed by client and enterprise organizations.
Collaborate with cross-functional Agile teams to design, develop, test, implement, and support technical solutions using full-stack development tools and technologies.
Support data science, research, and data analysis efforts by making data operationally available for products and services.
Partner with product managers, software engineers, and data scientists to deliver cloud-based solutions that drive powerful experiences.
Provide input to risk management; report risks as they are identified and participate in prioritization/follow up
Stay current with emerging technologies and advancements within existing technologies.
Positively and deliberately engage with colleagues – external and internal – to foster collaborative and productive relationships.
Stay curious, kind and contribute positively to the revology culture. The health + harmony of the team is everybody’s responsibility at revology.
QUALIFICATIONS:
3+ years of data engineering experience in big data solutions
Experience in database technology, architectures, ETL patterns and processes
Experience working with complex and varied data sources
Experience with relational SQL and NoSQL databases
Excellent analytical, strategic conceptual thinking, strategic planning, and execution skills
2+ years of experience working with Pyspark & AWS Glue
Demonstrated ability to merge technical and business requirements into solutions that can be consumed by all areas of the business
Experience working in HIPAA, HITRUST or advanced compliance environments
An understanding of the SaaS development, business model, and metrics
Familiarity with service business metrics such as “cost to serve”
Ability to engage productively in innovative and visionary brainstorming sessions while contributing your ideas, thoughts and suggestions
AWS certifications are especially desirable
Bachelor’s Degree in Information Technology, Computer Science, OR related field is preferred but not required
PHYSICAL REQUIREMENTS
Must be able to perform physical activities, such as, but not limited to: moving or handling (lifting, pushing, pulling and reaching overhead) office equipment and supplies weighing 1 to 25 lbs. unassisted. Frequently required to sit, stand or walk for extended periods during the workday. Manual dexterity and visual acuity required. Must be able to communicate effectively on the telephone and in person.
WORKING CONDITIONS
Work will generally be performed indoors in an office environment. Must maintain a professional appearance and manner.
EMPLOYMENT ELGIBILITY
Candidates must be legally authorized to work in the United States without sponsorship."
675,Data Engineer,University of Utah,"Salt Lake City, UT 84112","$88,000 - $101,649 a year","We are looking for a self-motivated, innovative, naturally curious person interested in solving complex health care issues in new and innovative ways. This individual must utilize expertise to query, design and translate complex data analysis into simple, user-friendly mediums that are easily digestible by health system executives and front-line clinicians. The ideal person for this role will have healthcare knowledge or be willing to learn healthcare terminology. Must be comfortable working on a team and have strong communication and presentation skills. The Data Engineer will have direct reporting responsibility to the Data Science Team Lead within the University of Utah Health Medical Group Analytics Team.
As a Data Engineer, you will be responsible for using APIs to integrate with the EPIC
Electronic Medical Record (EMR) system to build applications. In addition to
this, you will also be extracting data from third-party systems for use in
analytics. In this role, you will be expected to have strong programming
skills, particularly in languages such as Python or Java, and have experience
working with APIs. The ability to work with large datasets and experience with
data modeling and SQL is also highly desired. Overall, this role will involve
using your technical skills to bridge the gap between data sources and
analytics, and will be critical to the success of the data analytics efforts
within the organization.

Responsibilities
Essential Job Functions
Queries relational databases to produce data products used by Data Scientists and Business Analysts
Serves as a SME on ETL best practices
Uses APIs extensively to integrate within EPIC (EMR system) to build apps as well as extract data from third-party systems for use in analytics
Transforms data using SQL/Python/Java into a consumable structure
Aids in the development/implementation of new tools to improve data processes
Technical Skills
Proficient with standard office productivity tools, such as MS Office
Proficient with SQL
Proficient with Python and Java
Familiarity with DBT, Databricks, SSIS, Informatica etc. are preferred but not required

Work Environment and Level of Frequency typically required
Nearly Continuously: Office environment.
Physical Requirements and Level of Frequency that may be required
Nearly Continuously: Sitting, hearing, listening, talking.
Often: Repetitive hand motion (such as typing), walking.
Seldom: Bending, reaching overhead.

Minimum Qualifications
Requires a bachelor’s degree or equivalency in a related area and 2-4 years of experience in the field or in a related area.

Applicants must demonstrate the potential ability to perform the essential functions of the job as outlined in the position description.

Preferences
We enjoy a flexible/hybrid work environment.
This position has no responsibility for providing care to patients.

Type
Benefited Staff

Special Instructions Summary

Additional Information
The University of Utah values candidates who have experience working in settings with students from diverse backgrounds and possess a strong commitment to improving access to higher education for historically underrepresented students.

Individuals from historically underrepresented groups, such as minorities, women, qualified persons with disabilities and protected veterans are encouraged to apply. Veterans’ preference is extended to qualified applicants, upon request and consistent with University policy and Utah state law. Upon request, reasonable accommodations in the application process will be provided to individuals with disabilities.

The University of Utah is an Affirmative Action/Equal Opportunity employer and does not discriminate based upon race, ethnicity, color, religion, national origin, age, disability, sex, sexual orientation, gender, gender identity, gender expression, pregnancy, pregnancy-related conditions, genetic information, or protected veteran’s status. The University does not discriminate on the basis of sex in the education program or activity that it operates, as required by Title IX and 34 CFR part 106. The requirement not to discriminate in education programs or activities extends to admission and employment. Inquiries about the application of Title IX and its regulations may be referred to the Title IX Coordinator, to the Department of Education, Office for Civil Rights, or both.

To request a reasonable accommodation for a disability or if you or someone you know has experienced discrimination or sexual misconduct including sexual harassment, you may contact the Director/Title IX Coordinator in the Office of Equal Opportunity and Affirmative Action:

Director/ Title IX Coordinator

Office of Equal Opportunity and Affirmative Action ( OEO /AA)

135 Park Building

Salt Lake City, UT 84112

801-581-8365

oeo@utah.edu

Online reports may be submitted at oeo.utah.edu

For more information: https://www.utah.edu/nondiscrimination/

To inquire about this posting, email: employment@utah.edu or call 801-581-2300.


The University is a participating employer with Utah Retirement Systems (“URS”). Eligible new hires with prior URS service, may elect to enroll in URS if they make the election before they become eligible for retirement (usually the first day of work). Contact Human Resources at (801) 581-7447 for information. Individuals who previously retired and are receiving monthly retirement benefits from URS are subject to URS’ post-retirement rules and restrictions. Please contact Utah Retirement Systems at (801) 366-7770 or (800) 695-4877 or University Human Resource Management at (801) 581-7447 if you have questions regarding the post-retirement rules.

This position may require the successful completion of a criminal background check and/or drug screen.

Posting Specific Questions
Required fields are indicated with an asterisk (*).
* Do you have a related Bachelor's degree or equivalency? (2 years related work experience may be substituted for 1 year of education)
Yes
No
Will you now or in the future require sponsorship for employment visa status (e.g., H-1B status)?
Yes
No
Applicant Documents
Required Documents
Resume
Optional Documents
Cover Letter
Appropriate discharge document (such as a DD-214 – Member Copy 4) – Veteran Only – Call 801.581.2169
Addendum to the University of Utah - Veteran Only - Call 801.581.2169 after submission

Open Date
12/21/2022

Requisition Number
PRN33318B

Job Title
Data Architect

Working Title
Data Engineer

Job Grade
G

FLSA Code
Computer Employee

Patient Sensitive Job Code?
No

Standard Hours per Week
40

Full Time or Part Time?
Full Time

Shift
Day

Work Schedule Summary

VP Area
U of U Health - Academics

Department
00728 - U of U Medical Group

Location
Anywhere Utah

City
Salt Lake City, UT

Type of Recruitment
External Posting

Pay Rate Range
$88,000 - $101,649

Close Date

Open Until Filled
No"
676,"Architect, Data Engineer",Cincinnati Children's Hospital,"Cincinnati, OH 45219",$55.74 - $72.68 an hour,"Description

Expected Starting Salary Range: 55.74 - 72.68
SUBFUNCTION DEFINITION: Focuses on how to design, integrate, and manage complex data and analytic systems over their life cycles. Uses a combination of core software engineering principles and domain specific data and analytic knowledge to ensure the enterprise as seamless access to actionable, meaningful and well-governed data across all domains.

SCOPE: 0-7 Direct reports. Role applicable in centralized functions only

REPRESENTATIVE RESPONSIBILITIES

Enterprise Data Assets
Develop architecture and lead implementation of data warehouses, data lakes, data Hubs, data models, data virtualization and other enterprise data assets. Develop and continuously refine development & data integration standards in partnership with Business Intelligence, analytics and platform engineers. Collaborate with Information Architects, Platform architects, Application and Secuiry architects to understand and include impact on data architecture. Work across multiple deployment environments including cloud, on-premises and hybrid], multiple operating systems and through containerization techniques. Lead POCs/POVs to inform technology investments. Present and share data architecture principles/guidelines with both internal and external customers.

Metadata Management & Data Modelling
Implement standards and technology for Master data, metadata, reference data and data modelling, integrating data governance processes into development. Develop ETL and Data Quality standards and drive Automation through effective metadata management processes and strategies. Design integration of modern data preparation, integration and AI-enabled metadata management tools and techniques through industry best practice. Set & ensure documentation standards and ensure all code is properly controlled, documented and versioned.

Technical and Business Skill
Broad and Deep understanding of database architectures, data Modelling, Data Warehousing, Data Lake, Data Hub, etc. including knowledge of multiple infrastructure environments( public cloud, private cloud, virtualized, automated on-premises). Mastery of Data Management and platform practices and architectures. Advanced expertise in core Software & Data Engineering and automation technologies. Knowledge of SQL and demonstrated success in working with relational databases, data modeling, query authoring, performance tuning and security. Continuous improvement by enhancing the EA knowledge base of standards, innovation, evolving risks, and best practices.

Technical Support & Customer Service
Ensure outstanding end-user support is provided, including ongoing monitoring of Service Level Agreements for incident management and collaboration with other areas to ensure customer-centered incident management and support. Adhere to, develop/refine and promote continual adoption of change management policies and procedures. Model outstanding customer service behavior, including timely and effective follow-up with customers. Provide third-level problem resolution and technical support. Develop support documentation processes, including an update and review process to ensure junior staff can adequately support all production workflows. Set standards for the development of proactive monitoring systems (in collaboration with platform engineers) to identify problems before they impact production customers. Escalate with vendors when necessary to ensure CCHMC investments and requests are being adequately supported and enhanced. Serve as an escalation point for junior staff and escalate support issues with urgency.

Strategic Planning and Project Portfolio Management
Partner with business areas & internal IT teams to develop data architectures to meet organization's strategic goals. Anticipate needs and lead POVs/POCs/Pilots to inform technology investment. Develop standards, guidelines, processes and ensure compliance by leading and mentoring teams to deliver scalable data pipelines and data assets. Represent our architecture, tools, standards , policies and processes both within the medical center and outside entities.

Qualifications

EDUCATION/EXPERIENCE

Required:

Bachelor's degree in a computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field.
7+ years of work experience in a related job discipline

Preferred:

An advanced degree in computer science (MS), statistics, applied mathematics (Ph.D.), information science (MIS), data management, information systems, information science (postgraduate diploma or related) or a related quantitative field.

Unique Skills:

Data Management
Data management disciplines including data integration, modeling, optimization and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks.

Cincinnati Children's is proud to be an Equal Opportunity Employer that values and treasures Diversity, Equity, and Inclusion. We are committed to creating an environment of dignity and respect for all our employees, patients, and families (EEO/AA)."
677,Product Solutions Engineer- Data Integrations (Kargo Commerce),KARGO,"New York, NY 10003","$100,000 - $120,000 a year","Kargo unites the world's leading brands, retailers and premium publishers across screens using innovative technology and advanced creative ad formats. At Kargo, we're all about bringing together the best of the best with a spark of creativity to stand out from the crowd. The same is true for our employees. What makes Kargo and each Kargo team member exceptional makes our company special. Kargo believes differences should be celebrated and is committed to diversity in the workplace. As an Equal Opportunity employer, we do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, marital status, age, national origin, protected veteran status, disability or other legally protected status. Individuals with disabilities are provided reasonable accommodation to participate in the job application process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Kargo is 500 employees strong across offices in New York City, Chicago, Austin, Dallas, Los Angeles, Sydney, Auckland, Ireland, and London. But we're not stopping there, so stay tuned as we expand our map and our team.
For more information, visit Kargo at www.kargo.com or follow Kargo on Instagram (@kargomobile), Twitter (@kargo) and LinkedIn (Kargo).
Title: Product Solutions Engineer - Data Solutions
Job Type: Full time, Hybrid
Job Location: Los Angeles
Salary Range: $100,000-$120,000 OTE
Summary of Job's Purpose:
The Product Solutions Engineer will sit within the Product team and have close relationships with the Engineering & Customer Success teams. In this role, you will focus on support processes, technical solutions & diagnosing the origin of the issue between Kargo Commerce, social publishers, or user error. You will be the bridge between our technical & customer facing orgs, breaking silos as we support our customers in their innovative advertising programs. You will be responsible for bringing our technology to life each day by pushing existing features to their limits, ideating new solutions & testing prototypes that can one day become official products.
The Position:
In this role, you will
Solve complex & unique technical problems for the world's largest advertisers through ""outside the box"" methods including scripts, automation & more
Act as an escalation point for Customer Success teams to diffuse operational roadblocks that the team may be facing by managing the process and tools used to support our tech
Help our internal & external users understand, use, and troubleshoot Kargo Commerce's paid social ad management platform
Inform trainings (education) that our Learning & Development team create for users
Create and share best practices across Kargo Commerce users as the technical frontline.
Help to define product improvements based on user feedback & troubleshooting
The Requirements:
Minimum 3 years experience within an ad technology company or social media publisher, preferably in Customer Success, Engineering or QA
Computer Science or Engineering related education & background
Strong troubleshooting knowledge with HTML, JS, CSS, familiarity with JSON
Familiarity with Adobe Creative Suite (Photoshop, After Effects) a plus
Knowledge of Meta APIs - Snapchat, Pinterest and/or TikTok APIs a plus
Experience working with JIRA, Confluence and Asana
Ability to articulate technical issues to non-technical users & stakeholders
A genuine interest in helping our teams & customers solve problems in innovative ways
Comfortable with customer-facing communication
Experience working in a fast paced, ever-changing environment with the ability to work across multiple time zones
The Perks:
Flexible PTO + 10 sick days a year
Tuition reimbursement up to $1,000 a year
Monthly wellness benefit
Daily complimentary lunch when working in a Kargo office
The Recognition:
Kargo and Kargo leadership are regularly recognized for the company's growth and achievements, including:
B&T's Best Media Platform (2021)
Digiday Awards' Media Award for Best Digital Product Innovation (2021)
Martech Breakthrough Awards' Content Management Innovation (2021) (2017)
Three MMA Smarties (2018)
Adweek Innovative Mobile Ad Player (2017)
Best Places to Work (2017)"
678,"Senior Data Engineer, Data Orchestration",Salesforce,Colorado•Remote,"$156,800 - $215,600 a year","To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Products and Technology
Job Details
We’re Salesforce, the Customer Company. If you believe in bringing companies and customers together, in business as the greatest platform for change, in creating a more equitable and sustainable future for all – well, you’re in the right place. Through our #1 CRM, Customer 360, we help companies blaze new trails and connect with their customers in a whole new way. And, we empower you to be a Trailblazer, too — driving your performance and growth, charting new paths, and improving the state of the world.
Senior Data Engineer, Data Orchestration
Slack is looking for a senior data engineer to join the Data Orchestration Team. As part of the Data Engineering organization, we build and operate the platform that ingests data into our Data Warehouse. We write software to manage the ingestion for thousands of stateful hosts and stateless real time logging events. Currently, our infrastructure handles 65PB+ of storage, processes ~900B records a day, 400+ ETL pipelines and 900+ Active Airflow DAGs. As Slack’s data grows (along with the number of customers, features and employees), the goal of the team is to build a highly scalable and resilient orchestration capabilities for our downstream customers so that they can focus on their strengths.

You will build scalable services and tools to help partners implement, deploy and analyze data assets with a high level of autonomy and limited friction. You will play a meaningful role in making partner interactions with the Data Warehouse pleasant and productive. You will have deep technical skills, be a self-starter, detail and quality oriented, and passionate about driving data driven decisions and having a huge impact at Slack!

Here are a few blog posts that shed light into what we do here at Slack:
Data Lineage at Slack - https://slack.engineering/data-lineage-at-slack/
Reliably Upgrading Apache Airflow at Slack's Scale - https://slack.engineering/reliably-upgrading-apache-airflow-at-slacks-scale/
Introducing Data Residency for Slack - https://slack.com/blog/transformation/introducing-data-residency-for-slack
Data Wangling At Slack - https://slack.engineering/data-wrangling-at-slack/
What you will be doing
Design and develop highly scalable and resilient services/data pipelines for data ingestion and processing using modern big data technologies
Develop and maintain our real time analytics/low latency data access layer built on top of modern OLAP solutions
Optimize the end-to-end workflow for data users at Slack (from crafting libraries to schedule data pipelines and access data assets).
Automate and handle the lifecycle of data sets (schema evolution, metadata management, change and backfill management, deprecation and migration).
Improve the data quality and reliability of the pipelines through properly monitoring and failure detection.
Comfortably collaborate with cross functional partners and lead technical initiatives end to end.
Be a role model and a multiplier, coaching and mentoring other engineers across the org.
Write, review, or provide feedback on a technical design proposal from others.
What you should have
5+ years of software/data engineering experience, including experience with Big Data technologies, e.g. OLAP, Airflow, Spark, Kafka, Hadoop, etc.
Experience operating Airflow clusters and knowledge of Airflow best practices.
Experience working with cloud infrastructure (AWS preferred).
You have extensive experience of building and maintaining large scale ETL pipelines and in-depth knowledge of various big data frameworks and architectures
You are skilled at crafting and building robust distributed Microservices with tools like Docker, Kubernetes, AWS ECS/EKS etc.
Experience in real time analytics/low latency data access layer with OLAP stores such as Apache Pinot or Apache Druid is a huge plus.
You have strong dedication to code quality, automation and operational excellence: CI/CD pipelines, unit/integration tests.
You are proficient in object-oriented and/or functional programming languages: Python, Java/Scala, Chef, Terraform
You have excellent written and verbal communication and interpersonal skills; able to effectively collaborate with cross functional partners and explaining sophisticated technical concepts to non-technical stakeholders
You have high growth expectations for yourself and your team, and a willingness to push yourself and your team to achieve them
Bachelor's degree in Computer Science, Engineering or related field, or equivalent training, fellowship, or work experience.

Slack has a positive, diverse, and supportive culture—we look for people who are curious, inventive, and work to be a little better every single day. In our work together we seek to be smart, humble, hardworking and, above all, collaborative.
More details about our company benefits can be found at the following link: https://www.getsalesforcebenefits.com/
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form .
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at Salesforce and explore our benefits.
Salesforce, Inc . and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce, Inc . and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce, Inc . and Salesforce.org will not pay any third-party agency or company that does not have a signed agreement with Salesforce, Inc . or Salesforce.org .
Salesforce welcomes all.
For Colorado-based roles, the base salary hiring range for this position is $156,800 to $215,600.
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience.
Certain roles may be eligible for incentive compensation, equity, and benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com."
679,Senior Data Engineer,Agilon Health,Remote,N,"Position Summary:
agilon health is transforming healthcare by empowering community-based physicians with the resources and expertise they need to innovate the payment and delivery of care for seniors. The agilon health Total Care Model is powered by our purpose-built platform and enabled through a growing national network of like-minded physician partners.
With agilon health, physicians are freed from the constraints of the transactional fee-for-service reimbursement model and are able to practice team-based, coordinated care to serve the individual needs of their older adult patients and to transition to a sustainable and predictable, long-term business model.
As a member of agilon health’s Technology Group, step into a key role on an expanding data engineering team to build our data platforms, data pipelines and data transformation capabilities all running on AWS.
This is a great opportunity to help define and implement our data platform strategy on Cloud, have a meaningful impact on our customers and working in our high energy, innovative, fast paced Agile culture, in a leading tech company in healthcare space.
Job Functions:
· Develop and implement cloud native services to improve current data acquisition / ingestion and enrichment on all data layers of the SaaS platform.
· Construct, implement, and maintain extract load and transform (ELT) workflows with cloud native technology or open-source technologies (Snowflake, dbt, Airflow)
· Partner with Data Science team to define and implement best practices for data acquisition and transformation of data sets for ML training workflows.
· Leverage internal reporting tools to help validate and automate the data validation and data quality process.
· Management/Troubleshoot of all data pipeline services.
· Work with front-end developers / staff and customers to support reporting needs of business by restructuring data, creating tables/views, or virtual warehouses accessible in the cloud platform.
Qualifications:
· 4+ years of hands-on experience with designing and implementing data services in a cloud environment.
· Strong software engineering experience and deep familiarity programming languages such as Python, Java, Node.js, or equivalent.
· Experience working with at least one public cloud solution AWS, Azure, or GCP.
· Preferred experience working in scrum or agile teams & understanding of CI/CD pipelines.
· Experience working with cloud data warehouse / data lake solutions (Snowflake, Redshift, Google Big Query or equivalent platform.
· Undergraduate degree from accredited 4-year institution.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Retirement plan
Vision insurance
Compensation package:
Yearly pay
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you a US Citizen or Permanent Resident?
Education:
Bachelor's (Preferred)
Experience:
SQL: 4 years (Required)
Data warehouse: 3 years (Required)
Cloud architecture: 4 years (Required)
Software Engineering: 4 years (Required)
Agile: 2 years (Preferred)
Cloud computing: 3 years (Preferred)
Data lake: 2 years (Required)
Healthcare Industry: 2 years (Required)
Work Location: Remote

Health insurance"
680,Data Engineer,Cornerstone Defense,"Reston, VA",N,"Title: Data Engineer
Location: Reston, VA
*Clearance: *Active TS/SCI w/ Polygraph needed to apply *

Company Overview:
Cornerstone Defense, in partnership with our military, intelligence, and civil government customers, supports U.S. operations worldwide through the use of many different types of intelligence, satellite, and cyber technologies. Cornerstone’s Intelligence Sector provides solutions to the United States Government for information collection, operations, exploitation and dissemination, and research activities. Our Team specializes in software development, cloud architecture, systems and network engineering, systems integration, agile management, as well as targeting operations and intelligence analysis. Our support to our mission customers includes cyber network operations, exploitation and defense, signals intelligence, human intelligence, and critical missions and networks.

Job Description: Develop complex data integration and manipulation issues, to include multi-INT sources and cross-domain solutions, as well as mentor more junior staff.
These back-end focused developers will also independently handle ETL Engineering challenges that come with building applications and tools of this caliber.

???????"
681,Senior Quality & Data Engineer (Remote) Contract,Asics Digital,Remote,$75 - $90 an hour,"We are ASICS. And our five letters have meaning. ‘Anima Sana in Corpore Sano’ or a Sound Mind in a Sound Body.
We’ve always believed in the positive benefits of movement. And this year, we’ve recommitted to this founding purpose, supporting more people to experience the transformative power of movement, on the body and the mind.
So, this is your chance to join a truly purpose-driven brand.

Senior Quality & Data Engineer (Remote)
We are ASICS. And our five letters have meaning. ‘Anima Sana in Corpore Sano’ or a Sound Mind in a Sound Body. We’ve always believed in the positive benefits of movement. And this year, we’ve recommitted to this founding purpose, supporting more people to experience the transformative power of movement, on the body and the mind. So, this is your chance to join a truly purpose-driven brand.
ASICS Digital manages the e-Commerce and app development departments of ASICS. We are dedicated to expanding our employees’ skillset and creating a positive and freeing work environment such that our employees have a favorable work-life balance.
We are actively seeking an inquisitive, learning-minded Senior QA Engineer with experience working on data-centric projects and test automation. ASICS Digital is embarking on numerous projects this year related to consolidation and maintenance of customer data. For example, establishment of Single Sign-On feature across our online presences.
The ideal candidate will have sharp QA skills, established knowledge of coding principles, deep understanding of data structures and analytics, and ready to write well-packaged code for web-based testing.


Responsibilites:
Apply your expertise to help drive testing strategy and direction of major data-driven initiatives.
Work in partnership with software engineers, testers, and product managers to create new tests to the specifications of new features before release to the production website.
Write clear and concise test plans, track test coverage and key performance metrics, log and prioritize bugs as they are found. Coordinate testing efforts across multiple teams and time zones.
Increase regression test coverage by adding new automated tests and maintaining existing tests using Python and Selenium.
Apply abstraction techniques, design principles, and architectural code design daily to further a well-established testing repo.
Maintain CI/CD integrations between automated tests, Testrail, and AWS.
Contribute to the continuous improvement of the QA processes, ensure repeatability, be an advocate for quality.
Multitask with ease between projects, shifting focus as needed.
Create complex SQL statements, data audits, and automation of data processes.

Requirements:
Bachelor’s Degree in Computer Science, or related field.
5-8 years total experience in software engineering or quality assurance, with minimum 3 years of experience focused on large-scale data initiatives.
Experience with Sales Force Commerce Cloud, Tealium, Snowflake data storage.
Experience writing automated tests in Python, Selenium, Pytest, Pycharm, JavaScript.
Knowledge and application of both object oriented and functional design principles.
Intimate knowledge of advanced coding standards.
Ability to postulate positive and creative solutions to issues and present these ideas to the team.
Knowledge of SQL, HTML, CSS, Bash, React and API Testing.
Experience working in a fast-paced environment, using agile methodologies.

Nice to Haves:
Knowledge of Salesforce/Lightning, Allure, ContentStack, Launch Darkly, Yaml.
Familiarity with Jira, TestRail, GitHub, AWS.
Experience in performance testing.

Location:
Boston, MA is the preferred location for this position (Hybrid). The Company will consider a remote work arrangement in the following locations only: AZ, CO, CT, FL, GA, IL, KS, MA, MD, MI, MO, NH, NJ (except for Jersey City), NV, OH, OR, PA, NC, SC, TN, TX, VT, VA, or WI.

Additional Information:
The expected Hourly Range for this position is $ 75 to $ 90 per hour.
ASICS Applicant Privacy Notice:
https://bit.ly/3l35Une
$75 - $90 an hour
Pay may be adjusted based on applicant's work location.
Become a part of the ADI community:
ADI is taking active steps towards becoming a diverse, equitable, and inclusive workplace. We aim to engage in D&I work that permeates our organization and all employees are expected to be actively involved.
ADI is a strong, global community where we collaborate and care for each other.
We value a diversity of opinion, everyone’s input, and increasing the number of voices at the table.
You’ll have the opportunity to join the D&I task force, participate in affinity spaces, learn and grow on your anti-racist journey. We all need to know what anti-racist is so that everyone can talk about what it actually means.
We center our employees as full people. We don’t just accept difference, we celebrate it, support it, and thrive on it for the benefit of our employees, our products, and our community.

Equal Opportunity Employer Description:
At ADI, we don’t just accept diversity— we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products, and our community. ASICS Digital is proud to be an equal opportunity workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, veteran status, or fitness level."
682,Azure Data Engineer,DHL eCommerce,Remote,N,"Job Brief
Join the DHL eCommerce Data Engineering Team and be part of a very successful growing company. As Azure Data Engineer, you will be working with competent Data Engineers, Cloud Engineers, DevOps Engineers, Data Scientists and Architects.
SUMMARY:
Join the DHL eCommerce Data Engineering Team and be part of a very successful growing company. As Azure Data Engineer, you will be working with competent Data Engineers, Cloud Engineers, DevOps Engineers, Data Scientists and Architects to learn from and to share your experiences in a technologically diverse environment. You will be working with Data Engineering and Data Analytics technologies on the Azure Cloud platform.
WHAT YOU’LL DO
DHL eCommerce has set up a global big data architecture as part of our digitalization initiative. In this context, we are looking for an aspiring Cloud Data Engineer to join our growing team of data experts. You will be responsible for expanding and optimizing our Azure Data Lake, as well as optimizing the data flow and collection for cross-functional teams. Preferably, if you are interested in and have already some experiences in building data pipelines as well as processing (transforming, aggregating, wrangling) data. You will be involved in optimizing data systems and building them from the ground up. In your role, you will work with our software developers, database architects, data analysts, and data scientists on data initiatives and will ensure that the optimal data delivery is consistent throughout ongoing projects. It is mandatory that you work self-directed and are comfortable supporting the data needs of multiple teams, systems, and products. You are the right candidate to join our team if you are excited by the prospect of participating in optimizing and sometimes even re-designing our company’s data architecture to support our next generation of products and data initiatives. In addition to the development of data solutions, you are accountable for the customer’s Azure support experience driving resolution of complex critical problems from problem identification to full resolution, you will own and manage the customer/user experience and supporting key customer projects on Azure. Your day-to-day job will be about providing both technical expertise and about being an excellent communicator and a service oriented professional. You will debug, troubleshoot, correct code, and help resolve business user’s issues.
REQUIREMENTS/SKILLS:
2-4 years of experience working in a cloud data engineer role.
Practical experience in Python and at least 1 of the following programming languages: Scala, Java, Kotlin, Go, Rust, C#, F#, C, C++.
Experience building data ingestion pipelines for a cloud, with data consumable by business intelligence solutions (e. g. PowerBI) and/or data scientists.
Familiarity with the Azure cloud ecosystem, specifically with data intensive components like Azure Synapse, CosmosDB, Data Factory, Databricks.
Robust knowledge of cloud storage aspects (ADLS gen2)
Expertise in cloud security aspects
Routine command of observability approaches for the clouds: logging, monitoring, tracing.
EDUCATION AND WORK EXPERIENCE:
Bachelor’s degree in Computer Science, Information Systems, Engineering, Mathematics
2 to 4 years of professional experience with Data Management technologies and a minimum of 2 years Azure Data Factory, Azure Synapse (DWH), Azure SQL Database, Azure Data Bricks, Spark, and Data Explorer.
2 to 4 years of designing Big Data Pipelines supporting Data Science Use Cases.
6+ years of experience of IT platform implementation in a highly technical and analytical role.
PHYSICAL DEMANDS:
Physical demands are consistent with a professional office setting.
Regular sitting at workstation for 25 - 75% of the work shift.
Frequent standing and walking.
Travel (Rare, 5%)
Equal Opportunity Employer – Veterans/Disability"
683,"QA Automation Engineer II, Data",GOBankingRates,"Los Angeles, CA•Remote",N,"GOBankingRates™ is unique in the digital marketing and media industry - we combine marketing, digital, content and fintech. Our performance based approach increases brand awareness and generates targeted audience engagement on our internal web properties and partner sites.
Learn More About What We Do

How Will You Make an Impact?
Participate in Agile ceremonies, help analyze requirements, develop testing strategies, and write manual and automated test scripts for new and existing functionality.
Monitor all development cycles, prepare test data, design and execute test plans, and evaluate test results
Mentor fellow Test Engineers on the team on automation concepts and contribute towards maintaining full test coverage
Collaborate with product owners, project managers, data analysts, data scientists, and developers to refine user stories and acceptance criteria
Act as a key point of contact for all QA aspects of releases, providing QA services, UAT guidance, and coordinating QA resources internally and externally for the squad.
Suggest process improvements that enable efficient delivery and maintenance
Identify areas for cross functional testing to improve overall quality and, with peers or others, implement initiatives improving testing capability and efficiency
Have a leadership mentality when it comes to research and implementation of current and new technologies to help expand our current processes and time-to-market
What Do You Bring to Us?
Bachelors in Computer Science or Data Science or Information Systems or Mathematics or Statistics or equivalent years of relevant work experience
4+ years of experience with agile software development testing, reviewing user stories, acceptance criteria, and other available information in order to develop test plans and test scenarios, both manual and automated
4+ years of experience in database testing for relational databases( preferred Snowflake and MS SQL Server), ETL/ELT data solutions, and reporting and analytics tools
Knowledge of dimensional modeling and data warehouse concepts, such as star schemas, snowflakes, dimensions, facts
Experience in writing complex SQL queries, ability to determine the types of testing that must be conducted (i.e., data validation, regression, etc.), including evaluating the testability of requirements and create a comprehensive test plan that supports the business and technological solutions being delivered
3+ years of experience creating automated scripts using pytest for data validation
Expertise building test architecture and framework from ground up
1+ years of experience with API testing; manual (using tools like Postman/swagger) & automated (REST Assured/CURL).
Experience working with Cloud services and CI/CD tools like Jenkins
Ability to multi-task and adapt quickly to changes while maintaining urgency in completing assigned tasks
Benefits
Competitive salary with excellent growth opportunity; we pride ourselves in having a team that exudes leadership, high initiative, creativity and passion.
Awesome medical, dental and vision plans with heavy employer contribution
Paid maternity leave and paternity leave programs
Paid vacation, sick days and holidays
Company funding for outside classes and conferences to help you improve your skills
Contribution to student loan debt payments after the first year of employment
401(k) - employees can start contributing immediately. After the first year, GOBankingRates matches your contribution up to 4% of your salary
A note about our response to COVID -19 and our new norm: The world has changed and we know it's important to adapt and to do our part to take care of our teams in this global pandemic. Our number one priority is to have our team feel safe, balanced and connected. We're committed to providing our teams with the best resources and tools to navigate this new virtual world that we're living in. We've also reinvented the ways in which we recognize, celebrate, and engage with each other to keep our culture strong!
Here's a peek into our world at GOBankingRates -
Our teams are working remotely 100% for the foreseeable future and have flex time. We're in the digital media space so we're mobile and flexible!
Option to work from an office (if you need to get away!)
Tools & resources are available to keep our team connected across North America. (JIRA, Trello, Airtable, Slack, Zoom and so much more!)
To keep our community engaged and connected, virtual team building events are held weekly and monthly.
For wellness and balance, weekly virtual fitness classes such as yoga are available.
To care for the local communities that we're a part of across the U.S our team members host socially distanced philanthropic events every quarter.
And most importantly, we've committed to consistent and transparent communication to help us all stay informed, engaged and to keep us on our path to success and #greatness.
We are an equal-opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law."
684,Software Engineer - Applications & Data,"Nalu Medical, Inc.","Carlsbad, CA 92008","$80,000 - $110,000 a year","We are seeking a dynamic, highly motivated, collaborative, and talented Software Engineer with experience developing data-driven applications and database solutions.
Responsibilities:
Work with cross-functional teams to understand business requirements and develop corresponding process flows and use case definitions
Design, build, and maintain software for web database applications, cloud applications, and non-product mobile apps
Design, build, integrate, and automate scalable and secure database solutions that meet the company’s unique business needs including reporting and analytics
Evaluate technology options (including 3rd party solutions) and provide recommendation on scope and scale of effort required to develop solutions
Work with technical and non-technical staff to design features, lead implementation, and own solutions from development to production to live support
Implement security best practices to ensure data privacy and compliance
Create, maintain, and review software design specifications, interface descriptions, and other software documentation
Ensure feature quality through extensive testing (unit, integration, functional, and regression) with a focus on test automation
Requirements :
Bachelor's degree in Computer Science or related field; Master's degree preferred
Minimum of 3 years work experience with data driven applications and database solutions
Experience with cloud-based architectures, usage of RESTful APIs, SOAP APIs, and SQL databases
Experience with multiple object-oriented languages
Experience in development of applications for iOS and/or Android is preferred
Competent in software development tools (e.g. build tools, Git, Git branching strategies), best practices (e.g. unit testing, test automation, continuous integration, etc.) and defect tracking tools (e.g. JIRA)
Excellent communications skills both written and verbal"
685,"Lead Engineer, Atlas Data Federation (Storage)",MongoDB,"New York, NY 10019•Remote",N,"The worldwide data management software market is massive (IDC forecasts it to be $138 billion by 2026!). At MongoDB we are transforming industries and empowering developers to build amazing apps that people use every day. We are the leading modern data platform and the first database provider to IPO in over 20 years. Join our team and be at the forefront of innovation and creativity.
Atlas Data Lake (ADL) is a fully managed storage solution supporting large-scale analytics while maintaining the economics of cloud object storage. It automatically extracts data from MongoDB Atlas Clusters to isolate analytical workloads, allowing data scientists and business analysts to query analytical data without impacting production systems. It is also the foundation for Atlas Online Archive, which provides low-cost, tiered storage for querying infrequently-accessed data. By optimizing the storage layout for customer data during ingestion and rebalancing it as needed, Atlas Data Lake delivers great performance and scalability at reasonable cost for Atlas customers.
As the Team Lead for the ADL Storage team, you will be responsible for leading and growing the engineering team taking on challenging, high-visibility projects that improve and enhance the performance, scalability, and reliability of the distributed systems underlying Atlas Data Lake. The ADL team prides itself on building high-quality software using clean abstractions, and lives MongoDB cultural values every day – we value intellectual curiosity and honesty, and building together in an environment that prioritizes collaboration over competition. Our team is distributed across North America, supporting both in-office and remote work with convenient work hours for each time-zone.
If you are passionate about data management, distributed systems, and leadership, and are excited to work on high-impact, high-growth projects, we would love to hear from you. Join us as a Team Lead for the Atlas Data Lake Storage team and help us shape the future of analytics and data management!
This role can be based out of our New York City office or remotely in the US.
Successful candidates will have the following qualities
Track record in hiring, mentoring and growing strong software engineering teams
Excellent verbal and written technical communication skills and desire to collaborate with colleagues, mentor fellow engineers and lead projects
Solid experience in designing, writing, testing, and maintaining distributed systems and/or data storage software. Professional or advanced academic expertise in the domains of distributed computing and/or data storage is preferable but not required
Regardless of prior experience, they are willing, able, and excited to quickly learn new things in the domains of computer science and software engineering. They are curious about how people and organizations use MongoDB, and how MongoDB ADL could be improved to enable more use cases
Position Expectations
Onboard onto a highly technical product as an individual contributor to gain product knowledge before taking on managerial responsibilities
Actively participate in hiring for the ADL storage team
Lead and grow other engineers to coordinate seamless changes in a large, feature-rich code base
Work with product managers, program managers, and other teams to specify, prioritize and deliver new features that delight our users
Estimate task complexity, report progress, and voice risks for projects executed by reports
Understand and improve current functionality in ADL's data lake storage and processing system's performance and stability
Success Measures
In the first month, you'll have understood the surface area, the high level architecture of the MongoDB ADL's Data Lake storage
In three months, you'll have onboarded onto the team, and contributed to the design and development in Golang of a feature to be released to our customers
In six months, you will have taken on full responsibility for projects slated for the subsequent quarter of ADL
In nine months, you'll have initiated one-on-one's with your team members, started growth conversations with your team members and begun to understand their longer term career aspirations
In twelve months, you'll have led the development of multiple new features and performed a full annual cycle of performance and growth conversations with your team members. Additionally, you'll have contributed to the vision for the future of the ADL team and helped develop a plan to achieve that vision
To drive the personal growth and business impact of our employees, we're committed to developing a supportive and enriching culture for everyone. From employee affinity groups, to fertility assistance and a generous parental leave policy, we value our employees' wellbeing and want to support them along every step of their professional and personal journeys. Learn more about what it's like to work at MongoDB, and help us make an impact on the world!
MongoDB is committed to providing any necessary accommodations for individuals with disabilities within our application and interview process. To request an accommodation due to a disability, please inform your recruiter.
MongoDB, Inc. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type and makes all hiring decisions without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws."
686,Principal Big Data Solutions Engineer (remote option),National Instruments,United States•Remote,"$113,400 - $184,200 a year","NI is looking for a seasoned Solutions Engineer to join their Big Data analytics software division that provides end-to-end solutions enabling semiconductor and electronics brand owners to protect product quality and enhance their brand reputations in the market.
We are technically savvy and passionate about our technology. We are looking for motivated individuals who want to join our innovative team!
Description
As a Big Data Solutions Engineer you will be working with internal teams to support our big data solution which consists of the following technologies Linux, Microsoft Windows, Microsoft SQL, HP Vertica, Hadoop, and Kafka. You will be responsible for :
Working with our industry leading customers and internal teams to implement, troubleshoot and maintain enterprise big data eco systems (E.g. archiving tools, backup, data flow, data loading)
Deploying and configuring Optimal+ solutions using automation tools like Chef
Deploying and troubleshooting Optimal+ solutions in the cloud and customers on-premise data centers such as Amazon Web Services, Azure and others
Perform major and minor upgrades release and verify SQL, Vertica, Hadoop and Optimal+ patches, hotfix, new release planning
Run post upgrade system health check, daily health check, respond to monitoring alerts and log remediation
Monitor the OS, SQL, HP Vertica, Hadoop, Docker Containers and Kafka
Triage issues by review logs or examining system events to determine if the issue needs to be escalated to tier 3
Monitor and maintenance backups SQL, Vertica and Hadoop
Enterprise ticketing and support tools Service Now, Coralogix, Logic Monitor, Resolve IO, Ayehu.
Some travel and after hour support are required on an as needed basis defined by the customer 15%
Job Requirements
Degree in Computer/ Telecommunication Engineering/Information Technology/Electrical Engineering or a minimum of 10 years of progressive work experience with enterprise applications.
10+ years enterprise level support experience on Microsoft Windows, Linux RedHat/CentOS installation, support and troubleshooting
10+ years enterprise level support experience on Microsoft Server 2008-2016 installation, support and troubleshooting
Enterprise support experience, configuration and troubleshooting of highly available containerization solutions. Such as Kubernetes, Docker Swam, Etc.
Experience in support, configuration and troubleshooting of highly available enterprise networking and storage including philosophies and best practices (NAS, SANS, RAID, load balancers, VIP Etc.)
Enterprise cluster support, troubleshooting, installation of current versions of HP Vertica Clusters
Enterprise cluster support, troubleshooting, installation of current versions of EMR, and Cloudera distributions of Hadoop
Enterprise support, troubleshooting, installation of current versions of Kafka
Enterprise support experience in cloud service provides Azure, AWS, Etc.
Backup and restore strategies, industry best practices and principles RPO, RTO
Adaptable, driven, self-starter that can work independently or as part of a team that is willing go the extra mile with good interpersonal skills
Experience in semiconductor testing environment is an advantage
Knowledge in enterprise monitoring and log management tools is an advantage (Splunk, Elastic Search)
W e offer a base salary range of $113,400.00 - 184,200.00 plus competitive benefits as well as incentive compensation and/or equity for certain roles. Please note that the base salary range is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. Company benefits include health, dental and vision insurance, 401(k), flexible spending accounts, and paid leave.
Please note that pursuant to a government contract, this specific position requires U.S. citizenship status.

#LI-US1

Why NI?
There are many reasons to consider joining a company. Key among them are the people, the ideas, and the technology. At NI, we believe in the power and potential of connecting the three to create a path to success.
The people : We’re looking for curious and creative problem solvers who value diversity and fresh perspectives, are bold and kind, and willing to take chances.
The ideas : What did you want to be when you grew up? Did you want to program robots? Build flying cars? Leave the world better than you found it? At NI, we build on the big ideas of big dreamers to make their visions a reality.
The technology : With our tailored, software-connected approach, we support our customers through all phases of the product development cycle. From 5G and medical innovations to autonomous driving and the future of space travel, we help our customers Engineer Ambitiously every day.
We’ve long been globally recognized as a top employer. Our compensation and benefits are very competitive, as are our modern workspaces, career development and mobility opportunities, and a culture that fosters belonging and emphasizes community giving. We encourage our teammates to challenge the status quo and collaborate with one another to build innovative solutions.
No matter your career path, we’re here for you, for each other, and for the next generation of innovators who think bigger, aim higher, and go faster.
Are you up for the challenge of helping shape humanity for the next 100 years? If so, let’s get started, and let’s Engineer Ambitiously together.


NI is an equal opportunity and affirmative action employer, committed to providing a work environment free of discrimination on the basis of sex, race, religion, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, gender, gender identity, gender expression, age, sexual orientation, military status, veteran status, or any other basis protected by federal, state or local law.


We are changing how we work by offering more flexibility. NI has decided to take a hybrid approach (working both on- and off-site) at an aggregate level. We know that different roles have different requirements, so we’re embracing a full range of options.
Please be advised that if President Biden's Executive Order 14042, Ensuring Adequate COVID Safety Protocols for Federal Contractors, is ultimately upheld after the legal process concludes, NI will comply with the Executive Order. Accordingly, if the Executive Order is upheld in its current form, NI will require all U.S. employees to be fully vaccinated against COVID-19 and present acceptable proof of vaccination upon hire as a condition of employment. NI will consider requests for reasonable accommodation as required under applicable law. To qualify as being fully vaccinated against COVID-19, a two-week period must have passed after receiving the second dose in a 2-dose COVID-19 vaccine series or after receiving a single-dose in a single dose COVID-19 vaccine."
687,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
688,2023 Graduate - AI/ML Data Scientist/Engineer - Analytic Capabilities,Johns Hopkins Applied Physics Laboratory (APL),"Laurel, MD 20723",N,"Description

Are you searching for a place to build upon the foundation of your academic work?
Are you searching for engaging work with an employer that prioritizes impact, innovation, and personal development?
Are you looking for meaningful work in a driven intellectual environment?
If so, we're looking for someone like you to join our team at APL!
We are seeking recent college graduates to help us tackle the complex research, engineering, and analytical problems that present critical challenges to our nation. Our group is currently making critical contributions in the fight against online misinformation, developing the first truly autonomous UAV, and in the public health response to the COVID-19 pandemic. Our Covid 19 work has been recognized by Time Magazine as one of the ""Best Inventions of 2020"" and our team was named Fast Company's 2021 Innovative Team of the Year. To address these emerging national challenges, we develop software systems that enable the application of data science and artificial intelligence (AI) to a variety of domains, such as cyber operations, healthcare, social media analysis, and signal and video exploitation.
As a member of our team you will...
Work with dedicated team members charged with developing and delivering solutions that support national priorities
Work within a team environment and apply your skills to projects and tasks in areas such as Artificial Intelligence, Machine Learning, Data Science, Cybersecurity, Software DevOps, Signal and Image Processing, and Mathematics
You will meet the minimum qualifications for the job if you...
Possess a Bachelor's degree or Master’s degree in Computer Science, Mathematics, Engineering, or related technical field
Have maintained a minimum 3.0/4.0 GPA
Are able to obtain a Top Secret level security clearance. If selected, a government security clearance investigation will need to be conducted and the requirements met for access to classified information. Eligibility requirements include U.S. citizenship.

Qualifications

Why work at APL?

The Johns Hopkins University Applied Physics Laboratory (APL) brings world-class expertise to our nation’s most critical defense, security, space and science challenges. While we are dedicated to solving complex challenges and pioneering new technologies, what makes us truly outstanding is our culture. We offer a vibrant, welcoming atmosphere where you can bring your authentic self to work, continue to grow, and build strong connections with inspiring teammates.

At APL, we celebrate our differences and encourage creativity and bold, new ideas. Our employees enjoy generous benefits, including a robust education assistance program, unparalleled retirement contributions, and a healthy work/life balance. APL’s campus is located in the Baltimore-Washington metro area. Learn more about our career opportunities at www.jhuapl.edu/careers.


About Us

APL is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, creed, color, religion, sex, gender identity or expression, sexual orientation, national origin, age, physical or mental disability, genetic information, veteran status, occupation, marital or familial status, political opinion, personal appearance, or any other characteristic protected by applicable law.

APL is committed to promoting an innovative environment that embraces diversity, encourages creativity, and supports inclusion of new ideas. In doing so, we are committed to providing reasonable accommodation to individuals of all abilities, including those with disabilities. If you require a reasonable accommodation to participate in any part of the hiring process, please contact Accommodations@jhuapl.edu. Only by ensuring that everyone’s voice is heard are we empowered to be bold, do great things, and make the world a better place."
689,Senior AI Data Engineer,Tensure Consulting,"Cincinnati, OH•Remote","$130,000 - $148,825 a year","Tensure is a fully-remote technology consulting firm focused on Cloud, Applications and Data:
We started Tensure with the mission to create freedom for others: freedom to be who you are at work, freedom to pursue a career in technology, freedom from the constraints that keep good products from becoming great ones. Tensure is a fully-remote technology consulting firm focused on Cloud, Applications and Data.
Are you an AI Data Scientist with Vision AI experience? Do you have experience running after problems and being a great person to work with? We are looking for a Senior AI Data Scientist who has experience deploying and building machine learning models to various environments.
Responsibilities:
Developing and implementing machine learning models for tasks such as object recognition, image classification, and facial recognition.
Developing algorithms to combine result sets from multiple angles, clean result sets, and count objects
Monitoring and maintaining deployed models, troubleshooting and fixing problems as they arise
Building and deploying machine learning models using Vertex AI platform
Training and evaluating models using large datasets of labeled video and images
Optimizing model performance by experimenting with different algorithms and hyperparameters
Integrating the models with other systems, such as cloud services, to enable real-time data processing
Collaborating with other members of the team such as data scientists, engineers, and product managers to integrate vision AI models into products and services
Communicating results and findings to stakeholders and other teams, and participating in the development of technical reports and publications
What we are looking for:
6+ years experience software development, machine learning, and data analysis
Experience working Vision AI, Vertex AI
Understanding of Automated Machine Learning techniques
Development experience in Python
PySpark
Experience in database systems (SQL or NoSQL)
Experience with Google Cloud platform and its services
Technologies needed:
Vision AI
VertexAI
AutoML
Some of our best benefits:
100% Remote
Insurance (Health, Dental, Vision, Life)
We contribute 3% of your base salary to a 401k (regardless of your contribution)
5 weeks Paid Time Off + 10 Company Holiday
Parental leave
Referral program
A transparent pay structure with a clear path to promotion
What to expect during your interview process:
An initial screening interview
A technical interview (pairing, case studies and/or hypothetical questions about how you would solve certain challenges)
An interview with our sales team
A culture and values interview
Offer Letter sent via email (or a decline with feedback)
Salary: $130,000 - $148,825 (Depending on experience and skill level)"
690,Principal Data Engineer,Zignal Labs,Remote,"$170,000 - $190,000 a year","About Zignal Labs

Zignal Labs’ real-time intelligence technology helps the world’s largest organizations protect their people, places, and position. Analyzing billions of data points in real time, Zignal's AI-powered platform accelerates mission-critical decision making by empowering leaders with contextual situational awareness of the information environment.

Fully remote, with Silicon Valley roots and team members in over 20 states, Zignal serves customers around the world. Learn more at zignallabs.com.

As the Principal Data Engineer on the Platform team at Zignal Labs, you will get to use your Scala and Java experience to build a best-in-class distributed data and analytics infrastructure by leveraging open source technologies such as Apache Spark, Apache Storm, and Elasticsearch. We use social media, news, blogs and other media sources to empower our users with key insights based on real-time analysis.
In this role, you will have the opportunity to:
Solve complex real-time data collection & analysis problems with cutting edge technical solutions
Iterate on our high performance and scalable platform for massive data collection, real-time analytics, NLP, machine learning, and backend data services
Build high performance, scalable, real-time, server-side technologies
Write scalable code with extensive test coverage, working in a professional software engineering environment with source control, dev/stage/production release cycles, continuous integration, and deployment
Work closely with product management, design, quality assurance and operations teams to understand our customers’ needs and effectively translate them to technical specifications
Lead projects from translating product requirements into architecture to production

Tech Stack:
Scala, Java, Python
Apache Spark, Spark Streaming, Databricks/Delta Lake, Apache Storm, Elasticsearch, Apache Nifi
Kafka, MongoDB, Redis
AWS

In order to be successful in this role, you will need:
Bachelor's degree (or higher) in Computer Science, Engineering, or similar and/or relevant work experience
Experience providing technical leadership at the enterprise level for the design of information technology systems
Crafted and implemented operational data stores, as well as data lakes in production environments
Ability to analyze, diagnose and resolve complex architectural problems using industry standard engineering principles
Design and build data ingestion pipelines and ETL processing, including stream processing, while factoring in performance and cost
Identify and solve issues concerning data management to improve data quality
Clean, prepare and optimize data for ingestion and consumption
Experience solving performance problems with Lucene based search solutions like Elasticsearch or Solr
9+ years experience in server-side/back-end full cycle product development in a production environment
4+ years developing with Apache Spark, including Structured Streaming. Experience with Databricks is a big plus
Knowledge of Scala or Java with exposure to or interest in Scala
Leads and mentors other team members
Provides partners with coaching and feedback in order to build effective teams
Provides effective support to cross-functional teams

$170,000 - $190,000 a year
The salary range is applicable to the general US and may be adjusted based on geographic location. Compensation decisions depend on the circumstances of each case, including skill sets, experience, certifications, and business and organizational needs.
Why join Zignal Labs?
Competitive salary based on the work you do
Flexible time off – work with your manager to take the time you need
Excellent medical, dental, and vision coverage
Paid parental leave plan
Professional development and growth programs
A tight knit, collaborative, and transparent environment to help you succeed

Zignal Labs is proud to be an Equal Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.

Applicants must be authorized to work in the United States for any employer. No sponsorship is available for this position."
691,Data Engineer,Comcast,"Philadelphia, PA 19103",N,"Comcast’s Technology, Product & Experience organization works at the intersection of media and technology. Our innovative teams are continually developing and delivering products that transform the customer experience. From creating apps like TVGo to new features such as the Talking Guide on the X1 platform, we work every day to make a positive impact through innovation in the pursuit of building amazing products that are enjoyable, easy to use and accessible across all platforms. The team also develops and supports our evolving network architecture, including next-generation consumer systems and technologies, infrastructure and engineering, network integration and management tools, and technical standards. In most cases, Comcast prefers to have employees on-site collaborating unless the team has been designated as virtual due to the nature of their work. If a position is listed with both office locations and virtual offerings, Comcast may be willing to consider candidates who live greater than 100 miles from the office for the remote option.

Job Summary
Comcast is seeking a Data Engineer in the CONNECT organization. The role provides candidate with broad exposure and opportunity to up level the Cybersecurity maturity of multiple lines of business and next generation platforms utilized for providing high-speed Internet services. First and foremost an ideal candidate must demonstrate high degree of self-initiative and adaptability to work on multiple security work streams in a fast-paced environment. The candidate should ideally possess a technical background in security threat/risk management, background in database administration, and python. Lastly, the candidate is required to collaborate with stakeholders in peer security and technology groups to strengthen overall security posture.
Job Description
Core Responsibilities
Monitors reporting platform to support security remediation of security vulnerabilities across all platforms and services
Maintains SQL database for a reporting platform for security vulnerabilities to owners
Identifies, documents, and recommends reports and dashboards for trending data issues for teams
Stays current with security technologies, trends, vulnerabilities and threats
Follow our agile process, working in sprints together with the rest of the team to deliver on time with technical quality
Continuously improve yourself and the products you work on
Supports building a culture of security by educating others and advocating an open security
posture
Consistently exercises best judgment and discretion in matters of significance
Other duties and responsibilities as assigned
Skills & Experience
Experience in the area of Data Analysis
SQL databases
Frontend dashboard tools
Python
Hands on experience with Linux/Unix
Familiarity AWS, Openstack
Familiarity with Information Security (vulnerability testing, risk analysis, and security assessments)
Excellent written and verbal communication skills, interpersonal and collaborative skills
Must have strong problem-solving and analytical skills
High degree of initiative and be well organized
Ability to manage multiple projects with strict timelines
High level of personal integrity
Enjoys working in a demamding and dynamic enviroment
Disclaimer:
This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications.
About Our Perks & Benefits
We are determined to build an environment where our employees feel valued, understand our business goals, and are motivated.
Here's a look at just some of the perks and benefits we make available to our US-based employees:
Medical & Dental
401(k) Savings Plan
Generous paid time off
Life Milestones - from adoption assistance, childcare resources, pet insurance, and more, Comcast supports you at all life stages.
Courtesy Services - We offer all of our full-time employees in serviceable areas free digital TV and internet.
Discount tickets for Universal Resorts, including theme park tickets and onsite hotel rooms.
Learn more at https://jobs.comcast.com/life-at-comcast/benefits (https://jobs.comcast.com/life-at-comcast/benefits)
Reasonable Accommodation:
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment. Please contact us to request accommodation.
Comcast is an EOE/Veterans/Disabled/LGBT employer.
=== THIS POSITION IS INELIGIBLE FOR VISA SPONSORSHIP. TO BE CONSIDERED FOR THIS ROLE, YOU MUST BE LEGALLY AUTHORIZED TO WORK IN THE UNITED STATES AND NOT REQUIRE SPONSORSHIP FOR EMPLOYMENT NOW OR IN THE FUTURE. ===
Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other basis protected by applicable law.

Education
Bachelor's Degree
While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.
Relevant Work Experience
2-5 Years

Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That’s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality – to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details."
692,Sr. Data Engineer - HYBRID,Exelon Corporation,Remote,N,"Description
We're powering a cleaner, brighter future.

Exelon is leading the energy transformation, and we're calling all problem solvers, innovators, community builders and change makers. Work with us to deliver solutions that make our diverse cities and communities stronger, healthier and more resilient.

We're powered by purpose-driven people like you who believe in being inclusive and creative, and value safety, innovation, integrity and community service. We are a Fortune 200 company, 19,000 colleagues strong serving more than 10 million customers at six energy companies - Atlantic City Electric (ACE), Baltimore Gas and Electric (BGE), Commonwealth Edison (ComEd), Delmarva Power & Light (DPL), PECO Energy Company (PECO), and Potomac Electric Power Company (Pepco).

In our relentless pursuit of excellence, we elevate diverse voices, fresh perspectives and bold thinking. And since we know transforming the future of energy is hard work, we provide competitive compensation, incentives, excellent benefits and the opportunity to build a rewarding career.

Are you in?

PRIMARY PURPOSE OF POSITION
Performs and/or manages activities relating to planning, designing, building, and maintaining high quality solutions, products and processes. Creates and assigns detailed tasks to subordinates. Has budget responsibility for a small project, sub-project or process component. Expected to work under minimal supervision.

PRIMARY DUTIES AND ACCOUNTABILITIES
Perform, manage and appropriately document work activities relating to projects, sub-projects, or processes. For projects: plan, design, and build high quality IT software solutions in accordance with IT project management standards. For operations and application maintenance and support: plan and lead IT activities required to manage service level agreements. Assist others in planning and prioritizing work and work schedule.
Assist in creation of documentation for products and services. Use best practices to improve products and services provided to business unit partners, and monitor adherence within Team/Group to standards as defined within the Management Model
Maintain and enhance engagement with business and IT partners and other stakeholders
Establish positive team environment by proactively assisting and training less experienced personnel. Provide performance and development feedback as required
Maintain technical knowledge and business acumen within own discipline or function

JOB SCOPE
Utilize understanding of customer's business needs to determine requirements
Applies technical expertise to plan, design, build or support required products and processes
Help other team members learn appropriate theories, practices and principles that relate to their skill set portfolio
As assigned, manage budget for area of responsibility
As assigned, manage working relationship with outsourcing partners
Qualifications
MINIMUM QUALIFICATIONS
Bachelor's degree in Computer Science or related discipline and 4-7 years' experience (ex: Data warehouse design, business intelligence reporting, Customer Information Systems, Meter Data Management Systems and/or Outage Management Systems) or in lieu of experience, 6-9 years equivalent combination of education and work experience.
Appropriate technical skills and experience with tools used for data modelling, process modelling, documenting functional and technical requirements
Strong knowledge of business practices, processes, data and applications
Experience leading small IT projects or sub-teams and knowledge of IT project management.
Strong problem solving and analysis ability.
Excellent communications skills (written and verbal)
Strong facilitation/negotiation skills
Strong presentation skills
Ability to work with remote project teams
PREFERRED QUALIFICATIONS
Experience with SAP HANA databases
Experience with SAP CRM&B and Business Warehouse applications
Experience with developing reports in SAP Business Objects
Experience writing technical design specifications for data integrations and data transformations with structured and unstructured data
Experience with Scrum development methodology
Experience with writing business cases
Experience in capturing, analyzing and translating business needs into functional IT requirements
Experience developing data integrations and data transformations with structured and unstructured data
Application re-engineering, data modeling and performance tuning
Experience with Customer Information Systems (for example:CC&B, SAP CRM&B)
Experience with AMI Meter Data Management Systems (for example: Oracle Utilities MDM, Itron IEE)
Experience managing IT projects using Agile/Scrum and waterfall methodologies"
693,Data Engineer,Ford Motor Company,"Dearborn, MI•Remote",N,"Job Description & Qualifications
At Ford Motor Company, we believe freedom of movement drives human progress. We also believe in providing you with the freedom to define and realize your dreams. With our incredible plans for the future of mobility, we have a wide variety of opportunities for you to accelerate your career potential as you help us define tomorrow’s transportation.

Ford Motor Company is proud to be an Emerald sponsor of the Grace Hopper Conference, and we are excited to be a part of this event. Whether you are in person in Orlando, or attending virtually, we would love to hear from you!

As a Cloud Data Engineer, you will leverage your technical expertise in data, analytics, cloud technologies, and analytic software tools to identify best designs, improve business processes, and generate measurable business outcomes.

What you'll be able to do:
Develop EL/ELT/ETL pipelines to make data available in BigQuery analytical data store from disparate batch, streaming data sources for the Business Intelligence and Analytics teams
Work with on-prem data sources (Hadoop, SQL Server), understand the data model, business rules behind the data and build data pipelines (with GCP, Informatica) for one or more Ford verticals. This data will be landed in GCP BigQuery.
Build cloud-native services and APIs to support and expose data-driven solutions.
Partner closely with our data scientists to ensure the right data is made available in a timely manner to deliver compelling and insightful solutions.
Design, build and launch shared data services to be leveraged by the internal and external partner developer community.
Building out scalable data pipelines and choosing the right tools for the right job. Manage, optimize and monitor data pipelines.
Provide extensive technical, strategic advice and guidance to key stakeholders around data transformation efforts. Understand how data is useful to the enterprise.

The minimum requirements we seek:
Bachelor’s Degree in Computer Science, Electrical Engineering or a related field, or a combination of education or equivalent experience.
2+ years of experience with SQL and Python
2+ years of experience with GCP or AWS cloud services; Strong candidates with 5+ years in a traditional data warehouse environment (ETL pipelines with Informatica) will be considered
2+ years of experience building out data pipelines from scratch in a highly distributed and fault-tolerant manner.

Our preferred qualifications:
Master’s degree in Computer Engineering, Computer Science, or a related field of study.
Experience with GCP cloud services including BigQuery, Cloud Composer, Dataflow, CloudSQL, GCS, Cloud Functions and Pub/Sub.
Inquisitive, proactive, and interested in learning new tools and techniques.
Familiarity with big data and machine learning tools and platforms. Comfortable with open-source technologies including Apache Spark, Hadoop, Kafka.
1+ year experience with Hive, Spark, Scala, JavaScript.
Strong oral, written and interpersonal communication skills
Comfortable working in a dynamic environment where problems are not always well-defined.

What you will receive in return:
As part of the Ford family, you will enjoy excellent compensation and a comprehensive benefits package that includes generous PTO, retirement, savings and stock investment plans, incentive compensation, and much more. You will also experience exciting opportunities for professional and personal growth and recognition. If you have what it takes to help us redefine the future of mobility, we would love to have you join us. Candidates for positions with Ford Motor Company must be legally authorized to work in the United States. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is available for this position.

We are an Equal Opportunity Employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status. https://corporate.ford.com/content/dam/corporate/us/en-us/documents/careers/2022-benefits-and-comp-GSR-sal-plan-2.pdf

At Ford, the health and safety of our employees is our top priority. Vaccination has been proven to play a critical role in combating COVID-19. As a result, Ford has made the decision to require U.S. salaried employees to be fully vaccinated against COVID-19, unless employees require an accommodation for religious or medical reasons. Being fully vaccinated means that an individual is at least two weeks past their final dose of an authorized COVID-19 vaccine regimen. As a condition of employment, newly hired employees will be required to provide proof of their COVID-19 vaccination or an approved medical or religious exemption.

Auto req ID
64580BR
State
Michigan
Skill Team
Global Data Insight & Analytics (GDIA)
Sub-Component
Operations Analytics
Virtual/Remote Position
Yes"
694,Data Engineer,Jahnel Group,"Iowa City, IA 52240•Remote",N,"Jahnel Group’s mission is to provide the absolute best environment for software creators to pursue their passion by connecting them with great clients doing meaningful work. This is a full time position with one of our closest clients.
JOB OVERVIEW
Revology is a newly formed organization that provides technology-enabled revenue cycle management (RCM) services to hospitals, health systems, and physician groups.
We are currently searching for a Data Engineer to manage vital data infrastructure for large-scale data processing to support the functionality of Revology’s revenue cycle applications. This role will work with enterprise and client leaders to translate business and functional requirements into technical specifications.
The ideal candidate for Revology is analytical, strategic and a team player that is comfortable in a fast-paced, dynamic environment
RESPONSIBILITIES:
Develop and maintain efficient and scalable data pipelines that integrate data from multiple sources into common data models.
Transform raw data into usable information that can be consumed by client and enterprise organizations.
Collaborate with cross-functional Agile teams to design, develop, test, implement, and support technical solutions using full-stack development tools and technologies.
Support data science, research, and data analysis efforts by making data operationally available for products and services.
Partner with product managers, software engineers, and data scientists to deliver cloud-based solutions that drive powerful experiences.
Provide input to risk management; report risks as they are identified and participate in prioritization/follow up
Stay current with emerging technologies and advancements within existing technologies.
Positively and deliberately engage with colleagues – external and internal – to foster collaborative and productive relationships.
Stay curious, kind and contribute positively to the revology culture. The health + harmony of the team is everybody’s responsibility at revology.
QUALIFICATIONS:
3+ years of data engineering experience in big data solutions
Experience in database technology, architectures, ETL patterns and processes
Experience working with complex and varied data sources
Experience with relational SQL and NoSQL databases
Excellent analytical, strategic conceptual thinking, strategic planning, and execution skills
2+ years of experience working with Pyspark & AWS Glue
Demonstrated ability to merge technical and business requirements into solutions that can be consumed by all areas of the business
Experience working in HIPAA, HITRUST or advanced compliance environments
An understanding of the SaaS development, business model, and metrics
Familiarity with service business metrics such as “cost to serve”
Ability to engage productively in innovative and visionary brainstorming sessions while contributing your ideas, thoughts and suggestions
AWS certifications are especially desirable
Bachelor’s Degree in Information Technology, Computer Science, OR related field is preferred but not required
PHYSICAL REQUIREMENTS
Must be able to perform physical activities, such as, but not limited to: moving or handling (lifting, pushing, pulling and reaching overhead) office equipment and supplies weighing 1 to 25 lbs. unassisted. Frequently required to sit, stand or walk for extended periods during the workday. Manual dexterity and visual acuity required. Must be able to communicate effectively on the telephone and in person.
WORKING CONDITIONS
Work will generally be performed indoors in an office environment. Must maintain a professional appearance and manner.
EMPLOYMENT ELGIBILITY
Candidates must be legally authorized to work in the United States without sponsorship."
695,Data Engineer,Nyla Technology Solutions,"Washington, DC•Remote",N,"Description
We are pursuing individual(s) for the position of Data Engineer. If this position sounds like a good fit and you have a track record of delivering solid results, please apply.
The Data Engineer designs strategies for enterprise database systems and set standards for operations, programming, and security. Design and construct large relational databases. Integrate new systems with existing warehouse structure and refine system performance and functionality. The candidate will possess and apply expertise on multiple complex work assignments. Assignments may be broad in nature, requiring originality and innovation in determining how to accomplish tasks. Operates with appreciable latitude in developing methodology and presenting solutions to problems. Contributes to deliverables and performance metrics where applicable.
This is a full time remote role that has an as needed onsite requirement in the DC Metro Area.
Required Skills
The candidate will have 3 to 10 years of experience and a BA/BS or MA/MS degree. Candidate will be able to perform all functional duties independently.
About Nyla Technology Solutions
Nyla Technology Solutions is a Women-Owned Small Business that is forward-thinking and bold at every step which has earned us a solid reputation of being technical trendsetters within the industry. Headquartered in the heart of Baltimore City, Maryland, Nyla delivers exceptional software systems engineering services for the U.S. Government. Our customers like how we tackle their toughest problems., and so Nyla is adding people who have a passion for doing fun, impactful work. If you are a person who welcomes opportunities to apply your skills in new ways, Nyla has challenges for you. We seek out people with agile, diverse mindsets who are looking for a place to grow—professionally and personally. We create opportunities for you to share your knowledge and experience with the team, and learn from others via training, mentoring, and movement across the many contracts Nyla supports. Nyla endeavors to give back to our community—lending our energy and talents to support local area organizations helping people in need. At Nyla, you will have a place to grow, get, and give where you are passionate.
How We Show Up Every Day
Team Nyla shows up every day with the intention to be awesome—proactively working to accurately interpret and fully understand our client’s challenges and add value to solving those challenges. Knowing the mission, context, and end-users is how we deliver optimal, creative, and innovative solutions. We design our systems with precision, logic, and adaptability, solving the technical and engineering needs of today with an eye on scaling for tomorrow. We endeavor to meet the highest standards of personal conduct and act as a trusted-provider for our clients-this is how we develop and maintain our enduring relationships with customers and business partners.
Progress Through Our People
Nyla’s culture is built on the understanding that together, with our collective energy, talent, and fierce determination, we never stop believing and building the impossible. We are life-long learners, ever-curious and not afraid to dream.
If you have the unique experience and expertise we are seeking, along with the desire and determination to invest your time and energy as a part of Nyla’s Team, we will provide you with a first-class compensation package along with our uniquely Nyla benefits.
We Have the Total Package | Healthy Lifestyle, Learning, Health and Wellness, Financial Wellness, Financial Protection. From the start, Nyla is different. We never embraced the idea of a workplace and policies stuck in the industrial age-we treat our employees as valued contributors, not commodities. At Nyla, we strive to understand the needs of our workforce and are committed to continuously evolving practices and policies, making adjustments that meaningfully address the changing needs and desires of our workforce.
At Nyla we talk about employee investment, not employee costs—a small way we outwardly provide our great people with benefits that cover your overall well-being. Nyla offers group benefits, 100% paid by Nyla, that include CareFirst medical, dental, and vision coverage. To support your personal goals for overall well-being and health, we provide a healthy living and fitness benefit of up to $500.00 per anniversary year. We believe that an ongoing investment in your professional learning and development is a cornerstone to our success, so we go all-in and provide up to $5,000 per anniversary year to support your growth or tuition assistance of up to $5,250 per year. We want to protect you and your future—Nyla provides, at no cost to you, short-term and long-term disability, and life insurance. And, to augment your financial beyond today’s salary, Nyla offers up to 10% employer contribution/match in our retirement plan. Lastly, we understand the importance of recharging and replenishing yourself, so we provide new employees with 4 weeks of Paid Annual Leave and 11 holidays, plus each year you are with the company, you get an extra day off.
We are an equal opportunity employer-but we are more than that – we are open-minded, and care only about your capability, your drive, and your desire and determination to contribute your gifts and talents fully."
696,"Engineer, InfoSec Consulting - Data Protection",Royal Caribbean Group,United States,N,"POSITION SUMMARY:
The Engineer, InfoSec Consulting is an expert in multiple technologies and works on a “strike team” that deploys securely transformative technologies to enable the business. Their goal is to be a security and IT evangelist that identifies innovative solutions, creates secure patterns, and validates that they are deployed correctly. This may include deploying new security tools, embedding in high-profile projects, and rapid execution of mergers and acquisitions.

This role requires developing in-depth knowledge of our security standards and baselines, assessing new system architecture against them, and finding ways to deliver security with enabling business agility.

The Engineer, InfoSec Consulting reports to the Director of Business Enablement within the Global Information Security team.

ESSENTIAL DUTIES AND RESPONSIBILITIES:
Serve as highly technical security experts to bring security transformation to both new and legacy infrastructure.
Deploy new technologies quickly, and successfully transition new platforms to the Security Operations Team for ongoing support
Embed with project teams to understand complex architectures, apply correct security controls, and ensure they are governable in the future
Development of patterns and validation of those patterns through engagement with business and lab work
Generation of strategies that brings security ahead of business need and provide proactive solutions
Effective communication that is both written and spoken with great follow up skills
Ability to drive outcomes and become the multidisciplinary expert that enables engagement

QUALIFICATIONS:
6+ years of information technology experience, with 3+ years of specialization in Information Security roles that include multiple areas of specialization.
Professional-level knowledge in 2+ of the following:
Security Engineering & Architecture assessments
Network Security Design (including IPS, WAF, and cloud-native protections)
Zero Trust Design & Implementation
Cloud Security Architecture & Serverless Infrastructure
DevSecOps Practices
API Security and driving reuse of capabilities across the organization
Data Security (identification & controls of sensitive data, including DLP)
Secrets Management
Identity and Access Management (including User Lifecycle Management)
Endpoint Management
Encryption and Certificates/PKI
Entrepreneurial approach to solving security problems with consultative skills"
697,Software & Data Engineer (Remote - Latin America),Fractal River,Remote,N,"Fractal River is a consulting firm that helps growth-stage startups scale faster. It does so by working hands-on with the startups to build the technical and operational infrastructure they need to scale, while at the same time transferring knowledge and helping them develop their own internal capabilities.

We create and manage analytics infrastructure and cloud-based systems, with technologies such as Python, SQL, Docker/Linux, and serverless capabilities and infrastructure (Snowflake, PostgreSQL, RedShift, Lambda, etc.) hosted within the Amazon or Google clouds. We also configure and set up a variety of systems along the startup's customer journey including HubSpot, Salesforce, and ZenDesk.
We are seeking a Data Engineer to join our team.

What you'll do:
Help design and implement data pipelines and analytic infrastructures with the latest technologies.
Create beautiful dashboards and reports, and work with customers to create self-service exploration data cubes using Looker.
Develop Python & SQL components to integrate and automate a variety of processes and tools.
Leverage APIs from multiple systems to extract and update data, trigger and monitor processes and in general help tie our customers' infrastructures into cohesive platforms that power their growth.
Maintain and oversee cloud infrastructure to ensure it is running with the reliability and performance our customers expect.
Help create Data Models, best practices, and technical documents for our customers.
Develop our internal best practices, policies, and processes regarding DevOps and DataOps.
Coordinate projects, activities, and tasks to ensure objectives and key results are met.
Help identify opportunities, generate innovative solutions, and improve existing product features.
Who we are looking for:
Our ideal candidate is someone with 2-5 years of working experience in fast-paced environments, a high tolerance for ambiguity, and a passion for constant learning. We're looking for motivated, highly adaptive people who enjoy the challenge of working with brilliant people in multiple, innovative startups to put in place technical infrastructures that power their growth. They must be able to interact with customers and perform not only technical work but can understand needs, extract requirements, and design and propose solutions as well.

Candidates must ideally also:
Be adept at data analysis and process improvement.
Work collaboratively but are also able to own a project with little guidance
Thrive in an environment with constant and quick iterations.
Seek out creative solutions to challenging problems.
Have strong communication skills in English (CEFR level C1 is a must, see details (https://bit.ly/cefr-english)).
Have strong attention to detail.
Be wizards in Python and/or speak SQL as a second language.
Be very familiar with databases & data warehouses: Snowflake, Redshift, BigQuery, Postgres, MySQL, etc.
Be comfortable creating complex data models and visualizations.
Be familiar with development tools (Terraform, GitHub, VSCode, CircleCI, Docker, dbt).
Be knowledgeable of AWS environments (EC, Lambda, IAM, SQS, RDS, Kinesis, VPC, etc.)
Like to work in a very diverse work environment and be comfortable with non-hierarchical organizations.
Have a bachelor's degree in computer science or similar.
Google Professional Data Engineer / Cloud Developer certifications are a plus.
AWS associate or professional-level certifications (Solutions Architect, Developer, DevOps Engineer) are a plus.
Knowledge of Google Analytics, Salesforce, HubSpot, and/or ZenDesk is a plus.
Knowledge of Looker or Tableau is a plus.
Besides the benefits that as per law you are entitled to, we offer:

Personal development plan with accelerated career track.
Access to an extensive reference library of books, case studies, and best practices.
Unlimited budget for work-related books.
Support for certifications (time to study, sandboxes, all related costs) including AWS, Google, Microsoft, and other technologies.
Special bonuses for certifications acquired.
Online training (Udemy, nanodegrees, etc.), English language training.
Stretch assignments, pair-programming and code reviews with new technologies.
Yearly performance bonus based on personal performance.
Yearly success bonus based on company performance.
Home office setup including fast internet, large monitor, and your favorite keyboard and mouse. After a few months, gaming chair, espresso maker, standing desk, and speakers (or other items like these).
Monthly wellness budget to cover items such as sodas, tea, snacks, pet care, yoga, or other wellness-related items.
Company card for wellness budget and company expenses.
Private healthcare contribution.
Three floating holidays and unlimited (but reasonable) PTO starting the first year.
Fun company off-sites!
We are proud to be an equal opportunity employer."
698,Data Engineer,StackAdapt,United States•Remote,N,"StackAdapt is a self-serve advertising platform that specializes in multi-channel solutions including native, display, video, connected TV, audio, in-game, and digital out-of-home ads. We empower hundreds of digitally-focused companies to deliver outcomes and exceptional campaign performance everyday. StackAdapt was founded with a vision to be more than an advertising platform, it’s a hub of innovation, imagination and creativity.

We're looking to add Data Engineers to our data team! This team works on solving complex problems for StackAdapt's digital advertising platform. You'll be working directly with our data scientists, data engineers, Engineering team, and CTO on building pipelines and ad optimization models. With databases that process millions of requests per second, there's no shortage of data and problems to tackle.

Want to learn more about our Data Science Team: https://alldus.com/ie/blog/podcasts/aiinaction-ned-dimitrov-stackadapt/
Learn more about our team culture here: https://www.stackadapt.com/careers/data-science
Watch our talk at Amazon Tech Talks: https://www.youtube.com/watch?v=lRqu-a4gPuU

StackAdapt is a Remote First company, and we are open to candidates located anywhere in the US for this position.
What you'll be doing:
Design modular and scalable real time data pipelines to handle huge datasets
Understand and implement custom ML algorithms in a low latency environment
Work on microservice architectures that run training, inference, and monitoring on thousands of ML models concurrently
What you'll bring to the table:
Have the ability to take an ambiguously defined task, and break it down into actionable steps
Have deep understanding of algorithm and software design, concurrency, and data structures
Experience in implementing probabilistic or machine learning algorithms
Interest in designing scalable distributed systems
A high GPA from a well-respected Computer Science program
Enjoy working in a friendly, collaborative environment with others
StackAdapters enjoy:
Competitive salary + equity
401K matching
3 weeks vacation + 3 personal care days + 1 Culture & Belief day + birthdays off
Access to a comprehensive mental health care platform
Full benefits from day one of employment
Work from home reimbursements
Optional global WeWork membership for those who want a change from their home office
Robust training and onboarding program
Coverage and support of personal development initiatives (conferences, courses, etc)
Access to StackAdapt programmatic courses and certifications to support continuous learning
Mentorship opportunities with industry leaders
An awesome parental leave policy
A friendly, welcoming, and supportive culture
Our social and team events!
#LI-KR1

StackAdapt is a diverse and inclusive team of collaborative, hardworking individuals trying to make a dent in the universe. No matter who you are, where you are from, who you love, follow in faith, disability (or superpower) status, ethnicity, or the gender you identify with (if you’re comfortable, let us know your pronouns), you are welcome at StackAdapt. If you have any requests or requirements to support you throughout any part of the interview process, please let our Talent team know.

About StackAdapt

We've been recognized for our high performing campaign conversion rates, award-winning customer service, and innovation by numerous industry publications including:

2023 Best Workplaces for Women by Great Place to Work®
Top 20 on Ad Age's Best Places to Work 2023
#1 DSP on G2 and leader in the Video and Cross-Channel Advertising Categories
A Top Growing Company in Canada based on the Globe and Mail's 2022 Business Report
Named an Enterprise Fast 15 Winner for 2022, as part of the Technology Fast 50™ Program

#LI-Remote"
699,Data Engineer - Innovating for a Better Future,1904labs,"St. Louis, MO 63136",N,"About Us
At 1904labs, we are a human-centered technology company that focuses on using modern tools and technologies to solve enterprise organizations' digital transformation challenges. We look for innovative, courageous, intensely curious, and team-minded people. We value people skills and technical skills highly. At 1904labs, we are committed to providing each team member with opportunities that will allow personal and professional growth while working in a team-based model using our HCDAgile methodology. We strive to solve problems innovatively while keeping humans at the center.
The Role
As a Data Engineer at 1904labs, you will be part of a team working on complex projects that solve real-world problems for Fortune 500 and Global 2000 companies. We are seeking someone who is passionate about using modern tools and technologies to create solutions that work for people, meet and exceed the needs of businesses, and work elegantly and efficiently.
If you’re someone who is intensely curious, seeks first to understand, challenges assumptions, and always has the best interest of the client in mind, we’d like to meet you.
Basic Qualifications
Proficient in at least one programming language. More is better, as is experienced with functional and object-oriented languages. For example: Scala, Java, or Python.
Proven and recurring success applying software engineering practices like requirements gathering, design, and testing on a team.
Working knowledge of the Linux command line and shell scripting
Proficient in SQL
Comfortable leading discussions with clients, acting as a guide to help them accomplish their tech goals.
Impact
As a Data Engineer and Consultant at 1904labs, you will have the opportunity to make an impact in several ways:
Collaborate with intensely curious and innovative teammates to create data solutions that improve the lives of our clients
Guide clients to make data-driven decisions that positively impact their business and customers.
Use modern technologies and tools to create solutions that are innovative, efficient, and user-friendly.
Create inclusive, equitable, and user-friendly software & solutions that empower diverse communities and deliver high value to our clients.
A Day in The Life
Attend Agile ceremonies and other meetings to collaborate with your team, divide up user stories, and solve problems based on your individual skills.
Work with clients and internal teams to design, and implement data solutions using a variety of programming languages and technologies, including, but not limited to
Programming languages (ie. Scala, Java, Python)
Relational Databases (ie. PostgreSQL, Oracle, SQL Server)
Non-relational Databases (ie. MongoDB, ElasticSearch, Solr, Redis)
Big Data Technologies (ie. Spark, Hive, HBase/Cassandra)
Cloud-based data ecosystems (ie. AWS, Azure, or GCP)
Streaming Technologies (ie. Spark Streaming, Kafka, or Nifi)
Utilize a range of data modeling techniques to build data warehouses, data lakes, and operational applications.
Use your expertise to help clients make sense of their data and build insights for their businesses.
Analyze data to find trends, patterns, and insights and present those to the clients to help them make data-driven decisions.
Create and maintain documentation to ensure a smooth workflow, knowledge sharing, and efficiency.
Stay up-to-date with the latest trends in data engineering and share your knowledge with the team.

Why Choose 1904labs?
At 1904labs, we believe that innovation leads to progress, and we value creativity, inclusivity, and personal growth. Here are some reasons to choose 1904labs:
Innovation Hours: We believe in giving our employees time and space to innovate and work on their own ideas and projects.
Innovation First - work where it makes sense: What matters is collaborating with your team and meeting your individual commitments. So long as that remains true, each day we want you, in conversation with your team, to work from where it makes the most sense
Competitive salary, bonus, and benefits package, including medical, dental, and paid parental leave.
Personal and professional growth: We invest in the personal and professional growth of our employees, crafting career paths tailored to their aspirations.
Experienced team: Our teams consist of intensely curious and innovative thinkers from a variety of disciplines who inspire each other to come up with the best solutions possible.
Trusted advisor to our clients: We work with our clients to effect change within their organizations and create solutions that make an impact.
Inclusion inspires innovation: We believe that a diverse workforce builds better, more inclusive software.
Flexible schedules and human-centered approach to work.
Strong commitment to the community: Take advantage of the relationships we’ve built with our established community partners or pursue any other community endeavor you are passionate about.

Don’t meet every requirement? No problem. If your skills and experience align without meeting all requirements, we encourage you to apply. Studies have shown that women and people of color are less likely to apply for jobs if they don’t meet every requirement. At 1904labs, one of our core beliefs, Inclusion Inspires Innovation, reflects our belief that a diverse workforce builds better, more inclusive software.

At 1904labs, we believe that inclusion inspires innovation. Our goal is to hire innovative, courageous, intensely curious, and team-minded people, and we are committed to fair hiring practices and equal treatment and opportunity in all areas of recruitment, selection, and employment without regard to gender, race, religion, national origin, ethnicity, disability, gender identity/expression, sexual orientation, veteran or military status, or any other category protected under the law. 1904labs is an equal-opportunity employer; committed to an environment of inclusion, free from discrimination, harassment, and retaliation.

Flexible work from home options available."
700,Data Engineer - Oracle,"Infinity Consulting Solutions, Inc.","Houston, TX 77002",$80 - $90 an hour,"TITLE: Oracle Data Engineer
Location: Houston, Texas
Compensation Range : $ 60-90/hr

Data Engineer
The Sr. Data Engineer is responsible for designing, implementing and supporting full life-cycle data engineering projects.

These projects should include large data ingestion, persistence, transformation, and retrieval. Will develop, maintain, and deploy platform or application code, stored procedures, function, triggers in development, staging and production environments.

Perform fine tuning and optimization of SQL and PL/SQL code to improve query response. Debug and support production issues and assist with system integration and testing. Carry out complex DDL, DML and analytical functions to assist developers in application delivery.

Requirements:
Bachelor's degree in Computer Science, Engineering or other equivalent degree
5+ years data engineering experience or Master's degree in Computer Science, Engineering or other equivalent degree

Requires 2+ years' experience in the design, implementation and support of full life-cycle data engineering projects.

Experience in event driven and cloud based architectures and working knowledge of containerization and streaming technologies.

Detailed work experience on Oracle, SingleStore or IBM I DB2 database, SQL & PL/SQL, including ANSI SQL. Experience in database analysis, development and maintenance of business applications specific to oil & gas back-office operations.

Expertise in various ETL tools such as SQL*Loader, Equalum, Python scripts. Familiarity with No-SQL databases and pipelines (Elasticsearch, MongoDB).
Recent pl/sql development work experience with Oracle 19
Must have working experience with tools like Airflow, Git
Must have python coding experience and REST API development experience
Plus would be experience with microservices, docker, Kubernetes
Plus would be experience with accounting and financial data
About Infinity Consulting Solutions
At Infinity Consulting Solutions our mission is to cultivate successful long term relationships with candidates and clients matching the right candidate with the right client. We believe technology cannot replace the real personal relationships we cultivate. We reject the notion that technology alone is the answer to staffing which is why we our successful partnerships rely on collaboration NOT automation. ICS has been providing flexible staffing solutions for over 20 years in Information Technology, Compliance, Accounting / Finance and Corporate Support. Our staffing solutions include Contract, Temp to Perm and Permanent Placement.
ICS is an Equal Opportunity Employer.

*W2 employees of ICS are offered comprehensive benefits including health, dental and vision."
701,Data Engineer,EXL Services,"Detroit, MI",N,"Overview EXL (NASDAQ: EXLS) is a leading operations management and analytics company that designs and enables agile, customer-centric operating models to help clients improve their revenue growth and profitability. Our delivery model provides market-leading business outcomes using EXL’s proprietary Business EXLerator Framework™, cutting-edge analytics, digital transformation and domain expertise. At EXL, we look deeper to help companies improve global operations, enhance data-driven insights, increase customer satisfaction, and manage risk and compliance. EXL serves the insurance, healthcare, banking and financial services, utilities, travel, transportation and logistics industries. Headquartered in New York, New York, EXL has more than 32,000 professionals in locations throughout the United States, Europe, Asia (primarily India and Philippines), South America, Australia and South Africa. For more information, visit www.exlservice.com.

Data Engineer
Designation/ Title: Data Engineer
New or Replacement Role: New
Hire Type (Fulltime, Part-time, Independent. Contractor, Transfer): Fulltime
Location: Detroit, MI

Role and Responsibilities:
Manage and provide leadership around data infrastructure and data engineering by overseeing development, customization, and management of data integration tools, databases, warehouses, and analytical systems
Work across a wide range of problems in the data ingestion, data compute, data transport, data orchestration, data interoperability, experimentation, and data privacy areas
Review and integrate data into readily available formats while maintaining existing structures and govern their use according to business requirements
Manage the evaluation and integration of new data sources as they become available and monitor the performance, scalability, and security of data
Identify and improve the technical competencies within the team and establish practices for delivering engineering excellence
Partner with client engineering teams to understand their data needs and help create tools and automation that helps them derive value from their data
Works with the Business Intelligence delivery team to build out target information delivery architecture across PowerBI
Lead by example, role-modeling best practices for unit testing, CI/CD, performance testing, capacity planning, documentation, monitoring, alerting and incident response.
Requirement/Competencies:
Bachelor’s or Master’s (preferred) degree in in a quantitative or technical field such as Computer Science, Information Technology, Computer Engineering or equivalent
6+ years IT experience in software development for data systems at scale (building data warehouse, data marts, data lakes, ETL, analytic solutions, etc.)
4+ years of leadership and people management experience in a technology environment
3+ years of experience implementing solutions with AWS, specifically data services (EC2, EMR, RDS, Redshift, Aurora-Postgres, Glue ETL, Lambda, SNS, SQS etc)
Knowledge of market trends including cloud technology
Demonstrated interest in learning about emerging data platforms and Big Data technologies
Expert working knowledge of Python programing language, SQL, and data modeling, with the ability to understand the context of a business question and build a holistic answer
Experience modeling data in a warehouse such as Snowflake, Redshift, and or BigQuery
Managed agile/scrum team(s) before of different skill levels and know when to be a motivator, coach, or a player
Strong communication, problem solving, and organizational skills
Must be detail oriented, quality driven, and able to shift quickly to new tasks
Please be aware that EXL requires all employees to be vaccinated for COVID-19. This position will require the successful candidate to obtain and show proof of vaccination. EXL is an equal opportunity employer, and will provide
reasonable accommodation to those individuals who are unable to be vaccinated consistent with federal, state, and local law.

EEO/Minorities/Females/Vets/Disabilities

Please be aware that EXL requires all employees to be vaccinated for COVID-19. This position will require the successful candidate to obtain and show proof of a vaccination. EXL is an equal opportunity employer, and will provide reasonable accommodation to those individuals who are unable to be vaccinated consistent with federal, state, and local law.

Base Salary Range Disclaimer: The base salary range represents the low and high end of the EXL base salary range for this position. Actual salaries will vary depending on factors including but not limited to: location and experience. The base salary range listed is just one component of EXL's total compensation package for employees. Other rewards may include bonuses, as well as a Paid Time Off policy, and many region specific benefits.

Please also note that the data shared through the job application will be stored and processed by EXL in accordance with the EXL Privacy Policy."
702,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
703,Senior Data Engineer,Virginia Tech,"Blacksburg, VA•Remote",N,"Job no: 525109
Work type: Administrative & Professional
Senior management: Vice President-Info Technology
Department: Enterprise Systems
Location: Blacksburg, Virginia, Fully Remote, Hybrid
Categories: Information Systems / Technology
Job Description
Join a data team that works closely together and shares credit with our successes and responsibility with our failures. We debate architecture around best practices and new development ideas without tearing each other down. We provide great customer service to our business and technology partners by using relational collaboration and not transactional bureaucracy in order to help deliver data driven decisions to one of the top Universities in the country. We work with a wide range of technologies, and no we do not expect you to know all of them, but you will need to be a fast learner and be technically curious in order to be successful here.

There are many opportunities for growth, as VT has a wide range of technology experts who frequently give training in their area of expertise. As we move to a modern data-lake architecture, we need fast learners who are also passionate about supporting the existing processes that runs the University. The data management and analytics team is a part of central technology and runs Virginia Tech’s warehouse, data lake, data integrations and business intelligence solutions. We are looking for a problem solver who can help us deliver great service and technology solutions. Are you a good fit with the Hokies data management and analytics team?
Required Qualifications
Bachelor’s degree or equivalent relevant work experience in Information Technology related field
Architecture experience with ETL tools, like Talend, and data workflows where the candidate will setup framework and review code of other developers. This experience should include a cloud environment.
Extensive experience in relational databases, like Oracle, Postgres and Redshift including advanced SQL writing ability. Candidate will coach, guide and review work. Candidate will understand DBA role in order to communicate needs of the team
Experience with Business Intelligence tools such as MicroStrategy or Tableau
Demonstrated experience in working effectively in a team environment as a project leader. Strong communication and interpersonal skills with both technology team members and business partners. Ability to set competing priorities and meeting critical deadlines
Demonstrated experience in full system lifecycle development including requirements gathering, systems analysis, development of data services, testing and implementation of large enterprise systems as the subject matter expert
Preferred Qualifications
Extensive experience with process improvement that results in team buy-in and adoption
Demonstrated experience in managing priorities of others in an environment where individuals are working on multiple priorities with multiple skillsets
Extensive experience in data warehouse concepts and dynamic object architecture. Strong understanding in concepts and the features to maintain a well-structured and efficient database
Demonstrated experience with cloud technologies. Understanding architecture to where they can communicate needs to could engineers. Ideally AWS services such as EMR, Glue, Athena and Redshift
Experience in an object-oriented programming language like Python or Java.
Experience in an enterprise environment with DevOps best practices and technologies such as Docker, Version Control (GitLab), and Release Management (Jenkins)
Experience with data lakes in a Hadoop or AWS environment
Exposure to web services and APIs that return large data sets
Appointment Type
Regular
Salary Information
$90,000 - $110,000
Review Date
4/6/2023
Additional Information
The successful candidate will be required to have a criminal conviction check.
About Virginia Tech
Dedicated to its motto, Ut Prosim (That I May Serve), Virginia Tech pushes the boundaries of knowledge by taking a hands-on, transdisciplinary approach to preparing scholars to be leaders and problem-solvers. A comprehensive land-grant institution that enhances the quality of life in Virginia and throughout the world, Virginia Tech is an inclusive community dedicated to knowledge, discovery, and creativity. The university offers more than 280 majors to a diverse enrollment of more than 36,000 undergraduate, graduate, and professional students in eight undergraduate colleges, a school of medicine, a veterinary medicine college, Graduate School, and Honors College. The university has a significant presence across Virginia, including the Innovation Campus in Northern Virginia; the Health Sciences and Technology Campus in Roanoke; sites in Newport News and Richmond; and numerous Extension offices and research centers. A leading global research institution, Virginia Tech conducts more than $500 million in research annually.
Virginia Tech does not discriminate against employees, students, or applicants on the basis of age, color, disability, sex (including pregnancy), gender, gender identity, gender expression, genetic information, national origin, political affiliation, race, religion, sexual orientation, or military status, or otherwise discriminate against employees or applicants who inquire about, discuss, or disclose their compensation or the compensation of other employees or applicants, or on any other basis protected by law.
If you are an individual with a disability and desire an accommodation, please contact Brittany Bane at bmlester@vt.edu during regular business hours at least 10 business days prior to the event.
Advertised: March 23, 2023
Applications close:"
704,Data Integrations Engineer,"ShareThis, Inc","Palo Alto, CA 94306•Remote","$90,000 - $130,000 a year","About the Role:
ShareThis is seeking a skilled Data Integration Engineer for a client-facing role to ensure the smooth and efficient delivery of accurate data. The ideal candidate will work closely with the Data Engineering team to optimize and automate data integrations with data customers and data suppliers. The successful candidate will have a strong understanding of data integration processes, experience in optimizing data integrations, and excellent technical skills. This role will act as a liaison between clients, sales, operations, and engineering and will report to the VP of Product and Integrations.

About ShareThis:
ShareThis uncovers the topography of human behavior, unlocking the power of digital behavioral data while providing stewardship and respect for global consumers who share their data. From our free sharing service installed on 3 million websites around the world, ShareThis assembles data from billions of user page visits every month, under agreements with site owners and with the permission of site visitors. Using this data, ShareThis builds products to help marketers understand and connect to groups of consumers most likely to be interested in that marketer’s products and messages. Our products are used by companies, agencies, and analytics providers, and are widely available through leading technology platforms. Our clients and partners choose ShareThis data for its unparalleled scale and breadth in delivering insight into consumer interest and intent.

Responsibilities:

As a member of the Product Integrations team, you will

Directly and independently work with clients and suppliers to ensure that accurate data
is delivered smoothly and efficiently
Work with the Data Engineering team to understand the current data integrations with
data customers and data suppliers
Optimize and automate custom integrations to provide a seamless experience for clients
Implement a QA process to continually improve the success rate and customer
satisfaction
Identify and troubleshoot data integration issues in a timely manner, e.g. client access
(AWS/GCP buckets, Snowflake and more), internal data creation (Airflow DAGs, Scala
code) etc.
Develop and implement best practices for data integration
Collaborate with business partners and data engineers to understand business context,
technical needs, and review implementations to improve data integration process
efficiency
Maintain and update internal technical documents related to integrations to ensure that
processes are sustainable and repeatable

Requirements:

Bachelor’s degree in Computer Science, Computer Engineering, or a related field
5 or more years of experience in cloud-based data integration
Strong communication and interpersonal skills, with a client-facing mentality
Experience with cloud-based data integration tools for data management, ETL processes,
and database technologies (preferably AWS)
Hands-on expertise with access and security on Cloud
Expert level with SQL and scripting languages such as Python and Scala
Experience with Airflow DAGs and GitHub
Excellent analytical and problem-solving skills with a focus on data quality
Ability to work independently and collaboratively as part of a multifunctional team

This is a remote position."
705,Data Engineer,Ever.Ag,Remote,N,"Job Title: Data Engineer
Reports to: COO / SVP Services
FSLA Status: Full time, Salaried
Location: Remote, US
Position Title: Data Engineer
Position Summary
We are seeking a highly skilled Data Engineer to join our team. The ideal candidate will have a strong background in database design and management, with experience in developing data pipelines, ETL processes, and data integration. The candidate should be proficient in scripting, preferably Python, and have a drive to deliver exceptional customer experience. A background in CPG would be an added advantage.
Key Responsibilities
Design, develop, and maintain data pipelines, ETL processes, and data integration solutions.
Work with customers and internal stakeholders to identify and define data requirements, ensuring the accuracy, completeness, and timeliness of the data.
Build and maintain databases, ensuring optimal performance and scalability.
Write scripts to automate processes and tasks, with a focus on Python.
Develop and maintain data quality checks and monitoring processes.
Troubleshoot and resolve database and data-related issues, providing timely and effective solutions.
Stay up-to-date with the latest technologies and industry trends in database management, ETL processes, and data integration.
Work collaboratively with other members of the team to ensure the best possible customer experience.
Travel 15% to 25% to customer sites and in-person company meetings
Other duties as assigned
Qualifications
Bachelor's degree in Computer Science, Information Systems, or related field.
3+ years of experience in database design, management, and optimization.
Proficiency in SQL and experience with relational databases (MySQL preferred).
Strong scripting skills, preferably in Python.
Experience with ETL tools and data integration techniques.
Understanding of data modeling and data warehousing concepts.
Familiarity with cloud-based technologies such as AWS, Azure, or Google Cloud Platform.
Excellent problem-solving skills and attention to detail.
Strong communication skills and the ability to work collaboratively in a team environment.
Experience in CPG would be an added advantage.
ERP integration experience is preferred.
Competencies for Success
Excellent written and verbal communication: Presents oneself clearly and articulately when speaking, assuring that others fully comprehend the intended message; Uses appropriate grammar tailored to the audience
Analytical and Critical Thinking: Review and manage data with strong attention to detail; combine facts with likely possibilities; articulate and resolve complex problems
Quality Focused: A recognition of the value of doing things the right way; having a high sense of integrity and thoughtfulness in your actions
Reasoning Ability: Ability to think critically and solve problems with a variety of variables in situations where, at times, only limited standardization exists. Ability to define problems, collect data, establish facts, and draw conclusions. Ability to interpret a variety of technical instructions furnished in written, oral, diagram, or schedule form.
Action Oriented: A bias for action, when you see a problem, you solve it using your technical savvy and internal resources
Quality Focused: A recognition of the value of doing things the right way; having a high sense of integrity and thoughtfulness in your actions"
706,Data Engineer,Analytic Partners,"Miami, FL•Hybrid remote",N,"We’re Analytic Partners. Founded in 2000, Analytic Partners is the world’s premier technology driven, people enabled, analytic solution provider. Our mission is to turn data into expertise by enabling stronger brand connections to drive competitive advantage and deliver growth for our customers.
We offer a cloud-based, managed software platform which delivers adaptive solutions for deeper business understanding and right-time planning and optimization for marketing and beyond. We work with top-tier companies across the globe in a wide range of industries Including: Automotive, Consumer & Business Technology, Consumer Goods, Financial Services, Retail & Restaurants, Telco & Entertainment, Travel & Hospitality, and Wine, Beer & Spirits.
We’re growing fast with global operations across our full-service offices in New York City, Miami, Denver, Charlottesville, Dublin, London, Paris, Hamburg, Munich, Sydney, Melbourne, Singapore, and Shanghai.

The Data Engineer will help AP’s vision of turning data into expertise. At the core, this role is responsible for creating workflows for data ingestion, standardizing processes and ETL (Extract, Transform, Load). Leverage SQL scripts to design and automate workflows to ingest and load data to different target databases and systems. This exciting opportunity will put you at the forefront of cutting-edge technologies as we continue to expand our solutions over time. This role will be an integral part of shaping the development suite for our next-generation analytic solutions designed and built for our global clients. We work within an agile environment.
What we value in you:
You’ve got positive energy and you’re passionate about working with data. You’re optimistic about the future of using data to make informed business decisions.
You’re never tired of learning and are always looking for ways to grow personally and professionally
You thrive in collaborative work environments . You’re both an active communicator and an eager listener - because let’s face it, you can’t have one without the other.
You can pivot on the fly. Analytics is constantly changing, and we need to keep adapting and evolving that change too. What you worked on last month may not be what you work on today, and that excites you.
You have a “can do” attitude. Our teams create high-quality work on quick timelines.
Do you enjoy owning a problem and see it as an opportunity to innovate and challenge the status quo? We’re all about harnessing innovation and are constantly looking for ways to improve
You want to be part of a winning team. We’re stronger together, and you’re a person who embraces being pushed out of your comfort zone.
What you’ll be doing:
Cross-functional collaboration and promote teamwork.
Proactively find solutions and be receptive to feedback
Ideate and bring forth creative and practical solutions.
Challenge the status quo and always thinking about ways to improve and innovate
What we look for in you:
Bachelor's degree: Computer Science, Electrical Engineering, Marketing, Mathematics, Statistics or related field
Minimum 2 years of formal professional experience working with data
Solid SQL skills
Experience adapting in an agile environment to shifting priorities
A multitasker with excellent time management skills who can handle tight deadlines
Experience working with multiple data sources (SQL Server, MySQL, etc.)
Excellent written and verbal communication skills
Required: ETL, Excel, MySQL, SQL Server
Optional: JIRA
Our values and culture:
Our differentiator is – Our People! We hire the brightest talent and develop them into leaders. We foster a culture of PEOPLE, PASSION and GROWTH.

People: We value our people, clients, and partners
Passion: We love what we do
Growth: Unlimited growth means unlimited potential

AP is a client-focused, team-oriented organization where innovation and results are rewarded, and individuals can chart the course of their own careers.

As a woman-owned and led company, this has meant supporting a meritocracy where everyone has opportunities to achieve their best and ensure we foster an environment of diversity, equity, and inclusion. In practice this means we will not only work to recruit a diverse workforce, but also maximize the full potential of all of our people. You can read more about our commitment to DEI here
Additionally, Analytic Partners participates in the E-Verify program in certain locations, as required by law.

#LI-Hybrid"
707,Software Data Engineer,N,"Chicago, IL",N,"Role: Software Data Engineer
Location: Chicago or New York
At Aquatic, we are actively recruiting for a Software Data Engineer in our Chicago or New York office. In this role, you will work with quantitative researchers and software engineers to onboard new data sources and turn them into signals suitable for machine learning.
Responsibilities:
Support signals research by onboarding new data sources
Interact with vendors to understand data schemas and plan for changes
Respond to data outages during normal working hours
Work with signals researchers to optimize queries and refactor schemas as needed
Technical Skills:
2+ years full-time professional experience
Experience with SQL, relational databases, and structured data formats
Experience building batch data processing jobs in Python
Familiarity with cloud-based data solutions
Experience in finance is beneficial, but not required
Candidate Qualities:
Exceptional communication and coordination skills
Strong bias for action
Driven by accountability and internal urgency
Comfortable providing and receiving actionable feedback in a collaborative team setting
Motivated by an ambitious environment and driven colleagues
The base salary for this role is anticipated to be between $150,000 and $300,000, which is based on information at the time of posting. This position may also be eligible for additional forms of compensation, such as a discretionary bonus, and benefits. Discretionary bonus can be a significant portion of total compensation. Actual compensation for successful candidates will be carefully determined based on a number of factors, including their unique skills, qualifications and relevant experience.
Benefits:
Benefits: Fully paid medical, dental, and vision for employees and dependents, competitive 401k plan, employer-paid life & disability insurance
Perks: Wellness programs, casual dress, snacks, team and company events
Development: Open environment to maximize learning and knowledge sharing
Time: Generous PTO, paid holidays, competitive paid caregiver leaves
Aquatic Capital
Aquatic is a quantitative trading and investment company recently launched by Jon Graham. Prior to founding Aquatic, Jon was a Partner and Senior Managing Director at Citadel, where he worked for more than 13 years. At Citadel, Jon held numerous senior positions over the years, including head of Statistical Arbitrage and Equity High Frequency, culminating in leading the highly successful Global Quantitative Strategies business.
Aquatic develops systematic investment strategies, enabled by a leading-edge research and development platform. The vision is simple: to build a world class, quantitative trading company with a collaborative team of highly capable researchers and engineers.
This role represents a unique opportunity to join a quantitative investment manager at the foundational level of building a world class operation from scratch. The firm's culture will be shaped by collaboration, meritocracy, ambition, and calm determination.
Aquatic is a proud equal opportunity workplace. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity/expression, age, status as a protected veteran, status as an individual with a disability, or any other applicable legally protected characteristics."
708,Data Engineer II - Artificial Intelligence & Human Health Research,Mount Sinai,"New York, NY","$90,000 - $135,285 a year","Strength Through Diversity
Ground breaking science. Advancing medicine. Healing made personal.
Roles & Responsibilities:
The Thomas Fuchs Lab at the Windreich Dept. of AI & Human Health at Mount Sinai is inviting applications for a Data Engineer position in the novel field of computational pathology using supervised, weakly supervised, and unsupervised machine-learning algorithms and methods with applications in healthcare. The Data Engineer would aim to improve clinical practice in pathology by developing intelligent decision-support systems that not only automate cumbersome or repetitive tasks but also lead to more objective and reproducible results. The overarching goal is to lead the way in transforming pathology from a qualitative to a quantitative discipline. The Data Engineer will join a dynamic team of data scientists and clinicians and participate in unique opportunities to apply machine learning for important scientific breakthroughs and to directly impact patients’ lives in a clinical setting.

This position will be in the lab of Thomas J. Fuchs, Dr. Sc. housed in the Hasso Plattner Institute for Digital Health at Mount Sinai (HPI.MS) in the Windreich Department of AI and Human Health at the Icahn School of Medicine at Mount Sinai in Manhattan, New York, NY, USA. HPI.MS has a vested interest in using advanced data science methodologies on large-scale health data to push forward precision medicine.
Requirements:
BS and/or MS in a quantitative science-related field (e.g., biomedical informatics, clinical informatics, machine learning, biostatistics, genetics, etc.)
Experience in machine learning techniques is required, ideally with published work and/or code available. Experience with deep learning frameworks is preferred (e.g.,, Tensorflow, PyTorch, Keras). Experience with computational pathology, large vision models (CNNs, vision transformers), multi-GPU multi-node training, and federated learning strongly encouraged
Experience with programming and statistical software experience in Python and/or R
Experience working on an HPC cluster is encouraged
Publication track record including conference papers and preprints (e.g., arxiv)
Strong communication and presentation skills with fluency in spoken and written English

Strength Through Diversity
The Mount Sinai Health System believes that diversity, equity, and inclusion are key drivers for excellence. We share a common devotion to delivering exceptional patient care. When you join us, you become a part of Mount Sinai’s unrivaled record of achievement, education, and advancement as we revolutionize medicine together. We invite you to participate actively as a part of the Mount Sinai Health System team by:
Using a lens of equity in all aspects of patient care delivery, education, and research to promote policies and practices to allow opportunities for all to thrive and reach their potential
Serving as a role model confronting racist, sexist, or other inappropriate actions by speaking up, challenging exclusionary organizational practices, and standing side-by-side in support of colleagues who experience discrimination
Inspiring and fostering an environment of anti-racist behaviors among and between departments and co-workers
We work hard to acquire and retain the best people and to create an inclusive, welcoming and nurturing work environment where all feel they are valued, belong, and are able to professionally advance. We share the belief that all employees, regardless of job title or expertise, contribute to the patient experience and quality of patient care.
Explore more about this opportunity and how you can help us write a new chapter in our history!
Who We Are
Over 42,000 employees strong, the mission of the Mount Sinai Health System is to provide compassionate patient care with seamless coordination and to advance medicine through unrivaled education, research, and outreach in the many diverse communities we serve.
Formed in September 2013, The Mount Sinai Health System combines the excellence of the Icahn School of Medicine at Mount Sinai with seven premier hospitals, including Mount Sinai Beth Israel, Mount Sinai Brooklyn, The Mount Sinai Hospital, Mount Sinai Queens, Mount Sinai West (formerly Mount Sinai Roosevelt), Mount Sinai St. Luke’s, and New York Eye and Ear Infirmary of Mount Sinai.
The Mount Sinai Health System (MSHS) provides equal employment opportunity to all its employees and applicants for employment without unlawful discrimination on the basis of their actual or perceived race, creed, color, religion, national origin, sex, gender, gender expression, gender identity, age, disability, marital or parental status, sexual orientation, veteran, immigration, citizenship, or other protected status.
EOE including Veterans and Disabled
Compensation
The Mount Sinai Health System (MSHS) provides a salary range to comply with the New York City Law on Salary Transparency in Job Advertisements. The salary range for the role is $90,000.00 - $135,285.00 Annually. Actual salaries depend on a variety of factors, including experience, education, and hospital need. The salary range or contractual rate listed does not include bonuses/incentive, differential pay or other forms of compensation or benefits."
709,Sr Data Engineer,Talent Systems,"Los Angeles, CA•Remote","$140,000 - $150,000 a year","*This is a remote position.*

Company and Team Overview

Talent Systems, LLC is the leading technology solution provider for casting and auditioning to the entertainment industry. Casting directors and agents worldwide use Talent Systems’ portfolio of products to source and manage talent across film, television, commercials, theater and digital projects, powering an unparalleled, global casting software ecosystem. We are headquartered in Los Angeles and operate in the US, Canada, UK, Australia and India. Our portfolio brands include Casting Networks, Spotlight, Cast It Systems, Cast It Talent, Casting Frontier, Staff Me Up, and Cast It Reach.

The Talent Systems Data Engineering Team is based in Los Angeles and extends to New York, UK, Australia and Asia. The team delivers data solutions for Talent Systems, as well as maintains databases & data sets for strategic decision-making purposes.

Job purpose

The Lead/Sr Data Engineer is responsible for architecting, building and supporting the data and analytic technologies that support Talent Systems portfolio of applications and services. The Lead/Sr. Data Engineer’s primary focus is expanding and optimizing data and data pipeline architecture, as well as optimizing data flow and data collection, and visualizations for Talent System’s cross functional teams.

Duties and Responsibilities
Working with complex data sets coming from multiple data sources
Identifying key business requirements related to data models from business and technical perspectives
Identifying data quality issues and apply data cleansing strategies when applicable
Designing and implementing data pipelines, e2e data transformations, managing data models and organize data structure changes
Providing recommendations based on emerging technologies, and seeking areas for continuous improvement
Leading migration from a legacy data platform into the new business warehouse stack
Providing support during SDLC cycles, resolving escalated design and implementation issues whenever applicable
Building visualizations to provide actionable insights that can empower better decision-making processes

Qualification & Attributes
Strong understanding of ETL tools, processes and techniques
Hands-on experience with SQL databases & data warehouse engines
Ability to design and implement models and solutions to manage data within the enterprise (structured and unstructured)
Working knowledge of Python (core and data related libraries)
Strong hands-on experience with one more following technologies: Airflow, AWS (S3, Glue, Lambda, EMR, RDS), GCP (BigQuery), python, and others.
Excellent communication and problem-solving skills
Multi-tasking abilities"
710,Data Engineer,BlocPower,Remote,"$120,000 - $130,000 a year","BlocPower is a clean energy leader creating smarter, greener, healthier buildings for all by reducing the barriers to money-saving, quality-of-life-improving green building upgrade. We provide engineering, financing and project implementation services for our clients, with a special focus in historically left out communities across the country. These communities, and their buildings, are underserved by traditional energy services companies because they are considered too small, too costly, or too risky. Our portfolio of projects include houses of worship, schools, non-profits, small businesses and multifamily buildings. Through our work, we save our clients money, reduce greenhouse gas emissions, improve health and create local employment opportunities.

At BlocPower, we value our mission. We are trusted advisors that get things done for our customers by using data to make the right decisions. We support and expect excellence from our team members. We treat both our customers and ourselves with care and respect.

As our work is centered around systematically disenfranchised communities – including people of color, people from working class backgrounds, women and LGBTQ people – we strongly encourage applications from people with these identities or who are members of other marginalized communities.

About the Role
To further this mission, BlocPower is looking for a friendly Data Engineer to join our growing team. This engineer will be responsible for expanding our data pipeline architecture. You should have a depth of experience building data pipelines,, and see yourself adding to as much as benefitting from a supportive team environment.
This hire will help keep data consistent across projects and teams. This may look at times like helping our engineers to perform energy audits using our proprietary models, writing software that will help us analyze big data that comes into our platform via sensors installed across our projects, or optimizing the project pipelines of our construction and sales teams.

The ideal candidate is someone who is self-directed, believes in our mission, and is an excellent written communicator.
What You'll Do
Assemble large, complex, structured/unstructured data sets from various public/non-public data sources into the raw-zone of our data lake.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Design and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data and associated architecture secure.
Who You Are
Ability to write good SQL, and work with a variety of databases (eg. AWS RDS, MySQL, PostgreSQL, and MongoDB)
Autonomously stand up and maintain an ETL pipeline with little to no supervision, using tools such as AWS Glue, Informatica, or Talend
Coherently organize a data lake, making it easy easy to collaborate on data
Manipulate big data, including high-velocity streaming data using tools such as Spark, Kafka and Kinesis
Strong project management and organizational skills

Education/Experience
BA/BS or equivalent combination of work experience and education preferably in degree/course work/experience in computer science/data engineering
2+ years of experience as a Data Engineer
What You'll Get from Us
Base salary between $120,000 and $130,000
Competitive equity in a growing, Series A startup
Health, dental, vision benefits, plus perks like a One Medical membership
Bonus Points
Experience with AWS cloud services: EC2, EMR, RDS, Redshift, Glue, Sagemaker
Microservice architectures
Docker, Kubernetes, Docker-Compose
Git and Jenkins
Experience working with IoT/Sensors

This job description is not intended to be a comprehensive list of the duties and responsibilities of the position. The duties and responsibilities may change without notice.

BlocPower™ provides equal employment opportunities(EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, BlocPower™ complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.

BlocPower™ expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of BlocPower™ employees to perform their job duties may result in discipline up to and including discharge."
711,Principal Software Engineer - Azure Data,Microsoft,"Redmond, WA 98052","$133,600 - $256,800 a year","Microsoft is a company where passionate innovators come to collaborate, envision what can be and take their careers to levels they cannot achieve anywhere else. This is a world of more possibilities, more innovation, more openness, and the sky is the limit thinking a cloud-enabled world.

Microsoft’s Intelligence Platform engineering team is leading the transformation of analytics in the world of data with products like Power BI, Synapse Analytics, Azure Data Factory, Azure Data Explorer. We will bring the world’s data to the Microsoft Cloud, power a new class of data first applications, and empower everyone on the planet to make better decisions with data. The Azure Data team is looking for a Principal Software Engineer.

We do not just value differences or different perspectives. We seek them out and invite them in so we can tap into the collective power of everyone in the company. As a result, our ideas are better, our products are better, and our customers are better served.
Responsibilities
We are looking for passionate people with experiences working with all service aspects of high throughput and multi-tenant services, ability to design components carefully, properly handle errors, write clean and well-factored code with good tests and good maintainability.

In this role, the candidate will be designing and developing high performance, low latency, infinitely scalable, complex frameworks/libraries/experiences to be used by customers across the globe in business-critical workloads. The candidate will be expected to have strong customer passion and demonstrate the drive to take initiative and drive big goals. The role also comes with an opportunity to mentor members of the team.
Qualifications
Required Qualifications
Bachelor's Degree in Computer Science, or related technical discipline AND 6+ years technical engineering experience with coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python
OR equivalent experience.
Hands-on experience in shipping high quality large scale cloud services or distributed system.
Ability to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings:
Microsoft Cloud Background Check: This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter.
Preferred Qualifications
Experience with multi-threading, synchronous and asynchronous programming
Hands-on experience in shipping high quality large scale cloud services or distributed system.
Demonstrated solid working knowledge on cloud computing / Azure / AWS.
Evaluate current services and drive performance, availability, and supportability improvements.
Drive increased efficiencies through automation and reduction in operational expenditure.
Bachelor's Degree in Computer Science or related technical field AND 10+ years technical engineering experience with coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python
OR Master's Degree in Computer Science or related technical field AND 8+ years technical engineering experience with coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python
OR equivalent experience.
Software Engineering IC5 - The typical base pay range for this role across the U.S. is USD $133,600 - $256,800 per year. There is a different range applicable to specific work locations, within the San Francisco Bay area and New York City metropolitan area, and the base pay range for this role in those locations is USD $173,200 - $282,200 per year.

Certain roles may be eligible for benefits and other compensation. Find additional benefits and pay information here: https://careers.microsoft.com/us/en/us-corporate-pay

Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.

Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.

#azdat"
712,Data Engineer Job Ref #: 478039,Concentrix Catalyst,"Omaha, NE 68102",N,"Overview
Concentrix CVG Customer Management Group Inc. has multiple openings for the position of Data Engineer based out of its U.S. offices in Omaha, NE. The employee may also work at various unanticipated locations throughout the U.S. Travel and/or relocation to various unanticipated locations throughout the U.S. is required. Telecommuting may be permitted.
Responsibilities
The position of Data Engineer will write, update, and maintain software applications; perform production maintenance of code; gather solutions requirements. Own technical commitments to clients and work with the team to successful delivery of solutions. Analyze, design, and code for complex requirements as well as write programs of complexity. Responsible for defining problems, collecting data, establishing facts, drawing valid conclusions, and preparing appropriate reports.
Qualifications
The position of Data Engineer requires a Master’s degree in Computer Science, Engineering (any), or any technical/analytical field that is closely related to the specialty, plus knowledge of the following: Spark, Hadoop, HBase and Hive.
To apply, send Resumes to ctlyst_postings@concentrix.com with Job Ref# 478039 in the subject line of the email."
713,Senior Data Engineer - Data Semantic layer,LexisNexis,Remote,N,"Sr. Engineer - SEMANTIC LAYER - Remote
Do you have excellent experience building data pipelines?
Are you interested in being part of a high-profile Global Analytics team?
Join us in shaping a more just world.
About Us
LexisNexis, a part of RELX, is a leading global provider of legal, regulatory, and business information. We help customers increase productivity and improve decision-making and outcomes. Our 10,500 experts and innovative tools help us shape a better world for our customers and communities.
About the Role
You will own the development of a semantic layer for our analytics data. In this role, you will be responsible for crafting and building a comprehensive data architecture that will enable seamless data integration and enable the delivery of high-quality insights to our customers.
Responsibilities
Performing daily data loads ensuring recurring updates are logged and tracked
Interface with other technical personnel or team members to document, interpret, and finalize requirements.
Produce code that is efficient, repeatable, without defects, and adherent to best practices such as naming conventions, encapsulation, etc.Write and review portions of detailed specifications for the development of data components.
Design and implement a semantic layer that integrates analytics data from multiple sources in an efficient and effective manner.
Develop data models and mapping rules to transform raw data into actionable insights and reports.
Collaborate with the analytics and data science teams to understand their requirements and deliver solutions that meet their needs.
Ensure data quality and accuracy by developing data validation and reconciliation processes.
Play an active role in the development and maintenance of user documentation, including data models, mapping rules, and data dictionaries.
Collaborate with multi-functional teams to define and implement data governance policies and standards.
Stay informed about the latest developments in data analytics and data management technologies and recommend new tools and methodologies to improve the semantic layer.
Requirements
Demonstrate expertise in in data management area with a technical competencies required
Expert experience with SQL languages (ANSI, PostGres, MySQL, etc.)
Strong experience with Python
Demonstrated experience in building and deploying semantic layers for analytics data is a plus.
Expertise in data modeling, data warehousing, and ETL concepts.
Proven experience in data engineering, data architecture, or a related field.
Experience with AWS services like EC2, Redshift, Databricks, RDS, and S3 ,Athena, Lambda, Glue, Cloud formation and EMR.
Experience in building and deploying semantic layers for analytics data is preferred
Proficiency with one major data analytics platform, such as Hadoop, Spark, or Snowflake.
Experience with data integration and data governance tools, such as Talend, Informatica, or Collibra.
Excellent problem-solving and analytical skills, and the ability to work well under tight deadlines.
Excellent interpersonal skills and the ability to collaborate effectively with multi-functional teams.
Work in a way that works for You
We promote a healthy work/life balance across the organization, with various flexible and remote working options available to employees
Working with Us
LexisNexis Legal & Professional is proud to be an equal-opportunity employer. We are committed to equal opportunity employment regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Together, we are building a diverse and inclusive workplace.
Working for You
We believe in a healthy work/life balance. We know that your well-being and happiness are key to a long and successful career. These are some of the benefits we are delighted to offer: - Comprehensive, multi-carrier health plan benefits - Disability insurance - Dependent care and commuter spending accounts - Life and accident insurance - Retirement benefits (salary investment plan/employer stock purchase plan) - Modern family benefits, including adoption and surrogacy
About our Team
LexisNexis is a data and analytics company with 10,500 colleagues serving customers in more than 150 countries. We’re one of the largest information and analytics companies on the planet. We design solutions that help our customers increase productivity, improve decision-making and outcomes, and be more successful.
Job Type: Full-time
Work Location: Remote"
714,Senior Data Engineer (REMOTE),National Grid,"Waltham, MA 02451•Remote","$112,000 - $158,000 a year","Senior Data Engineer (REMOTE)
Location: Waltham, MA, US, 02451
Division: Chief Information Officer
Job Type:
Requisition Number: 41864
Department:
Job Function: Information Technology
About us
Come be a part of driving National Grid’s digital transformation! We are digital creators, continuous learners and daring innovators. We leverage digital innovative ways to create products and catalyze the transformation of National Grid's business units into more agile and digitally native organizations in our shared purpose of bringing energy to life.
What you'll do
Take on a critical role of turning business needs into technology solutions
Develop, test, document and support scalable data pipelines and data integrations for customer facing systems serving millions of users
You’re comfortable with system/software design and development, and possesses solid communication and documentation skills
Provide support and mentorship to the engineering team
Possesses the ability to grow technically and maintain a customer-first mentality, the motivation to take the initiative to own and resolve issues
What you'll need
Education: 4-year college degree or equivalent combination of education and experience. Prefer an academic background in Computer Science, Mathematics, Statistics, or related technical field.
5+ years of technical contribution in a structured team environment
Experience designing data schemas and working with CosmosDB, Atlas or other SQL/NoSQL database systems
Strong experience architecting and implementing applications in EventHubs and Kafka. Experience with features like KSQL
Azure cloud knowledge and experience in architecting, configuring, and deploying with Advanced Kubernetes and AKS
Experience with designing and building ETL/ELT data pipelines, data analysis, CI/CD
Architected and led the development of complex enterprise systems
Previous experience introducing technical innovation resulting in significant value to the organization
Excellent problem solving and troubleshooting skills
Experience using SQL queries as well as writing and perfecting SQL queries in a business environment with large-scale, complex datasets.
Experience architecting with Change Data Replication (CDC) and Qlik is a nice to have
Experience with analytics systems like Snowflake nice to have
More Information
At National Grid, we keep the lights on and homes warm. But it’s so much more than that. We keep people connected and society moving. This is no easy feat, and it takes all of us. But National Grid supplies us with the environment to make it happen. As we generate momentum in the energy transition for all, we don’t plan on leaving any of our customers in the dark. So, join us and help bring energy to life.

#LI-TF1
Salary
$112,000 - $158,000 a year
This position has a career path which provides for advancement opportunities within and across bands as you develop and evolve in the position; gaining experience, expertise and acquiring and applying technical skills. Candidates will be assessed and provided offers against the minimum qualifications of this role and their individual experience.

National Grid is an equal opportunity employer that values a broad diversity of talent, knowledge, experience and expertise. We foster a culture of inclusion that drives employee engagement to deliver superior performance to the communities we serve. National Grid is proud to be an affirmative action employer. We encourage minorities, women, individuals with disabilities and protected veterans to join the National Grid team.


Nearest Major Market: Waltham
Nearest Secondary Market: Boston"
715,Data Engineer,GameOn Technology,"San Francisco, CA•Remote",N,"At GameOn, we believe in the potential of AI to better the world and our mission is to make AI accessible through conversational user experiences.

To further our mission, we are looking for our first Data Engineer to join our growing data team. As GameOn’s first data engineer, you’ll play a central role in scaling our data and capabilities as we grow, and you’ll be a primary influence on our company’s data practices. You’ll build and maintain ETL pipelines, to provide new data sources and insights to our company and customers. You’ll help to expand our data infrastructure to meet the demands of our products as they evolve. And you’ll collaborate on teams to produce data products and applications to complement our ChatOS.

Come join us! GameOn is an open and ever-evolving work environment where you’ll have ample opportunity to contribute and grow in areas of the company that interest you. We are a people-first organization that embraces a flexible work environment in order to meet the needs of our team members. Whether you prefer working-from-home, coming into our energy-rich office in the Financial District of San Francisco, or maybe a combination of the two - we support you!
Responsibilities
Integrate and surface new data sources into our warehouse, from both internal and external origins.
Initiate and execute testing and monitoring on our data, to ensure data quality and pipeline health.
Monitor and advise on upcoming data infrastructure needs as we scale
Promote data best practices within the company for data storage, access, and pipeline architecture.
Collaborate on the design of data-related apps and services.
Requirements
Strong Proficiency in SQL and Python.
2+ years experience maintaining and building ETL/ELT pipelines.
Understanding of system design for data infrastructure, toward evolving our current data infrastructure to meet near-future needs.
Experience with the GCP platform and data stack is preferred.
Understanding of best practices in maintaining data quality at volume.
Ability to work both independently and collaboratively, as our first data engineer.
Experience on teams/projects supporting data product deployment through API is preferred.
Benefits & Perks
Competitive salary and equity packages
Medical
Dental
Vision
Life
Short-term Disability
Long-term Disability
401k Program
Professional Development Budget
Home Office Stipend
Parental Leave (12 weeks fully paid)
Time Off (15 company holidays + Unlimited PTO)
Virtual and in-person team events
Company Background
GameOn is the industry-leading intelligent chat platform that powers authentic conversational experiences for some of the world's largest and most popular brands, teams and content properties. Established in 2014 and based in San Francisco, GameOn's omnichannel technology engages fans, drives profitability and saves time and money for partners like the NBA, NFL, NHL, PGA Tour, FIFA, TIME Inc., among others. Founded by proven entrepreneurs Alex Beckman, Kalin Stanojev and Nate Simmons, GameOn has raised $54 million from leading VC firms like Quest Ventures, Mirae Asset Venture Investment, Mighty Capital and celebrity investors like Snoop Dogg, Joe Montana and Gary Payton.

About GameOn
GameOn Technology is proud to be an equal opportunity workplace and we are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Our company is based on a culture of support, collaboration and passion driven by the success of our customers in the spirit of developing fans out of users."
716,Data Engineer,"U.S. Xpress Enterprises, Inc.","Chattanooga, TN 37421",N,"Data Engineer
CHATTANOOGA TN
Monday - Friday 8am - 5pm
Position open to remote: Yes
Who We Are:
Relentlessly Delivering Big Ideas. U.S. Xpress is one of the nation’s largest asset-based trucking companies. But the most valuable asset we offer isn’t tractors, trailers, or even our exclusive, cutting-edge technology. It’s the collective brainpower of thousands of visionaries and problem-solvers. Together, we are revolutionizing the transportation industry by providing innovative, custom solutions. And, here, we believe in the sanctity of a promise—both to our customers, and our people. When we focus our varied talents on reshaping the future of transportation, that’s what we call the POWER OF U.S.
Why U.S. Xpress?
Right Role. Right Tools. Right People. We invest in our talent starting on day one. You will be provided with personal and professional development opportunities that complement your interests and encourage you to build a career you’re passionate about. Whether it is employee stock options, profit-sharing, 401K, professional development, or our competitive pay, we help prepare you for the future. Be part of an organization that values out-of-the-box thinking and rewards employees for going above and beyond. Curious about the other benefits of working with us? Check out other perks below!
Vacation, Sick, & Personal Time
Paid Holidays
Paid Parental Leave
Medical, Dental and Vision
What You’ll Do:
Build tools that make data available to company’s freight platform and future-proof architecture and implementations to scale for years to come. Proficiently develop data pipelines, packages, stored procedures, functions, triggers, and complex SQL statements applying experience with ETL tools and cloud (AWS/Azure/Google). Work with multiple data sources and data types (structured/semi-structured/unstructured) and assemble complex and large data sets for the business. Partner with leadership, engineers, program managers and data scientists to understand data needs. Design, build and launch efficient and reliable data pipelines to move and transform data; find trends in data and build algorithms to optimize data delivery. Write high quality distributed system software using programming in Java & Python. Use data engineering best practices to ensure a high standard of quality for all the team deliverables. Partner with Data Owners to design solutions that align to data governance and data management principles best practices.
What We’re Looking for:
MINIMUM REQUIREMENTS: Bachelor’s degree in Mathematics, Economics, Business, Computer Science, Information Systems or a related field of study plus five (5) years of progressive software or data engineering experience, including experience with ETL tools and technologies, such as FiveTran, dbt, Apache Druid, Apache Cassandra, Event Sourcing, Python, Java, Scala, SSIS, Actor Systems or AKKA, Cloud (AWS/Azure/Google); three (3) years of experience with Kafka, Elasticsearch, Snowflake, RDBMS and/or NoSQL.
ALTERNATE REQUIREMENTS: Master’s degree in Mathematics, Economics, Business, Computer Science, Information Systems or a related field of study plus two (2) years of software or data engineering experience, including experience with ETL tools and technologies, such as FiveTran, dbt, Apache Druid, Apache Cassandra, Event Sourcing, Python, Java, Scala, SSIS, Actor Systems or AKKA, Cloud (AWS/Azure/Google); two (2) years of experience with Kafka, Elasticsearch, Snowflake, RDBMS and/or NoSQL.

SPECIAL REQUIREMENTS: Must pass technical interview
#LI-DNP #LI-DNI
Work Environment / Physical Requirements – Normal office settings.
This job description indicates the general nature and level of work expected for this position. It is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities. Employees performing this job may be asked to perform other duties as required and the responsibilities of the position may change. Reasonable accommodations will be made to enable individuals with disabilities to perform the essential functions.

U.S. Xpress is an Equal Opportunity Employer committed to creating and maintaining a diverse workforce.

NA"
717,"Data Modeler and Database Engineer, Lead",Booz Allen Hamilton,"Lexington, MA 02421•Hybrid remote","$93,300 - $212,000 a year","The Opportunity:
For an organization to transform in today’s digital world, it needs to properly collect, store, and organize its data. Effective data management can enable more efficient operations, yielding more growth. As a data architect, you know how to apply your creative-thinking and analytical mindset to help organizations manage their data assets. We’re looking for a data architecture lead like you to solve complex challenges and deliver leading-edge solutions for building software and systems, from vision to production ready.
As a data architect on our team, you’ll use your extensive technical expertise to lead the design of data architecture solutions for a system of systems architecture. You’ll resolve routine data architecture issues in collaboration with business analysts and technology teams by working with a cross-functional team to make decisions and recommendations on architecture modernization activities.
In this role, you’ll share your technical expertise, introduce best practices, and use tools like ERWin and MySQL Workbench. You’ll lead your team as it designs, defines, develops, and tests Cloud solution components, and you’ll serve as a liaison between clients and developers to ensure that requirements are met, and solutions are delivered.
With your motivation to establish processes and facilitate technological innovation, you’ll make a lasting impact on our nation's security.
Join us. The world can’t wait.
You Have:
8+ years of experience as a data operations lead, including a senior data director
2+ years of experience with ERWin, MySQL Workbench, or leading data modeling tools
Experience with Java, Javascript, and SQL
Experience with data migration from on-prem to AWS and Cloud, data synchronization, data replication for high availability and resiliency, backup and recovery, and RTO and RPO
Experience with AWS Outposts and AWS Edge services
Experience with leading a team of 10+ people to form data models for database transitions and larger change management activities
Knowledge of AWS and Cloud data architecture, data services, data API, and synchronous and asynchronous messaging
Secret clearance
Bachelor’s degree in Science, Technology, Engineering, or Mathematics

Nice If You Have:
Knowledge of C2 applications, Open Mission Systems, and Universal Command and Control Interface
Knowledge of DevSecOps
Possession of excellent verbal and written communication skills
Master’s degree

Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Secret clearance is required.
Create Your Career:
Grow With Us
Your growth matters to us—that’s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.
A Place Where You Belong
Diverse perspectives cultivate collective ingenuity. Booz Allen’s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you’ll build your community in no time.
Support Your Well-Being
Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we’ll support you as you pursue a balanced, fulfilling life—at work and at home.
Your Candidate Journey
At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we’ve compiled a list of resources so you’ll know what to expect as we forge a connection with you during your journey as a candidate with us.
Compensation
At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen’s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.
Salary at Booz Allen is determined by various factors, including but not limited to location, the individual’s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $93,300.00 to $212,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen’s total compensation package for employees.
Work Model
Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.
If this position is listed as remote or hybrid, you’ll periodically work from a Booz Allen or client site facility.
If this position is listed as onsite, you’ll work with colleagues and clients in person, as needed for the specific role.
EEO Commitment
We’re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change – no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law."
718,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
719,Data Engineer,Massachusetts General Hospital(MGH),"Charlestown, MA 02129",N,"Data Engineer
- (3227183)

GENERAL SUMMARY/ OVERVIEW STATEMENT:
The Data Engineer will be part of a diverse interdisciplinary team of computer and neuroscientists with broad expertise spanning computer science, neuroscience, psychology, psychiatry, neuropsychology, cognitive neuroscience, neuroimaging, bioinformatics, biostatistics, epidemiology, and neurophysiology. The project will involve data processing and coordination for new multisite data collection networks, as well as develop and apply stratification tools to identify and validate biomarkers to predict outcome trajectories in individuals at high risk to develop psychosis.
This position joins the data management, processing and archiving team. The successful candidate will help develop, deploy and maintain the bioinformatics infrastructure of several large projects. Our group (spanning MGH and BWH) is also actively developing new technologies to characterize brain structure and function, which has led to the design of state-of-the-art image analysis pipelines capable of robustly processing hundreds of neuroimaging datasets. The role of the engineer will be to maintain and enhance existing image processing pipelines and to develop new pipelines based on the latest research with an emphasis on version tracking, data provenance, and high performance computing.
PRINCIPAL DUTIES AND RESPONSIBILITIES:
Relevant activities include, but are not limited to the following:
Maintain and enhance existing image processing pipelines.
Design new image processing pipelines, with an emphasis on version tracking, data provenance, and high performance computing.
Develop neuroinformatics tools to track data provenance and project management.
Test and evaluate a range of neuroimaging packages to determine their suitability for research goals.
Regular, direct interaction with neuroscientists from within and outside the DPACC to assist them with neuroimaging data analysis using a range of methods including FSL, SPM, Slicer and other specialized tools.
Design, implement, test, maintain and support applications to capture, manage, archive and monitor multi-site, multi-modal study data. Applications may include but are not limited to study monitoring systems, data management systems, workflow execution and monitoring systems, interactive viewers, and reporting tools.
Support web and application server configuration and deployment.
Support data engineering efforts, including database and API design, data extraction/transformation/load, and data aggregation/integration.
Containerize and deploy software and workflows on local high performance computing platforms and cloud computing infrastructure (AWS).

Required:
Bachelor’s Degree in Computer Science, Mathematics, Physical Sciences, Engineering, or related field
Excellent programming skills in Python, Bash, MATLAB
Superior Linux/Unix skills and comfort with command line programs – the ability to get new programs and packages running, overcoming hurdles as they arise, is particularly helpful.
Familiarity with standard software evolution method—version controlling (Git), pull requests, code reviews, issue and release management
Ability to work in an interdisciplinary, diverse, and international team in a highly collaborative and intellectually challenging environment.
Excellent oral and written communication skills
Basic knowledge of neuroscience and neuroanatomy.
Understanding of structural, diffusion, and functional Magnetic Resonance Imaging.
Preferred:
Master’s Degree in Computer Science, Mathematics, Physical Sciences, Engineering, or related field
Familiarity with C/C++ programming
Experience in neuroimaging software FSL, FreeSurfer, Slicer, DIPY, Nipype, Neurodocker, REDCap, XNAT
Experience with database management systems (e.g., SQL, PostgreSQL, MongoDB, CouchDB)
Experience with Linux container engines (e.g., Docker, rkt) and container orchestration systems (e.g., Kubernetes)
Experience with JavaScript libraries for interactive data visualization (e.g. d3, Recharts, Charts.js).
Experience with at least one web framework for building single-page web applications (e.g., React, Angular, Vue)
EEO Statement

Massachusetts General Hospital is an Equal Opportunity Employer. By embracing diverse skills, perspectives and ideas, we choose to lead. Applications from protected veterans and individuals with disabilities are strongly encouraged.
Primary Location MA-Charlestown-MGH 13th Street
Other Locations MA-Charlestown
Work Locations MGH 13th Street 149 13th Street Charlestown 02129
Job IT/Health IT/Informatics-Engineer
Organization Massachusetts General Hospital(MGH)
Schedule Full-time
Standard Hours 40
Shift Day Job
Employee Status Regular
Recruiting Department MGH Psychiatry
Job Posting Jan 3, 2023"
720,Data Engineer,"U.S. Xpress Enterprises, Inc.","Chattanooga, TN 37421",N,"Data Engineer
CHATTANOOGA TN
Monday - Friday 8am - 5pm
Position open to remote: Yes
Who We Are:
Relentlessly Delivering Big Ideas. U.S. Xpress is one of the nation’s largest asset-based trucking companies. But the most valuable asset we offer isn’t tractors, trailers, or even our exclusive, cutting-edge technology. It’s the collective brainpower of thousands of visionaries and problem-solvers. Together, we are revolutionizing the transportation industry by providing innovative, custom solutions. And, here, we believe in the sanctity of a promise—both to our customers, and our people. When we focus our varied talents on reshaping the future of transportation, that’s what we call the POWER OF U.S.
Why U.S. Xpress?
Right Role. Right Tools. Right People. We invest in our talent starting on day one. You will be provided with personal and professional development opportunities that complement your interests and encourage you to build a career you’re passionate about. Whether it is employee stock options, profit-sharing, 401K, professional development, or our competitive pay, we help prepare you for the future. Be part of an organization that values out-of-the-box thinking and rewards employees for going above and beyond. Curious about the other benefits of working with us? Check out other perks below!
Vacation, Sick, & Personal Time
Paid Holidays
Paid Parental Leave
Medical, Dental and Vision
What You’ll Do:
Build tools that make data available to company’s freight platform and future-proof architecture and implementations to scale for years to come. Proficiently develop data pipelines, packages, stored procedures, functions, triggers, and complex SQL statements applying experience with ETL tools and cloud (AWS/Azure/Google). Work with multiple data sources and data types (structured/semi-structured/unstructured) and assemble complex and large data sets for the business. Partner with leadership, engineers, program managers and data scientists to understand data needs. Design, build and launch efficient and reliable data pipelines to move and transform data; find trends in data and build algorithms to optimize data delivery. Write high quality distributed system software using programming in Java & Python. Use data engineering best practices to ensure a high standard of quality for all the team deliverables. Partner with Data Owners to design solutions that align to data governance and data management principles best practices.
What We’re Looking for:
MINIMUM REQUIREMENTS: Bachelor’s degree in Mathematics, Economics, Business, Computer Science, Information Systems or a related field of study plus five (5) years of progressive software or data engineering experience, including experience with ETL tools and technologies, such as FiveTran, dbt, Apache Druid, Apache Cassandra, Event Sourcing, Python, Java, Scala, SSIS, Actor Systems or AKKA, Cloud (AWS/Azure/Google); three (3) years of experience with Kafka, Elasticsearch, Snowflake, RDBMS and/or NoSQL.
ALTERNATE REQUIREMENTS: Master’s degree in Mathematics, Economics, Business, Computer Science, Information Systems or a related field of study plus two (2) years of software or data engineering experience, including experience with ETL tools and technologies, such as FiveTran, dbt, Apache Druid, Apache Cassandra, Event Sourcing, Python, Java, Scala, SSIS, Actor Systems or AKKA, Cloud (AWS/Azure/Google); two (2) years of experience with Kafka, Elasticsearch, Snowflake, RDBMS and/or NoSQL.

SPECIAL REQUIREMENTS: Must pass technical interview
#LI-DNP #LI-DNI
Work Environment / Physical Requirements – Normal office settings.
This job description indicates the general nature and level of work expected for this position. It is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities. Employees performing this job may be asked to perform other duties as required and the responsibilities of the position may change. Reasonable accommodations will be made to enable individuals with disabilities to perform the essential functions.

U.S. Xpress is an Equal Opportunity Employer committed to creating and maintaining a diverse workforce.

NA"
721,Software Engineer: Data Platform,"Observe, Inc.","San Mateo, CA•Hybrid remote",N,"Observe is building an Observability Cloud that allows customers to bring all their machine data (logs, metrics, traces, etc.) together in one place. At the core of the service is our data management platform which enables our users to build sophisticated streaming pipelines and express complex temporal queries all without learning complex database concepts. The platform is built on top of a cloud data warehouse platform (Snowflake).

As a data platform engineer, your will work on making the platform perform better, adding new capabilities, and reducing its cost. Typical work involves:
Compiling and optimizing user queries so that they can be executed more efficiently
Support new query functions to make the platform more powerful and flexible
Make streaming more cost-effective and reduce the end-to-end latency

The ideal candidate should be:

Experience with relational database internals
Experience with query optimization and stream processing is a strong plus
Experience with Go is a plus but not required
Self-motivated, fearless, likes to dive in the code and get stuff done
Strong teamplayer who can work independently but cares about context and knows when to reach out"
722,Data Engineer with PowerBI,Bradley Corporation,"Menomonee Falls, WI 53051","$95,000 - $120,000 a year","Bradley Corporation Is A 5th Generation Family-Owned And Operated Business Leading The Way In Commercial Washrooms And Safety Solutions. Celebrating 100 Years In Business, We Take Pride In All We Do As A US Manufacturer. With A Culture Focused On Innovation, Quality, And Family, You Will Know Your Colleagues On A First Name Basis, All While In A Friendly, Yet Professional Environment Where Your Voice Matters.

Here’s Why You’ll Love Working Here:
Work within an innovative company, meeting passionate colleagues and partners with diverse backgrounds and experiences
Generous time off program, with the option to purchase additional Paid Time Off. Twelve paid holidays a year, including a company-wide paid holiday shutdown between Christmas and New Year’s
Work schedules that support work/life balance
Competitive salary and comprehensive benefits package including medical, free basic dental, life insurance, 401(k) with company match and more

Here’s Your Opportunity:
Data Engineer is responsible for designing, building, and maintaining our data infrastructure, as well as developing ETL processes to move and transform data from various sources. Utilize Power BI skills to create insightful visualizations and reports that communicate complex data in a user-friendly manner. Work closely with Business Project Owners, users, analysts, programmers, third-party resources and other people to coordinate the various phases of systems requirements gathering, analysis, development, coordination, implementation, training and ongoing sustainability of the system(s). Overall, the key success factors for this position are a combination of technical expertise, attention to detail, analytical and problem-solving skills, communication skills, and adaptability. The successful candidate should possess these skills and be able to apply them effectively to support the organization's data engineering and visualization needs.

Here’s how you’ll contribute:
Develop, maintain and optimize our data architecture, including databases, data warehouses, and data lakes.
Create and manage ETL processes to extract, transform and load data from various sources into our data infrastructure.
Implement data quality checks and ensure data integrity and accuracy.
Work closely with cross-functional teams to understand their data needs and provide solutions to support their business objectives.
Use Power BI to create engaging and insightful visualizations and reports that communicate complex data in a user-friendly manner.
Develop and maintain data models, dashboards, and reports that provide actionable insights to stakeholders.
Manage data security and access control, ensuring compliance with regulatory and company policies.
Keep up-to-date with emerging technologies and industry trends, and make recommendations for improvements to our data infrastructure and visualization capabilities.
Other related duties and projects as required.
Here’s why we want you:
Bachelor’s degree in computer science, Information Systems, or a related field or equivalent experience
3+ years of experience in data engineering, with expertise in designing, building, and maintaining data infrastructure.
Strong proficiency in SQL and ETL processes.
Familiarity with Azure Resources - key vault, data lake, Synapse. (pipelines and notebooks), Azure SQL Server.
Apache Spark and Python/Pyspark
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema construction - modeling source data into dimensions and facts, and setting up relationships between tables.
Familiarity with Azure cloud platforms.
Excellent problem-solving and analytical skills, with the ability to communicate complex data in a simple and understandable way.
Strong attention to detail and ability to prioritize and manage multiple tasks simultaneously.
Ability to work independently and as part of a team in a fast-paced and dynamic environment.
At Bradley, we seek diversity. Embracing diversity only enhances our work culture, and is essential to living our values and achieving our business goals. Because of our differences, we are able to drive innovation while enhancing our customer experience. We are committed to creating a family environment where all employees are included, and treated with dignity and respect.

Equal Opportunity Employer of Minorities, Females, Protected Veterans, and Individuals with Disabilities

Pre-employment drug screen required.

Your career with Bradley can be just a few clicks away."
723,Data Engineer,Comcast,"Philadelphia, PA 19103",N,"Comcast’s Technology, Product & Experience organization works at the intersection of media and technology. Our innovative teams are continually developing and delivering products that transform the customer experience. From creating apps like TVGo to new features such as the Talking Guide on the X1 platform, we work every day to make a positive impact through innovation in the pursuit of building amazing products that are enjoyable, easy to use and accessible across all platforms. The team also develops and supports our evolving network architecture, including next-generation consumer systems and technologies, infrastructure and engineering, network integration and management tools, and technical standards. In most cases, Comcast prefers to have employees on-site collaborating unless the team has been designated as virtual due to the nature of their work. If a position is listed with both office locations and virtual offerings, Comcast may be willing to consider candidates who live greater than 100 miles from the office for the remote option.

Job Summary
Comcast is seeking a Data Engineer in the CONNECT organization. The role provides candidate with broad exposure and opportunity to up level the Cybersecurity maturity of multiple lines of business and next generation platforms utilized for providing high-speed Internet services. First and foremost an ideal candidate must demonstrate high degree of self-initiative and adaptability to work on multiple security work streams in a fast-paced environment. The candidate should ideally possess a technical background in security threat/risk management, background in database administration, and python. Lastly, the candidate is required to collaborate with stakeholders in peer security and technology groups to strengthen overall security posture.
Job Description
Core Responsibilities
Monitors reporting platform to support security remediation of security vulnerabilities across all platforms and services
Maintains SQL database for a reporting platform for security vulnerabilities to owners
Identifies, documents, and recommends reports and dashboards for trending data issues for teams
Stays current with security technologies, trends, vulnerabilities and threats
Follow our agile process, working in sprints together with the rest of the team to deliver on time with technical quality
Continuously improve yourself and the products you work on
Supports building a culture of security by educating others and advocating an open security
posture
Consistently exercises best judgment and discretion in matters of significance
Other duties and responsibilities as assigned
Skills & Experience
Experience in the area of Data Analysis
SQL databases
Frontend dashboard tools
Python
Hands on experience with Linux/Unix
Familiarity AWS, Openstack
Familiarity with Information Security (vulnerability testing, risk analysis, and security assessments)
Excellent written and verbal communication skills, interpersonal and collaborative skills
Must have strong problem-solving and analytical skills
High degree of initiative and be well organized
Ability to manage multiple projects with strict timelines
High level of personal integrity
Enjoys working in a demamding and dynamic enviroment
Disclaimer:
This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications.
About Our Perks & Benefits
We are determined to build an environment where our employees feel valued, understand our business goals, and are motivated.
Here's a look at just some of the perks and benefits we make available to our US-based employees:
Medical & Dental
401(k) Savings Plan
Generous paid time off
Life Milestones - from adoption assistance, childcare resources, pet insurance, and more, Comcast supports you at all life stages.
Courtesy Services - We offer all of our full-time employees in serviceable areas free digital TV and internet.
Discount tickets for Universal Resorts, including theme park tickets and onsite hotel rooms.
Learn more at https://jobs.comcast.com/life-at-comcast/benefits (https://jobs.comcast.com/life-at-comcast/benefits)
Reasonable Accommodation:
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment. Please contact us to request accommodation.
Comcast is an EOE/Veterans/Disabled/LGBT employer.
=== THIS POSITION IS INELIGIBLE FOR VISA SPONSORSHIP. TO BE CONSIDERED FOR THIS ROLE, YOU MUST BE LEGALLY AUTHORIZED TO WORK IN THE UNITED STATES AND NOT REQUIRE SPONSORSHIP FOR EMPLOYMENT NOW OR IN THE FUTURE. ===
Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other basis protected by applicable law.

Education
Bachelor's Degree
While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.
Relevant Work Experience
2-5 Years

Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That’s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality – to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details."
724,Senior Data Engineer - Data Semantic layer,LexisNexis,Remote,N,"Sr. Engineer - SEMANTIC LAYER - Remote
Do you have excellent experience building data pipelines?
Are you interested in being part of a high-profile Global Analytics team?
Join us in shaping a more just world.
About Us
LexisNexis, a part of RELX, is a leading global provider of legal, regulatory, and business information. We help customers increase productivity and improve decision-making and outcomes. Our 10,500 experts and innovative tools help us shape a better world for our customers and communities.
About the Role
You will own the development of a semantic layer for our analytics data. In this role, you will be responsible for crafting and building a comprehensive data architecture that will enable seamless data integration and enable the delivery of high-quality insights to our customers.
Responsibilities
Performing daily data loads ensuring recurring updates are logged and tracked
Interface with other technical personnel or team members to document, interpret, and finalize requirements.
Produce code that is efficient, repeatable, without defects, and adherent to best practices such as naming conventions, encapsulation, etc.Write and review portions of detailed specifications for the development of data components.
Design and implement a semantic layer that integrates analytics data from multiple sources in an efficient and effective manner.
Develop data models and mapping rules to transform raw data into actionable insights and reports.
Collaborate with the analytics and data science teams to understand their requirements and deliver solutions that meet their needs.
Ensure data quality and accuracy by developing data validation and reconciliation processes.
Play an active role in the development and maintenance of user documentation, including data models, mapping rules, and data dictionaries.
Collaborate with multi-functional teams to define and implement data governance policies and standards.
Stay informed about the latest developments in data analytics and data management technologies and recommend new tools and methodologies to improve the semantic layer.
Requirements
Demonstrate expertise in in data management area with a technical competencies required
Expert experience with SQL languages (ANSI, PostGres, MySQL, etc.)
Strong experience with Python
Demonstrated experience in building and deploying semantic layers for analytics data is a plus.
Expertise in data modeling, data warehousing, and ETL concepts.
Proven experience in data engineering, data architecture, or a related field.
Experience with AWS services like EC2, Redshift, Databricks, RDS, and S3 ,Athena, Lambda, Glue, Cloud formation and EMR.
Experience in building and deploying semantic layers for analytics data is preferred
Proficiency with one major data analytics platform, such as Hadoop, Spark, or Snowflake.
Experience with data integration and data governance tools, such as Talend, Informatica, or Collibra.
Excellent problem-solving and analytical skills, and the ability to work well under tight deadlines.
Excellent interpersonal skills and the ability to collaborate effectively with multi-functional teams.
Work in a way that works for You
We promote a healthy work/life balance across the organization, with various flexible and remote working options available to employees
Working with Us
LexisNexis Legal & Professional is proud to be an equal-opportunity employer. We are committed to equal opportunity employment regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Together, we are building a diverse and inclusive workplace.
Working for You
We believe in a healthy work/life balance. We know that your well-being and happiness are key to a long and successful career. These are some of the benefits we are delighted to offer: - Comprehensive, multi-carrier health plan benefits - Disability insurance - Dependent care and commuter spending accounts - Life and accident insurance - Retirement benefits (salary investment plan/employer stock purchase plan) - Modern family benefits, including adoption and surrogacy
About our Team
LexisNexis is a data and analytics company with 10,500 colleagues serving customers in more than 150 countries. We’re one of the largest information and analytics companies on the planet. We design solutions that help our customers increase productivity, improve decision-making and outcomes, and be more successful.
Job Type: Full-time
Work Location: Remote"
725,Sr Data Engineer,Talent Systems,"Los Angeles, CA•Remote","$140,000 - $150,000 a year","*This is a remote position.*

Company and Team Overview

Talent Systems, LLC is the leading technology solution provider for casting and auditioning to the entertainment industry. Casting directors and agents worldwide use Talent Systems’ portfolio of products to source and manage talent across film, television, commercials, theater and digital projects, powering an unparalleled, global casting software ecosystem. We are headquartered in Los Angeles and operate in the US, Canada, UK, Australia and India. Our portfolio brands include Casting Networks, Spotlight, Cast It Systems, Cast It Talent, Casting Frontier, Staff Me Up, and Cast It Reach.

The Talent Systems Data Engineering Team is based in Los Angeles and extends to New York, UK, Australia and Asia. The team delivers data solutions for Talent Systems, as well as maintains databases & data sets for strategic decision-making purposes.

Job purpose

The Lead/Sr Data Engineer is responsible for architecting, building and supporting the data and analytic technologies that support Talent Systems portfolio of applications and services. The Lead/Sr. Data Engineer’s primary focus is expanding and optimizing data and data pipeline architecture, as well as optimizing data flow and data collection, and visualizations for Talent System’s cross functional teams.

Duties and Responsibilities
Working with complex data sets coming from multiple data sources
Identifying key business requirements related to data models from business and technical perspectives
Identifying data quality issues and apply data cleansing strategies when applicable
Designing and implementing data pipelines, e2e data transformations, managing data models and organize data structure changes
Providing recommendations based on emerging technologies, and seeking areas for continuous improvement
Leading migration from a legacy data platform into the new business warehouse stack
Providing support during SDLC cycles, resolving escalated design and implementation issues whenever applicable
Building visualizations to provide actionable insights that can empower better decision-making processes

Qualification & Attributes
Strong understanding of ETL tools, processes and techniques
Hands-on experience with SQL databases & data warehouse engines
Ability to design and implement models and solutions to manage data within the enterprise (structured and unstructured)
Working knowledge of Python (core and data related libraries)
Strong hands-on experience with one more following technologies: Airflow, AWS (S3, Glue, Lambda, EMR, RDS), GCP (BigQuery), python, and others.
Excellent communication and problem-solving skills
Multi-tasking abilities"
726,Data Visualization Engineer,Blue Margin,Colorado,"$65,000 - $105,000 a year","Data Visualization Engineer I-III
Blue Margin, Inc. is on a mission to build great places to work. We help companies improve company culture and consequently, performance. We believe remarkable things are possible when the whole team knows the score and is rowing in the same direction. Using Power BI as the primary catalyst, we partner with our clients to align employees, improve meetings, and increase employee satisfaction.
Why are we looking?
We are expanding our Microsoft Power BI team and are looking for people who are flexible and capable of putting themselves in the client's shoes. We are looking for a clever, creative, data-savvy person to produce reports while working alongside a fantastic team of developers and consultants. Our growth means we are looking for people who will help us, and our clients, build great places to work. It also means we provide an excellent opportunity for someone serious about learning and advancing their career.
We are seeking a candidate to work as a full-time employee in our local office in Fort Collins.
Please note that we are interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas.
We are interested in considering candidates for all three of our developer tiers. The sample report you create will help distinguish your skill level:
Data Visualization Engineer I
Data Visualization Engineer II
Data Visualization Engineer III
Responsibilities:
Develop accurate reports in Power BI that are not only visually engaging, but also make customers’ data accessible and actionable.
Regularly interact with clients for project updates and inquiries
Create, enhance, and troubleshoot data models in Power BI and Visual Studio
Author documentation of customer reporting requirements and finished reports
Craft and use T-SQL queries for data validation
Candidates MUST possess the following qualifications:
Working knowledge of Power BI Desktop and Power BI Service
Ability to create DAX calculations
Some familiarity with data modeling
Proficient ability to talk to executives clearly and concisely in English
Professional demeanor
Desire to be a team player and do meaningful work
Ideal candidates would possess these additional qualifications:
1-3 years of experience in Power BI Desktop creating tables, graphs, drill downs, drill throughs, bookmarks, and KPIs
Working knowledge of Power BI Service and administration
Ability to create intermediate to advanced DAX calculations using functions such as Calculate, Summarize and Filter
Experience creating T-SQL queries in SSMS
Comprehensive grasp of data visualization methods
Experience using Visual Studio 2017/2019, DAX Studio, Tabular Editor, ALM Toolkit
Familiarity with tabular data models
Experience manipulating data in Power Query Editor
Our Culture:
Company Core Values: Commit to Quality, Embrace Transparency, Choose to Be Positive, Be Efficient/Systematize, Pursue Learning, Be Generous
We believe that in-person interaction is vital to solid work relationships and our great place to work
Weekly personal and professional development programs for all
Teamwork—we maintain company-wide interaction and communication
Entrepreneurism – we want everyone on our team to be eager to adapt and evolve with our advancing business. We are looking for someone who is comfortable wearing more than one hat.
Work Environment and Physical Requirements:
This position may require minimal physical effort including lifting materials and equipment of less than 10 pounds. This position requires viewing a computer screen more than 80 percent of the time. The job will take place in a normal office environment with controlled temperature and lighting conditions. This position may require some travel and occasional participation in off-site functions.
Salary and Benefits:
The starting salary for a Level 1 position is between $65-85K and is commensurate with experience and qualifications.
The starting salary for a Level II position is between $75-95K and is commensurate with experience and qualifications.
The starting salary for a Level III position is between $85-105K and is commensurate with experience and qualifications.
All positions come with a comprehensive benefits package consisting of medical and dental coverage, paid sick leave, vacation, disability/life insurance, and a retirement plan."
727,Data Engineer II - Artificial Intelligence & Human Health Research,Mount Sinai,"New York, NY","$90,000 - $135,285 a year","Strength Through Diversity
Ground breaking science. Advancing medicine. Healing made personal.
Roles & Responsibilities:
The Thomas Fuchs Lab at the Windreich Dept. of AI & Human Health at Mount Sinai is inviting applications for a Data Engineer position in the novel field of computational pathology using supervised, weakly supervised, and unsupervised machine-learning algorithms and methods with applications in healthcare. The Data Engineer would aim to improve clinical practice in pathology by developing intelligent decision-support systems that not only automate cumbersome or repetitive tasks but also lead to more objective and reproducible results. The overarching goal is to lead the way in transforming pathology from a qualitative to a quantitative discipline. The Data Engineer will join a dynamic team of data scientists and clinicians and participate in unique opportunities to apply machine learning for important scientific breakthroughs and to directly impact patients’ lives in a clinical setting.

This position will be in the lab of Thomas J. Fuchs, Dr. Sc. housed in the Hasso Plattner Institute for Digital Health at Mount Sinai (HPI.MS) in the Windreich Department of AI and Human Health at the Icahn School of Medicine at Mount Sinai in Manhattan, New York, NY, USA. HPI.MS has a vested interest in using advanced data science methodologies on large-scale health data to push forward precision medicine.
Requirements:
BS and/or MS in a quantitative science-related field (e.g., biomedical informatics, clinical informatics, machine learning, biostatistics, genetics, etc.)
Experience in machine learning techniques is required, ideally with published work and/or code available. Experience with deep learning frameworks is preferred (e.g.,, Tensorflow, PyTorch, Keras). Experience with computational pathology, large vision models (CNNs, vision transformers), multi-GPU multi-node training, and federated learning strongly encouraged
Experience with programming and statistical software experience in Python and/or R
Experience working on an HPC cluster is encouraged
Publication track record including conference papers and preprints (e.g., arxiv)
Strong communication and presentation skills with fluency in spoken and written English

Strength Through Diversity
The Mount Sinai Health System believes that diversity, equity, and inclusion are key drivers for excellence. We share a common devotion to delivering exceptional patient care. When you join us, you become a part of Mount Sinai’s unrivaled record of achievement, education, and advancement as we revolutionize medicine together. We invite you to participate actively as a part of the Mount Sinai Health System team by:
Using a lens of equity in all aspects of patient care delivery, education, and research to promote policies and practices to allow opportunities for all to thrive and reach their potential
Serving as a role model confronting racist, sexist, or other inappropriate actions by speaking up, challenging exclusionary organizational practices, and standing side-by-side in support of colleagues who experience discrimination
Inspiring and fostering an environment of anti-racist behaviors among and between departments and co-workers
We work hard to acquire and retain the best people and to create an inclusive, welcoming and nurturing work environment where all feel they are valued, belong, and are able to professionally advance. We share the belief that all employees, regardless of job title or expertise, contribute to the patient experience and quality of patient care.
Explore more about this opportunity and how you can help us write a new chapter in our history!
Who We Are
Over 42,000 employees strong, the mission of the Mount Sinai Health System is to provide compassionate patient care with seamless coordination and to advance medicine through unrivaled education, research, and outreach in the many diverse communities we serve.
Formed in September 2013, The Mount Sinai Health System combines the excellence of the Icahn School of Medicine at Mount Sinai with seven premier hospitals, including Mount Sinai Beth Israel, Mount Sinai Brooklyn, The Mount Sinai Hospital, Mount Sinai Queens, Mount Sinai West (formerly Mount Sinai Roosevelt), Mount Sinai St. Luke’s, and New York Eye and Ear Infirmary of Mount Sinai.
The Mount Sinai Health System (MSHS) provides equal employment opportunity to all its employees and applicants for employment without unlawful discrimination on the basis of their actual or perceived race, creed, color, religion, national origin, sex, gender, gender expression, gender identity, age, disability, marital or parental status, sexual orientation, veteran, immigration, citizenship, or other protected status.
EOE including Veterans and Disabled
Compensation
The Mount Sinai Health System (MSHS) provides a salary range to comply with the New York City Law on Salary Transparency in Job Advertisements. The salary range for the role is $90,000.00 - $135,285.00 Annually. Actual salaries depend on a variety of factors, including experience, education, and hospital need. The salary range or contractual rate listed does not include bonuses/incentive, differential pay or other forms of compensation or benefits."
728,Data Engineer/Data Scientist,Ampcus Incorporated,"Richmond, VA",N,"Qualifications:
Skills associated with being a data engineer/data scientist Must be well versed in data manipulation, data organization, and process improvement

Must have extensive knowledge of Tableau, creating metrics reports, and creating dashboards with proper data visualization

Must be proficient in Python, R, Spark and/or Power Bi Some experience using Git, GitHub and GitLab for code repository

Some knowledge of Hadoop and connecting/setting up live Tableau dashboards Some knowledge of AI/Machine Learning

Some knowledge of RPA and how to automate processes, including automation of existing dashboard and reporting processes

Some knowledge of agile working processes, Scrumban and product management processes

Responsibilities:
Contractor would be responsible for accomplishing the following objectives:

Review and validate the current process, data management practices and data organization to ensure data is appropriately obtained, combined and protected within data repositories, published dashboards and reporting processes

Automate the process where possible – suggest using Python and/or Tableau but are open to other tools as long as they are consistent with tools used in the FR; many data sources and/or data refresh processes are manual – looking to reduce the level of manual intervention to support analytic solutions

Refine and modernize the current reports by creating a new look and feel that is sustainable and allows for the expansion of customers to be reported on.

The idea is to utilize productivity gains from automation to offset the expansion of customer base and ability to use dashboards are the main reporting tool for our users

Create documentation that describes the new processes end-to-end

Create user access processes to support expansion of customer base, and ensure data is appropriately secured and accessible only by those who have a business need to the data

Contractor must be able to work in small teams and be comfortable with knowledge sharing Risks Existing data sources are moving to new solutions which may impact the existing data collection, analysis and reporting processes.

These processes will need to be updated as new solutions are implemented, and we will need to ensure updates enable automation as much as possible to further support expansion of analytic capabilities."
729,Software Data Engineer,N,"Chicago, IL",N,"Role: Software Data Engineer
Location: Chicago or New York
At Aquatic, we are actively recruiting for a Software Data Engineer in our Chicago or New York office. In this role, you will work with quantitative researchers and software engineers to onboard new data sources and turn them into signals suitable for machine learning.
Responsibilities:
Support signals research by onboarding new data sources
Interact with vendors to understand data schemas and plan for changes
Respond to data outages during normal working hours
Work with signals researchers to optimize queries and refactor schemas as needed
Technical Skills:
2+ years full-time professional experience
Experience with SQL, relational databases, and structured data formats
Experience building batch data processing jobs in Python
Familiarity with cloud-based data solutions
Experience in finance is beneficial, but not required
Candidate Qualities:
Exceptional communication and coordination skills
Strong bias for action
Driven by accountability and internal urgency
Comfortable providing and receiving actionable feedback in a collaborative team setting
Motivated by an ambitious environment and driven colleagues
The base salary for this role is anticipated to be between $150,000 and $300,000, which is based on information at the time of posting. This position may also be eligible for additional forms of compensation, such as a discretionary bonus, and benefits. Discretionary bonus can be a significant portion of total compensation. Actual compensation for successful candidates will be carefully determined based on a number of factors, including their unique skills, qualifications and relevant experience.
Benefits:
Benefits: Fully paid medical, dental, and vision for employees and dependents, competitive 401k plan, employer-paid life & disability insurance
Perks: Wellness programs, casual dress, snacks, team and company events
Development: Open environment to maximize learning and knowledge sharing
Time: Generous PTO, paid holidays, competitive paid caregiver leaves
Aquatic Capital
Aquatic is a quantitative trading and investment company recently launched by Jon Graham. Prior to founding Aquatic, Jon was a Partner and Senior Managing Director at Citadel, where he worked for more than 13 years. At Citadel, Jon held numerous senior positions over the years, including head of Statistical Arbitrage and Equity High Frequency, culminating in leading the highly successful Global Quantitative Strategies business.
Aquatic develops systematic investment strategies, enabled by a leading-edge research and development platform. The vision is simple: to build a world class, quantitative trading company with a collaborative team of highly capable researchers and engineers.
This role represents a unique opportunity to join a quantitative investment manager at the foundational level of building a world class operation from scratch. The firm's culture will be shaped by collaboration, meritocracy, ambition, and calm determination.
Aquatic is a proud equal opportunity workplace. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity/expression, age, status as a protected veteran, status as an individual with a disability, or any other applicable legally protected characteristics."
730,Data Engineer - Oracle,"Infinity Consulting Solutions, Inc.","Houston, TX 77002",$80 - $90 an hour,"TITLE: Oracle Data Engineer
Location: Houston, Texas
Compensation Range : $ 60-90/hr

Data Engineer
The Sr. Data Engineer is responsible for designing, implementing and supporting full life-cycle data engineering projects.

These projects should include large data ingestion, persistence, transformation, and retrieval. Will develop, maintain, and deploy platform or application code, stored procedures, function, triggers in development, staging and production environments.

Perform fine tuning and optimization of SQL and PL/SQL code to improve query response. Debug and support production issues and assist with system integration and testing. Carry out complex DDL, DML and analytical functions to assist developers in application delivery.

Requirements:
Bachelor's degree in Computer Science, Engineering or other equivalent degree
5+ years data engineering experience or Master's degree in Computer Science, Engineering or other equivalent degree

Requires 2+ years' experience in the design, implementation and support of full life-cycle data engineering projects.

Experience in event driven and cloud based architectures and working knowledge of containerization and streaming technologies.

Detailed work experience on Oracle, SingleStore or IBM I DB2 database, SQL & PL/SQL, including ANSI SQL. Experience in database analysis, development and maintenance of business applications specific to oil & gas back-office operations.

Expertise in various ETL tools such as SQL*Loader, Equalum, Python scripts. Familiarity with No-SQL databases and pipelines (Elasticsearch, MongoDB).
Recent pl/sql development work experience with Oracle 19
Must have working experience with tools like Airflow, Git
Must have python coding experience and REST API development experience
Plus would be experience with microservices, docker, Kubernetes
Plus would be experience with accounting and financial data
About Infinity Consulting Solutions
At Infinity Consulting Solutions our mission is to cultivate successful long term relationships with candidates and clients matching the right candidate with the right client. We believe technology cannot replace the real personal relationships we cultivate. We reject the notion that technology alone is the answer to staffing which is why we our successful partnerships rely on collaboration NOT automation. ICS has been providing flexible staffing solutions for over 20 years in Information Technology, Compliance, Accounting / Finance and Corporate Support. Our staffing solutions include Contract, Temp to Perm and Permanent Placement.
ICS is an Equal Opportunity Employer.

*W2 employees of ICS are offered comprehensive benefits including health, dental and vision."
731,"Sr. Consultant, Data Engineer - BI&A Sustain",Cardinal Health,United States•Remote,"$97,700 - $139,600 a year","Business Intelligence and Analytics sustain organization is a centralized organization to continue to provide support to multiple business areas to help unlock the true value of Reporting and Analytics.
The Sr. Consultant Strategic Planning/Execution of Business Intelligence and Analytics is responsible for deeply understanding the business in multiple functional areas and across Business Units, as well as understanding the application landscape and capabilities. They maintain relationships with the business and leverage requirements engineering practices and methods to enable improved business processes to drive business success. They align business strategies and capability needs into business and technical requirements to ensure IT systems enable the desired value.
What Strategic Planning/Execution contributes to Cardinal Health

Strategy is responsible for leadership, direction and expertise in the development of business strategy, strategic management disciplines and business analytics that support the company's mission vision and valuation objectives, in close collaboration with business leaders.

Strategic Planning/Execution is responsible for developing and supporting the realization of strategic plans and management agendas.

Responsibilities
Demonstrates knowledge of legacy and future BI&A capabilities, data domain knowledge, tools and practices to enable various Business Analytics teams
Identifies business BI&A problem areas, data & capabilities gaps, in alignment with the needs of the BI&A business users and/or industry and potential solutions to resolve problems/gaps
Knows and executes best practices for business analysis processes and functions (from an industry and strategic standpoint), including requirements elicitation methods
Drive key priorities and initiatives for the BI&A Sustain team
Partner with Business Supported Applications (BSA) owners or business analytics users in supporting their remediation efforts by leveraging new capabilities or developing critical business knowledge
Provide data consultant expertise in outlining data impacts during transition state
Focused on adoption of future state platforms by providing BI&A/Data coordination with legacy data and reporting teams
Lead training, drive adoption, develop data literacy leverage self-service data marts for reporting and analytics
Partner with critical business users from Legacy solutions to migrate to future state solutions -enabling path of legacy reporting system retirement
Qualifications
Bachelor's degree preferred
5+ years of experience in Business Intelligence and Analytics and Data with proven techno-functional and data leadership experience preferred
Knowledge of BI&A capabilities, data domain expertise or techno functional knowledge is preferred to work with various Business Data Analytics teams
Possess SAP process knowledge and its translation to future state data
Possess techno-functional knowledge of HANA/Business Objects self-service data marts, GCP/BQ, Tableau, AtScale, Alteryx, and Teradata
Proven results and process orientation
Proven planning, analytical, and detail-orientation
Excellent communication skills - written, oral and presentation
Demonstrated ability to develop working relationships at various business analytics areas

What is expected of you and others at this level
Applies advanced knowledge and understanding of concepts, principles, and technical capabilities to manage a wide variety of projects
Participates in the development of policies and procedures to achieve specific goals
Recommends new practices, processes, metrics, or models
Works on or may lead complex projects of large scope
Projects may have significant and long-term impact
Provides solutions which may set precedent
Independently determines method for completion of new projects
Receives guidance on overall project objectives
Acts as a mentor to less experienced colleagues
Anticipated salary range: $97,700 - $139,600
Bonus eligible: Yes
Benefits: Health insurance, 401k Contributions, Paid Time Off, Vacation, STD/LTD
#LI-Remote
Cardinal Health is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, ancestry, age, physical or mental disability, sex, sexual orientation, gender identity/expression, pregnancy, veteran status, marital status, creed, status with regard to public assistance, genetic status or any other status protected by federal, state or local law."
732,Data Engineer; W2,Synchrony Systems,"Quincy, MA","From $100,000 a year","Title: Data Engineer
Location:: Quincy MA
Python, SQL, Cloud Computing, REST
Leads in the design, development, and support of Data Engineering projects
Consults with business unit management as needed to analyze requirements and identify the optimal technical solution to fit within existing technologies and comply with overall strategic direction
Proactively ensures minimum delays in systems deliverables, focuses on performance optimization for the total system
Provides subject matter expertise in addressing projects and issues that encompass a wide range of internal and external systems, components, and processes
Leverages industry best practices; shares same with team.
Job Type: Full-time
Salary: From $100,000.00 per year
Benefits:
Health insurance
Paid time off
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Experience:
Python: 3 years (Required)
SQL: 2 years (Required)
Cloud computing: 2 years (Required)
Work Location: On the road

Health insurance"
733,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
734,Principal Data Engineer,Tickets.com,"New York, NY 10020•Hybrid remote","$150,000 - $180,000 a year","Tickets.com, an MLB company, delivers innovative, cutting-edge technologies to enable frictionless and unforgettable fan experiences in venues across the globe. Together with MLB, Tickets.com is changing the landscape of the live sports and entertainment industry, delivering new digital venue and ticketing experiences to millions of fans. Our Technology team builds platforms and products that provide a new smart ticketing solution and venue experience. Using cutting-edge technology, our platform and applications are consumed by fans, stadiums, and MLB teams
We are assembling a world-class team to build on these experiences, to scale platforms and products that anticipate emerging opportunities, including dynamic pricing and offers and digital, contactless ticketing. Our mission is to provide premium, innovative live experiences for our clients and their patrons.
Tickets.com is looking for a Principal Data Engineer passionate about building engaging products for our fans.
The Opportunity: The Data Engineering team at MLB builds and maintains data pipelines to integrate baseball fan interaction and transaction data from hundreds of sources into our Data Warehouse for use by our Business Intelligence, Data Science, Analytics, Marketing, Ticketing, Finance, and other internal business teams. In addition, we share this data with all 30 MLB Clubs, and interact with Club staff on a regular basis to ensure they are able to better understand and interact with their fans as well as run ticketing operations through the use of this data.
We leverage modern, real-time data-streaming solutions for all of our new data pipeline development, and we love it! We are moving towards Google BigQuery as our Data Warehouse platform. We’re Pythonistas, and regularly develop complex Python scripts to interact with APIs and data stores. And SQL is the lingua franca across all of the teams in our organization that interact with our Data Warehouse, so we speak it fluently.
The Principal Data Engineer will be responsible for accelerating and solidifying the Data Engineering team’s development, deployment, and operational support practices by building tools leveraged by the entire Data Engineering team. In addition, the Principal Data Engineer will build new data pipelines and enhance and troubleshoot existing ones. The Principal Data Engineer will report directly to the Director, Ticketing Engineering and interact regularly with our team of Senior, Mid, and Junior Data
Essential Job Functions:
High Impact - You’ll develop tools and processes leveraged by the entire Data
Engineering team, and work on data sets leveraged to measure fan behavior, enhance
our products, optimize marketing pipelines, and directly impact our business’ bottom line.
High Impact - You’ll develop tools and processes leveraged by the entire Data Engineering team, and work on data sets leveraged to measure fan behavior, enhance our products, optimize marketing pipelines, and directly impact our business’ bottom line.
Broad Range of Technology - You’ll have the freedom to use the right tools for the job, whether it’s vanilla SQL or a distributed processing framework such as Apache Spark. We run our processes within Google Cloud Platforms’ ecosystems, so we can take advantage of their managed services such as Oracle and BigQuery database administration to help us get the job done better or faster.
Diverse Data Sources - You’ll work with hundreds of fan engagement data sources, including: Ticket sales from TicketMaster, Tickets.com, StubHub and other ticket brokers; additionally you will help construct data pipelines and analytics solutions to support the buildout of an enhanced MLB-controlled ticketing eco-system that supports broker-to-fan and fan-to-fan ticket exchanges.
Diverse Data Sizes - Sure, we’ve got “Big Data” - but we also have plenty of valuable small and mid-size data sets too. You won’t be forced to use complex distributed technologies on small but valuable data sets where using them would be overkill - you’ll get to make decisions on the proper framework for each particular use case.
Lead and Coach – You’ll mentor other Data Engineers, and review design and code produced by them.
Build and Support – You’ll embrace the DevOps mentality to build and support data applications in the cloud. You’ll deploy using infrastructure as code, ensuring that servers are treated like cattle, and not pets. You will help define processes that support CI/CD principles.
Requirements:
10+years Experience working on Big Data and Data pipelines. With focus on data engineering technical leadership building scalable and secure data platforms and systems powering intelligent applications.
Expertise in Python, specifically interacting with data APIs and automating tasks.
Expertise in SQL.
Experience working with large (Terabyte-scale) data sets
Experience with an MPP Data Warehouse such as BigQuery, Oracle Exadata, Redshift, or Teradata
Experience with GCP, OCI, AWS or Snowflake.
Comfort in a Linux environment and with basic server administration tasks.
Significant previous experience in a Data Engineering or ETL Engineering role.
Experience with Git.
Bachelor’s degree or better in Computer Science or a related technical field or equivalent job experience
Experience with any/all of the following:
● Apache Airflow
● Google BigQuery
● DevOps - Jenkins/Ansible/Terraform
● Docker / Kubernetes
● Informatica PowerCenter, Mulesoft, RabbitMQ (used for legacy data pipelines)
● Looker and Jaspersoft (for operational reporting)
Required Education:
or equivalent job
experience
We offer an Outstanding Benefits Package that includes:
Medical
Dental
Vision
STD & LTD
401K Retirement Plan
Basic Life & AD&D
Supplemental Life Insurance
Paid Time Off (PTO, STO, Holidays including Year-End Holiday Break)
HSA & FSA
Legal Plan
Pet Insurance
Tuition Reimbursement
Flexible Hybrid Work Environment
MLB Tickets
Tickets.com is an Equal Opportunity Employer.
Job Type: Full-time
Pay: $150,000.00 - $180,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Experience level:
10 years
Schedule:
8 hour shift
Work Location: Hybrid remote in New York, NY 10020

Health insurance"
735,Data Engineer,SVS Technologies,"Memphis, TN 38125",N,"Data Engineer, Memphis, TN: Limited domestic travel, telecommute and/or occasional relocation to client sites nationwide to work on every stage of data science projects; develop predictive models, identify key processes for improvement using advanced analytics/data science. Work on data gathering, sampling, EDA. Work in Hadoop, Hive Kafka, Maven, JUnit, MySQL, Jenkins environment. Reply to: SVS Technologies Limited, 8700 Trail Lake Drive, #228 Memphis, TN 38125"
736,Data Engineer Job Ref #: 478039,Concentrix Catalyst,"Omaha, NE 68102",N,"Overview
Concentrix CVG Customer Management Group Inc. has multiple openings for the position of Data Engineer based out of its U.S. offices in Omaha, NE. The employee may also work at various unanticipated locations throughout the U.S. Travel and/or relocation to various unanticipated locations throughout the U.S. is required. Telecommuting may be permitted.
Responsibilities
The position of Data Engineer will write, update, and maintain software applications; perform production maintenance of code; gather solutions requirements. Own technical commitments to clients and work with the team to successful delivery of solutions. Analyze, design, and code for complex requirements as well as write programs of complexity. Responsible for defining problems, collecting data, establishing facts, drawing valid conclusions, and preparing appropriate reports.
Qualifications
The position of Data Engineer requires a Master’s degree in Computer Science, Engineering (any), or any technical/analytical field that is closely related to the specialty, plus knowledge of the following: Spark, Hadoop, HBase and Hive.
To apply, send Resumes to ctlyst_postings@concentrix.com with Job Ref# 478039 in the subject line of the email."
737,Senior Data Engineer,Virginia Tech,"Blacksburg, VA•Remote",N,"Job no: 525109
Work type: Administrative & Professional
Senior management: Vice President-Info Technology
Department: Enterprise Systems
Location: Blacksburg, Virginia, Fully Remote, Hybrid
Categories: Information Systems / Technology
Job Description
Join a data team that works closely together and shares credit with our successes and responsibility with our failures. We debate architecture around best practices and new development ideas without tearing each other down. We provide great customer service to our business and technology partners by using relational collaboration and not transactional bureaucracy in order to help deliver data driven decisions to one of the top Universities in the country. We work with a wide range of technologies, and no we do not expect you to know all of them, but you will need to be a fast learner and be technically curious in order to be successful here.

There are many opportunities for growth, as VT has a wide range of technology experts who frequently give training in their area of expertise. As we move to a modern data-lake architecture, we need fast learners who are also passionate about supporting the existing processes that runs the University. The data management and analytics team is a part of central technology and runs Virginia Tech’s warehouse, data lake, data integrations and business intelligence solutions. We are looking for a problem solver who can help us deliver great service and technology solutions. Are you a good fit with the Hokies data management and analytics team?
Required Qualifications
Bachelor’s degree or equivalent relevant work experience in Information Technology related field
Architecture experience with ETL tools, like Talend, and data workflows where the candidate will setup framework and review code of other developers. This experience should include a cloud environment.
Extensive experience in relational databases, like Oracle, Postgres and Redshift including advanced SQL writing ability. Candidate will coach, guide and review work. Candidate will understand DBA role in order to communicate needs of the team
Experience with Business Intelligence tools such as MicroStrategy or Tableau
Demonstrated experience in working effectively in a team environment as a project leader. Strong communication and interpersonal skills with both technology team members and business partners. Ability to set competing priorities and meeting critical deadlines
Demonstrated experience in full system lifecycle development including requirements gathering, systems analysis, development of data services, testing and implementation of large enterprise systems as the subject matter expert
Preferred Qualifications
Extensive experience with process improvement that results in team buy-in and adoption
Demonstrated experience in managing priorities of others in an environment where individuals are working on multiple priorities with multiple skillsets
Extensive experience in data warehouse concepts and dynamic object architecture. Strong understanding in concepts and the features to maintain a well-structured and efficient database
Demonstrated experience with cloud technologies. Understanding architecture to where they can communicate needs to could engineers. Ideally AWS services such as EMR, Glue, Athena and Redshift
Experience in an object-oriented programming language like Python or Java.
Experience in an enterprise environment with DevOps best practices and technologies such as Docker, Version Control (GitLab), and Release Management (Jenkins)
Experience with data lakes in a Hadoop or AWS environment
Exposure to web services and APIs that return large data sets
Appointment Type
Regular
Salary Information
$90,000 - $110,000
Review Date
4/6/2023
Additional Information
The successful candidate will be required to have a criminal conviction check.
About Virginia Tech
Dedicated to its motto, Ut Prosim (That I May Serve), Virginia Tech pushes the boundaries of knowledge by taking a hands-on, transdisciplinary approach to preparing scholars to be leaders and problem-solvers. A comprehensive land-grant institution that enhances the quality of life in Virginia and throughout the world, Virginia Tech is an inclusive community dedicated to knowledge, discovery, and creativity. The university offers more than 280 majors to a diverse enrollment of more than 36,000 undergraduate, graduate, and professional students in eight undergraduate colleges, a school of medicine, a veterinary medicine college, Graduate School, and Honors College. The university has a significant presence across Virginia, including the Innovation Campus in Northern Virginia; the Health Sciences and Technology Campus in Roanoke; sites in Newport News and Richmond; and numerous Extension offices and research centers. A leading global research institution, Virginia Tech conducts more than $500 million in research annually.
Virginia Tech does not discriminate against employees, students, or applicants on the basis of age, color, disability, sex (including pregnancy), gender, gender identity, gender expression, genetic information, national origin, political affiliation, race, religion, sexual orientation, or military status, or otherwise discriminate against employees or applicants who inquire about, discuss, or disclose their compensation or the compensation of other employees or applicants, or on any other basis protected by law.
If you are an individual with a disability and desire an accommodation, please contact Brittany Bane at bmlester@vt.edu during regular business hours at least 10 business days prior to the event.
Advertised: March 23, 2023
Applications close:"
738,"Senior Data Engineer, Data Orchestration",Salesforce,Colorado•Remote,"$156,800 - $215,600 a year","To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Products and Technology
Job Details
We’re Salesforce, the Customer Company. If you believe in bringing companies and customers together, in business as the greatest platform for change, in creating a more equitable and sustainable future for all – well, you’re in the right place. Through our #1 CRM, Customer 360, we help companies blaze new trails and connect with their customers in a whole new way. And, we empower you to be a Trailblazer, too — driving your performance and growth, charting new paths, and improving the state of the world.
Senior Data Engineer, Data Orchestration
Slack is looking for a senior data engineer to join the Data Orchestration Team. As part of the Data Engineering organization, we build and operate the platform that ingests data into our Data Warehouse. We write software to manage the ingestion for thousands of stateful hosts and stateless real time logging events. Currently, our infrastructure handles 65PB+ of storage, processes ~900B records a day, 400+ ETL pipelines and 900+ Active Airflow DAGs. As Slack’s data grows (along with the number of customers, features and employees), the goal of the team is to build a highly scalable and resilient orchestration capabilities for our downstream customers so that they can focus on their strengths.

You will build scalable services and tools to help partners implement, deploy and analyze data assets with a high level of autonomy and limited friction. You will play a meaningful role in making partner interactions with the Data Warehouse pleasant and productive. You will have deep technical skills, be a self-starter, detail and quality oriented, and passionate about driving data driven decisions and having a huge impact at Slack!

Here are a few blog posts that shed light into what we do here at Slack:
Data Lineage at Slack - https://slack.engineering/data-lineage-at-slack/
Reliably Upgrading Apache Airflow at Slack's Scale - https://slack.engineering/reliably-upgrading-apache-airflow-at-slacks-scale/
Introducing Data Residency for Slack - https://slack.com/blog/transformation/introducing-data-residency-for-slack
Data Wangling At Slack - https://slack.engineering/data-wrangling-at-slack/
What you will be doing
Design and develop highly scalable and resilient services/data pipelines for data ingestion and processing using modern big data technologies
Develop and maintain our real time analytics/low latency data access layer built on top of modern OLAP solutions
Optimize the end-to-end workflow for data users at Slack (from crafting libraries to schedule data pipelines and access data assets).
Automate and handle the lifecycle of data sets (schema evolution, metadata management, change and backfill management, deprecation and migration).
Improve the data quality and reliability of the pipelines through properly monitoring and failure detection.
Comfortably collaborate with cross functional partners and lead technical initiatives end to end.
Be a role model and a multiplier, coaching and mentoring other engineers across the org.
Write, review, or provide feedback on a technical design proposal from others.
What you should have
5+ years of software/data engineering experience, including experience with Big Data technologies, e.g. OLAP, Airflow, Spark, Kafka, Hadoop, etc.
Experience operating Airflow clusters and knowledge of Airflow best practices.
Experience working with cloud infrastructure (AWS preferred).
You have extensive experience of building and maintaining large scale ETL pipelines and in-depth knowledge of various big data frameworks and architectures
You are skilled at crafting and building robust distributed Microservices with tools like Docker, Kubernetes, AWS ECS/EKS etc.
Experience in real time analytics/low latency data access layer with OLAP stores such as Apache Pinot or Apache Druid is a huge plus.
You have strong dedication to code quality, automation and operational excellence: CI/CD pipelines, unit/integration tests.
You are proficient in object-oriented and/or functional programming languages: Python, Java/Scala, Chef, Terraform
You have excellent written and verbal communication and interpersonal skills; able to effectively collaborate with cross functional partners and explaining sophisticated technical concepts to non-technical stakeholders
You have high growth expectations for yourself and your team, and a willingness to push yourself and your team to achieve them
Bachelor's degree in Computer Science, Engineering or related field, or equivalent training, fellowship, or work experience.

Slack has a positive, diverse, and supportive culture—we look for people who are curious, inventive, and work to be a little better every single day. In our work together we seek to be smart, humble, hardworking and, above all, collaborative.
More details about our company benefits can be found at the following link: https://www.getsalesforcebenefits.com/
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form .
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at Salesforce and explore our benefits.
Salesforce, Inc . and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce, Inc . and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce, Inc . and Salesforce.org will not pay any third-party agency or company that does not have a signed agreement with Salesforce, Inc . or Salesforce.org .
Salesforce welcomes all.
For Colorado-based roles, the base salary hiring range for this position is $156,800 to $215,600.
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience.
Certain roles may be eligible for incentive compensation, equity, and benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com."
739,Staff Software Engineer - Data Visualization - Full Stack,ServiceNow,"Atlanta, GA 30328•Remote",N,"Company Description

At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can’t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you.
With more than 7,400+ customers, we serve approximately 80% of the Fortune 500, and we're proud to be one of FORTUNE's 100 Best Companies to Work For® and World's Most Admired Companies® 2022.
Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow.
Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.

Job Description

You will be part of the ServiceNow Cloud Services Big Data Team. The Big Data team is building the next-generation platform that collects, stores and provides real-time access to large amounts of data.
You will be driving the design and implementation of ServiceNow in-house real-time data visualization and analytics platform to support the growth of ServiceNow.

What you get to do in this role:
Bring your innovation and experience in designing and developing the next generation data analytics platform using cutting-edge technologies.
Lead a global engineering team to drive end to end product design and implementation.
Standardize processes for complete development cycle including design, implementation, unit testing, code review, testing automation etc.
Research and adopt the right technologies to improve the scalability and productivity of the engineering group.
Work closely with key stakeholders and product owners to drive technical design for requirements of various use cases.
Coordinate with cross-function teams (DevOps, network, QA, etc.) to ensure a smooth cycle from development to deployment.

Qualifications

To be successful in this role you have:
6+ years of software development experience along with strong troubleshooting and debugging skills.
Expert level skills with JavaScript, ReactJS strongly preferred along with NodeJS, Webpack or other modern UI frameworks, Java and REST API development.
Hands-on experience architecting enterprise data analytics products with high scalability and performance.
Background with data analytics, data visualization, BI tools and Hadoop ecosystem.
Ability to produce high-quality software that is unit tested, code reviewed, and checked in regularly for continuous integration.
Solid background in complicated SQL & data analytics.
Zeal for learning and adopting new ideas and patterns.
Strong Computer Science fundamentals, data structures, algorithms, and software design.

Additional Information

ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law.
At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office.
If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance.
For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government.
Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.

From Fortune. © 2022 Fortune Media IP Limited All rights reserved. Used under license.
Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow."
740,Data Engineer,N,"Oakland, CA•Remote","$120,000 - $180,000 a year","Welcome to Hayden AI, where the future is ours to make. From bus lane and bus stop enforcement to digital twin modeling and more, our clients use our mobile perception system to speed up transit, make streets safer, and create a more sustainable future.
Led by experts in AI, computer vision, data science, transportation, and government technology, Hayden AI is pioneering real world problem solving powered by AI and machine learning.
Our mobile perception platform is currently utilized by New York City's MTA for automated bus lane enforcement. We recently raised $20 million in Series A funding and have been featured in Bloomberg CityLab, Forbes, Smart Cities Dive, and Politico.
Come join us, and help us build a vision-based data platform for smart enforcement and smart cities with our six core values in mind:
Customer Focus
Passion
Collaboration
Empowerment
Transparency
Integrity
Data Engineer

Hayden AI is hiring its first Data Engineer to support the company's effort in revolutionizing data analytics in public transit, by building out Hayden's data lake that enables advanced analytics and geographic information products. Reporting to VP of Data Science, this role will collaborate closely with data scientists, backend software engineers, and site reliability engineers, driving high impact data initiatives that are critical to the company's rapid growth.
Responsibilities:
Manage cloud based data storage for structured and unstructured data sets.
Deploy and monitor machine learning models produced by data scientists.
Design, deploy and maintain performant and scalable processes to acquire and manipulate data.
Participate actively in the agile software development process, including design reviews, code reviews, and brainstorming sessions. Keep documents accurate and updated.
Proactively remove roadblocks and escalate issues when encountered.
Required Qualifications:
Experience implementing a data lake architecture, enterprise data solutions
Experience with AWS Kinesis, Kafka, Storm, Spark, EMR.
Extensive experience with AWS data services: Redshift, RDS and DynamoDB.
Experience working with data scientists and/or machine learning engineers.
5+ years programming in one or more languages (Python, Scala, C/C++, ruby)
Experience with Agile process methodology, CI/CD automation, Test Driven Development.
BS or MS in Computer Science, Electrical Engineering, a related field
Excellent communication, verbal and written.
Desired Qualifications:
Experience with start-ups
Experience with NoSQL databases such as MongoDB and couchDB
Experience with GIS software and geospatial data.


Salary Range

120,000-180,000

There are endless learning and development opportunities from a highly diverse and talented peer group, including experts in a wide range of fields (AI, Computer Vision, Government Contracting, Systems & Device Engineering, Operations, Communications, and more)!
Just a few of our perks include:
Options for 100% company paid medical, dental, and vision coverage for employees and dependents (for US employees)
Flexible Spending Account (FSA) and Dependent Care Flexible Spending Account (DCFSA)
Life, AD&D, Short and Long Term Disability Insurance
Aflac Critical Illness, Accident Insurance & Hospital Indemnity Insurance
MetLife Legal Plan(s) & Pet Insurance
Farmers GroupSelect Auto & Home Insurance
401(k) with 3% company matching
Professional development reimbursement
Wellness stipends
Unlimited PTO
Remote work opportunities
Home office & technology reimbursement
Daily catered lunches in our Oakland office
Hayden AI is committed to creating a diverse and inclusive environment that fosters learning from each other. We celebrate people of diverse backgrounds, experiences, abilities, and perspectives. We are an equal opportunity employer and are committed to providing a work environment free of harassment and discrimination. Hayden AI is also committed to working with and providing reasonable accommodations to individuals with disabilities. Please let your recruiter know if you need an accommodation at any point during the interview process.
To all recruitment agencies: Hayden AI does not accept agency resumes. Please do not forward resumes to our jobs alias, Hayden AI employees or any other company location. Hayden AI is not responsible for any fees related to unsolicited resumes.
Privacy Policy"
741,Frontend Big Data Engineer - PySpark,Logic20/20 Inc.,Remote,"$130,000 - $162,500 a year","Company Description
We’re a seven-time “Best Company to Work For,” where intelligent, talented people come together to do outstanding work—and have a lot of fun while they’re at it. Because we’re a full-service consulting firm with a diverse client base, you can count on a steady stream of opportunities to work with cutting-edge technologies on projects that make a real difference.
Logic20/20's Global Delivery Model creates a connected experience for Logicians across geographies. You'll have access to projects in different locations, the technology to support Connected Teams, and in-person and online culture events in our Connected Hub cities.
Job Description
Bring your skillset to an exciting and meaningful initiative where we are leveraging data science, artificial intelligence, and machine learning to mitigate wildfires. By proactively identifying and addressing issues with power lines and related equipment, we’re increasing safety, saving lives, and protecting the environment. This is a highly visible, highly impactful project with implications for millions of customers.
As a Frontend Big Data Engineer, you’ll join our Data Management team to design and develop scalable data processing infrastructure. Applying an Agile approach, you’ll work closely with our team of analysts, technical product owners, and data scientists to provide the structure for a highly anticipated solution. You’ll leverage your balance of technical skills and business acumen to help the client better understand their core needs and technical capabilities. Your efforts will result in greater protection for the community and our environment, long-term.
About the team
The Logic20/20 Advanced Analytics team is where skilled professionals in data engineering, data science, and visual analytics join forces to build simple solutions for complex data problems. We make it look like magic, but for us, it’s all in a day’s work. As part of our team, you’ll collaborate on projects that help clients spin their data into a high-performance asset, all while enjoying the company of kindred spirits who are as committed to your success as you are. And when you’re ready to level up in your career, you’ll have access to the training, the project opportunities, and the mentorship to get you where you want to go.
“We build an environment where we really operate as one team, building up each other’s careers and capabilities.” – Adam Cornille, Director, Advanced Analytics
About you
You are collaborative, working with partners to understand business needs and pain points
You are determined and able to manage obstacles while maintaining a positive outlook
You are patient and savvy in explaining technical benefits and deficits to non-technical audiences
You have a passion for learning new data tools and best practices
You have built large-scale machine learning pipelines, quickly developing and iterating solutions
Qualifications
5+ years of data engineering experience
3+ years of implementation experience using PySpark
Solid experience with TypeScript or JavaScript
Strong understanding of high-performance ETL development with Python
Experience with Big Data Technologies (Hadoop, Spark, MongoDB)
Experience designing and developing cloud ELT and date pipeline with various technologies such as Python, Spark, PySpark, SparkSQL, Airflow, Talend, Matillion, DBT, and/or Fivetran
Demonstrated ability to identify business and technical impacts of user requirements and incorporate them into the project schedule
Ideal, but not required
An undergraduate degree in technology or business
Agile, Scrum, and/or SAFe experience and certifications
Experience building data and computational systems that support machine learning
Knowledge of AWS services
Experience with modern software delivery practices, including source control, testing, and continuous delivery
Experience with streaming data in Spark
Additional Information
All your information will be kept confidential according to EEO guidelines.
Compensation range: $130,000 - $162,500 annually
About Logic20/20
To learn more about Logic20/20, please visit: https://www.logic2020.com/careers/life-at-logic
Core Values
At Logic20/20, we are guided by three core values: Drive toward Excellence, Act with Integrity & Foster a Culture of We. These values were generated and agreed upon by our employees—and they help us pursue our goal of being one of the best companies to work for and to work with. Learn more at https://www.logic2020.com/company/our-values.
Logic20/20 Benefits
Why Logic20/20? It’s our goal to be one of the best companies to work for. One piece of the puzzle is an evolving set of benefits that extend past medical, dental, and 401(k).You will have
Career Development – A built-in program from day 1, providing a mentor and individually-directed training opportunities, plus access to leaders across the company
PTO, Paid Holidays, & Voluntary Leave – Worry-free time off to recharge and pursue your personal goals
Community & Committees – As part of our “Culture of We,” Logic20/20 invests in providing many social, interest, and learning opportunities
Recognition – From peer recognition, swag, and the chance to win a once-in-a-lifetime type of award, we make your Logic20/20 journey stand out
Referral Programs & Bonuses – Employee, project, and sales referral programs with paid incentives
Equal Opportunity Statement
We believe that people should be celebrated: for their talents, ideas, and skills, but most of all, for what makes them unique. We prohibit harassment and/or discrimination based on age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status, or any other basis as protected by federal, state, or local law. To learn more about our DE&I initiatives, please visit: https://www.logic2020.com/company/diversity-equity-inclusion
Privacy Policy
During the recruitment and hiring process, we gather, process, and store some of your personal data. We consider data privacy a priority. For further information, please view our company privacy policy.
Job Type: Full-time
Pay: $130,000.00 - $162,500.00 per year
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote"
742,Data Engineer,Impact Advisors LLC,United States,N,"Healthcare Data Engineer

Work You’ll Do:

As a Healthcare Data Engineer, you will work closely with a multidisciplinary Agile team to build high-quality data pipelines driving analytic solutions. Utilizing your deep understanding of data architecture, data engineering, data analysis, reporting, and basic understanding of data science, the solutions you create will generate insights from the organization’s connected data which will enable the advancement of data-driven decision-making capabilities within the enterprise. You will utilize your strong problem-solving skills, ability to work as part of a technical, cross-functional analytics team, and desire to solve complex data problems to deliver the insights which enable analytics strategies.

About Impact Advisors:

We deliver Best in KLAS advisory, implementation and optimization services to healthcare organizations. At Impact Advisors, we are committed to exceeding our clients’ expectations. We are a nationally recognized partner to many of the nation’s top healthcare organizations. Our commitment to patient-centered, value-driven outcomes has earned us some of the industry’s most prestigious awards. Please visit our website at www.impact-advisors.com for additional information.

Your Responsibilities:
Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals
Solve complex data problems to deliver insights that help business to achieve goals
Create data products for analytics and data scientist team members to improve their productivity
Advise, consult, mentor and coach other data & analytics professionals on data standards and practices
Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions
Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering
Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes
Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Learn about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics

Your Expertise:
Bachelor’s degree preferred; Computer Science, MIS, or Engineering preferred
Certification in one or more of the following Epic Systems modules: Cogito, Caboodle, Clarity, Reporting Workbench
5 years of experience working in data engineering or architecture role, 7+ preferred (3 years preferred for Jr. role)
Expertise in SQL and data analysis and experience with at least one programming language (Python preferred)
Significant experience developing and maintaining data warehouses in big data solutions (e.g., Snowflake, SAP Hana, Oracle, SQL Server, Teradata, etc.)
Experience with developing solutions on cloud computing services and infrastructure in the data and analytics space (preferred)
Database development experience using Hadoop or BigQuery and experience with a variety of relational, NoSQL, and cloud database technologies
Worked with BI tools such as Tableau, Power BI, Looker
Deep knowledge of data and analytics, such as dimensional modeling, ETL, reporting tools, data governance, data warehousing, structured and unstructured data.
Big Data Development experience using Hive, Impala, Spark and familiarity with Kafka
Exposure to machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics
Experience in using data base connections, SSIS, API, ODBC, etc.
Healthcare experience preferred but not required.

Our People and Culture:

We believe in a caring, fun, honest and autonomous work environment and we recognize that our dedication to our associates drives our success. Our mission to create a Positive Impact fuels our associates to innovate and deliver high value services to our clients.

In healthcare, many of the greatest ideas and discoveries come from a diverse mix of minds, backgrounds and experiences, and we are committed to cultivating an inclusive work environment. Impact Advisors provides equal opportunities to all employees and applicants for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, genetic disposition, neurodiversity, disability, veteran status, or any other protected category under federal, state and local law"
743,Jr-Mid Level Data Engineer/Analyst with Front-end experience,Ryan Consulting Group,Remote,"$70,000 - $80,000 a year","- Gather and create business requirements, understand client needs and communicate those needs internally and
externally.
- Gauge development size and timeline ranking projects/features/enhancements from S, M, L, XL, XXL. These were
backed by data (research) and were used by the business to determine which projects took priority, as well as
internally to apply necessary human resources to those projects.
- Utilize SQL to create/configure products based on requirements from the business partners
- Resolved application defects tracked in Jira. Analysis includes troubleshooting SQL, Perl, Java, and some Angular.
Actual code fixes pertain to SQL and Perl, including complex SQL update statements across multiple tables /
environments, and Perl ternary operators and statements.
- Consolidate hierarchal data structures into readable tabular data via SQL.
- Cleaned MySQL databases. Fixed bad data and poorly constructed tables, improved structure and readability of
stored procedures and admin scripts.
- Loaded via SQL inserts > 5 million distinct prices for dental and vision plans derived from incomplete data,
requiring complex update statements to be applied to the records for employer paid plans. Multiple criteria such
as risk class, group size, region (zip code), coverage level, etc. were factors in applying these updates.
- Collaborated with other developers and expanded SQL scripts repository in Bitbucket via GIT to track changes.
- Successfully implemented many plan introductions for states across the country, in addition to global features and
enhancements that apply to the products.
- Supported overnight deployments for updates that included all developers’ code changes over a given release
cycle, fixing any production support issues that arose in a timely manner.
HR requirements-
Minimum of 4 years’ experience
Bachelor’s Degree in Computer Science, Information Systems or programming
PERM, Remote role, 80k max salary
Job Type: Full-time
Pay: $70,000.00 - $80,000.00 per year
Benefits:
401(k)
Dental insurance
Flexible schedule
Health insurance
Paid time off
Vision insurance
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote

Health insurance"
744,Senior Materials Engineer - Data Management,TerraPower,"Bellevue, WA 98008","$105,237 - $154,855 a year","TITLE: Senior Materials Engineer - Data Management

LOCATION: Bellevue, WA, USA

TerraPower is a nuclear technology company based in Bellevue, Washington. At its core, the company is working to raise living standards globally through a more affordable, secure and environmentally friendly form of nuclear energy along with innovations in medical isotopes to improve human health. In 2006, TerraPower originated with Bill Gates and a group of like-minded visionaries who evaluated the fundamental challenges to raising living standards around the world. They recognized energy access was crucial to the health and economic well-being of communities and decided that the private sector needed to take action and create energy sources that would advance global energy deployment. TerraPower’s mission is to be a world leader in new nuclear technologies, while developing innovators and future leaders in the nuclear field. As a result, the company’s activities in the fields of nuclear energy and related sciences are yielding significant innovations in the safety and economics of nuclear power, hybrid energy and medical applications – all for significant human health benefits.

TerraPower is seeking to hire highly motivated and forward-thinking professionals who are interested in focusing on advanced nuclear reactor research and development and influencing change within the nuclear power landscape and bringing forward the critical production of medical isotopes. TerraPower is an Equal Opportunity Employer. We do not discriminate in hiring on the basis of sex, gender identity, sexual orientation, race, color, religious creed, national origin, physical or mental disability, protected Veteran status, or any other characteristic protected by federal, state, or local law. In addition, as a federal contractor, TerraPower has instituted an Affirmative Action Plan (AAP) in an effort to proactively recruit, hire, and promote women, minorities, disabled persons and veterans.

Senior Materials Engineer - Data Management
TerraPower, LLC is seeking a highly motivated Intern to support the engineering efforts of the Materials Selection & Data Management team. This role will work as part of a high functioning team of materials engineers focused on developing a database of materials properties for use in design of components for sodium cooled fast reactors. A primary function of this role will be to review and qualify digitized literature data to inform the development of a materials property database.

Responsibilities
Consolidate material property data from a variety of published sources as well as internal testing programs
Use fundamental materials engineering principles to develop material property correlations and material performance models as input for reactor design and licensing
Update and maintain a Python-based material database as well as associated design handbooks for Natrium
Inform materials test programs to validate models of material performance and ensure materials development and testing efforts meet programmatic objectives of the Natrium reactor

Key Qualifications and Skills
Minimum of 5 years of related experience with a B.S. degree; or 3 years with an M.S. degree; or equivalent experience; a degree in Materials Science/Engineering, Welding Engineering, or a related technical discipline is desired.
The successful candidate will possess a high degree of trust and integrity, communicate openly and display respect and a desire to foster teamwork.
Experience in analyzing and interpreting materials or environmental data is preferred
Understanding of common test methods and characterization for materials, such as creep, fracture toughness, charpy impact, and/or fatigue is preferred.
Experience with ferritic-martensitic and austenitic steels is preferred.
Actual position starting level and title will be determined based on assessment of qualifications.

Job Functions

Job Functions are physical actions and/or working conditions associated with the position. These functions may also constitute essential functions for the job which the employee must be able to fulfill, with or without accommodation. Information provided below is to help describe the job so that the applicant has a reasonable understanding of the job duties/expectations. An applicant's ability to perform and/or tolerate these actions and conditions will be discussed and workplace accommodations may be made on a case-by-case basis following an individualized assessment of the applicant and other considerations, including but not limited to any governing safety standards.
Motor Abilities: Sitting and/or standing for extended periods, bending/stooping, grasping/gripping, fine motor control (hands)
Physical exertion and/or requirements: Minimal, with ability to safely lift up to 25 pounds
Repetitive work: Prolonged
Special Senses: Visual and audio focused work
Work Conditions: Stairs, typing/keyboard, standard and/or sitting working environment of >8 hrs./day
Travel required: 0-5%

TerraPower’s technology is controlled for export by various agencies of the U.S. Government. TerraPower must evaluate applicants who are foreign nationals (other than asylees, refugees, or lawful permanent residents) in accordance with U.S. Government export control requirements. To facilitate TerraPower’s export control reviews, you will be asked as part of the application process to identify whether you are a U.S. Citizen or national, asylee, refugee, or lawful permanent resident of the United States. Government export authorization approval times vary. Based on the business needs for a particular position, TerraPower may not consider a foreign national from a country if it is impracticable to obtain timely Government export approval.
Job details
Salary Range: $105,237 - $154,855
Typically, our employee salaries are within .90 – 1.0 of the mid-point of the posted salary band. Any salary offered within the posted salary band is based on market data and commensurate with the selected individual’s qualifications and experience. This range is specific to Washington State.
Job Type: Full-time
Benefits:
Competitive Compensation
Salary, eligible to participate in discretionary short-term incentive payments
Comprehensive Medical and Wellness Benefits Medical
Vision
Dental
Life
Life and Disability
Gender Affirmation Benefits
Parental Leave
401k Plan
Generous Paid Time Off (PTO)
21 days of annually accrued PTO
Generous Holiday Schedule
10 paid holidays
Relocation Assistance
Professional and Educational Support Opportunities
Flexible Work Schedule
TerraPower Career and Benefits information: https://www.terrapower.com/contact-us/careers/
Please visit www.terrapower.com to apply"
745,"Engineer, InfoSec Consulting - Data Protection",Royal Caribbean Group,United States,N,"POSITION SUMMARY:
The Engineer, InfoSec Consulting is an expert in multiple technologies and works on a “strike team” that deploys securely transformative technologies to enable the business. Their goal is to be a security and IT evangelist that identifies innovative solutions, creates secure patterns, and validates that they are deployed correctly. This may include deploying new security tools, embedding in high-profile projects, and rapid execution of mergers and acquisitions.

This role requires developing in-depth knowledge of our security standards and baselines, assessing new system architecture against them, and finding ways to deliver security with enabling business agility.

The Engineer, InfoSec Consulting reports to the Director of Business Enablement within the Global Information Security team.

ESSENTIAL DUTIES AND RESPONSIBILITIES:
Serve as highly technical security experts to bring security transformation to both new and legacy infrastructure.
Deploy new technologies quickly, and successfully transition new platforms to the Security Operations Team for ongoing support
Embed with project teams to understand complex architectures, apply correct security controls, and ensure they are governable in the future
Development of patterns and validation of those patterns through engagement with business and lab work
Generation of strategies that brings security ahead of business need and provide proactive solutions
Effective communication that is both written and spoken with great follow up skills
Ability to drive outcomes and become the multidisciplinary expert that enables engagement

QUALIFICATIONS:
6+ years of information technology experience, with 3+ years of specialization in Information Security roles that include multiple areas of specialization.
Professional-level knowledge in 2+ of the following:
Security Engineering & Architecture assessments
Network Security Design (including IPS, WAF, and cloud-native protections)
Zero Trust Design & Implementation
Cloud Security Architecture & Serverless Infrastructure
DevSecOps Practices
API Security and driving reuse of capabilities across the organization
Data Security (identification & controls of sensitive data, including DLP)
Secrets Management
Identity and Access Management (including User Lifecycle Management)
Endpoint Management
Encryption and Certificates/PKI
Entrepreneurial approach to solving security problems with consultative skills"
746,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
747,"Data Modeler and Database Engineer, Lead",Booz Allen Hamilton,"Lexington, MA 02421•Hybrid remote","$93,300 - $212,000 a year","The Opportunity:
For an organization to transform in today’s digital world, it needs to properly collect, store, and organize its data. Effective data management can enable more efficient operations, yielding more growth. As a data architect, you know how to apply your creative-thinking and analytical mindset to help organizations manage their data assets. We’re looking for a data architecture lead like you to solve complex challenges and deliver leading-edge solutions for building software and systems, from vision to production ready.
As a data architect on our team, you’ll use your extensive technical expertise to lead the design of data architecture solutions for a system of systems architecture. You’ll resolve routine data architecture issues in collaboration with business analysts and technology teams by working with a cross-functional team to make decisions and recommendations on architecture modernization activities.
In this role, you’ll share your technical expertise, introduce best practices, and use tools like ERWin and MySQL Workbench. You’ll lead your team as it designs, defines, develops, and tests Cloud solution components, and you’ll serve as a liaison between clients and developers to ensure that requirements are met, and solutions are delivered.
With your motivation to establish processes and facilitate technological innovation, you’ll make a lasting impact on our nation's security.
Join us. The world can’t wait.
You Have:
8+ years of experience as a data operations lead, including a senior data director
2+ years of experience with ERWin, MySQL Workbench, or leading data modeling tools
Experience with Java, Javascript, and SQL
Experience with data migration from on-prem to AWS and Cloud, data synchronization, data replication for high availability and resiliency, backup and recovery, and RTO and RPO
Experience with AWS Outposts and AWS Edge services
Experience with leading a team of 10+ people to form data models for database transitions and larger change management activities
Knowledge of AWS and Cloud data architecture, data services, data API, and synchronous and asynchronous messaging
Secret clearance
Bachelor’s degree in Science, Technology, Engineering, or Mathematics

Nice If You Have:
Knowledge of C2 applications, Open Mission Systems, and Universal Command and Control Interface
Knowledge of DevSecOps
Possession of excellent verbal and written communication skills
Master’s degree

Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Secret clearance is required.
Create Your Career:
Grow With Us
Your growth matters to us—that’s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.
A Place Where You Belong
Diverse perspectives cultivate collective ingenuity. Booz Allen’s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you’ll build your community in no time.
Support Your Well-Being
Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we’ll support you as you pursue a balanced, fulfilling life—at work and at home.
Your Candidate Journey
At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we’ve compiled a list of resources so you’ll know what to expect as we forge a connection with you during your journey as a candidate with us.
Compensation
At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen’s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.
Salary at Booz Allen is determined by various factors, including but not limited to location, the individual’s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $93,300.00 to $212,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen’s total compensation package for employees.
Work Model
Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.
If this position is listed as remote or hybrid, you’ll periodically work from a Booz Allen or client site facility.
If this position is listed as onsite, you’ll work with colleagues and clients in person, as needed for the specific role.
EEO Commitment
We’re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change – no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law."
748,Data Science - Software Engineer,Plaxonic Technologies,"Austin, TX",$50 - $60 an hour,"Data Science - Software Engineer
Location: Austin, Texas
 API and UI so that users can fetch image data from Azure
 Lightweight task to check SharePoint to see if new images exist and if so, send them to our
image processing pipeline.
 A lightweight API to update our database when image tests change state
Job Type: Contract
Salary: $50.00 - $60.00 per hour
Schedule:
8 hour shift
Experience:
Data science: 8 years (Required)
API: 8 years (Required)
Python: 8 years (Required)
Work Location: On the road
Speak with the employer
+91 (727) 241- 5640"
749,Senior Data Engineer,Morgan Stanley,"New York, NY 10036",N,"Senior Data Engineer
Job Number:
3227245
POSTING DATE: Mar 21, 2023
PRIMARY LOCATION: Americas-United States of America-New York-New York
EDUCATION LEVEL: Bachelor's Degree
JOB: Global Sustainable Finance
EMPLOYMENT TYPE: Full Time
JOB LEVEL: Director
DESCRIPTION
The Global Sustainability Office (GSO) aims to drive the growth of sustainable investing through ongoing development of products and solutions, economic analysis, thought leadership and capacity building initiatives.

Within GSO, the Global Sustainable Finance (GSF) team is seeking a Senior Data Engineer to support the team's activities, with a particular focus on the team's expanding efforts in geospatial, high-frequency and distributed ledger data and analytics.

Successful candidates will have a dual passion for sustainability and technology, as well as demonstrated expertise in data engineering and distributed systems.

Additionally, successful candidates will be well-organized and detail oriented, and will work well in a collaborative team environment.

DESCRIPTION

Responsibilities include:

Working with the GSF data team and various internal partners to maintain and grow the existing infrastructure for ingesting, fusing, and distributing sustainability data
Develop and automate high quality data management to drive GSF's business growth and improve dataset usability across the firm
Build resilient data pipelines leveraging an internal framework to combine multiple internal and external sources
Design our data models for optimal storage and retrieval and to meet critical product and business requirements
Understand and influence logging to support our data flow, architecting logging best practices where needed
Contribute to shared Data Engineering tooling & standards to improve the productivity and quality of output for Data Engineers across the company
Improve data quality by using & improving internal and external tools to automatically detect issues
QUALIFICATIONS
Required:

5+ years of relevant industry experience
Bachelor's and/or master's degree, preferably in Computer Science or Software Engineering, or equivalent experience
Demonstrated ability to analyze data sets to identify gaps and inconsistencies, provide data insights, and advance effective product solutions
Experience designing and deploying systems with reliable monitoring and logging practices.
Working knowledge of data scraping, relational databases, query authoring (SQL)
Experience with maintaining and curating meta-data libraries and Git code repositories and supporting notebook-based data science, and workflow scheduling tools
Excellent communication skills, both written and verbal.

Exposure to one or more of the following areas is a plus:

Experience with dashboarding frameworks such as Dash or Bokeh is preferred
Experience with geospatial data integration, management and processing
Working knowledge of Scala, Java, and/or kdb/Q, and cloud-based data solutions including Snowflake and Databricks

Expected base pay rates for the role will be between 85,000.00 and 135,000.00 per year at the commencement of employment. However, base pay if hired will be determined on an individualized basis and is only part of the total compensation package, which, depending on the position, may also include commission earnings, incentive compensation, discretionary bonuses, other short and long-term incentive packages, and other Morgan Stanley sponsored benefit programs.

Morgan Stanley's goal is to build and maintain a workforce that is diverse in experience and background but uniform in reflecting our standards of integrity and excellence. Consequently, our recruiting efforts reflect our desire to attract and retain the best and brightest from all talent pools. We want to be the first choice for prospective employees.

It is the policy of the Firm to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, religion, creed, age, sex, sex stereotype, gender, gender identity or expression, transgender, sexual orientation, national origin, citizenship, disability, marital and civil partnership/union status, pregnancy, veteran or military service status, genetic information, or any other characteristic protected by law.

Morgan Stanley is an equal opportunity employer committed to diversifying its workforce (M/F/Disability/Vet)."
750,Data Engineer,B. Riley Financial,Remote,N,"B. Riley Financial (“B. Riley”) provides collaborative solutions tailored to fit the capital raising and business advisory needs of its clients and partners. B. Riley operates through several subsidiaries that offer a diverse range of complementary end-to-end capabilities spanning investment banking and institutional brokerage, private wealth and investment management, financial consulting, corporate restructuring, operations management, risk and compliance, due diligence, forensic accounting, litigation support, appraisal and valuation, auction and liquidation services. B. Riley is headquartered in Los Angeles with offices across the U.S. as well as an international presence.
At B. Riley Financial, we are building the next generation data infrastructure that will support our growing business intelligence and application development initiatives. One of the key roles for building and supporting this infrastructure will be the DevOps Data Engineer. This role will manage the internal data infrastructure on which we run our business. This includes capacity planning, installation, automation, configuration, migration, and operational duties such as availability monitoring, deployments, performance, security and enforcing segregation of duties for developer access to production. We are a small, highly capable development team backed up by offshore resources as needed.
Required Skills and Expertise:
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Education, Experience and additional skills that would be a plus:
Bachelor’s degree in computer science or related field
At least five (5) years experience in data engineering, or the equivalent combination of education and experience.
Proficiency in Python, C++, Java, R, and/or SQL.
Familiarity with Hadoop or suitable equivalent.
Excellent analytical and problem-solving skills.
Capacity to successfully manage a pipeline of duties with minimal supervision.
Microsoft SSRS
Power BI
Experience working at an investment bank
Experience with large data infrastructures
B. Riley Financial, Inc. employees enjoy competitive salaries, access to our 401(k) profit-sharing retirement plan, and our other benefits including paid holidays, vacation, and sick leave, voluntary group medical, dental, and vision insurance, and company-paid life and disability coverage.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Retirement plan
Vision insurance
Compensation package:
Bonus pay
Schedule:
Day shift
Education:
Bachelor's (Required)
Experience:
SQL: 3 years (Required)
Power BI: 3 years (Required)
SSIS: 3 years (Preferred)
Work Location: Remote

Health insurance"
751,Data Engineer,BI Labs,United States,N,"Position ID
SE
Job Title
Data Engineer
Description
Implementing the Project in AGILE based development Environment and participate in Daily standups, Backlog Refinement, Sprint Planning 1 & 2 and Retrospective meetings.
Involving in the phases of SDLC Analysis, Design phase, Development, UAT and Production phase of the application.
Conduct logical and physical database design.
Assemble large, complex data sets that meet functional/non-functional business requirements.
Design ETL processes and data pipelines to build large, complex datasets.
Build, test and validate analytical and statistical models.
Create centralized data stores to feed other applications.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies.
Implement, configure, administer and monitor Hadoop clusters.
Provide technical assistance to junior team members.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Using Maven build tool to build and deploy the application.
Design and develop micro-services that read data from Kafka and stores to MaprDB.
Create docker images and spin up containers on which the applications can run as a micro-service.
Creating JAR files and loading it to the central artifactory for other teams to use.
Working with source code version control GIT/GitHub and Used GIT for branching and merging of source code management.
Design, Development and create program specifications and unit test plans for Quality Management.
Qualification
Bachelorâ€™s degree in Computer Science or a closely related field
Location
USA"
752,Data Engineer,Massachusetts General Hospital(MGH),"Charlestown, MA 02129",N,"Data Engineer
- (3227183)

GENERAL SUMMARY/ OVERVIEW STATEMENT:
The Data Engineer will be part of a diverse interdisciplinary team of computer and neuroscientists with broad expertise spanning computer science, neuroscience, psychology, psychiatry, neuropsychology, cognitive neuroscience, neuroimaging, bioinformatics, biostatistics, epidemiology, and neurophysiology. The project will involve data processing and coordination for new multisite data collection networks, as well as develop and apply stratification tools to identify and validate biomarkers to predict outcome trajectories in individuals at high risk to develop psychosis.
This position joins the data management, processing and archiving team. The successful candidate will help develop, deploy and maintain the bioinformatics infrastructure of several large projects. Our group (spanning MGH and BWH) is also actively developing new technologies to characterize brain structure and function, which has led to the design of state-of-the-art image analysis pipelines capable of robustly processing hundreds of neuroimaging datasets. The role of the engineer will be to maintain and enhance existing image processing pipelines and to develop new pipelines based on the latest research with an emphasis on version tracking, data provenance, and high performance computing.
PRINCIPAL DUTIES AND RESPONSIBILITIES:
Relevant activities include, but are not limited to the following:
Maintain and enhance existing image processing pipelines.
Design new image processing pipelines, with an emphasis on version tracking, data provenance, and high performance computing.
Develop neuroinformatics tools to track data provenance and project management.
Test and evaluate a range of neuroimaging packages to determine their suitability for research goals.
Regular, direct interaction with neuroscientists from within and outside the DPACC to assist them with neuroimaging data analysis using a range of methods including FSL, SPM, Slicer and other specialized tools.
Design, implement, test, maintain and support applications to capture, manage, archive and monitor multi-site, multi-modal study data. Applications may include but are not limited to study monitoring systems, data management systems, workflow execution and monitoring systems, interactive viewers, and reporting tools.
Support web and application server configuration and deployment.
Support data engineering efforts, including database and API design, data extraction/transformation/load, and data aggregation/integration.
Containerize and deploy software and workflows on local high performance computing platforms and cloud computing infrastructure (AWS).

Required:
Bachelor’s Degree in Computer Science, Mathematics, Physical Sciences, Engineering, or related field
Excellent programming skills in Python, Bash, MATLAB
Superior Linux/Unix skills and comfort with command line programs – the ability to get new programs and packages running, overcoming hurdles as they arise, is particularly helpful.
Familiarity with standard software evolution method—version controlling (Git), pull requests, code reviews, issue and release management
Ability to work in an interdisciplinary, diverse, and international team in a highly collaborative and intellectually challenging environment.
Excellent oral and written communication skills
Basic knowledge of neuroscience and neuroanatomy.
Understanding of structural, diffusion, and functional Magnetic Resonance Imaging.
Preferred:
Master’s Degree in Computer Science, Mathematics, Physical Sciences, Engineering, or related field
Familiarity with C/C++ programming
Experience in neuroimaging software FSL, FreeSurfer, Slicer, DIPY, Nipype, Neurodocker, REDCap, XNAT
Experience with database management systems (e.g., SQL, PostgreSQL, MongoDB, CouchDB)
Experience with Linux container engines (e.g., Docker, rkt) and container orchestration systems (e.g., Kubernetes)
Experience with JavaScript libraries for interactive data visualization (e.g. d3, Recharts, Charts.js).
Experience with at least one web framework for building single-page web applications (e.g., React, Angular, Vue)
EEO Statement

Massachusetts General Hospital is an Equal Opportunity Employer. By embracing diverse skills, perspectives and ideas, we choose to lead. Applications from protected veterans and individuals with disabilities are strongly encouraged.
Primary Location MA-Charlestown-MGH 13th Street
Other Locations MA-Charlestown
Work Locations MGH 13th Street 149 13th Street Charlestown 02129
Job IT/Health IT/Informatics-Engineer
Organization Massachusetts General Hospital(MGH)
Schedule Full-time
Standard Hours 40
Shift Day Job
Employee Status Regular
Recruiting Department MGH Psychiatry
Job Posting Jan 3, 2023"
753,Data Engineer - Data Science & Analytics,Costco Wholesale,"Seattle, WA 98134","$100,000 - $135,000 a year","This is an environment unlike anything in the high-tech world and the secret of Costco’s success is its culture. The value Costco puts on its employees is well documented in articles from a variety of publishers including Bloomberg and Forbes. Our employees and our members come FIRST. Costco is well known for its generosity and community service and has won many awards for its philanthropy. The company joins with its employees to take an active role in volunteering by sponsoring many opportunities to help others. In 2021, Costco contributed over $58 million to organizations such as United Way and Children's Miracle Network Hospitals.

Costco IT is responsible for the technical future of Costco Wholesale, the third largest retailer in the world with wholesale operations in fourteen countries. Despite our size and explosive international expansion, we continue to provide a family, employee centric atmosphere in which our employees thrive and succeed. As proof, Costco ranks seventh in Forbes “World’s Best Employers”.
The Data Engineer - Data Analytics is responsible for the end to end data pipelines to power analytics and data services. This role is focused on data engineering to build and deliver automated data pipelines from a plethora of internal and external data sources. The Data Engineer will partner with product owners, engineering and data platform teams to design, build, test, and automate data pipelines that are relied upon across the company as the single source of truth.


If you want to be a part of one of the worldwide BEST companies “to work for”, simply apply and let your career be reimagined.

ROLE
Develops and operationalizes data pipelines to make data available for consumption (BI, Advanced analytics, Services).
Works with data architects and data/BI engineers to design data pipelines and recommends ongoing optimization of data storage, data ingestion, data quality, and orchestration.
Designs, develops, and implements ETL/ELT processes using IICS (informatica cloud).
Uses Azure services such as Azure SQL DW (Synapse), ADLS, Azure Event Hub, Azure Data Factory to improve and speed up delivery of our data products and services.
Implements big data and NoSQL solutions by developing scalable data processing platforms to drive high-value insights to the organization.
Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery.
Identifies ways to improve data reliability, efficiency, and quality of data management.
Communicates technical concepts to non-technical audiences both written and verbal.
Performs peer reviews for other data engineer’s work.

REQUIRED
5+ years’ experience engineering and operationalizing data pipelines with large and complex datasets.
5+ years’ of hands on experience with Informatica PowerCenter.
2+ years’ of hands on experience with Informatica IICS.
3+ years’ experience working with Cloud technologies; such as ADLS, Azure Databricks, Spark, Azure Synapse, Cosmos DB, and other big data technologies.
5+ years’ experience with Data Modeling, ETL, and Data Warehousing.
2+ years’ hands on experience implementing data integration techniques such as event / message based integration (Kafka, Azure Event Hub), ETL.
3+ years’ hands on experience with Git / Azure DevOps
Extensive experience working with various data sources; SQL,Oracle database, flat files (csv, delimited), Web API, XML.
Advanced SQL skills; Understanding of relational databases, business data, and the ability to write complex SQL queries against a variety of data sources.
Strong understanding of database storage concepts; Data Lake, Relational Databases, NoSQL, Graph, Data Warehousing.
Able to work in a fast-paced agile development environment.

Recommended
Microsoft Azure/similar certifications.
Experience delivering data solutions through agile software development methodologies.
Exposure to the retail industry.
Excellent verbal and written communication skills.
Experience working with SAP integration tools including BODS.
Experience with UC4 Job Scheduler.
BA/BS in Computer Science, Engineering, or equivalent software/services experience.

Required Documents
Cover Letter
Resume


California applicants, please click here to review the Costco Applicant Privacy Notice.

Apart from any religious or disability considerations, open availability is needed to meet the needs of the business. If hired, you will be required to provide proof of authorization to work in the United States. Applicants and employees for this position will not be sponsored for work authorization, including, but not limited to H1-B visas.
Pay Ranges:
Level 2 - $100,000 - $135,000,
Level 3 - $125,000 - $165,000
Level 4 - $155,000 - $195,000, Bonus and Restricted Stock Unit (RSU) eligible
We offer a comprehensive package of benefits including paid time off, health benefits — medical/dental/vision/hearing aid/pharmacy/behavioral health/employee assistance, health care reimbursement account, dependent care assistance plan, commuter benefits, short-term disability and long-term disability insurance, AD&D insurance, life insurance, 401(k), stock purchase plan, SmartDollar financial wellness program, to eligible employees.
Costco is committed to a diverse and inclusive workplace. Costco is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or any other legally protected status. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to IT-Recruiting@costco.com
If hired, you will be required to provide proof of authorization to work in the United States."
754,Data Engineer,O'Reilly Media,Remote,"$110,000 - $140,000 a year","About Your Team
Our data engineering team has a strong focus on delivering high-quality, reliable data to platforms and people within O’Reilly as well as building high-performance, scalable and extensible systems. We are intentional in our search for teammates who are helpful, respectful, communicate openly, and are always willing to do what’s best for our users. We keep a close eye on our pipelines and processes to make sure we’re delivering useful, timely improvements to aid decision-making and data visualization within O’Reilly. The team is broadly distributed across the US in multiple cities and time zones and constantly encourages each other to deliver work that instills pride and fulfillment.

Salary Range: $110,000 - $140,000
About the Job
We are looking for a thoughtful and experienced data engineer to help grow a suite of systems and tools written primarily in Python. The ideal candidate will have a deep understanding of modern data engineering concepts and will have shipped or supported code and infrastructure with a user base in the millions and datasets with billions of records. The candidate will be routinely implementing features, fixing bugs, performing maintenance, consulting with product managers, and troubleshooting problems. Changes you make will be accompanied by tests to confirm desired behavior. Code reviews, in the form of pull requests reviewed by peers, are a regular and expected part of the job as well.
Job Details
In a normal week, you might:
Develop a new feature from a user story using Python and PostgreSQL or BigQuery
Collaborate with product managers to define clear requirements, deliverables, and milestones
Team up with other groups within O’Reilly (e.g. data science or machine learning) to leverage experience and consult on data engineering best practices
Review a pull request from a coworker and pair on a tricky problem
Provide a consistent and reliable estimate to assess risk for a project manager
Learn about a new technology or paper and present it to the team
Identify opportunities to improve our pipelines through research and proof-of-concepts
Help QA and troubleshoot a pesky production problem
Participate in agile process and scrum ceremonies

Why you'll love working on our team:
You'll be working for a company that embraces and pursues new technology
You'll be working with a company that trusts and engages its employees
We believe in giving engineers the tools and hardware that they need to do their job
Bi-weekly virtual team hangouts and space to learn new skills (we’re a learning company after all!)
Great company benefits (health/dental/vision insurance, 401k, etc.)
We care deeply about work-life balance and treat everyone like human beings first

About You
What we like to see for anyone joining our data engineering teams:
Proficiency in building highly scalable ETL and streaming-based data pipelines using Google Cloud Platform services and products
Proficiency in large scale data platforms and data processing systems such as Google BigQuery and Amazon Redshift
Excellent Python and PostgreSQL development and debugging skills
Experience building systems to retrieve and aggregate data from event-driven messaging frameworks (e.g. RabbitMQ and Pub/Sub)
Strong drive to experiment, learn and improve your skills
Respect for the craft—you write self-documenting code with modern techniques
Great written communication skills—we do a lot of work asynchronously in Slack and Google Docs
Empathy for our users—a willingness to spend time understanding their needs and difficulties is central to the team
Desire to be part of a compact, fun, and hard-working team

Not required, but for bonus points:
Experience with Google Cloud Dataflow/Apache Beam
Experience with Django RESTful endpoints
Experience working in a distributed team
Knowledge and experience with machine learning pipelines
Contributions to open source projects
Knack for benchmarking and optimization
Minimum Qualifications
2+ years of professional data engineering (or equivalent) experience
1+ year experience of working in an agile environment

About O’Reilly Media
O’Reilly’s mission is to change the world by sharing the knowledge of innovators. For over 40 years, we’ve inspired companies and individuals to do new things—and do things better—by providing them with the skills and understanding that’s necessary for success.
At the heart of our business is a unique network of experts and innovators who share their knowledge through us. O’Reilly Learning offers exclusive live training, interactive learning, a certification experience, books, videos, and more, making it easier for our customers to develop the expertise they need to get ahead. And our books have been heralded for decades as the definitive place to learn about the technologies that are shaping the future. Everything we do is to help professionals from a variety of fields learn best practices and discover emerging trends that will shape the future of the tech industry.
Our customers are hungry to build the innovations that propel the world forward. And we help you do just that.
Learn more: https://www.oreilly.com/about/

Diversity
At O’Reilly, we believe that true innovation depends on hearing from, and listening to, people with a variety of perspectives. We want our whole organization to recognize, include, and encourage people of all races, ethnicities, genders, ages, abilities, religions, sexual orientations, and professional roles.
Learn more: https://www.oreilly.com/diversity"
755,"Bioinformatics Software Engineer III, Data Scientist",Memorial Sloan Kettering Cancer Center,"New York, NY 10065","$116,800 - $192,800 a year","Pay Range: $116,800.00-$192,800.00 Company Overview:
At Memorial Sloan Kettering (MSK), we’re not only changing the way we treat cancer, but also the way the world thinks about it. By working together and pushing forward with innovation and discovery, we’re driving excellence and improving outcomes. We’re treating cancer, one patient at a time. Join us and make a difference every day.

In compliance with applicable State regulatory authorities, COVID-19 vaccination is mandatory for all MSK staff. Staff are considered fully vaccinated upon completion of a primary vaccination series for COVID-19 (i.e., one dose of a single dose vaccine or a final dose of a multi-dose vaccine series). Exceptions from the COVID-19 vaccine requirement are permitted for those who request and receive an approved medical or fully remote exemption. Requests for medical exemption can take up to two weeks to review. Vaccination or an approved medical exemption is required to start work.

Staff working at a MSK New Jersey location must be up to date with COVID-19 vaccination, which includes having completed the primary COVID-19 vaccination series and booster once eligible as mandated by New Jersey State. All New Jersey staff not yet eligible for a booster must receive a booster within 3 weeks of becoming eligible as a condition of continued employment at MSK.

Note: Individuals are eligible to receive a COVID-19 booster two months after completion of a primary vaccination series for COVID-19. Because vaccine-induced immunity can wane over time, MSK continues to strongly urge all staff to get a COVID-19 booster as soon as they become eligible to maintain their protective immunity for a longer period.
Job Description:
Are you passionate about using technology to solve life impacting challenges? Help us change how the world treats cancer! We can provide you with the opportunity to make a difference in your career. We are searching for a dedicated and hard-working Data Scientist to contribute to the analysis of large-scale cancer genomics and imaging datasets. You will work closely with software and data engineers, clinicians, molecular biologists, and other data scientists to develop and implement cutting-edge data analysis approaches that integrate multi-omics and clinical data to better understand the molecular basis of cancer and improve patient outcomes.

You Are:
Interested in applying advanced statistical and machine learning methods to extract meaningful insights from complex and diverse data types, including transcriptomic, proteomic, epigenomic, imaging, and clinical data
An excellent communicator, highly collaborative, and with the ability to work in a multidisciplinary team
Analytical with strong problem-solving skills and keen attention to detail
Able to work effectively in a fast-paced environment and eager to solve complex problems
Passionate about reproducibility in data science
Resilient in recovering from setbacks and skilled at finding detours around problems
You Will:
Analyze large-scale imaging, genomics, and clinical datasets to identify genetic and molecular signatures associated with cancer progression and treatment response
Collaborate with interdisciplinary teams to develop and implement data analysis pipelines, from data pre-processing and quality control to statistical modeling and interpretation
Communicate results and insights effectively to scientific and clinical teams, as well as external partners and collaborators
Stay up-to-date with the latest advancements in cancer genomics
Contribute to scientific publications, presentations, and grant applications
You Have:
A Ph.D. in computational science or a Master’s degree with 2+ years experience
Proficiency in one or more programming languages such as Python or R, and ability to work efficiently with associated data analysis packages
Experience with statistical and machine learning methods analysis, such as regression, clustering, dimensionality reduction, and deep learning
Familiarity with data storage and management systems, such as SQL and NoSQL databases, data lakes, and data warehouses
Familiarity with genomics and imaging data formats and standards, such as BAM, VCF, DICOM, and NIfTI
#LI-POST Location : 323 E 61st Street (Macklowe)

Are you ready to learn more about our Benefits?

Pay Range $116,800.00-$192,800.00

Please click to learn more about MSK’s compensation philosophy.
Closing:
MSK is an equal opportunity and affirmative action employer committed to diversity and inclusion in all aspects of recruiting and employment. All qualified individuals are encouraged to apply and will receive consideration without regard to race, color, gender, gender identity or expression, sexual orientation, national origin, age, religion, creed, disability, veteran status or any other factor which cannot lawfully be used as a basis for an employment decision.

Federal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment."
756,Data Engineer,N,"Fort Worth, TX 76155","$100,000 - $110,000 a year","Role: Data Engineer/ Admin
Location: Fort Worth, TX 76155 (Hybrid 2 days a week onsite)
Duration: Direct Hire
Education/Credentials/Certifications:
· Bachelor’s Degree in the field of Computer Science, MIS, IT, or Engineering
Experience
Minimum three (3) years of experience in data modeling, analytics, and reporting.
Minimum two (2) years of experience with Power BI, Tableau, and/or SAS.
Minimum one (1) year experience with Microsoft SQL Administration.
Specific Skills Required:
Prior experience with database and model design and partitioning techniques.
Practical experience in statistical analysis and reporting utilizing statistical packages such as Excel, SPSS, Power BI, and SAS.
Working knowledge in Python. Experience with C# preferred.
Experience with Snowflake, Oracle, Hadoop, MongoDB, and other SQL and No-SQL systems preferred.
Basic Microsoft SQL Server Administration/SSIS.
Job Type: Full-time
Salary: $100,000.00 - $110,000.00 per year
Experience level:
9 years
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Fort Worth, TX 76155: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 4 years (Required)
Analytics: 4 years (Required)
Reporting: 4 years (Required)
Power BI: 3 years (Required)
Tableau: 3 years (Preferred)
SAS: 3 years (Preferred)
Python: 1 year (Required)
C#: 1 year (Preferred)
Snowflake: 1 year (Preferred)
Hadoop: 1 year (Preferred)
SSIS: 1 year (Required)
Security clearance:
Secret (Preferred)
Work Location: One location
Speak with the employer
+91 7035961600"
757,Senior Business Intelligence Engineer - Medicaid Encounter Data Submissions,Humana,"Louisville, KY 40202•Remote",N,"The Senior Business Intelligence Engineer solves complex business problems and issues using data from internal and external sources to provide insight to decision-makers. The Senior Business Intelligence Engineer work assignments involve moderately complex to complex issues where the analysis of situations or data requires an in-depth evaluation of variable factors.
Responsibilities
The Senior Business Intelligence Engineer describes the tools, technologies, applications and practices used to collect, integrate, analyze, and present an organization's raw data in order to create insightful and actionable business information. Begins to influence department’s strategy. Makes decisions on moderately complex to complex issues regarding technical approach for project components, and work is performed without direction. Exercises considerable latitude in determining objectives and approaches to assignments.
Required Qualifications
Bachelor's degree and 5 or more years of technical experience in data analysis OR Master's degree and 3 or more years of experience
3+ years experience with reading/writing SQL
Comprehensive knowledge of Microsoft Office Applications including Word, Excel, Access and PowerPoint
Advanced experience working with big and complex data sets within large organizations
Excellent communication skills; both written and verbal
Experience analyzing data to solve a wide variety of business problems and create data visualizations that drive strategic direction
Proficiency in understanding Healthcare related data
Proficiency in verbal and written communication to senior and executive leadership
Must be passionate about contributing to an organization focused on continuously improving consumer experiences
Preferred Qualifications
Advanced Degree in a quantitative discipline, such as Mathematics, Economics, Finance, Statistics, Computer Science, Engineering or related field
Advanced in SAS and other data systems
Experience with PowerBI for creating data visualizations
Expertise in data mining, forecasting, simulation, and/or predictive modeling
Prior experience with claims processing and/or claims analysis
Additional Information
This position is open to working remote in the eastern time zone and required to attend training meetings in Louisville, Kentucky as needed
Humana and its subsidiaries require vaccinated associates who work outside of their home to submit proof of vaccination, including COVID-19 boosters. Associates who remain unvaccinated must either undergo weekly negative COVID testing OR wear a mask at all times while in a Humana facility or while working in the field. Every associate and contractor who work inside a Humana facility or in the field, regardless of vaccination status, must complete a daily health screening questionnaire.
WAH requirements: Must have the ability to provide a high-speed DSL or cable modem for a home office. Associates or contractors who live and work from home in the state of California will be provided payment for their internet expense. A minimum standard speed for optimal performance of 25x10 (25mpbs download x 10mpbs upload) is required. Satellite and Wireless Internet service is NOT allowed for this role. A dedicated space lacking ongoing interruptions to protect member PHI / HIPAA information.
As part of our hiring process for this opportunity, we will be using an exciting interviewing technology called Modern Hire to enhance our hiring and decision-making ability. Modern Hire allows us to quickly connect and gain valuable information from you pertaining to your relevant skills and experience at a time that is best for your schedule.
Scheduled Weekly Hours
40

Not Specified
0"
758,Data Engineer,Ixaris,"Malta, MT",N,"Ixaris is recruiting a Data Engineer to join our diverse, ambitious team.

Who we are
From launching Europe’s first virtual cards to shaping the future of payments, innovation is in our DNA. Ixaris is a Principal Issuing Member of Visa and Mastercard. We help customers in more than 50 countries make smarter payment choices, processing.

We are on a mission to make payments smarter because we know payments can be a driver for innovation and growth across every business and industry. Our challenges are unique, so we need as many different voices as possible to join us in solving them. Voices like yours.

The Data Engineer
The Data Engineer role is part of the Business Intelligence function at Ixaris. The Data Engineer collaborates closely with stakeholders across the company to design, develop and maintain our payments data warehouse and business intelligence solutions. Data Engineers draw their expertise and experience to assist in engineering high-quality solutions to complex problems in the domain, helping to shape the future of our platform and constantly raise the bar.

What you'll do

Acquire and maintain an in-depth understanding of the business logic embodied in our data solutions, its development infrastructure and project delivery process.
Work in a multi-functional team to design and implement phases of product and technical initiatives, making effective data engineering decisions.
Be accountable for quality code and a champion of best industry practices, keeping up to date with technological trends.
Actively participate in guilds and community of practice teams and initiatives, shaping the technology roadmap
Eager to learn about new technologies and adapt to team dynamics.
Ability to support the running of data solutions in a critical environment.
Urge to learn about the dynamics of the payments eco-system.

Who you are
2+ years of experience working in data or business intelligence environments.
Well-rounded knowledge of cutting-edge technologies and data engineering practices and processes, up to date with the latest developments.
Good knowledge of data warehouse design and concepts
Good knowledge of an object-oriented language such as Python, Java or Scala
Expert in SQL
Experience with Database Management Systems such as Microsoft SQL Server and MySQL.
Proficient in using a data integration tool such as Talend, Pentaho or SSIS.
Experience using a data visualisation tool such as PowerBI, Tableau or QlikView.
Experience of an analytics engine such as Apache Spark and ability to write applications using libraries such as DataFrames and Spark Streaming is desirable.
Knowledge of statistical computing language such as R will be considered an asset.

Perks of the job
Assistance relocating to Malta.
Full health insurance coverage.
Annual wellness stipend for sports equipment or gym memberships.
Free-flowing, healthy snacks at our Malta office location.
Professional training and development opportunities.
Paid maternity and paternity leave."
759,Data Engineer,Arrive Logistics,"Austin, TX 78744•Hybrid remote",N,"Who We Are
Arrive Logistics is one of the fastest-growing freight brokerage firms in the US, with over $2 billion in annual revenue and plans to grow significantly year over year. Our success is a testament to our remarkable team and what we’re building together. We’re committed to providing employees with a meaningful work experience and have established an award-winning culture that supports personal and career development in a fun, casual and collaborative environment. There’s never been a more exciting time to get on board, so read on to learn more and apply today!

Who We Want
As a Data Engineer at Arrive Logistics, you’re joining a small team with big plans to improve the way our teams in the analytical space work with data. We’re building a modern Data Platform from the ground up. Arrive’s data capabilities are expanding and you’ll have a great opportunity to help define new design patterns, frameworks, and best practices working with data to support Analytics and Data Science. You’ll work alongside data and analytics engineers, software engineers, and data scientists within Arrive’s Tech arm to define and deploy new data infrastructure and tooling as well as own the pipelines that ingest and transform data integral to our analytics, BI, and data science initiatives.

You’ll sit within the Data organization (Data Engineering, Machine Learning Engineering, Analytics Engineering, Data Science) supporting internal technology powering Arrive Logistics in the fast-paced freight industry. If you’re interested in building the foundation that enables high-business value machine learning applications and data-driven decision making alongside a passionate team, read on.

Arrive Logistics is unable to provide visa sponsorship for this position now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.
What You’ll Do
Build and maintain data systems and ELT/ETL pipelines
Build and maintain pipelines to extract data from various source systems and organize raw data within our data warehouse
Build and maintain data pipelines to engineer features for Arrive’s machine learning systems
Collaborate with other members of the Data Engineering team to review code and provide feedback on new data-related systems and design patterns
Write new and contribute to existing technical/functional documentation on Data Engineering-owned systems and standards
Build data quality frameworks to enhance observability and reliability of Data Engineering-owned systems and pipelines
Collaborate with analysts and business stakeholders to improve data systems that feed Business Intelligence, dashboarding, and reporting tools
Configure secure integrations between Engineering and Analytics systems needing to interact with our Data Platform
Take part in an on-call rotation to respond to and debug data pipeline issues
Qualifications
2+ years of experience in a technical role focused on data
Experience building and deploying ETL/ELT pipelines
2+ years of SQL experience
1+ years of Python experience
Degree in Computer Science, IT, Analytics, or related field
Experience with modern and industry standard data tooling such as Snowflake, Airflow, dbt, ADF and BI tools
Excellent written and oral communication and presentation skills
The Perks of Working With Us
Take advantage of excellent benefits, including health, dental, vision, and life coverage.
Invest in your future with our matching 401K program.
Enjoy the flexibility of a hybrid work-from-home schedule based on position and tenure.
Build relationships and find your home at Arrive through our Employee Resource Groups.
Get recognized through our employee rewards program.
Leave the suit and tie at home; our dress code is casual.
Eat for free on Mondays and Fridays! Breakfast is provided on Mondays and lunch on Fridays. Snack and lunch options are also available daily.
Work in the booming city of Austin, TX - we are in a convenient location close to the airport and downtown.
Park your car for free on site!
Sweat it out using local gym discounts or with the team at our onsite gym.
Maximize your wellness with free counseling sessions through our Employee Assistance Program
Start your morning with a specialty drink from our fully stocked coffee bar, Broker’s Brew.
Get paid to work with your friends through our Referral Program!
Get relocation assistance! If you are not local to the area, we offer relocation packages and have a Relocation Specialist who can help you along the way.
Your Arrive Experience
When we say “award-winning culture,” we mean it. We’ve already earned “Best Place to Work” honors from Inc. Magazine (three years in a row!), Austin Business Journal and the Chicago Tribune. We intend on topping many more of those lists in the years to come, but we’re not in it for the trophies. We’re committed to culture because it keeps us connected to each other and invested in our shared success while having a blast along the way. Our employee-founded resource groups create communities within Arrive’s walls, including Women in Logistics, Emerging Professionals, PRISMS, Black Logistics Group, and Salute."
760,Principal Software/Data Engineer,Liberty Mutual,Remote,"$140,000 - $189,000 a year","Description
We are looking for Principal Software/Data Engineer to bring their passion and skill for software, architecture, engineering practices, and data architecture to lead the transformation and aggressive modernization of our platforms. This person will be a key member of an agile development team building out services and data transformations. This person will have extensive development experience in JAVA and Python in an AWS Cloud native platform. The role will act as a thought leader in this space, helping your squad deliver innovative solutions, as well as sharing best practices across the broader portfolio.
Knowledge:
In-depth knowledge of software engineering best practices (coding standards, code reviews, source control management, build processes, testing)
In-depth knowledge of design patterns and architecture principles
Familiar with security and data privacy policies, especially related to cloud service providers
Aware of emerging technologies and their use cases
Understands how to interpret and leverage Agile delivery concepts including backlog tracking, metrics, and incremental delivery
Qualifications
Bachelor or Master`s degree in technical or business discipline or equivalent experience, technical degree preferred
Generally 8+ years of professional experience
Technical leadership and mentoring engineers
Java, Python and SQL and or other programming languages
AWS including RDS Databases, Aurora, DynamoDB, Lambda, API Gatgeway, S3, SQS, SNS and EventBridge
CI/CD deployment methodology
Experience with Data Management and Data Movement skills (AWS Glue)
In depth experience with API design and concepts
Conversant in software architecture and design patterns
In-depth knowledge of IT concepts, strategies and methodologies
In-depth knowledge of diverse and emerging technologies and new architectural concepts and principles
Knowledgeable in data engineering languages and tools; proficient in new and emerging technologies
About Us
At Liberty Mutual, our purpose is to help people embrace today and confidently pursue tomorrow. That’s why we provide an environment focused on openness, inclusion, trust and respect. Here, you’ll discover our expansive range of roles, and a workplace where we aim to help turn your passion into a rewarding profession.

Liberty Mutual has proudly been recognized as a “Great Place to Work” by Great Place to Work® US for the past several years. We were also selected as one of the “100 Best Places to Work in IT” on IDG’s Insider Pro and Computerworld’s 2020 list. For many years running, we have been named by Forbes as one of America’s Best Employers for Women and one of America’s Best Employers for New Graduates—as well as one of America’s Best Employers for Diversity. To learn more about our commitment to diversity and inclusion please visit: https://jobs.libertymutualgroup.com/diversity-equity-inclusion/

We value your hard work, integrity and commitment to make things better, and we put people first by offering you benefits that support your life and well-being. To learn more about our benefit offerings please visit: https://LMI.co/Benefits

Liberty Mutual is an equal opportunity employer. We will not tolerate discrimination on the basis of race, color, national origin, sex, sexual orientation, gender identity, religion, age, disability, veteran’s status, pregnancy, genetic information or on any basis prohibited by federal, state or local law."
761,Data Engineer - Python,Ampcus Incorporated,Remote,N,"Position: Data Engineer

Location: Fully Remote

Duration: 6 months assignment with the possibility of extension

AWS, Python , All Access, ServiceNow, Partner Gateway

Job Description:
Qualifications :
Bachelor’s degree or equivalent experience.
Experience in Data Engineering and Processing tools and services in AWS.
Experience with Python scripting.
Practical experience on a Data Migration effort from on-prem to AWS using DMS and/or similar data migration services.
Strong working knowledge in AWS covering networking, security and data services
Experience with Data lakes and S3 centric data processing designs.
Ability to research, design and implement DBOps paradigm and have it incorporated in CICD automation using tools like GitLab.
Experience working on agile projects and participate in daily scrums and updates
Strong analytical ability and technical skill, as well as the ability to provide innovative solutions to technical needs and business requirements.
Strong attention to detail with a high level of data integrity and accuracy.
Proficient oral and written communication, ability to interact on required information and concepts with people at all levels of the organization.
Proficient ability to translate highly technical information into non-technical terms.
Broad knowledge of the concepts, practices, and principles of programming including design, implementation, and testing.
Ability to interact with customers, understand business requirements and collaborate with team members to explore existing system, determine areas of complexity, identify potential risk to successful implementations.
Responsibilities :
Contribute in an agile and collaborative environment to the development, testing, implementation, and review and evaluation of complex solutions.
Work as part of a team to move on-prem applications to a Cloud environment.
Contribute to the design of technology infrastructure and configurations, recommend process improvements.
Compile and maintain technical documentation, including use cases and scripts; conduct technical research and maintain viable knowledge of technology trends.
Contribute in an agile and collaborative environment to the development, testing, implementation, and review and evaluation of complex solutions.
Work as part of a team to move on-prem applications to a Cloud environment.
Contribute to the design of technology infrastructure and configurations, recommend process improvements.
Compile and maintain technical documentation, including use cases and scripts; conduct technical research and maintain viable knowledge of technology trends."
762,Remote Data Engineer,N,"Saint Paul, MN•Remote",N,"AiRCare Health is on a mission.
We are transforming healthcare.
We are empowering individuals to put emotional health first.

Who we are
We are on a mission to transform mental and emotional healthcare. Using data and “machine smarts” we find the people who need help before they crash into the system. We do not wait for things to get worse. Our unique approach combines both the heart of clinical care management and the science of machine learning to transform the health and wellbeing of large populations. From workplace pressures to financial challenges, marital struggles, and behavioral health issues, our teams deliver the kind of support that makes a measurable difference in the lives of the individuals and families we serve.

Who you are
You are passionate, open minded, and you make a daily difference in the lives of those around you. You are the person people ask for help, guidance, and direction. You are an optimist. You remove obstacles. And just like us, you wear your H.E.A.R.T. on your sleeve with Hope, Empathy, a preference for Action, a willingness to Raise the bar, and a belief that Trust is at the core of every meaningful relationship.

Why we need you
We are growing and building a hand-picked team of positive, optimistic, empathetic rockstars. We are looking for passionate humans to join our multi-disciplinary team of professionals and inspire hope for “the other eleven” to live their best life. Let’s do this.

Required qualifications:
You are an individual with at least 3 – 5 years data engineering experience developing and running data pipelines in Azure and Databricks.
3-5 years’ experience data warehousing and data modeling experience.
Experience with various data pipeline tools and approaches: our tech stack includes Databricks, Python, Delta Lake, PySpark, Spark streaming, GraphFrames, Azure Data Factory, Synapse, containers, and the full Azure stack.
Experience with pipeline orchestration and monitoring techniques.
Azure DevOps experience.
Experience consuming REST services, debugging stored procedures (T-SQL), and triaging serverless functions.
Testing and performance tuning in a distributed Azure environment.
Experience with data ingestion from multiple third-party sources, match / merge /update problems, handling schema drift.
You have a bachelor’s degree in IT, business systems, engineering, or another relevant field OR equivalent work experience.
Have an obvious sense of urgency and purpose.
You are focused and determined, optimistic and creative.
Strong verbal and written communication skills and ability to present data visually.
You are interested in working remotely and reside in Minnesota, Wisconsin, Washington, Virginia, North Carolina, Florida, Texas, Georgia, Arkansas, or Nevada.
You can pass a substance abuse test and criminal background check.
Preferred qualifications:
3+ years' development and design experience with medical and insurance data (eligibility, coverage, claims / 837, EMR / HL7 / FHIR) preferred.

What you’ll do (Essential Job Functions)
Design, implement, and test data pipeline components
Design and implement data models to support streamlined analytics
Implement and test data pipeline integration
Monitor and fix data pipeline operations failures
Monitor and flag data quality issues
Collaborate with data engineering team and others to root cause data quality issues, data operations performance issues
Collaborate with operations and architecture to improve quality, scalability, and capability of the platform
Maintain strict confidentiality with data and show utmost respect for individual’s data

This role is a great fit if you are:
Able to thrive in this environment: your colleagues see you as a detail-oriented, organized, responsive, multi-tasker who is always seeking new information to improve everything you do.
Able to balance a wide variety of projects in a fast-paced work environment and flexible and willing to accept a change in priorities as necessary.
Able to demonstrate initiative and work independently and collaboratively as a member of an integrated team.
Someone with an Agile background.
Comfortable working in a team environment.
Passionate about improving lives and believe in the greater good
Tenacious: you persist and bring creative solutions to tough challenges
Accountable and collaborative. Working remotely, you will be expected to work independently, proactively take ownership of key tasks, and keep in regular communication with your team
Not afraid to have tough conversations or make recommendations
Thorough: you are detail-oriented and committed to getting the job done
A time management guru: you efficiently move through a to-do list and effectively prioritize and triage competing demands
Metric-driven towards the goal of successfully connecting the maximum number of callers to resources
Growth-oriented: you thrive on constructive feedback, which you view as an opportunity to polish skills
Open-minded, non-judgmental, compassionate, flexible, and have good humor
Someone who thrives in a fast-paced and evolving environment. We move quickly to evolve tools and protocols based on data.
Adept at learning to use various techniques, tools and viewing them to make your work easier, and enhance your impact
What we offer
Full-time employee position, not a contractor. We are happy to offer you a generous benefits package, that includes medical, dental, STD, LTD, vision, FSA opportunities, life insurance, PTO/sick leave/holiday pay, 401K.
PLEASE NOTE: To ensure you do not miss important communications from us regarding your application status and/or next steps, please be sure to add the following email address to your approved contacts list. notifications@app.bamboohr.com. You should receive an email confirming your application was received once you submit. If you do not receive an auto confirmation email, please check your spam/promotions inbox!

AiRCare is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: AiRCare is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at AiRCare are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. AiRCare will not tolerate discrimination or harassment based on any of these characteristics.

All applicants are required to be able to perform the essential functions of the position, with or without reasonable accommodations."
763,Data Engineer,Adstra,United States,N,"Overview:
This is a remote position within the United States

At Adstra we know great things happen when people come together with one shared goal. The future of data is happening at Adstra. Come join our growing team.

We are currently seeking a Data Engineer to join our team of professionals.

You will be responsible for build and maintain processes and workflows leveraging the latest big data technologies to serve our clients and partners.

Our “ideal candidate” is an individual who is an individual who is naturally curious, detailed oriented, have a sharp analytical mind, proficient with a variety of cutting-edge toolsets, a penchant to drive efficiency through automation, and ability to communicate complex concepts to technical and non-technical people alike.

About Adstra
Adstra, a privately held, PE backed company, provides a comprehensive and unique suite of identity, data, and audience solutions to a diverse base of blue-chip clients.

Adstra delivers the new model for data-driven enterprises. Based on a culture of innovation, quality, and inclusion, Adstra offers a comprehensive suite of transparent, compliant, and cross media solutions. Adstra’s goal is to liberate brands from legacy marketing technology and data approaches to maximum value and efficiency from their data and customer touchpoints.

Adstra works across industry verticals, both for profit and nonprofit, as a leading provider of identity resolution, customer acquisition, consumer data, and data management services. Adstra is headquartered in Princeton, New Jersey with associates working across the United States.

Adstra offers excellent benefits including medical, dental, vision, life, LTD, 401k w/ company match, paid time off, business casual dress, summer hours, customized training & more! If you are one of the “best and brightest” in your field and you’re looking to position yourself for the future, this is a great opportunity.

To learn more about why Adstra is the new ideal in places to work check out our Adstra Careers page today!

Equal Employment Opportunity
Adstra provides equal employment opportunities to all employees and applicants for employment. All practices such as recruitment, selection, promotions, and other terms and conditions of employment are administered in a manner that ensures there is no discrimination on the basis of race, color, national origin, ancestry, religion, citizenship, age, gender, gender expression or identity, sexual orientation, marital status, disability, pregnancy, medical condition, genetic information, protected veterans, or any other protected characteristic under applicable law.
For immediate consideration, please create a profile and make sure you provide your resume and current contact information. We look forward to learning more about you and the value you can bring to Adstra!

We want to be your Employer of Choice!

EOE/M/F/D/V
Responsibilities:
Responsibilities for this opportunity include, but are not limited to:

Build and maintain scalable data pipelines and ETL processes using Python and Airflow.
Design, implement and maintain data processes within different data warehouses.
Work with cross-functional teams to gather data requirements, identify data quality issues, and resolve data-related problems
Work with the product team to design and implement custom customer-related solutions
Develop and maintain customer data integrations and data pipelines
Automate and optimize data processing workflows
Collaborate with Data Scientists and Analysts to ensure data quality and accuracy
Ensure data security, integrity, and privacy across all systems
Monitor system performance and troubleshoot issues as necessary
Keep up-to-date with the latest developments in data engineering, big data, and cloud computing technologies
Qualifications:
In addition, our ""ideal candidate"" has the following skills & experience:

Bachelor’s degree in CS, Engineering, Mathematics, Economics, Statistics, or related field.
You are an analytical, result driven individual with high attention to detail.
You are a self-starter, quick learner, problem-solver and someone who relishes challenges.
3+ years using SQL or other query and scripting languages to aggregate, gather, and manipulate data.
2+ years using scripting languages such as Python or Bash.
Deep understanding of relational databases, ETL tools, data conversion and data cleansing methodologies
Experience with (or at least exposure to) cloud-based databases like Redshift, Snowflake, or BigQuery.
Your verbal and written communication skills are excellent.
You have a love for all things data and data-driven solutions.
Pride in your work, and high standards for yourself and the company you work for.
Strong communication, analytical and collaborative problem-solving skills.
Proven ability to learn quickly, work independently, and adapt to change in a fast-paced environment.
Experience with Big Data technologies such as Spark and Hadoop
Experience with Cloud Platforms such as AWS or Google Cloud
Previous AdTech or MarTech industry experience"
764,Data Engineer,Zoll Medical Corporation,"Broomfield, CO•Remote","$125,000 - $140,000 a year","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote"
